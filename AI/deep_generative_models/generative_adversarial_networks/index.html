
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/AI/deep_generative_models/generative_adversarial_networks/">
      
      
        <link rel="prev" href="../recap_at_this_point/">
      
      
        <link rel="next" href="../../../math/probability/probability_and_counting/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Generative Adversarial Networks - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#generative-adversarial-networks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Generative Adversarial Networks
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/probability/probability_and_counting/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction-gans-as-a-paradigm-shift" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction: GANs as a Paradigm Shift
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction: GANs as a Paradigm Shift">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-traditional-likelihood-based-paradigm" class="md-nav__link">
    <span class="md-ellipsis">
      The Traditional Likelihood-Based Paradigm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gans-a-different-approach" class="md-nav__link">
    <span class="md-ellipsis">
      GANs: A Different Approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#likelihood-free-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Likelihood-Free Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Likelihood-Free Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-likelihood-vs-sample-quality-disconnect" class="md-nav__link">
    <span class="md-ellipsis">
      The Likelihood vs. Sample Quality Disconnect
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-two-sample-test-framework" class="md-nav__link">
    <span class="md-ellipsis">
      The Two-Sample Test Framework
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#application-to-generative-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      Application to Generative Modeling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-this-approach-makes-sense" class="md-nav__link">
    <span class="md-ellipsis">
      Why this approach makes sense
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gan-objective" class="md-nav__link">
    <span class="md-ellipsis">
      GAN Objective
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GAN Objective">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#components" class="md-nav__link">
    <span class="md-ellipsis">
      Components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-minimax-game" class="md-nav__link">
    <span class="md-ellipsis">
      The Minimax Game
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#formal-objective" class="md-nav__link">
    <span class="md-ellipsis">
      Formal Objective
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#understanding-the-objective" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding the Objective
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimal-discriminator" class="md-nav__link">
    <span class="md-ellipsis">
      Optimal Discriminator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#connection-to-jensen-shannon-divergence" class="md-nav__link">
    <span class="md-ellipsis">
      Connection to Jensen-Shannon Divergence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#properties-of-jsd" class="md-nav__link">
    <span class="md-ellipsis">
      Properties of JSD
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimal-solution" class="md-nav__link">
    <span class="md-ellipsis">
      Optimal Solution
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gan-training-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      GAN Training Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#challenges" class="md-nav__link">
    <span class="md-ellipsis">
      Challenges
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#selected-gans" class="md-nav__link">
    <span class="md-ellipsis">
      Selected GANs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Selected GANs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#f-gan" class="md-nav__link">
    <span class="md-ellipsis">
      f-GAN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="f-GAN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#understanding-the-requirements" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding the Requirements
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#f-divergence-examples" class="md-nav__link">
    <span class="md-ellipsis">
      f-Divergence Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setting-up-the-f-gan-objective" class="md-nav__link">
    <span class="md-ellipsis">
      Setting up the f-GAN Objective
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#steps-in-f-gan" class="md-nav__link">
    <span class="md-ellipsis">
      Steps in f-GAN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages-of-f-gan" class="md-nav__link">
    <span class="md-ellipsis">
      Advantages of f-GAN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Considerations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="generative-adversarial-networks">Generative Adversarial Networks</h1>
<h2 id="introduction-gans-as-a-paradigm-shift">Introduction: GANs as a Paradigm Shift</h2>
<p>GANs are unique from all the other model families that we have seen so far, such as autoregressive models, VAEs, and normalizing flow models, because we do not train them using maximum likelihood.</p>
<h3 id="the-traditional-likelihood-based-paradigm">The Traditional Likelihood-Based Paradigm</h3>
<p>All the generative models we've explored so far follow a similar training paradigm:</p>
<ol>
<li><strong>Autoregressive Models</strong>: Maximize <span class="arithmatex">\(\log p_\theta(x) = \sum_{i=1}^N \log p_\theta(x_i|x_{&lt;i})\)</span></li>
<li><strong>Variational Autoencoders</strong>: Maximize the ELBO <span class="arithmatex">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z))\)</span></li>
<li><strong>Normalizing Flow Models</strong>: Maximize <span class="arithmatex">\(\log p_\theta(x) = \log p_z(f^{-1}_\theta(x)) + \log |\det(\frac{\partial f^{-1}_\theta(x)}{\partial x})|\)</span></li>
</ol>
<p><strong>Common Theme</strong>: All these models are trained by maximizing some form of likelihood or likelihood approximation.</p>
<h3 id="gans-a-different-approach">GANs: A Different Approach</h3>
<p><strong>GANs break away from this paradigm entirely.</strong> Instead of maximizing likelihood, GANs use <strong>adversarial training</strong> - a fundamentally different approach to generative modeling. We'll get to what a Generator and a Discriminator are in a bit but here is a quick table showing how GAN is different.</p>
<p><strong>Key Differences:</strong></p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Likelihood-Based Models</th>
<th>GANs</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Training Objective</strong></td>
<td>Maximize likelihood/ELBO</td>
<td>Minimax game between generator and discriminator</td>
</tr>
<tr>
<td><strong>Loss Function</strong></td>
<td><span class="arithmatex">\(\mathcal{L} = -\log p_\theta(x)\)</span></td>
<td><span class="arithmatex">\(\mathcal{L}_G = -\log D(G(z))\)</span>, <span class="arithmatex">\(\mathcal{L}_D = -\log D(x) - \log(1-D(G(z)))\)</span></td>
</tr>
<tr>
<td><strong>Model Evaluation</strong></td>
<td>Direct likelihood computation</td>
<td>No explicit likelihood computation</td>
</tr>
<tr>
<td><strong>Training Stability</strong></td>
<td>Generally stable</td>
<td>Can be unstable, requires careful tuning</td>
</tr>
<tr>
<td><strong>Sample Quality</strong></td>
<td>May produce blurry samples</td>
<td>Often produces sharp, realistic samples</td>
</tr>
</tbody>
</table>
<h2 id="likelihood-free-learning">Likelihood-Free Learning</h2>
<p><strong>Why not use maximum likelihood?</strong> In fact, it is not so clear that better likelihood numbers necessarily correspond to higher sample quality. We know that the optimal generative model will give us the best sample quality and highest test log-likelihood. However, models with high test log-likelihoods can still yield poor samples, and vice versa.</p>
<h3 id="the-likelihood-vs-sample-quality-disconnect">The Likelihood vs. Sample Quality Disconnect</h3>
<p>To see why, consider pathological cases in which our model is comprised almost entirely of noise, or our model simply memorizes the training set:</p>
<ol>
<li>
<p><strong>Noise Model</strong>: A model that outputs pure noise might assign some probability to real data points, leading to a non-zero (though poor) likelihood, but produces completely useless samples.</p>
</li>
<li>
<p><strong>Memorization Model</strong>: A model that perfectly memorizes the training set will have very high likelihood on training data but will only reproduce exact training examples, lacking generalization and diversity.</p>
</li>
</ol>
<p>Therefore, we turn to <strong>likelihood-free training</strong> with the hope that optimizing a different objective will allow us to disentangle our desiderata of obtaining high likelihoods as well as high-quality samples.</p>
<h3 id="the-two-sample-test-framework">The Two-Sample Test Framework</h3>
<p>Recall that maximum likelihood required us to evaluate the likelihood of the data under our model <span class="arithmatex">\(p_\theta\)</span>. A natural way to set up a likelihood-free objective is to consider the <strong>two-sample test</strong>, a statistical test that determines whether or not a finite set of samples from two distributions are from the same distribution using only samples from <span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(Q\)</span>.</p>
<p><strong>Concretely</strong>, given <span class="arithmatex">\(S_1 = \{x \sim P\}\)</span> and <span class="arithmatex">\(S_2 = \{x \sim Q\}\)</span>, we compute a test statistic <span class="arithmatex">\(T\)</span> according to the difference in <span class="arithmatex">\(S_1\)</span> and <span class="arithmatex">\(S_2\)</span> that, when less than a threshold <span class="arithmatex">\(\alpha\)</span>, accepts the null hypothesis that <span class="arithmatex">\(P = Q\)</span>.</p>
<h3 id="application-to-generative-modeling">Application to Generative Modeling</h3>
<p>Analogously, we have in our generative modeling setup access to our training set <span class="arithmatex">\(S_1 = \{x \sim p_{data}\}\)</span> and <span class="arithmatex">\(S_2 = \{x \sim p_\theta\}\)</span>. The key idea is to train the model to minimize a two-sample test objective between <span class="arithmatex">\(S_1\)</span> and <span class="arithmatex">\(S_2\)</span>.</p>
<p><strong>However</strong>, this objective becomes extremely difficult to work with in high dimensions, so we choose to optimize a <strong>surrogate objective</strong> that instead maximizes some distance between <span class="arithmatex">\(S_1\)</span> and <span class="arithmatex">\(S_2\)</span>.</p>
<h3 id="why-this-approach-makes-sense">Why this approach makes sense</h3>
<p><strong>1. Avoiding pathological cases:</strong>
- The two-sample test framework naturally avoids the noise and memorization problems
- It forces the model to learn the true underlying distribution structure</p>
<p><strong>3. Flexibility:</strong>
- We can choose different distance metrics or test statistics
- This allows us to focus on different aspects of sample quality</p>
<p><strong>Key Insight:</strong> GANs implement likelihood-free learning by using a neural network (the discriminator) to learn an optimal test statistic for distinguishing between real and generated data, and then training the generator to minimize this learned distance.</p>
<h2 id="gan-objective">GAN Objective</h2>
<p>We thus arrive at the generative adversarial network formulation. There are two components in a GAN: (1) a generator and (2) a discriminator. The generator <span class="arithmatex">\(G_\theta\)</span> is a directed latent variable model that deterministically generates samples <span class="arithmatex">\(x\)</span> from <span class="arithmatex">\(z\)</span>, and the discriminator <span class="arithmatex">\(D_\phi\)</span> is a function whose job is to distinguish samples from the real dataset and the generator.</p>
<h3 id="components">Components</h3>
<ul>
<li><strong>Generator <span class="arithmatex">\(G_\theta\)</span></strong>: A neural network that transforms noise <span class="arithmatex">\(z \sim p(z)\)</span> to samples <span class="arithmatex">\(G_\theta(z)\)</span></li>
<li><strong>Discriminator <span class="arithmatex">\(D_\phi\)</span></strong>: A neural network that outputs a probability <span class="arithmatex">\(D_\phi(x) \in [0,1]\)</span> indicating whether <span class="arithmatex">\(x\)</span> is real or generated</li>
</ul>
<p><img alt="GAN graphical model" src="../gan.png" /></p>
<h3 id="the-minimax-game">The Minimax Game</h3>
<p>The generator and discriminator both play a two player minimax game, where:
- <strong>Generator</strong>: Minimizes a two-sample test objective (<span class="arithmatex">\(p_{data} = p_\theta\)</span>)
- <strong>Discriminator</strong>: Maximizes the objective (<span class="arithmatex">\(p_{data} \neq p_\theta\)</span>)</p>
<p>Intuitively, the generator tries to fool the discriminator to the best of its ability by generating samples that look indistinguishable from <span class="arithmatex">\(p_{data}\)</span>.</p>
<h3 id="formal-objective">Formal Objective</h3>
<p>The GAN objective can be written as:</p>
<div class="arithmatex">\[\min_\theta \max_\phi V(G_\theta, D_\phi) = \mathbb{E}_{x \sim p_{data}}[\log D_\phi(x)] + \mathbb{E}_{z \sim p(z)}[\log(1-D_\phi(G_\theta(z)))]\]</div>
<h3 id="understanding-the-objective">Understanding the Objective</h3>
<p>Let's unpack this expression:</p>
<p><strong>For the Discriminator (maximizing with respect to <span class="arithmatex">\(\phi\)</span>):</strong>
- Given a fixed generator <span class="arithmatex">\(G_\theta\)</span>, the discriminator performs binary classification
- It tries to assign probability 1 to data points from the training set <span class="arithmatex">\(x \sim p_{data}\)</span>
- It tries to assign probability 0 to generated samples <span class="arithmatex">\(x \sim p_G\)</span></p>
<p><strong>For the Generator (minimizing with respect to <span class="arithmatex">\(\theta\)</span>):</strong>
- Given a fixed discriminator <span class="arithmatex">\(D_\phi\)</span>, the generator tries to maximize <span class="arithmatex">\(D_\phi(G_\theta(z))\)</span>
- This is equivalent to minimizing <span class="arithmatex">\(\log(1-D_\phi(G_\theta(z)))\)</span></p>
<h3 id="optimal-discriminator">Optimal Discriminator</h3>
<p>In this setup, the optimal discriminator is:</p>
<div class="arithmatex">\[D^*_G(x) = \frac{p_{data}(x)}{p_{data}(x) + p_G(x)}\]</div>
<p><strong>Derivation:</strong>
The discriminator's objective is to maximize:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{x \sim p_G}[\log(1-D(x))]\]</div>
<p>This is maximized when:</p>
<div class="arithmatex">\[D(x) = \frac{p_{data}(x)}{p_{data}(x) + p_G(x)}\]</div>
<p>On the other hand, the generator minimizes this objective for a fixed discriminator <span class="arithmatex">\(D_\phi\)</span>.</p>
<h3 id="connection-to-jensen-shannon-divergence">Connection to Jensen-Shannon Divergence</h3>
<p>After performing some algebra, plugging in the optimal discriminator <span class="arithmatex">\(D^*_G(\cdot)\)</span> into the overall objective <span class="arithmatex">\(V(G_\theta, D^*_G(x))\)</span> gives us:</p>
<div class="arithmatex">\[2D_{JSD}[p_{data}, p_G] - \log 4\]</div>
<p>The <span class="arithmatex">\(D_{JSD}\)</span> term is the <strong>Jensen-Shannon Divergence</strong>, which is also known as the symmetric form of the KL divergence:</p>
<div class="arithmatex">\[D_{JSD}[p,q] = \frac{1}{2}\left(D_{KL}\left[p, \frac{p+q}{2}\right] + D_{KL}\left[q, \frac{p+q}{2}\right]\right)\]</div>
<h3 id="properties-of-jsd">Properties of JSD</h3>
<p>The JSD satisfies all properties of the KL divergence, and has the additional perk that <span class="arithmatex">\(D_{JSD}[p,q] = D_{JSD}[q,p]\)</span> (symmetry).</p>
<p><strong>Key Properties:</strong></p>
<ol>
<li>
<p><strong>Non-negative</strong>: <span class="arithmatex">\(D_{JSD}[p,q] \geq 0\)</span></p>
</li>
<li>
<p><strong>Symmetric</strong>: <span class="arithmatex">\(D_{JSD}[p,q] = D_{JSD}[q,p]\)</span></p>
</li>
<li>
<p><strong>Zero iff equal</strong>: <span class="arithmatex">\(D_{JSD}[p,q] = 0\)</span> if and only if <span class="arithmatex">\(p = q\)</span></p>
</li>
<li>
<p><strong>Bounded</strong>: <span class="arithmatex">\(D_{JSD}[p,q] \leq \log 2\)</span></p>
</li>
</ol>
<h3 id="optimal-solution">Optimal Solution</h3>
<p>With this distance metric, the optimal generator for the GAN objective becomes <span class="arithmatex">\(p_G = p_{data}\)</span>, and the optimal objective value that we can achieve with optimal generators and discriminators <span class="arithmatex">\(G^*(\cdot)\)</span> and <span class="arithmatex">\(D^*_{G^*}(x)\)</span> is <span class="arithmatex">\(-\log 4\)</span>.</p>
<p><strong>Why <span class="arithmatex">\(-\log 4\)</span>?</strong>
- When <span class="arithmatex">\(p_G = p_{data}\)</span>, we have <span class="arithmatex">\(D_{JSD}[p_{data}, p_G] = 0\)</span>
- Therefore, <span class="arithmatex">\(V(G^*, D^*) = 2 \cdot 0 - \log 4 = -\log 4\)</span></p>
<h2 id="gan-training-algorithm">GAN Training Algorithm</h2>
<p>Thus, the way in which we train a GAN is as follows:</p>
<p><strong>For epochs <span class="arithmatex">\(1, \ldots, N\)</span> do:</strong></p>
<ol>
<li><strong>Sample minibatch of size <span class="arithmatex">\(m\)</span> from data</strong>: <span class="arithmatex">\(x^{(1)}, \ldots, x^{(m)} \sim p_{data}\)</span></li>
<li><strong>Sample minibatch of size <span class="arithmatex">\(m\)</span> of noise</strong>: <span class="arithmatex">\(z^{(1)}, \ldots, z^{(m)} \sim p_z\)</span></li>
<li><strong>Take a gradient descent step on the generator parameters <span class="arithmatex">\(\theta\)</span></strong>:</li>
</ol>
<div class="arithmatex">\[\nabla_\theta V(G_\theta, D_\phi) = \frac{1}{m}\nabla_\theta \sum_{i=1}^m \log(1-D_\phi(G_\theta(z^{(i)})))\]</div>
<ol>
<li><strong>Take a gradient ascent step on the discriminator parameters <span class="arithmatex">\(\phi\)</span></strong>:</li>
</ol>
<div class="arithmatex">\[\nabla_\phi V(G_\theta, D_\phi) = \frac{1}{m}\nabla_\phi \sum_{i=1}^m [\log D_\phi(x^{(i)}) + \log(1-D_\phi(G_\theta(z^{(i)})))]\]</div>
<p><strong>Key Points:</strong></p>
<ol>
<li><strong>Alternating Updates</strong>: We update the generator and discriminator in alternating steps</li>
<li><strong>Minibatch Training</strong>: We use minibatches of both real data and noise samples</li>
<li><strong>Generator Update</strong>: Minimizes the probability that the discriminator correctly identifies generated samples</li>
<li><strong>Discriminator Update</strong>: Maximizes the probability of correctly classifying real vs. generated samples</li>
</ol>
<p><strong>Practical Considerations:</strong></p>
<ul>
<li><strong>Learning Rate Balance</strong>: The learning rates for generator and discriminator must be carefully balanced</li>
<li><strong>Update Frequency</strong>: Often the discriminator is updated multiple times per generator update</li>
<li><strong>Convergence Monitoring</strong>: Training progress is monitored through discriminator accuracy and sample quality</li>
<li><strong>Early Stopping</strong>: Training may be stopped when the discriminator can no longer distinguish real from fake</li>
</ul>
<p>This formulation shows that GANs are essentially implementing an adaptive two-sample test, where the discriminator learns the optimal way to distinguish between real and generated data, and the generator learns to minimize this learned distance.</p>
<h2 id="challenges">Challenges</h2>
<p>Although GANs have been successfully applied to several domains and tasks, working with them in practice is challenging because of their: (1) unstable optimization procedure, (2) potential for mode collapse, (3) difficulty in evaluation.</p>
<p><strong>1. Unstable Optimization Procedure</strong></p>
<p>During optimization, the generator and discriminator loss often continue to oscillate without converging to a clear stopping point. Due to the lack of a robust stopping criteria, it is difficult to know when exactly the GAN has finished training.</p>
<p><strong>Causes of Instability:</strong>
- <strong>Minimax Nature</strong>: The adversarial game creates competing objectives
- <strong>Gradient Issues</strong>: Vanishing/exploding gradients can occur
- <strong>Learning Rate Sensitivity</strong>: Small changes in learning rates can cause divergence
- <strong>Network Capacity Imbalance</strong>: If one network becomes too powerful, training collapses</p>
<p><strong>Symptoms:</strong>
- Oscillating loss curves
- No clear convergence pattern
- Sudden collapse of training
- Generator or discriminator loss going to zero/infinity</p>
<p><strong>2. Mode Collapse</strong></p>
<p>The generator of a GAN can often get stuck producing one of a few types of samples over and over again (mode collapse). This occurs when the generator finds a few "safe" modes that consistently fool the discriminator and stops exploring the full data distribution.</p>
<p><strong>What is Mode Collapse:</strong>
- <strong>Definition</strong>: Generator only produces samples from a subset of the true distribution modes
- <strong>Example</strong>: In image generation, only producing images of one type (e.g., only front-facing faces)
- <strong>Problem</strong>: Lack of diversity in generated samples</p>
<p><strong>Causes:</strong>
- <strong>Discriminator Overfitting</strong>: Discriminator becomes too good at detecting certain types of fake samples
- <strong>Generator Optimization</strong>: Generator finds local optima that work well against current discriminator
- <strong>Training Imbalance</strong>: One network becomes too powerful relative to the other</p>
<p><strong>3. Difficulty in Evaluation</strong></p>
<p>Unlike likelihood-based models, GANs don't provide explicit likelihood values, making evaluation challenging.</p>
<p><strong>Evaluation Challenges:</strong>
- <strong>No Likelihood</strong>: Can't use traditional metrics like log-likelihood
- <strong>Subjective Quality</strong>: Sample quality is often subjective and domain-specific
- <strong>Diversity vs. Quality Trade-off</strong>: Hard to balance sample quality with diversity
- <strong>Mode Coverage</strong>: Difficult to measure if all modes of the data distribution are captured</p>
<p><strong>Addressing the Challenges:</strong></p>
<p>Most fixes to these challenges are empirically driven, and there has been a significant amount of work put into developing new architectures, regularization schemes, and noise perturbations in an attempt to circumvent these issues.</p>
<h2 id="selected-gans">Selected GANs</h2>
<p>Next, we focus our attention to a few select types of GAN architectures and explore them in more detail.</p>
<h3 id="f-gan">f-GAN</h3>
<p>The f-GAN optimizes the variant of the two-sample test objective that we have discussed so far, but using a very general notion of distance: the <strong>f-divergence</strong>. Given two densities <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(q\)</span>, the f-divergence can be written as:</p>
<div class="arithmatex">\[D_f(p,q) = \sup_{T} \left(\mathbb{E}_{x \sim p}[T(x)] - \mathbb{E}_{x_{fake} \sim q}[f^*(T(x_{fake}))]\right)\]</div>
<p>where <span class="arithmatex">\(f\)</span> is any convex, lower-semicontinuous function with <span class="arithmatex">\(f(1) = 0\)</span>. Several of the distance "metrics" that we have seen so far fall under the class of f-divergences, such as KL and Jensen-Shannon.</p>
<h4 id="understanding-the-requirements">Understanding the Requirements</h4>
<p><strong>What is a convex function?</strong>
A function <span class="arithmatex">\(f\)</span> is convex if for any two points <span class="arithmatex">\(x, y\)</span> and any <span class="arithmatex">\(\lambda \in [0,1]\)</span>, we have:</p>
<div class="arithmatex">\[f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)\]</div>
<p>This means that the line segment between any two points on the function lies above or on the function itself. </p>
<p><strong>Understanding the Line Segment Property:</strong></p>
<p>Let's break down what this means geometrically:</p>
<p><strong>Two Points</strong>: Consider any two points <span class="arithmatex">\((x, f(x))\)</span> and <span class="arithmatex">\((y, f(y))\)</span> on the graph of the function <span class="arithmatex">\(f\)</span></p>
<p><strong>Line Segment</strong>: The line segment connecting these points consists of all points of the form:</p>
<div class="arithmatex">\[(\lambda x + (1-\lambda)y, \lambda f(x) + (1-\lambda)f(y))\]</div>
<p>where <span class="arithmatex">\(\lambda \in [0,1]\)</span></p>
<p><strong>Function Value</strong>: At the same <span class="arithmatex">\(x\)</span>-coordinate <span class="arithmatex">\(\lambda x + (1-\lambda)y\)</span>, the function value is:</p>
<div class="arithmatex">\[f(\lambda x + (1-\lambda)y)\]</div>
<p><strong>Convexity Condition</strong>: The inequality <span class="arithmatex">\(f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)\)</span> means that the function value at any point on the line segment is less than or equal to the corresponding point on the straight line connecting <span class="arithmatex">\((x, f(x))\)</span> and <span class="arithmatex">\((y, f(y))\)</span></p>
<p><strong>Visual Interpretation:</strong>
If you draw a straight line between any two points on a convex function's graph. The entire function between those points must lie on or below that straight line.</p>
<p>Convex functions have important properties:</p>
<ul>
<li>
<p><strong>Single minimum</strong>: If a minimum exists, it's global</p>
</li>
<li>
<p><strong>Well-behaved gradients</strong>: Useful for optimization</p>
</li>
<li>
<p><strong>Jensen's inequality</strong>: <span class="arithmatex">\(\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])\)</span> for convex <span class="arithmatex">\(f\)</span></p>
</li>
</ul>
<p><strong>What is a lower-semicontinuous function?</strong>
A function <span class="arithmatex">\(f\)</span> is lower-semicontinuous at a point <span class="arithmatex">\(x_0\)</span> if:</p>
<div class="arithmatex">\[\liminf_{x \to x_0} f(x) \geq f(x_0)\]</div>
<p><strong>Understanding <span class="arithmatex">\(\liminf\)</span> and the Infimum:</strong></p>
<p>The notation <span class="arithmatex">\(\liminf_{x \to x_0} f(x)\)</span> involves two concepts:</p>
<ol>
<li>
<p><strong>Infimum (inf)</strong>: The infimum of a set is the greatest lower bound. For a set <span class="arithmatex">\(S\)</span>, <span class="arithmatex">\(\inf S\)</span> is the largest number that is less than or equal to all elements in <span class="arithmatex">\(S\)</span>.</p>
</li>
<li>
<p><strong>Limit Inferior</strong>: <span class="arithmatex">\(\liminf_{x \to x_0} f(x)\)</span> is the infimum of all limit points of <span class="arithmatex">\(f(x)\)</span> as <span class="arithmatex">\(x\)</span> approaches <span class="arithmatex">\(x_0\)</span>.</p>
</li>
</ol>
<p><strong>How <span class="arithmatex">\(\liminf\)</span> works:</strong></p>
<p>Consider all sequences <span class="arithmatex">\(\{x_n\}\)</span> that converge to <span class="arithmatex">\(x_0\)</span>. For each sequence, we look at the limit of <span class="arithmatex">\(f(x_n)\)</span> (if it exists). The <span class="arithmatex">\(\liminf\)</span> is the infimum of all these possible limit values.</p>
<p><strong>Mathematical Definition:</strong></p>
<div class="arithmatex">\[\liminf_{x \to x_0} f(x) = \inf \left\{ \lim_{n \to \infty} f(x_n) : x_n \to x_0 \text{ and } \lim_{n \to \infty} f(x_n) \text{ exists} \right\}\]</div>
<p><strong>Why consider Sequences even for Continuous Functions?</strong></p>
<p>You might wonder: "If <span class="arithmatex">\(f\)</span> is continuous, why do we need sequences? Can't we just use the regular limit?"</p>
<p><strong>Key Insight:</strong> Lower-semicontinuity is a <strong>weaker condition</strong> than continuity. A function can be lower-semicontinuous without being continuous.</p>
<p><strong>The Relationship:</strong></p>
<ol>
<li><strong>Continuous functions</strong> are always lower-semicontinuous</li>
<li><strong>Lower-semicontinuous functions</strong> may have discontinuities (but only "jumps up")</li>
</ol>
<p><strong>Example of Lower-Semicontinuous but NOT Continuous:</strong>
Consider <span class="arithmatex">\(f(x) = \begin{cases} 0 &amp; \text{if } x &lt; 0 \\ 1 &amp; \text{if } x \geq 0 \end{cases}\)</span>. This function is lower-semicontinuous at <span class="arithmatex">\(x = 0\)</span> (no "jump down"). But it's NOT continuous at <span class="arithmatex">\(x = 0\)</span> (there's a "jump up")</p>
<p><strong>Example of NOT Lower-Semicontinuous:</strong>
Consider <span class="arithmatex">\(f(x) = \begin{cases} 0 &amp; \text{if } x &lt; 0 \\ 1 &amp; \text{if } x = 0 \\ 0 &amp; \text{if } x &gt; 0 \end{cases}\)</span> at <span class="arithmatex">\(x_0 = 0\)</span>:</p>
<p><span class="arithmatex">\(\liminf_{x \to 0^-} f(x) = 0\)</span> (approaching from left)</p>
<p><span class="arithmatex">\(\liminf_{x \to 0^+} f(x) = 0\)</span> (approaching from right)</p>
<p><span class="arithmatex">\(\liminf_{x \to 0} f(x) = 0\)</span> (overall limit inferior)</p>
<p>Since <span class="arithmatex">\(f(0) = 1\)</span> and <span class="arithmatex">\(0 \not\geq 1\)</span>, this function is NOT lower-semicontinuous at <span class="arithmatex">\(x = 0\)</span></p>
<p><strong>Why the Sequence definition is uiversal:</strong></p>
<p>The sequence-based definition works for ALL functions, whether they're:
- Continuous everywhere
- Lower-semicontinuous but not continuous
- Neither continuous nor lower-semicontinuous</p>
<p><strong>For Continuous Functions:</strong>
If <span class="arithmatex">\(f\)</span> is continuous at <span class="arithmatex">\(x_0\)</span>, then:</p>
<div class="arithmatex">\[\liminf_{x \to x_0} f(x) = \lim_{x \to x_0} f(x) = f(x_0)\]</div>
<p>So the sequence definition "reduces" to the regular limit, but it's still the same mathematical concept.</p>
<p><strong>Why this matters for f-Divergences:</strong>
The f-divergence framework needs to work with functions that might not be continuous everywhere, so we need the more general sequence-based definition.</p>
<p><strong>Why do we need <span class="arithmatex">\(f(1) = 0\)</span>?</strong>
This requirement ensures that the f-divergence has the correct properties for a distance measure:</p>
<p><strong>Zero when distributions are equal</strong>: When <span class="arithmatex">\(p = q\)</span>, we have <span class="arithmatex">\(\frac{p(x)}{q(x)} = 1\)</span> everywhere, so:</p>
<div class="arithmatex">\[D_f(p,p) = \mathbb{E}_{x \sim p}[f(1)] = \mathbb{E}_{x \sim p}[0] = 0\]</div>
<p><strong>Distance-like behavior</strong>: This property ensures that the f-divergence behaves like a proper distance measure, being zero only when the distributions are identical</p>
<p><strong>Example</strong>: For KL divergence, <span class="arithmatex">\(f(u) = u \log u\)</span> satisfies <span class="arithmatex">\(f(1) = 1 \cdot \log 1 = 0\)</span>, making it a valid choice for an f-divergence.</p>
<p><strong>Important Clarification: KL Divergence and Convexity</strong></p>
<p>You might be wondering: "But <span class="arithmatex">\(\log u\)</span> is concave, so how can KL divergence be an f-divergence?" This is a great observation! The key is that the <span class="arithmatex">\(f\)</span> function for KL divergence is <span class="arithmatex">\(f(u) = u \log u\)</span>, not just <span class="arithmatex">\(\log u\)</span>.</p>
<p><strong>The KL Divergence Formula:</strong>
The KL divergence between distributions <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(q\)</span> is:</p>
<div class="arithmatex">\[D_{KL}(p||q) = \mathbb{E}_{x \sim p}\left[\log\frac{p(x)}{q(x)}\right] = \mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)}\log\frac{p(x)}{q(x)}\right]\]</div>
<p>Notice that the second form matches the f-divergence formula with <span class="arithmatex">\(f(u) = u \log u\)</span>.</p>
<h4 id="f-divergence-examples">f-Divergence Examples</h4>
<p><strong>Common f-divergences:</strong></p>
<ol>
<li><strong>KL Divergence</strong>: <span class="arithmatex">\(f(u) = u \log u\)</span></li>
<li><strong>Reverse KL</strong>: <span class="arithmatex">\(f(u) = -\log u\)</span></li>
<li><strong>Jensen-Shannon</strong>: <span class="arithmatex">\(f(u) = u \log u - (u+1) \log \frac{u+1}{2}\)</span></li>
<li><strong>Total Variation</strong>: <span class="arithmatex">\(f(u) = \frac{1}{2}|u-1|\)</span></li>
<li><strong>Pearson χ²</strong>: <span class="arithmatex">\(f(u) = (u-1)^2\)</span></li>
</ol>
<h4 id="setting-up-the-f-gan-objective">Setting up the f-GAN Objective</h4>
<p>To set up the f-GAN objective, we borrow two commonly used tools from convex optimization: the <strong>Fenchel conjugate</strong> and <strong>duality</strong>. Specifically, we obtain a lower bound to any f-divergence via its Fenchel conjugate:</p>
<div class="arithmatex">\[D_f(p,q) \geq \sup_{T \in \mathcal{T}} \left(\mathbb{E}_{x \sim p}[T(x)] - \mathbb{E}_{x_{fake} \sim q}[f^*(T(x_{fake}))]\right)\]</div>
<p>Where <span class="arithmatex">\(f^*\)</span> is the Fenchel conjugate of <span class="arithmatex">\(f\)</span>:</p>
<div class="arithmatex">\[f^*(t) = \sup_{u \in \text{dom}(f)} (tu - f(u))\]</div>
<p><strong>What is the Fenchel Conjugate?</strong></p>
<p>The Fenchel conjugate <span class="arithmatex">\(f^*\)</span> of a function <span class="arithmatex">\(f\)</span> is defined as:</p>
<div class="arithmatex">\[f^*(t) = \sup_{u \in \text{dom}(f)} (tu - f(u))\]</div>
<p><strong>Detailed Geometric Interpretation:</strong></p>
<p>The Fenchel conjugate <span class="arithmatex">\(f^*(t)\)</span> has a beautiful geometric interpretation that helps us understand what it represents. For each value of <span class="arithmatex">\(t\)</span>, we consider the family of lines <span class="arithmatex">\(y = tu + c\)</span> with slope <span class="arithmatex">\(t\)</span>. We look for the line <span class="arithmatex">\(y = tu + c\)</span> that touches the graph of <span class="arithmatex">\(f(u)\)</span> from below and has the largest <span class="arithmatex">\(y\)</span>-intercept <span class="arithmatex">\(c\)</span>. The Fenchel conjugate <span class="arithmatex">\(f^*(t)\)</span> is the negative of this <span class="arithmatex">\(y\)</span>-intercept: <span class="arithmatex">\(f^*(t) = -c\)</span></p>
<p><strong>Economic Intuition for the Fenchel Conjugate:</strong></p>
<p>One of the most intuitive ways to understand the convex conjugate function <span class="arithmatex">\(f^*(t)\)</span> is through an economic lens. Imagine <span class="arithmatex">\(f(u)\)</span> as the cost function representing the total expense incurred to produce a quantity <span class="arithmatex">\(u\)</span> of a certain product. The variable <span class="arithmatex">\(y\)</span> corresponds to the market price per unit of that product.</p>
<p>In this context, the product <span class="arithmatex">\(xy\)</span> represents the revenue generated by selling <span class="arithmatex">\(x\)</span> units at price <span class="arithmatex">\(t\)</span>. The term <span class="arithmatex">\(f(u)\)</span>, as mentioned, is the cost of producing those units. Therefore, the expression <span class="arithmatex">\(tu - f(u)\)</span> represents the profit earned by producing and selling <span class="arithmatex">\(u\)</span> units at price <span class="arithmatex">\(t\)</span>.</p>
<p>The convex conjugate <span class="arithmatex">\(f^*(t)\)</span> is defined as the supremum (or maximum) of this profit over all possible production quantities <span class="arithmatex">\(u\)</span>:</p>
<div class="arithmatex">\[f^*(t) = \sup_u (tu - f(u))\]</div>
<p>Thus, <span class="arithmatex">\(f^*(t)\)</span> gives the optimal profit achievable at the market price <span class="arithmatex">\(t\)</span>, assuming the producer chooses the best production quantity <span class="arithmatex">\(u\)</span> to maximize profit.</p>
<p><strong>Geometric Interpretation in Economics:</strong></p>
<p>Now, consider the graph of the cost function <span class="arithmatex">\(f(u)\)</span>. Assume <span class="arithmatex">\(f\)</span> is convex, continuous, and differentiable, which is a reasonable assumption for many cost functions in economics.</p>
<p>The slope of the cost curve at any point <span class="arithmatex">\(u\)</span> is given by the derivative <span class="arithmatex">\(f'(u)\)</span>. This derivative represents the marginal cost — the additional cost to produce one more unit at quantity <span class="arithmatex">\(u\)</span>.</p>
<p>The condition for optimal production quantity <span class="arithmatex">\(u\)</span> at price <span class="arithmatex">\(t\)</span> arises from maximizing profit:</p>
<div class="arithmatex">\[\max_u \{tu - f(u)\}\]</div>
<p>Taking the derivative with respect to <span class="arithmatex">\(u\)</span> and setting it to zero for an optimum:</p>
<div class="arithmatex">\[t - f'(u) = 0 \implies t = f'(u)\]</div>
<p>This means the optimal production quantity <span class="arithmatex">\(u\)</span> is found where the price <span class="arithmatex">\(t\)</span> equals the marginal cost <span class="arithmatex">\(f'(u)\)</span>.</p>
<p>Geometrically, this corresponds to finding a tangent line to the graph of <span class="arithmatex">\(f(u)\)</span> that has slope <span class="arithmatex">\(t\)</span>. Using a ruler, you can "slide" the line around until it just touches the cost curve without crossing it. The point of tangency corresponds to the optimal <span class="arithmatex">\(u\)</span>.</p>
<p>Importantly, the vertical intercept of this tangent line relates directly to the optimal profit. The tangent line can be expressed as:</p>
<div class="arithmatex">\[\ell(u) = f(u_0) + f'(u_0)(u - u_0)\]</div>
<p>At <span class="arithmatex">\(u = 0\)</span>, the intercept is:</p>
<div class="arithmatex">\[\ell(0) = f(u_0) - u_0 f'(u_0)\]</div>
<p>Notice that:</p>
<div class="arithmatex">\[-(u_0 t - f(u_0)) = f(u_0) - u_0 t\]</div>
<p>Since <span class="arithmatex">\(t = f'(u_0)\)</span>, the intercept equals the negative of the optimal profit. Therefore, the intercept of the tangent line with slope <span class="arithmatex">\(t\)</span> gives <span class="arithmatex">\(-f^*(t)\)</span>.</p>
<p><strong>Key Properties:</strong>
1. <strong>Convexity</strong>: If <span class="arithmatex">\(f\)</span> is convex, then <span class="arithmatex">\(f^*\)</span> is also convex
2. <strong>Duality</strong>: <span class="arithmatex">\((f^*)^* = f\)</span> (the conjugate of the conjugate is the original function)
3. <strong>Domain</strong>: The domain of <span class="arithmatex">\(f^*\)</span> depends on the behavior of <span class="arithmatex">\(f\)</span></p>
<p><strong>Examples of Fenchel Conjugates:</strong></p>
<p><strong>Example 1: KL Divergence</strong></p>
<p>For <span class="arithmatex">\(f(u) = u \log u\)</span>:</p>
<div class="arithmatex">\[f^*(t) = \sup_{u &gt; 0} (tu - u \log u)\]</div>
<p>To find this, we set the derivative to zero:</p>
<div class="arithmatex">\[\frac{d}{du}(tu - u \log u) = t - \log u - 1 = 0\]</div>
<div class="arithmatex">\[\log u = t - 1\]</div>
<div class="arithmatex">\[u = e^{t-1}\]</div>
<p>Substituting back:</p>
<div class="arithmatex">\[f^*(t) = te^{t-1} - e^{t-1}(t-1) = e^{t-1}\]</div>
<p><strong>Example 2: Reverse KL</strong></p>
<p>For <span class="arithmatex">\(f(u) = -\log u\)</span>:</p>
<div class="arithmatex">\[f^*(t) = \sup_{u &gt; 0} (tu + \log u)\]</div>
<p>Setting derivative to zero:</p>
<div class="arithmatex">\[\frac{d}{du}(tu + \log u) = t + \frac{1}{u} = 0\]</div>
<div class="arithmatex">\[u = -\frac{1}{t}\]</div>
<p>Substituting back:</p>
<div class="arithmatex">\[f^*(t) = t(-\frac{1}{t}) + \log(-\frac{1}{t}) = -1 + \log(-\frac{1}{t}) = -1 - \log(-t)\]</div>
<p><strong>Example 3: Total Variation</strong></p>
<p>For <span class="arithmatex">\(f(u) = \frac{1}{2}|u-1|\)</span>:</p>
<div class="arithmatex">\[f^*(t) = \sup_{u} (tu - \frac{1}{2}|u-1|)\]</div>
<p>This gives:</p>
<div class="arithmatex">\[f^*(t) = \begin{cases} t &amp; \text{if } |t| \leq \frac{1}{2} \\ +\infty &amp; \text{otherwise} \end{cases}\]</div>
<p><strong>The Duality Principle:</strong></p>
<p>The Fenchel conjugate provides a way to transform optimization problems. The key insight is that:</p>
<p><strong>Primal Problem</strong>: <span class="arithmatex">\(\inf_{u} f(u)\)</span></p>
<p><strong>Dual Problem</strong>: <span class="arithmatex">\(\sup_{t} -f^*(t)\)</span></p>
<p><strong>The Primal-Dual Relationship:</strong></p>
<p>The f-divergence can be expressed in both forms:</p>
<div class="arithmatex">\[D_f(p,q) = \mathbb{E}_{x \sim q}\left[f\left(\frac{p(x)}{q(x)}\right)\right] = \sup_{T} \left(\mathbb{E}_{x \sim p}[T(x)]- \mathbb{E}_{x_{fake} \sim q}[f^*(T(x_{fake}))]\right)\]</div>
<p>Where:</p>
<ul>
<li>
<p><strong>Primal form</strong>: <span class="arithmatex">\(\mathbb{E}_{x \sim q}\left[f\left(\frac{p(x)}{q(x)}\right)\right]\)</span> (direct computation)</p>
</li>
<li>
<p><strong>Dual form</strong>: <span class="arithmatex">\(\sup_{T} \left(\mathbb{E}_{x \sim p}[T(x)] - \mathbb{E}_{x_{fake} \sim q}[f^*(T(x_{fake}))]\right)\)</span> (optimization problem)</p>
</li>
</ul>
<p><strong>Derivation: From Primal to Dual Form</strong></p>
<p>Let's walk through the step-by-step derivation of how we transform the primal form into the dual form:</p>
<p><strong>Step 1: Start with the Primal Form</strong></p>
<p>The f-divergence is defined as:</p>
<div class="arithmatex">\[D_f(p,q) = \mathbb{E}_{x \sim q}\left[f\left(\frac{p(x)}{q(x)}\right)\right]\]</div>
<p>This is the "primal form" - it directly computes the divergence by evaluating the function <span class="arithmatex">\(f\)</span> at the ratio <span class="arithmatex">\(\frac{p(x)}{q(x)}\)</span>.</p>
<p><strong>Step 2: Apply the Fenchel Conjugate Identity</strong></p>
<p>The key insight comes from the Fenchel conjugate identity. For any convex function <span class="arithmatex">\(f\)</span> and any point <span class="arithmatex">\(u\)</span>, we have:</p>
<div class="arithmatex">\[f(u) = \sup_{t} (tu - f^*(t))\]</div>
<p>This is a fundamental result in convex analysis known as the <strong>Fenchel-Moreau theorem</strong>. It states that a convex function can be recovered from its conjugate.</p>
<p><strong>Step 3: Substitute the Identity</strong></p>
<p>We substitute this identity into our primal form:</p>
<div class="arithmatex">\[D_f(p,q) = \mathbb{E}_{x \sim q}\left[\sup_{t} \left(t \cdot \frac{p(x)}{q(x)} - f^*(t)\right)\right]\]</div>
<p><strong>Step 4: Exchange Supremum and Expectation</strong></p>
<p>This is the crucial step. We can exchange the supremum and expectation under certain conditions (satisfied for convex <span class="arithmatex">\(f\)</span>):</p>
<div class="arithmatex">\[D_f(p,q) = \sup_{T} \mathbb{E}_{x \sim q}\left[T(x) \cdot \frac{p(x)}{q(x)} - f^*(T(x))\right]\]</div>
<p>Here, we've replaced the variable <span class="arithmatex">\(t\)</span> with a function <span class="arithmatex">\(T(x)\)</span> that can depend on <span class="arithmatex">\(x\)</span>.</p>
<p><strong>Step 5: Simplify the Expression</strong></p>
<p>We can rewrite the expectation:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim q}\left[T(x) \cdot \frac{p(x)}{q(x)} - f^*(T(x))\right] = \mathbb{E}_{x \sim q}\left[T(x) \cdot \frac{p(x)}{q(x)}\right] - \mathbb{E}_{x \sim q}[f^*(T(x))]\]</div>
<p>The first term can be simplified using the definition of expectation:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim q}\left[T(x) \cdot \frac{p(x)}{q(x)}\right] = \int T(x) \cdot \frac{p(x)}{q(x)} \cdot q(x) dx = \int T(x) \cdot p(x) dx = \mathbb{E}_{x \sim p}[T(x)]\]</div>
<p><strong>Step 6: Arrive at the Dual Form</strong></p>
<p>Putting it all together:</p>
<div class="arithmatex">\[D_f(p,q) = \sup_{T} \left(\mathbb{E}_{x \sim p}[T(x)] - \mathbb{E}_{x \sim q}[f^*(T(x))]\right)\]</div>
<p>In other words (to distinguish the two different inputs to the Discriminator):</p>
<div class="arithmatex">\[D_f(p,q) = \sup_{T} \left(\mathbb{E}_{x \sim p}[T(x)] - \mathbb{E}_{x_{fake} \sim q}[f^*(T(x_{fake}))]\right)\]</div>
<p>This is the <strong>dual form</strong> of the f-divergence.</p>
<p><strong>Why This Derivation Works:</strong></p>
<ol>
<li><strong>Convexity</strong>: The convexity of <span class="arithmatex">\(f\)</span> ensures that the Fenchel conjugate identity holds</li>
<li><strong>Exchange of Supremum and Expectation</strong>: This is valid because we're optimizing over a convex set of functions</li>
<li><strong>Duality Gap</strong>: Under certain conditions, there is no duality gap, meaning the primal and dual forms give the same value</li>
</ol>
<p><strong>Key Insights from the Derivation:</strong></p>
<ol>
<li>
<p><strong>From Direct Computation to Optimization</strong>: The primal form requires direct computation of <span class="arithmatex">\(f(\frac{p(x)}{q(x)})\)</span>, while the dual form transforms this into an optimization problem over functions <span class="arithmatex">\(T\)</span>.</p>
</li>
<li>
<p><strong>Role of the Fenchel Conjugate</strong>: The conjugate <span class="arithmatex">\(f^*\)</span> appears naturally in the dual form.</p>
</li>
<li>
<p><strong>Connection to GANs</strong>: The dual form is perfect for GANs because we can parameterize <span class="arithmatex">\(T\)</span> as a neural network (the discriminator). The optimization becomes a minimax game. We can use gradient-based optimization.</p>
</li>
</ol>
<p><strong>Example: KL Divergence Derivation</strong></p>
<p>Let's see this in action for KL divergence where <span class="arithmatex">\(f(u) = u \log u\)</span>:</p>
<p><strong>Primal Form:</strong></p>
<div class="arithmatex">\[D_{KL}(p||q) = \mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)} \log \frac{p(x)}{q(x)}\right]\]</div>
<p><strong>Step 1:</strong> Use the Fenchel conjugate identity for <span class="arithmatex">\(f(u) = u \log u\)</span></p>
<p>We know that <span class="arithmatex">\(f^*(t) = e^{t-1}\)</span> (from our earlier examples)</p>
<p><strong>Step 2:</strong> Substitute:</p>
<div class="arithmatex">\[D_{KL}(p||q) = \mathbb{E}_{x \sim q}\left[\sup_{t} \left(t \cdot \frac{p(x)}{q(x)} - e^{t-1}\right)\right]\]</div>
<p><strong>Step 3:</strong> Exchange supremum and expectation:</p>
<div class="arithmatex">\[D_{KL}(p||q) = \sup_{T} \mathbb{E}_{x \sim q}\left[T(x) \cdot \frac{p(x)}{q(x)} - e^{T(x)-1}\right]\]</div>
<p><strong>Step 4:</strong> Simplify:</p>
<div class="arithmatex">\[D_{KL}(p||q) = \sup_{T} \left(\mathbb{E}_{x \sim p}[T(x)] - \mathbb{E}_{x \sim q}[e^{T(x)-1}]\right)\]</div>
<p>This gives us the dual form for KL divergence, which can be used in f-GAN training.</p>
<h4 id="steps-in-f-gan">Steps in f-GAN</h4>
<p><strong>Step 1: Choose an f-divergence</strong>
Select a convex function <span class="arithmatex">\(f\)</span> with <span class="arithmatex">\(f(1) = 0\)</span> (e.g., KL divergence, Jensen-Shannon, etc.)</p>
<p><strong>Step 2: Compute the Fenchel conjugate</strong>
Find <span class="arithmatex">\(f^*\)</span> analytically or numerically</p>
<p><strong>Step 3: Parameterize the dual variable</strong>
Replace <span class="arithmatex">\(T\)</span> with a neural network <span class="arithmatex">\(T_\phi\)</span> parameterized by <span class="arithmatex">\(\phi\)</span></p>
<p><strong>Step 4: Set up the minimax game</strong></p>
<div class="arithmatex">\[\min_\theta \max_\phi F(\theta,\phi) = \mathbb{E}_{x \sim p_{data}}[T_\phi(x)] - \mathbb{E}_{x_{fake} \sim p_{G_\theta}}[f^*(T_\phi(x_{fake}))]\]</div>
<p><strong>Understanding the Roles:</strong></p>
<ul>
<li><strong>Generator (<span class="arithmatex">\(G_\theta\)</span>)</strong>: Tries to minimize the divergence estimate</li>
<li><strong>Discriminator (<span class="arithmatex">\(T_\phi\)</span>)</strong>: Tries to tighten the lower bound by maximizing the dual objective</li>
</ul>
<p><strong>Key Insight:</strong>
The discriminator <span class="arithmatex">\(T_\phi\)</span> is not a binary classifier like in standard GANs, but rather a function.</p>
<p><strong>Important Distinction: Vanilla GAN vs f-GAN Generator</strong></p>
<p>There is a fundamental difference between how generators work in vanilla GANs versus f-GANs:</p>
<p><strong>Vanilla GAN Generator:</strong></p>
<ul>
<li>
<p><strong>Explicit generator network</strong>: <span class="arithmatex">\(G_\theta(z)\)</span> where <span class="arithmatex">\(z \sim p(z)\)</span> (noise)</p>
</li>
<li>
<p><strong>Direct transformation</strong>: Noise <span class="arithmatex">\(z\)</span> → Generated sample <span class="arithmatex">\(G_\theta(z)\)</span></p>
</li>
<li>
<p><strong>Objective</strong>: <span class="arithmatex">\(\min_\theta \max_\phi V(G_\theta, D_\phi) = \mathbb{E}_{x \sim p_{data}}[\log D_\phi(x)] + \mathbb{E}_{z \sim p(z)}[\log(1-D_\phi(G_\theta(z)))]\)</span></p>
</li>
</ul>
<p><strong>f-GAN Generator:</strong></p>
<ul>
<li>
<p><strong>No explicit generator network</strong>: We work with <span class="arithmatex">\(p_{G_\theta}\)</span> as a distribution directly</p>
</li>
<li>
<p><strong>Implicit generator</strong>: The "generator" is whatever mechanism produces samples from <span class="arithmatex">\(p_{G_\theta}\)</span></p>
</li>
<li>
<p><strong>Objective</strong>: <span class="arithmatex">\(\min_\theta \max_\phi F(\theta,\phi) = \mathbb{E}_{x \sim p_{data}}[T_\phi(x)] - \mathbb{E}_{x_{fake} \sim p_{G_\theta}}[f^*(T_\phi(x_{fake}))]\)</span></p>
</li>
</ul>
<p><strong>The Key Insight:</strong></p>
<p>In f-GAN, the "generator" is <strong>implicit</strong> - it's whatever mechanism produces samples from the distribution <span class="arithmatex">\(p_{G_\theta}\)</span>. This could be:</p>
<ol>
<li><strong>A neural network</strong> <span class="arithmatex">\(G_\theta\)</span> that transforms noise (like in vanilla GANs)</li>
<li><strong>A flow-based model</strong> that transforms a base distribution</li>
<li><strong>A VAE decoder</strong> that generates from a latent space</li>
<li><strong>Any other generative model</strong> that produces samples from <span class="arithmatex">\(p_{G_\theta}\)</span></li>
</ol>
<p><strong>Why This Matters:</strong></p>
<p>The f-GAN framework is <strong>more general</strong> than vanilla GANs because:</p>
<ul>
<li><strong>Vanilla GANs</strong>: Require a specific generator architecture <span class="arithmatex">\(G_\theta(z)\)</span></li>
<li><strong>f-GANs</strong>: Can work with any generative model that produces samples from <span class="arithmatex">\(p_{G_\theta}\)</span></li>
</ul>
<p><strong>Practical Implementation:</strong></p>
<p>In practice, when implementing f-GAN, you would typically:</p>
<ol>
<li><strong>Choose a generative model</strong> (e.g., a neural network <span class="arithmatex">\(G_\theta\)</span>)</li>
<li><strong>Use it to generate samples</strong> from <span class="arithmatex">\(p_{G_\theta}\)</span></li>
<li><strong>Apply the f-GAN objective</strong> to train both the generator and discriminator</li>
</ol>
<p>So in the f-GAN formulation, there's no explicit <span class="arithmatex">\(G_\theta\)</span> network like in vanilla GANs. The "generator" is the abstract distribution <span class="arithmatex">\(p_{G_\theta}\)</span>, and the actual implementation depends on what generative model you choose to use.</p>
<h4 id="advantages-of-f-gan">Advantages of f-GAN</h4>
<ol>
<li><strong>Unified Framework</strong>: One formulation covers many different GAN variants</li>
<li><strong>Theoretical Rigor</strong>: Based on well-established convex optimization theory</li>
<li><strong>Flexibility</strong>: Can adapt the divergence measure to the specific problem</li>
<li><strong>Stability</strong>: Some f-divergences may lead to more stable training</li>
</ol>
<h4 id="practical-considerations">Practical Considerations</h4>
<ul>
<li><strong>Choice of f</strong>: Different f-divergences have different properties</li>
<li><strong>Fenchel Conjugate</strong>: Must be computable for the chosen f-divergence</li>
<li><strong>Training</strong>: Similar alternating optimization as standard GANs</li>
<li><strong>Evaluation</strong>: Same challenges as other GAN variants</li>
</ul>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>