
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/AI/deep_generative_models/score_based_diffusion_models/">
      
      
        <link rel="prev" href="../evaluating_generative_models/">
      
      
        <link rel="next" href="../../../math/probability/probability_and_counting/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Score Based Diffusion Models - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#score-based-diffusion-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Score Based Diffusion Models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/probability/probability_and_counting/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_generative_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Generative Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluating_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluating Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#quick-recap-score-based-models" class="md-nav__link">
    <span class="md-ellipsis">
      Quick Recap: Score Based Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#diffusion-models-as-score-based-models-hierarchical-vaes" class="md-nav__link">
    <span class="md-ellipsis">
      Diffusion Models as Score Based Models &amp; Hierarchical VAEs
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="score-based-diffusion-models">Score Based Diffusion Models</h1>
<h2 id="quick-recap-score-based-models">Quick Recap: Score Based Models</h2>
<p>From our exploration of score-based generative modeling, we learned several key concepts:</p>
<p><strong>Score Function</strong>: The gradient of the log probability density, <span class="arithmatex">\(\nabla_x \log p(x)\)</span>, which points "uphill" in the probability landscape toward high-density regions.</p>
<p><strong>Score Matching</strong>: A training objective that learns the score function by minimizing the Fisher divergence between the learned and true score functions.</p>
<p><strong>Score Matching Objective</strong>: The original score matching objective is:</p>
<div class="arithmatex">\[\mathcal{L}(\theta) = \mathbb{E}_{x \sim p_{data}(x)} \left[ \frac{1}{2} \| s_\theta(x) \|_2^2 + \text{tr}(\nabla_x s_\theta(x)) \right]\]</div>
<p>where <span class="arithmatex">\(\text{tr}(\nabla_x s_\theta(x))\)</span> is the trace of the Jacobian of the score function, which is computationally expensive to evaluate.</p>
<p><strong>Denoising Score Matching (DSM)</strong>: A practical variant that trains the score function to predict the direction from noisy to clean data, avoiding the need to compute the true score function.</p>
<p><strong>DSM Objective</strong>: The denoising score matching objective is:</p>
<div class="arithmatex">\[\mathcal{L}(\theta) = \mathbb{E}_{y \sim p_{data}(y)} \mathbb{E}_{x \sim \mathcal{N}(x; y, \sigma^2 I)} \left[ \frac{1}{2} \left\| s_\theta(x) - \frac{y - x}{\sigma^2} \right\|_2^2 \right]\]</div>
<p>where <span class="arithmatex">\(s_\theta(x)\)</span> learns to predict the score function of the noise-perturbed distribution, and <span class="arithmatex">\(\frac{y - x}{\sigma^2}\)</span> is the target score function that points from noisy sample <span class="arithmatex">\(x\)</span> toward clean data <span class="arithmatex">\(y\)</span>.</p>
<p><strong>Langevin Dynamics</strong>: A continuous-time stochastic process that uses the score function to guide sampling:</p>
<div class="arithmatex">\[dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t\]</div>
<p><strong>Discretized Form</strong>: For practical implementation:</p>
<div class="arithmatex">\[x_{t+1} = x_t + \frac{\epsilon}{2} \cdot s_\theta(x_t) + \sqrt{2\epsilon} \cdot \eta_t\]</div>
<p><strong>Mode Collapse</strong>: Standard Langevin dynamics struggles with multi-modal distributions and low-density regions.</p>
<p><strong>Annealed Langevin Dynamics</strong>: Addresses this by using multiple noise scales <span class="arithmatex">\(\sigma_1 &lt; \sigma_2 &lt; \ldots &lt; \sigma_L\)</span>, creating a sequence of increasingly noisy distributions that are easier to sample from.</p>
<p><strong>Stochastic Differential Equations (SDEs)</strong>: General framework for continuous-time stochastic processes:</p>
<div class="arithmatex">\[dx = f(x, t)dt + g(t)dw\]</div>
<p><strong>Reverse SDE</strong>: Any SDE has a corresponding reverse process for sampling:</p>
<div class="arithmatex">\[dx = [f(x, t) - g^2(t)\nabla_x \log p_t(x)]dt + g(t)d\bar{w}\]</div>
<p><strong>Time-Dependent Score Models</strong>: Neural networks that learn <span class="arithmatex">\(s_\theta(x, t) \approx \nabla_x \log p_t(x)\)</span> for continuous-time processes.</p>
<p><strong>Key insights:</strong></p>
<ol>
<li><strong>Score functions act as denoisers</strong>: They point from noisy to clean data</li>
<li><strong>Multiple noise scales help</strong>: Annealing from high to low noise improves sampling</li>
<li><strong>Continuous-time generalizes discrete</strong>: SDEs provide a unified framework</li>
<li><strong>Reverse processes enable generation</strong>: The reverse SDE naturally incorporates the score function for sampling</li>
</ol>
<h2 id="diffusion-models-as-score-based-models-hierarchical-vaes">Diffusion Models as Score Based Models &amp; Hierarchical VAEs</h2>
<p><strong>Iterative Denoising perspective</strong>: In annealed Langevin dynamics with multiple noise scales, the sampling process can be viewed as <strong>iterative denoising</strong>. Starting from high noise levels and gradually reducing noise, each step uses the score function to denoise the sample, progressively refining it from a noisy state toward the clean data distribution.</p>
<p><strong>Training perspective</strong>: The inverse process involves <strong>iteratively adding Gaussian noise</strong> to clean data during training. By corrupting data with increasing levels of noise, the model learns to predict the score function at each noise level, enabling it to reverse the corruption process during sampling.</p>
<p><img alt="Iterative denoising" src="../iter_denoise.png" /></p>
<p><strong>VAE Perspective</strong>: This entire framework can be viewed as a <strong>VAE</strong> where:</p>
<ul>
<li>
<p><strong>Encoder process</strong>: The forward process that converts clean data to noise through iterative corruption</p>
</li>
<li>
<p><strong>Decoder process</strong>: The reverse process that generates samples by iteratively denoising from noise</p>
</li>
</ul>
<p><strong>Noise Perturbation process</strong>: Each <span class="arithmatex">\(x_t\)</span> represents a noise-perturbed density that is obtained by adding Gaussian noise to <span class="arithmatex">\(x_{t-1}\)</span>. This creates a Markov chain where each step adds a small amount of noise to the previous state.</p>
<p>We can write the forward process as a conditional distribution:</p>
<div class="arithmatex">\[q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)\]</div>
<p>where <span class="arithmatex">\(\beta_t\)</span> is the noise schedule that determines how much noise is added at each step.</p>
<p>The joint distribution of the entire forward process is:</p>
<div class="arithmatex">\[q(x_1, x_2, \ldots, x_T | x_0) = \prod_{t=1}^T q(x_t | x_{t-1})\]</div>
<p>This factorization follows from the <strong>chain rule of probability</strong> and the <strong>Markov property</strong> of the forward process:</p>
<p><strong>Chain Rule</strong>: For any joint distribution, we can write:</p>
<div class="arithmatex">\[q(x_1, x_2, \ldots, x_T | x_0) = q(x_1 | x_0) \cdot q(x_2 | x_0, x_1) \cdot q(x_3 | x_0, x_1, x_2) \cdots q(x_T | x_0, x_1, \ldots, x_{T-1})\]</div>
<p><strong>Markov Property</strong>: In the forward process, each <span class="arithmatex">\(x_t\)</span> depends only on <span class="arithmatex">\(x_{t-1}\)</span>, not on earlier states:</p>
<div class="arithmatex">\[q(x_t | x_0, x_1, \ldots, x_{t-1}) = q(x_t | x_{t-1})\]</div>
<p>Substituting the Markov property into the chain rule:</p>
<div class="arithmatex">\[q(x_1, x_2, \ldots, x_T | x_0) = q(x_1 | x_0) \cdot q(x_2 | x_1) \cdot q(x_3 | x_2) \cdots q(x_T | x_{T-1})\]</div>
<p>This can be written compactly as:</p>
<div class="arithmatex">\[q(x_1, x_2, \ldots, x_T | x_0) = \prod_{t=1}^T q(x_t | x_{t-1})\]</div>
<p>This represents the probability of the entire noise corruption sequence, where each step depends only on the previous step (Markov property).</p>
<p><strong>Comparison with VAEs</strong>: In a typical VAE, you would take <span class="arithmatex">\(x_0\)</span> and map it via a neural network to obtain some mean and standard deviation to parameterize the distribution of the latent variable. Here, we obtain the distribution of the latent variables through the <strong>predefined noise corruption procedure</strong> we defined above, rather than learning it with a neural network.</p>
<p><strong>Multistep transitions</strong>: A key advantage of this process is that we can compute transitions between any two time steps efficiently. For example, we can directly compute <span class="arithmatex">\(q(x_t | x_0)\)</span> without going through all intermediate steps.</p>
<p>Starting from <span class="arithmatex">\(x_0\)</span>, we can write:</p>
<div class="arithmatex">\[x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}\]</div>
<p>where <span class="arithmatex">\(\alpha_t = 1 - \beta_t\)</span> and <span class="arithmatex">\(\epsilon_{t-1} \sim \mathcal{N}(0, I)\)</span>.</p>
<p>Recursively substituting:</p>
<div class="arithmatex">\[x_t = \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2}) + \sqrt{1 - \alpha_t} \epsilon_{t-1}\]</div>
<p>Continuing this recursion, we get:</p>
<div class="arithmatex">\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\]</div>
<p>where <span class="arithmatex">\(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\)</span> and <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, I)\)</span>.</p>
<p><strong>Result</strong>: The multistep transition is:</p>
<div class="arithmatex">\[q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\]</div>
<p>This allows us to sample <span class="arithmatex">\(x_t\)</span> directly from <span class="arithmatex">\(x_0\)</span> in a single step, making training much more efficient.</p>
<p><strong>Diffusion analogy</strong>: We can think of this as a <strong>diffusion process</strong>. This is like a diffuser where given an initial state, we keep adding noise at every step. This is analogous to <strong>heat diffusion</strong> in a space- just as heat spreads out and becomes more uniform over time, our data distribution becomes increasingly noisy and uniform Gaussian as we add more noise at each step.</p>
<p>The process gradually "diffuses" the structured information in the data into random noise, creating a smooth transition from the complex data distribution to a simple Gaussian noise distribution.</p>
<p><img alt="Diffusion" src="../diff.png" /></p>
<p>The ideal sampling process would be:</p>
<ol>
<li>Sample <span class="arithmatex">\(x_T\)</span> from <span class="arithmatex">\(\pi(x_T)\)</span>. Start with pure noise from the prior distribution</li>
<li>Iteratively sample from the true denoising distribution <span class="arithmatex">\(q(x_{t-1} | x_t)\)</span>.</li>
</ol>
<p>This would generate samples by following the exact reverse of the forward diffusion process, gradually denoising from pure noise back to clean data.</p>
<p>The challenge however, is that we don't know the true denoising distributions <span class="arithmatex">\(q(x_{t-1} | x_t)\)</span>. While the forward process <span class="arithmatex">\(q(x_t | x_{t-1})\)</span> is predefined and tractable, the reverse process is not.</p>
<p>However, we can learn an approximation <span class="arithmatex">\(p_\theta(x_{t-1} | x_t)\)</span> which is a Gaussian distribution with learned parameters:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\]</div>
<p>where <span class="arithmatex">\(\mu_\theta(x_t, t)\)</span> is a neural network that learns the mean of the denoising distribution, and <span class="arithmatex">\(\sigma_t^2 I\)</span> is the fixed variance schedule.</p>
<p>This is similar to a VAE decoder:</p>
<p><strong>VAE Decoder</strong>:</p>
<div class="arithmatex">\[p_\theta(x | z) = \mathcal{N}(x; \mu_\theta(z), \sigma_\theta^2(z) I)\]</div>
<p><strong>Diffusion reverse process</strong>:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\]</div>
<p>The diffusion decoder <span class="arithmatex">\(p_\theta(x_{t-1} | x_t)\)</span> is trying to learn to approximate the true denoising distributions <span class="arithmatex">\(q(x_{t-1} | x_t)\)</span>.</p>
<p>The joint distribution of the learned reverse process is:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = \prod_{t=1}^T p_\theta(x_{t-1} | x_t)\]</div>
<p>Let's derive the joint distribution of the learned reverse process step by step.</p>
<p>In the general case of <span class="arithmatex">\(n\)</span> random variables <span class="arithmatex">\(X_1, X_2, \ldots, X_n\)</span>, the values of an arbitrary subset of variables can be known and one can ask for the joint probability of all other variables. For example, if the values of <span class="arithmatex">\(X_{k+1}, X_{k+2}, \ldots, X_n\)</span> are known, the probability for <span class="arithmatex">\(X_1, X_2, \ldots, X_k\)</span> given these known values is:</p>
<div class="arithmatex">\[P(X_1, X_2, \ldots, X_k|X_{k+1}, X_{k+2}, \ldots, X_n) = \frac{P(X_1, X_2, \ldots, X_n)}{P(X_{k+1}, X_{k+2}, \ldots, X_n)}\]</div>
<p>This is the fundamental definition of conditional probability for multiple random variables.</p>
<p>For any three events <span class="arithmatex">\(A\)</span>, <span class="arithmatex">\(B\)</span>, and <span class="arithmatex">\(C\)</span>, the joint conditional probability is defined as:</p>
<div class="arithmatex">\[P(A, B|C) = \frac{P(A, B, C)}{P(C)}\]</div>
<p>We can write the joint probability <span class="arithmatex">\(P(A, B, C)\)</span> using the chain rule:</p>
<div class="arithmatex">\[P(A, B, C) = P(A|B, C) \cdot P(B, C)\]</div>
<p>Substituting this into our definition:</p>
<div class="arithmatex">\[P(A, B|C) = \frac{P(A|B, C) \cdot P(B, C)}{P(C)}\]</div>
<p>We can write <span class="arithmatex">\(P(B, C)\)</span> as:</p>
<div class="arithmatex">\[P(B, C) = P(B|C) \cdot P(C)\]</div>
<div class="arithmatex">\[P(A, B|C) = \frac{P(A|B, C) \cdot P(B|C) \cdot P(C)}{P(C)}\]</div>
<p>The <span class="arithmatex">\(P(C)\)</span> terms cancel out:</p>
<div class="arithmatex">\[P(A, B|C) = P(A|B, C) \cdot P(B|C)\]</div>
<p>The learned reverse process consists of a sequence of conditional distributions:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\]</div>
<p>where <span class="arithmatex">\(\mu_\theta(x_t, t)\)</span> is a neural network that learns the mean of the denoising distribution.</p>
<p>For the joint distribution <span class="arithmatex">\(p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T)\)</span>, we can apply the chain rule:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = p_\theta(x_0 | x_1, \ldots, x_T) \cdot p_\theta(x_1 | x_2, \ldots, x_T) \cdots p_\theta(x_{T-1} | x_T)\]</div>
<p>In the reverse process, we assume that each <span class="arithmatex">\(x_{t-1}\)</span> depends only on <span class="arithmatex">\(x_t\)</span>, not on future states. This is the <strong>reverse Markov property</strong>:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t, x_{t+1}, \ldots, x_T) = p_\theta(x_{t-1} | x_t)\]</div>
<p>Substituting the reverse Markov property into the chain rule:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = p_\theta(x_0 | x_1) \cdot p_\theta(x_1 | x_2) \cdots p_\theta(x_{T-1} | x_T)\]</div>
<p>This can be written compactly as:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = \prod_{t=1}^T p_\theta(x_{t-1} | x_t)\]</div>
<p>To get the complete joint distribution, we need to include the prior distribution over <span class="arithmatex">\(x_T\)</span>:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_T) = p(x_T) \cdot p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T)\]</div>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_T) = p(x_T) \cdot \prod_{t=1}^T p_\theta(x_{t-1} | x_t)\]</div>
<p>A crucial aspect of the diffusion process is choosing the values of <span class="arithmatex">\(\bar{\alpha}_t\)</span> such that after many steps, we are left with pure noise. This ensures that the forward process converges to a simple, known distribution.</p>
<p>Common choices for the noise schedule include:</p>
<ol>
<li><strong>Linear Schedule</strong>: <span class="arithmatex">\(\beta_t = \frac{t}{T} \cdot \beta_{\text{max}}\)</span></li>
<li><strong>Cosine Schedule</strong>: <span class="arithmatex">\(\beta_t = \cos\left(\frac{t}{T} \cdot \frac{\pi}{2}\right)\)</span></li>
<li><strong>Quadratic Schedule</strong>: <span class="arithmatex">\(\beta_t = \left(\frac{t}{T}\right)^2 \cdot \beta_{\text{max}}\)</span></li>
</ol>
<p><strong>Example</strong>: For a linear schedule with <span class="arithmatex">\(\beta_{\text{max}} = 0.02\)</span> and <span class="arithmatex">\(T = 1000\)</span>, we get <span class="arithmatex">\(\beta_1 = 0.00002\)</span>, <span class="arithmatex">\(\beta_{500} = 0.01\)</span> and <span class="arithmatex">\(\beta_{1000} = 0.02\)</span>.</p>
<p>Once we have trained the diffusion model and learned the reverse process <span class="arithmatex">\(p_\theta(x_{t-1} | x_t)\)</span>, we can generate new samples by running the reverse process. Here's how sampling works. </p>
<p>Sample <span class="arithmatex">\(x_T\)</span> from the prior distribution <span class="arithmatex">\(x_T \sim \mathcal{N}(x_T; 0, I)\)</span>.</p>
<p>For <span class="arithmatex">\(t = T, T-1, \ldots, 1\)</span>, sample from the learned reverse process <span class="arithmatex">\(x_{t-1} \sim p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\)</span>.</p>
<p>After <span class="arithmatex">\(T\)</span> steps, we obtain <span class="arithmatex">\(x_0\)</span>, which is our generated sample.</p>
<p>This entire diffusion framework can be viewed as a <strong>Variational Autoencoder (VAE)</strong> with a crucial difference: <strong>the encoder is fixed and predefined, while only the decoder is learned</strong>.</p>
<p><strong>Standard VAE Structure</strong>:</p>
<ul>
<li>
<p><strong>Encoder</strong>: <span class="arithmatex">\(q_\phi(z | x) = \mathcal{N}(z; \mu_\phi(x), \sigma_\phi^2(x) I)\)</span></p>
</li>
<li>
<p><strong>Decoder</strong>: <span class="arithmatex">\(p_\theta(x | z) = \mathcal{N}(x; \mu_\theta(z), \sigma_\theta^2(z) I)\)</span></p>
</li>
<li>
<p><strong>Prior</strong>: <span class="arithmatex">\(p(z) = \mathcal{N}(z; 0, I)\)</span></p>
</li>
</ul>
<p><strong>Vanilla VAE ELBO (Non-KL form)</strong>:</p>
<div class="arithmatex">\[ELBO_{\text{VAE}} = \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z|x)} \right]\]</div>
<p><strong>Hierarchical VAE Structure</strong> (z₂ → z₁ → x):</p>
<ul>
<li>
<p><strong>Encoder</strong>: <span class="arithmatex">\(q_\phi(z_1, z_2 | x) = q_\phi(z_1 | x) \cdot q_\phi(z_2 | z_1)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(q_\phi(z_1 | x) = \mathcal{N}(z_1; \mu_\phi(x), \sigma_\phi^2(x) I)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(q_\phi(z_2 | z_1) = \mathcal{N}(z_2; \mu_\phi(z_1), \sigma_\phi^2(z_1) I)\)</span></p>
</li>
<li>
<p><strong>Decoder</strong>: <span class="arithmatex">\(p_\theta(x, z_1 | z_2) = p_\theta(x | z_1) \cdot p_\theta(z_1 | z_2)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(p_\theta(x | z_1) = \mathcal{N}(x; \mu_\theta(z_1), \sigma_\theta^2(z_1) I)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(p_\theta(z_1 | z_2) = \mathcal{N}(z_1; \mu_\theta(z_2), \sigma_\theta^2(z_2) I)\)</span></p>
</li>
<li>
<p><strong>Prior</strong>: <span class="arithmatex">\(p(z_2) = \mathcal{N}(z_2; 0, I)\)</span></p>
</li>
</ul>
<p><strong>Hierarchical VAE ELBO (Non-KL form)</strong>:</p>
<div class="arithmatex">\[ELBO_{\text{HVAE}} = \mathbb{E}_{q_\phi(z_1,z_2|x)} \left[ \log \frac{p_\theta(x, z_1, z_2)}{q_\phi(z_1, z_2|x)} \right]\]</div>
<p>Following the hierarchical VAE formulation, we can write the ELBO for diffusion models. In diffusion models, we have a sequence of latent variables <span class="arithmatex">\(x_1, x_2, \ldots, x_T\)</span> where <span class="arithmatex">\(x_T\)</span> is the most abstract (pure noise) and <span class="arithmatex">\(x_0\)</span> is the data.</p>
<p><strong>Diffusion Model Structure</strong> (x_T → x_{T-1} → ... → x_1 → x_0):</p>
<ul>
<li>
<p><strong>Encoder</strong>: <span class="arithmatex">\(q(x_1, x_2, \ldots, x_T | x_0) = \prod_{t=1}^T q(x_t | x_{t-1})\)</span> - Fixed noise corruption process</p>
</li>
<li>
<p><strong>Decoder</strong>: <span class="arithmatex">\(p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = \prod_{t=1}^T p_\theta(x_{t-1} | x_t)\)</span> - Learned denoising process</p>
</li>
<li>
<p><strong>Prior</strong>: <span class="arithmatex">\(p(x_T) = \mathcal{N}(x_T; 0, I)\)</span></p>
</li>
</ul>
<p><strong>Diffusion Model ELBO (Non-KL form)</strong>:</p>
<div class="arithmatex">\[ELBO_{\text{Diff}} = \mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ \log \frac{p_\theta(x_0, x_1, \ldots, x_T)}{q(x_1, \ldots, x_T|x_0)} \right]\]</div>
<p>The Negative Evidence Lower BOund (NELBO) is the negative of the ELBO, which is what we actually minimize during training:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = -\mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ \log \frac{p_\theta(x_0, x_1, \ldots, x_T)}{q(x_1, \ldots, x_T|x_0)} \right]\]</div>
<p>This can be rewritten as:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = \mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ -\log \frac{p_\theta(x_0, x_1, \ldots, x_T)}{q(x_1, \ldots, x_T|x_0)} \right]\]</div>
<p>The decoder learns to predict the mean function <span class="arithmatex">\(\mu_\theta(x_t, t)\)</span> for the reverse process. Let's derive how this function is parameterized.</p>
<p>The true reverse process <span class="arithmatex">\(q(x_{t-1} | x_t, x_0)\)</span> can be derived using Bayes' theorem. For Gaussian distributions, this gives us:</p>
<div class="arithmatex">\[q(x_{t-1} | x_t, x_0) = \mathcal{N}(x_{t-1}; \mu_t(x_t, x_0), \sigma_t^2 I)\]</div>
<p>where it can be shown that:</p>
<div class="arithmatex">\[\mu_t(x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon \right)\]</div>
<p>and:</p>
<div class="arithmatex">\[\sigma_t^2 = \frac{\beta_t(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}\]</div>
<p>The learned reverse process is:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\]</div>
<p>Since we want the learned process to approximate the true reverse process, we parameterize <span class="arithmatex">\(\mu_\theta(x_t, t)\)</span> to match the form of <span class="arithmatex">\(\mu_t(x_t, x_0)\)</span>:</p>
<div class="arithmatex">\[\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right)\]</div>
<p>where <span class="arithmatex">\(\epsilon_\theta(x_t, t)\)</span> is a neural network that predicts the noise <span class="arithmatex">\(\epsilon\)</span> given <span class="arithmatex">\(x_t\)</span> and <span class="arithmatex">\(t\)</span>.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>