
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/">
      
      
        <link rel="prev" href="../introduction/">
      
      
        <link rel="next" href="../regularization/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Image Classification with Linear Classifiers - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#image-classification-with-linear-classifiers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Image Classification with Linear Classifiers
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-tabs__link">
          
  
  
  Productivity

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Learning for Computer Vision
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning for Computer Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Image Classification with Linear Classifiers
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Image Classification with Linear Classifiers
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-image-classification-task" class="md-nav__link">
    <span class="md-ellipsis">
      The Image Classification Task
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#machine-learning-data-driven-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Machine Learning: Data-Driven Approach
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k-nearest-neighbors-k-nn-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      k-Nearest Neighbors (k-NN) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-classifiers" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Classifiers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear Classifiers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameterized-mapping-from-images-to-label-scores" class="md-nav__link">
    <span class="md-ellipsis">
      Parameterized mapping from images to label scores
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#template-matching-interpretation" class="md-nav__link">
    <span class="md-ellipsis">
      Template Matching interpretation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bias-trick" class="md-nav__link">
    <span class="md-ellipsis">
      Bias Trick
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-function" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Function
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Loss Function">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multiclass-support-vector-machine-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Multiclass Support Vector Machine loss
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-classifier" class="md-nav__link">
    <span class="md-ellipsis">
      Softmax Classifier
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Softmax Classifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-kl-divergence-connection" class="md-nav__link">
    <span class="md-ellipsis">
      The KL Divergence connection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-issues-numeric-stability" class="md-nav__link">
    <span class="md-ellipsis">
      Practical issues: numeric stability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#svm-vs-softmax" class="md-nav__link">
    <span class="md-ellipsis">
      SVM vs. Softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#final-notes-on-terminology" class="md-nav__link">
    <span class="md-ellipsis">
      Final notes on terminology
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-references" class="md-nav__link">
    <span class="md-ellipsis">
      Additional References
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Regularization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../backpropagation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backpropagation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../backpropagation_for_a_linear_layer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backpropagation for a Linear Layer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../neural_networks_setting_up_the_architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Setting up the Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../neural_networks_setting_up_the_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Setting up the Data
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../neural_networks_learning_and_evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Learning and Evaluation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../putting_it_together_minimal_neural_network_case_study/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Putting it together- Minimal Neural Network Case Study
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convolutional_neural_networks_architectures_convolution_pooling_layers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolutional Neural Networks- Architectures, Convolution / Pooling Layers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recurrent_neural_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recurrent Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../long_short_term_memory_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Long-Short Term Memory (LSTM) Networks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Natural Language Processing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Natural Language Processing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/representing_words/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Representing words
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/svd_based_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SVD based methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/distributional_semantics_and_word2vec/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributional semantics and Word2vec
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/score_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/score_based_generative_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Generative Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/evaluating_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluating Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_generative_models/score_based_diffusion_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors, vector addition, and scalar multiplication
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector geometry in Rn and correlation coefficients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/planes_in_r3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Planes in R3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/span_subspaces_and_dimension/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Span, subspaces, and dimension
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/basis_and_orthogonality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basis and orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Applications of projections in Rn- orthogonal bases of planes and linear regression
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/story_proofs_and_axioms_of_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Story Proofs and Axioms of Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/some_famous_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Some famous problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/indicator_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Indicator Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/poisson_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Poisson Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/continuous_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Continuous Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/exponential_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exponential Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/joint_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Joint Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/independence_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Independence of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/multinomial_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multinomial Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/covariance_and_correlation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Covariance and Correlation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/transformations_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformations of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/convolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/beta_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Beta Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/jensens_inequality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jensen's inequality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/central_limit_theorem/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Central Limit Theorem
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/multivariate_normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multivariate Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/markov_chains/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Markov Chains
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Multivariate Calculus
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Multivariate Calculus
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/multivariate_calculus/the_tl_dr_version_of_derivatives/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    The TL;DR version of Derivatives
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/multivariate_calculus/multivariable_functions_level_sets_and_contour_plots/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multivariable functions, level sets, and contour plots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Productivity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Productivity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    How to Build Your Career in AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            How to Build Your Career in AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Three Steps to Career Growth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_1_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 1- Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_2_projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 2- Projects
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_3_job/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 3- Job
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/make_every_day_count/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Make Every Day Count
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="image-classification-with-linear-classifiers">Image Classification with Linear Classifiers</h1>
<h2 id="the-image-classification-task">The Image Classification Task</h2>
<p>Image classification is a fundamental computer vision task where, given an image and a set of labels, the goal is to assign the image to one of the labels. This is essentially a supervised learning problem where we want to learn a mapping from input images to discrete class labels.</p>
<p>Formally given a dataset <span class="arithmatex">\(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n\)</span> where:</p>
<ul>
<li>
<p><span class="arithmatex">\(x_i\)</span> is an input image (typically represented as a high-dimensional vector of pixel values)</p>
</li>
<li>
<p><span class="arithmatex">\(y_i\)</span> is the corresponding class label from a predefined set of categories <span class="arithmatex">\(\{1, 2, \ldots, C\}\)</span></p>
</li>
<li>
<p>The task is to learn a function <span class="arithmatex">\(f: \mathbb{R}^d \rightarrow \{1, 2, \ldots, C\}\)</span> that can accurately predict the class label for new, unseen images</p>
</li>
</ul>
<p><strong>Key challenges</strong>:</p>
<ul>
<li>
<p><strong>High dimensionality</strong>: Images are typically represented as very high-dimensional vectors (e.g., a 224Ã—224 RGB image has 150,528 dimensions)</p>
</li>
<li>
<p><strong>Variability</strong>: The same object can appear in different poses, lighting conditions, scales, backgrounds, camera movements, with background clutter, at different scales (zoom) within the image, with partial occlusion, deformation, and varying contextual information</p>
</li>
<li>
<p><strong>Intra-class variation</strong>: Objects within the same class can look very different</p>
</li>
<li>
<p><strong>Inter-class similarity</strong>: Objects from different classes can sometimes look very similar</p>
</li>
</ul>
<h2 id="machine-learning-data-driven-approach">Machine Learning: Data-Driven Approach</h2>
<p>The machine learning approach to image classification follows a systematic, data-driven methodology that can be broken down into three main steps:</p>
<p><strong>1. Collect a Dataset of Images and Labels:</strong> The first step involves gathering a comprehensive dataset where each image is paired with its corresponding class label.</p>
<p><strong>2. Use ML Algorithms to Train a Classifier:</strong> Once we have the dataset, we employ machine learning algorithms like Linear Regression, Support Vector Machines (SVM), or Logistic Regression to learn a mapping from images to class labels. The choice of algorithm depends on the complexity of the problem, dataset size, and computational constraints. </p>
<p><strong>3. Evaluate the Classifier on New Images:</strong> The final step is to assess how well the trained classifier performs on previously unseen images. This evaluation process includes measuring accuracy, precision, recall, and F1-score on the held-out test set. Also includes validation techniques like k-fold cross-validation to get robust performance estimates.</p>
<h2 id="k-nearest-neighbors-k-nn-algorithm">k-Nearest Neighbors (k-NN) Algorithm</h2>
<p>The k-Nearest Neighbors algorithm is one of the simplest and most intuitive machine learning algorithms for classification. It's a non-parametric, instance-based learning method that makes predictions based on the similarity of new examples to previously seen training examples.</p>
<p><strong>Training Phase</strong>: k-NN is a "lazy learner" - it doesn't actually learn a model during training. Instead, it simply stores all the training examples and their labels.</p>
<p><strong>Prediction Phase</strong>: For a new test image, k-NN:</p>
<ol>
<li>
<p>Computes the distance between the test image and all training images</p>
</li>
<li>
<p>Identifies the k nearest training examples (neighbors)</p>
</li>
<li>
<p>Assigns the class label that appears most frequently among these k neighbors</p>
</li>
</ol>
<p>The choice of distance metric is crucial for k-NN performance:</p>
<ul>
<li>
<p><strong>Euclidean Distance</strong>: <span class="arithmatex">\(d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}\)</span></p>
</li>
<li>
<p><strong>Manhattan Distance</strong>: <span class="arithmatex">\(d(x, y) = \sum_{i=1}^{n}|x_i - y_i|\)</span></p>
</li>
<li>
<p><strong>Cosine Similarity</strong>: <span class="arithmatex">\(d(x, y) = 1 - \frac{x \cdot y}{||x|| \cdot ||y||}\)</span></p>
</li>
</ul>
<p>Understanding the computational efficiency of k-NN requires analyzing its time complexity using Big O notation.</p>
<p><strong>Big O Notation</strong>: <span class="arithmatex">\(O(f(n))\)</span> describes how the runtime of an algorithm grows as the input size <span class="arithmatex">\(n\)</span> increases. It provides an upper bound on the worst-case performance, focusing on the dominant term and ignoring constants and lower-order terms.</p>
<p><strong>k-NN Time Complexity</strong>:</p>
<ul>
<li>
<p><strong>Training Time</strong>: <span class="arithmatex">\(O(1)\)</span> - k-NN doesn't perform any computation during training; it simply stores the training data</p>
</li>
<li>
<p><strong>Prediction Time</strong>: <span class="arithmatex">\(O(N)\)</span> - For each prediction, k-NN must compute distances to all <span class="arithmatex">\(N\)</span> training examples</p>
</li>
</ul>
<p>In real-world applications, we typically want classifiers that are <strong>fast at prediction</strong> and can tolerate <strong>slow training</strong> because:</p>
<ol>
<li><strong>Training happens once</strong>: We train the model offline, often overnight or over days, so training time is less critical</li>
<li><strong>Prediction happens repeatedly</strong>: Once deployed, the model makes thousands or millions of predictions per day</li>
<li><strong>Real-time requirements</strong>: Many applications (autonomous vehicles, medical diagnosis, security systems) need immediate predictions</li>
<li><strong>Scalability</strong>: As the dataset grows, k-NN prediction time grows linearly, making it impractical for large-scale systems</li>
</ol>
<p>This is why we often prefer <strong>parametric models</strong> (like linear classifiers) that invest computational effort upfront during training to enable fast predictions later.</p>
<h2 id="linear-classifiers">Linear Classifiers</h2>
<p>kNN has a number of disadvantages:</p>
<ul>
<li>The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.</li>
<li>Classifying a test image is expensive since it requires a comparison to all training images.</li>
</ul>
<p>We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to Neural Networks. The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.</p>
<h3 id="parameterized-mapping-from-images-to-label-scores">Parameterized mapping from images to label scores</h3>
<p>The first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class. We will develop the approach with a concrete example. Let's assume a training dataset of images <span class="arithmatex">\(x_i \in \mathbb{R}^D\)</span>, each associated with a label <span class="arithmatex">\(y_i\)</span>. Here <span class="arithmatex">\(i = 1 \ldots N\)</span> and <span class="arithmatex">\(y_i \in 1 \ldots K\)</span>. That is, we have <span class="arithmatex">\(N\)</span> examples (each with a dimensionality <span class="arithmatex">\(D\)</span>) and <span class="arithmatex">\(K\)</span> distinct categories. For example, in CIFAR-10 we have a training set of <span class="arithmatex">\(N = 50,000\)</span> images, each with <span class="arithmatex">\(D = 32 \times 32 \times 3 = 3072\)</span> pixels, and <span class="arithmatex">\(K = 10\)</span>, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function <span class="arithmatex">\(f: \mathbb{R}^D \mapsto \mathbb{R}^K\)</span> that maps the raw image pixels to class scores.</p>
<p>We will start out with arguably the simplest possible function, a linear mapping:</p>
<div class="arithmatex">\[f(x_i, W, b) = Wx_i + b\]</div>
<p>In the above equation, we are assuming that the image <span class="arithmatex">\(x_i\)</span> has all of its pixels flattened out to a single column vector of shape <span class="arithmatex">\([D \times 1]\)</span>. The matrix <span class="arithmatex">\(W\)</span> (of size <span class="arithmatex">\([K \times D]\)</span>), and the vector <span class="arithmatex">\(b\)</span> (of size <span class="arithmatex">\([K \times 1]\)</span>) are the parameters of the function. In CIFAR-10, <span class="arithmatex">\(x_i\)</span> contains all pixels in the <span class="arithmatex">\(i\)</span>-th image flattened into a single <span class="arithmatex">\([3072 \times 1]\)</span> column, <span class="arithmatex">\(W\)</span> is <span class="arithmatex">\([10 \times 3072]\)</span> and <span class="arithmatex">\(b\)</span> is <span class="arithmatex">\([10 \times 1]\)</span>, so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in <span class="arithmatex">\(W\)</span> are often called the <strong>weights</strong>, and <span class="arithmatex">\(b\)</span> is called the <strong>bias vector</strong> because it influences the output scores, but without interacting with the actual data <span class="arithmatex">\(x_i\)</span>.</p>
<p>There are a few things to note:</p>
<ol>
<li>
<p><strong>First</strong>, note that the single matrix multiplication <span class="arithmatex">\(Wx_i\)</span> is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of <span class="arithmatex">\(W\)</span>.</p>
</li>
<li>
<p><strong>Notice also</strong> that we think of the input data <span class="arithmatex">\((x_i, y_i)\)</span> as given and fixed, but we have control over the setting of the parameters <span class="arithmatex">\(W, b\)</span>. Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set. We will go into much more detail about how this is done, but intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes.</p>
</li>
<li>
<p><strong>An advantage</strong> of this approach is that the training data is used to learn the parameters <span class="arithmatex">\(W, b\)</span>, but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores.</p>
</li>
<li>
<p><strong>Lastly</strong>, note that classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images.</p>
</li>
</ol>
<p><strong>Geometric Interpretation</strong></p>
<p>To understand linear classifiers geometrically, let's first consider the general form of a plane in â„Â³:</p>
<div class="arithmatex">\[ax + by + cz = d\]</div>
<p>where <span class="arithmatex">\((a, b, c)\)</span> is the <strong>normal vector</strong> to the plane and <span class="arithmatex">\(d\)</span> determines the plane's position in space.</p>
<ul>
<li>
<p>The normal vector <span class="arithmatex">\((a, b, c)\)</span> is perpendicular to the plane</p>
</li>
<li>
<p>The plane divides 3D space into two half-spaces</p>
</li>
<li>
<p>Points on one side of the plane satisfy <span class="arithmatex">\(ax + by + cz &gt; d\)</span></p>
</li>
<li>
<p>Points on the other side satisfy <span class="arithmatex">\(ax + by + cz &lt; d\)</span></p>
</li>
<li>
<p>Points on the plane satisfy <span class="arithmatex">\(ax + by + cz = d\)</span></p>
</li>
</ul>
<p><strong>Example</strong>: The plane <span class="arithmatex">\(2x - 3y + 4z = 12\)</span> has normal vector <span class="arithmatex">\((2, -3, 4)\)</span>.</p>
<p>Now, let's see how this relates to linear classifiers. In a binary classification problem, our linear classifier computes:</p>
<div class="arithmatex">\[f(x) = w^T x + b\]</div>
<p>where <span class="arithmatex">\(w\)</span> is the weight vector and <span class="arithmatex">\(b\)</span> is the bias.</p>
<p><strong>The Decision Boundary</strong>: The equation <span class="arithmatex">\(w^T x + b = 0\)</span> defines a <strong>hyperplane</strong> in the input space that separates the two classes. This is exactly analogous to the plane equation <span class="arithmatex">\(ax + by + cz = d\)</span>. This is how:</p>
<ul>
<li>
<p><span class="arithmatex">\(w\)</span> is the <strong>normal vector</strong> to the hyperplane (just like <span class="arithmatex">\((a, b, c)\)</span> in the plane equation)</p>
</li>
<li>
<p><span class="arithmatex">\(b\)</span> determines the <strong>position</strong> of the hyperplane (just like <span class="arithmatex">\(d\)</span> in the plane equation)</p>
</li>
<li>
<p>The hyperplane divides the input space into two half-spaces</p>
</li>
<li>
<p>Points in one half-space are classified as class 1 (<span class="arithmatex">\(w^T x + b &gt; 0\)</span>)</p>
</li>
<li>
<p>Points in the other half-space are classified as class 2 (<span class="arithmatex">\(w^T x + b &lt; 0\)</span>)</p>
</li>
</ul>
<p><strong>Multi-class extension</strong>: For <span class="arithmatex">\(K\)</span> classes, we have <span class="arithmatex">\(K\)</span> hyperplanes, each defined by a row of the weight matrix <span class="arithmatex">\(W\)</span>. </p>
<p>In the multi-class case, we have <span class="arithmatex">\(K\)</span> score functions:</p>
<div class="arithmatex">\[f_1(x) = w_1^T x + b_1\]</div>
<div class="arithmatex">\[f_2(x) = w_2^T x + b_2\]</div>
<div class="arithmatex">\[\vdots\]</div>
<div class="arithmatex">\[f_K(x) = w_K^T x + b_K\]</div>
<p>The classifier predicts class <span class="arithmatex">\(i\)</span> if <span class="arithmatex">\(f_i(x) &gt; f_j(x)\)</span> for all <span class="arithmatex">\(j \neq i\)</span>.</p>
<p>The decision boundary between classes <span class="arithmatex">\(i\)</span> and <span class="arithmatex">\(j\)</span> is the set of points where the classifier is indifferent between the two classes, i.e., where <span class="arithmatex">\(f_i(x) = f_j(x)\)</span>.</p>
<p>Starting with:</p>
<div class="arithmatex">\[f_i(x) = f_j(x)\]</div>
<p>Substituting the score functions:</p>
<div class="arithmatex">\[w_i^T x + b_i = w_j^T x + b_j\]</div>
<p>Rearranging terms:</p>
<div class="arithmatex">\[w_i^T x - w_j^T x = b_j - b_i\]</div>
<p>Factoring out <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[(w_i - w_j)^T x = b_j - b_i\]</div>
<p>Moving all terms to one side:</p>
<div class="arithmatex">\[(w_i - w_j)^T x + (b_i - b_j) = 0\]</div>
<p>The decision boundary between classes <span class="arithmatex">\(i\)</span> and <span class="arithmatex">\(j\)</span> is the hyperplane:</p>
<div class="arithmatex">\[(w_i - w_j)^T x + (b_i - b_j) = 0\]</div>
<p>Note:</p>
<ul>
<li>
<p>The normal vector to this decision boundary is <span class="arithmatex">\((w_i - w_j)\)</span></p>
</li>
<li>
<p>The bias term is <span class="arithmatex">\((b_i - b_j)\)</span></p>
</li>
<li>
<p>Points on one side of this hyperplane are classified as class <span class="arithmatex">\(i\)</span></p>
</li>
<li>
<p>Points on the other side are classified as class <span class="arithmatex">\(j\)</span></p>
</li>
<li>
<p>Points on the hyperplane are exactly at the decision boundary</p>
</li>
</ul>
<p><strong>Why this matters</strong>: This geometric interpretation helps us understand that:</p>
<ol>
<li>
<p>Linear classifiers create <strong>linear decision boundaries</strong></p>
</li>
<li>
<p>The weight vector <span class="arithmatex">\(w\)</span> determines the <strong>orientation</strong> of the decision boundary</p>
</li>
<li>
<p>The bias <span class="arithmatex">\(b\)</span> determines the <strong>position</strong> of the decision boundary</p>
</li>
<li>
<p>The classifier's performance depends on how well the data can be separated by these linear boundaries</p>
</li>
</ol>
<p>This connection between planes in â„Â³ and hyperplanes in linear classifiers provides an intuitive way to visualize and understand how linear classifiers work geometrically.</p>
<p>Coming back to CIFAR-10, we cannot visualize 3072-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing.</p>
<p><img alt="Multi-class extension" src="../linear_classifier.jpeg" /></p>
<p>Above figure shows a cartoon representation of the image space, where each image is a single point, and three classifiers are visualized. For example, take the car classifier (in red), where the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores.</p>
<p>As we saw above, every row of <span class="arithmatex">\(W\)</span> is a classifier for one of the classes. The geometric interpretation of these numbers is that as we change one of the rows of <span class="arithmatex">\(W\)</span>, the corresponding line in the pixel space will rotate in different directions. The biases <span class="arithmatex">\(b\)</span>, on the other hand, allow our classifiers to translate the lines.</p>
<h3 id="template-matching-interpretation">Template Matching interpretation</h3>
<p>Another interpretation for the weights <span class="arithmatex">\(W\)</span> is that each row of <span class="arithmatex">\(W\)</span> corresponds to a template (or sometimes also called a prototype) for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using an inner product (or dot product) one by one to find the one that "fits" best. With this terminology, the linear classifier is doing template matching, where the templates are learned. Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class.</p>
<h3 id="bias-trick">Bias Trick</h3>
<p>Recall that we defined the score function as:</p>
<div class="arithmatex">\[f(x_i, W, b) = Wx_i + b\]</div>
<p>It is a little cumbersome to keep track of two sets of parameters (the biases <span class="arithmatex">\(b\)</span> and weights <span class="arithmatex">\(W\)</span>) separately. A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector <span class="arithmatex">\(x_i\)</span> with one additional dimension that always holds the constant 1- a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply:</p>
<div class="arithmatex">\[f(x_i, W) = Wx_i\]</div>
<p>With our CIFAR-10 example, <span class="arithmatex">\(x_i\)</span> is now <span class="arithmatex">\([3073 \times 1]\)</span> instead of <span class="arithmatex">\([3072 \times 1]\)</span> - (with the extra dimension holding the constant 1), and <span class="arithmatex">\(W\)</span> is now <span class="arithmatex">\([10 \times 3073]\)</span> instead of <span class="arithmatex">\([10 \times 3072]\)</span>. The extra column that <span class="arithmatex">\(W\)</span> now corresponds to the bias <span class="arithmatex">\(b\)</span>. An illustration might help clarify this transformation.</p>
<p><img alt="Bias trick" src="../bias_trick.jpeg" /></p>
<h3 id="loss-function">Loss Function</h3>
<p>We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if weâ€™re doing a poor job of classifying the training data, and it will be low if weâ€™re doing well.</p>
<h4 id="multiclass-support-vector-machine-loss">Multiclass Support Vector Machine loss</h4>
<p>The SVM loss is set up so that the SVM "wants" the correct class for each image to have a score higher than the incorrect classes by some fixed margin <span class="arithmatex">\(\Delta\)</span>.</p>
<p>Recall that for the <span class="arithmatex">\(i\)</span>-th example we are given the pixels of image <span class="arithmatex">\(x_i\)</span> and the label <span class="arithmatex">\(y_i\)</span> that specifies the index of the correct class. The score function takes the pixels and computes the vector <span class="arithmatex">\(f(x_i,W)\)</span> of class scores, which we will abbreviate to <span class="arithmatex">\(s\)</span> (short for scores). For example, the score for the <span class="arithmatex">\(j\)</span>-th class is the <span class="arithmatex">\(j\)</span>-th element: <span class="arithmatex">\(s_j = f(x_i,W)_j\)</span>. The Multiclass SVM loss for the <span class="arithmatex">\(i\)</span>-th example is then formalized as follows:</p>
<div class="arithmatex">\[L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)\]</div>
<p><strong>Example.</strong> Let's unpack this with an example to see how it works. Suppose that we have three classes that receive the scores <span class="arithmatex">\(s = [13, -7, 11]\)</span>, and that the first class is the true class (i.e. <span class="arithmatex">\(y_i = 0\)</span>). Also assume that <span class="arithmatex">\(\Delta\)</span> (a hyperparameter we will go into more detail about soon) is 10. The expression above sums over all incorrect classes (<span class="arithmatex">\(j \neq y_i\)</span>), so we get two terms:</p>
<div class="arithmatex">\[L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)\]</div>
<p>You can see that the first term gives zero since <span class="arithmatex">\([-7 - 13 + 10]\)</span> gives a negative number, which is then thresholded to zero with the <span class="arithmatex">\(\max(0, -)\)</span> function. We get zero loss for this pair because the correct class score (13) was greater than the incorrect class score (-7) by at least the margin 10. In fact the difference was 20, which is much greater than 10 but the SVM only cares that the difference is at least 10; any additional difference above the margin is clamped at zero with the max operation. The second term computes <span class="arithmatex">\([11 - 13 + 10]\)</span> which gives 8. That is, even though the correct class had a higher score than the incorrect class (13 &gt; 11), it was not greater by the desired margin of 10. The difference was only 2, which is why the loss comes out to 8 (i.e. how much higher the difference would have to be to meet the margin). In summary, the SVM loss function wants the score of the correct class <span class="arithmatex">\(y_i\)</span> to be larger than the incorrect class scores by at least by <span class="arithmatex">\(\Delta\)</span> (delta). If this is not the case, we will accumulate loss.</p>
<p><img alt="Multiclass SVM loss" src="../svm_loss_margin.jpg" /></p>
<h3 id="softmax-classifier">Softmax Classifier</h3>
<p>Unlike the SVM which treats the outputs <span class="arithmatex">\(f(x_i,W)\)</span> as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping <span class="arithmatex">\(f(x_i;W) = Wx_i\)</span> stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss. The function <span class="arithmatex">\(f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}\)</span> is called the softmax function: it takes a vector of arbitrary real-valued scores (in <span class="arithmatex">\(z\)</span>) and squashes it to a vector of values between zero and one that sum to one.</p>
<p>The cross-entropy between a "true" distribution <span class="arithmatex">\(p\)</span> and an estimated distribution <span class="arithmatex">\(q\)</span> is defined as:</p>
<div class="arithmatex">\[H(p, q) = -\sum_x p(x) \log q(x)\]</div>
<p><em>Note: This derivation is for the <span class="arithmatex">\(i\)</span>-th sample, where <span class="arithmatex">\(y_i\)</span> is the correct class label for that sample.</em></p>
<p>For the Softmax classifier, we have:</p>
<ul>
<li>
<p>True distribution: <span class="arithmatex">\(p_i = [0, \ldots, 1, \ldots, 0]\)</span> (one-hot vector with 1 at position <span class="arithmatex">\(y_i\)</span>)</p>
</li>
<li>
<p>Estimated distribution: <span class="arithmatex">\(q_{i,j} = \frac{e^{f_j}}{\sum_k e^{f_k}}\)</span> (softmax probabilities)</p>
</li>
</ul>
<p>Substituting into the cross-entropy formula:</p>
<div class="arithmatex">\[H(p_i,q_i) = -\sum_{j=1}^{K} p_{i,j} \log q_{i,j}\]</div>
<p>Since <span class="arithmatex">\(p_i\)</span> is a one-hot vector, only <span class="arithmatex">\(p_{i,y_i} = 1\)</span> and all other <span class="arithmatex">\(p_{i,j} = 0\)</span>:</p>
<div class="arithmatex">\[H(p_i,q_i) = -p_{i,y_i} \log q_{i,y_i} - \sum_{j \neq y_i} p_{i,j} \log q_{i,j} = -1 \cdot \log q_{i,y_i} - \sum_{j \neq y_i} 0 \cdot \log q_{i,j}\]</div>
<div class="arithmatex">\[H(p_i,q_i) = -\log q_{i,y_i} = -\log\left(\frac{e^{f_{y_i}}}{\sum_k e^{f_k}}\right) = L_i\]</div>
<div class="arithmatex">\[L_i = -\log\left(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}\right) \quad \text{or equivalently} \quad L_i = -f_{y_i} + \log\sum_j e^{f_j}\]</div>
<p>where we are using the notation <span class="arithmatex">\(f_j\)</span> to mean the <span class="arithmatex">\(j\)</span>-th element of the vector of class scores <span class="arithmatex">\(f\)</span>. As before, the full loss for the dataset is the mean of <span class="arithmatex">\(L_i\)</span> over all training examples together with a regularization term <span class="arithmatex">\(R(W)\)</span>. The function <span class="arithmatex">\(f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}\)</span> is called the softmax function: it takes a vector of arbitrary real-valued scores (in <span class="arithmatex">\(z\)</span>) and squashes it to a vector of values between zero and one that sum to one.</p>
<p>Therefore, the cross-entropy between the true one-hot distribution and the estimated softmax distribution is exactly the Softmax loss <span class="arithmatex">\(L_i\)</span>.</p>
<p>The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities (<span class="arithmatex">\(q_{i,j} = e^{f_{y_i}}/\sum_j e^{f_j}\)</span> as seen above) and the "true" distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. <span class="arithmatex">\(p_i = [0, \ldots, 1, \ldots, 0]\)</span> contains a single 1 at the <span class="arithmatex">\(y_i\)</span>-th position.).</p>
<h4 id="the-kl-divergence-connection">The KL Divergence connection</h4>
<p>Before diving into KL divergence, let's understand entropy. The entropy <span class="arithmatex">\(H(p)\)</span> of a distribution <span class="arithmatex">\(p\)</span> measures the average amount of information (or uncertainty) in the distribution:</p>
<div class="arithmatex">\[H(p) = -\sum_x p_{x} \log p_{x}\]</div>
<ul>
<li>
<p><strong>High entropy</strong>: Distribution is spread out, high uncertainty (e.g., uniform distribution)</p>
</li>
<li>
<p><strong>Low entropy</strong>: Distribution is concentrated, low uncertainty (e.g., one-hot distribution)</p>
</li>
</ul>
<p><em>Note: This derivation is for the <span class="arithmatex">\(i\)</span>-th sample, where <span class="arithmatex">\(y_i\)</span> is the correct class label for that sample.</em></p>
<p>For our one-hot true distribution <span class="arithmatex">\(p_i\)</span> where <span class="arithmatex">\(p_{i,y_i} = 1\)</span> and <span class="arithmatex">\(p_{i,j} = 0\)</span> for <span class="arithmatex">\(j \neq y_i\)</span>:</p>
<div class="arithmatex">\[H(p_i) = -p_{i,y_i} \log p_{i,y_i} - \sum_{j \neq y_i} p_{i,j} \log p_{i,j} = -1 \cdot \log 1 - \sum_{j \neq y_i} 0 \cdot \log 0 = 0\]</div>
<p>The entropy is zero because there's no uncertainty - we know exactly which class is correct.</p>
<p>The KL divergence <span class="arithmatex">\(D_{KL}(p||q)\)</span> measures how much information is lost when we use distribution <span class="arithmatex">\(q\)</span> to approximate distribution <span class="arithmatex">\(p\)</span>. It's defined as:</p>
<div class="arithmatex">\[D_{KL}(p||q) = \sum_x p_{x} \log\left(\frac{p_{x}}{q_{x}}\right)\]</div>
<p><em>Note: This derivation is for the <span class="arithmatex">\(i\)</span>-th sample, where <span class="arithmatex">\(y_i\)</span> is the correct class label for that sample.</em></p>
<p>For our one-hot true distribution <span class="arithmatex">\(p_i\)</span> where <span class="arithmatex">\(p_{i,y_i} = 1\)</span> and <span class="arithmatex">\(p_{i,j} = 0\)</span> for <span class="arithmatex">\(j \neq y_i\)</span>:</p>
<div class="arithmatex">\[D_{KL}(p_i||q_i) = p_{i,y_i} \log\left(\frac{p_{i,y_i}}{q_{i,y_i}}\right) + \sum_{j \neq y_i} p_{i,j} \log\left(\frac{p_{i,j}}{q_{i,j}}\right)\]</div>
<div class="arithmatex">\[D_{KL}(p_i||q_i) = 1 \cdot \log\left(\frac{1}{q_{i,y_i}}\right) + \sum_{j \neq y_i} 0 \cdot \log\left(\frac{0}{q_{i,j}}\right) = -\log q_{i,y_i}\]</div>
<p>Since <span class="arithmatex">\(H(p_i) = 0\)</span> (the entropy of a deterministic distribution is zero), we have:</p>
<div class="arithmatex">\[H(p_i,q_i) = H(p_i) + D_{KL}(p_i||q_i) = 0 + (-\log q_{i,y_i}) = -\log q_{i,y_i} = L_i\]</div>
<p>This shows that minimizing the cross-entropy loss is exactly equivalent to minimizing the KL divergence between the true one-hot distribution and the predicted softmax distribution. The KL divergence is zero only when <span class="arithmatex">\(q_{i,y_i} = 1\)</span> (perfect confidence in the correct class), and it increases as the predicted probability of the correct class decreases.</p>
<h4 id="practical-issues-numeric-stability">Practical issues: numeric stability</h4>
<p>When you're writing code for computing the Softmax function in practice, the intermediate terms <span class="arithmatex">\(e^{f_{y_i}}\)</span> and <span class="arithmatex">\(\sum_j e^{f_j}\)</span> may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant <span class="arithmatex">\(C\)</span> and push it into the sum, we get the following (mathematically equivalent) expression:</p>
<div class="arithmatex">\[\frac{e^{f_{y_i}}}{\sum_j e^{f_j}} = \frac{C e^{f_{y_i}}}{C \sum_j e^{f_j}} = \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}\]</div>
<p>We are free to choose the value of <span class="arithmatex">\(C\)</span>. This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for <span class="arithmatex">\(C\)</span> is to set <span class="arithmatex">\(\log C = -\max_j f_j\)</span>. This simply states that we should shift the values inside the vector <span class="arithmatex">\(f\)</span> so that the highest value is zero.</p>
<p><strong>Example code:</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">123</span><span class="p">,</span> <span class="mi">456</span><span class="p">,</span> <span class="mi">789</span><span class="p">])</span> <span class="c1"># example with 3 classes and each having large scores</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">f</span><span class="p">))</span> <span class="c1"># Bad: Numeric problem, potential blowup</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="c1"># instead: first shift the values of f so that the highest number is 0:</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="n">f</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="c1"># f becomes [-666, -333, 0]</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">f</span><span class="p">))</span> <span class="c1"># safe to do, gives the correct answer</span>
</code></pre></div>
<h3 id="svm-vs-softmax">SVM vs. Softmax</h3>
<p>A picture might help clarify the distinction between the Softmax and SVM classifiers using an example.</p>
<p><img alt="SVM vs Softmax" src="../svmvssoftmax.png" /></p>
<h3 id="final-notes-on-terminology">Final notes on terminology</h3>
<p><strong>1. Softmax</strong></p>
<p>The softmax function is not a classifier itself, but it is an essential component of a classifier. The term "softmax classifier" is often used informally to describe a multi-class classification model that uses the softmax function in its final layer. However, the actual classification workâ€”the process of learning to predict the correct classesâ€”is done by the entire model, not just the softmax layer.</p>
<p><strong>2. SVM</strong></p>
<p>A "linear classifier + SVM loss function" is a Linear SVM. This is not a new or different kind of model; it is simply a Support Vector Machine that uses a linear function to find the optimal decision boundary.</p>
<p>The key components of a Linear SVM are:</p>
<ul>
<li>
<p><strong>Linear decision function</strong>: <span class="arithmatex">\(f(x) = Wx + b\)</span></p>
</li>
<li>
<p><strong>Hinge loss</strong>: <span class="arithmatex">\(L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)\)</span></p>
</li>
<li>
<p><strong>Margin maximization</strong>: The SVM tries to maximize the margin between classes</p>
</li>
</ul>
<p>The term "Linear SVM" emphasizes that the decision boundary is linear (a hyperplane), as opposed to kernel SVMs that can have non-linear decision boundaries through the kernel trick.</p>
<h2 id="additional-references">Additional References</h2>
<p>Here are some (optional) links you may find interesting for further reading:</p>
<p><a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">A Few Useful Things to Know about Machine Learning</a>, where especially section 6 is related but the whole paper is a warmly recommended reading.</p>
<p><a href="https://people.csail.mit.edu/torralba/shortCourseRLOC/index.html">Recognizing and Learning Object Categories</a>, a short course of object categorization at ICCV 2005.</p>
<p><a href="https://arxiv.org/abs/1306.0239">Deep Learning using Linear Support Vector Machines</a> from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>