
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/ai/deep_generative_models/score_based_models/">
      
      
        <link rel="prev" href="../energy_based_models/">
      
      
        <link rel="next" href="../score_based_generative_modeling/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Score Based Models - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#score-based-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Score Based Models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-tabs__link">
          
  
  
  Productivity

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#score-matching" class="md-nav__link">
    <span class="md-ellipsis">
      Score Matching
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Score Matching">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#score-matching-comparing-distributions-via-vector-fields" class="md-nav__link">
    <span class="md-ellipsis">
      Score Matching: Comparing Distributions via Vector Fields
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#score-matching-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Score Matching Algorithm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recap-distances-for-training-ebms" class="md-nav__link">
    <span class="md-ellipsis">
      Recap: Distances for Training EBMs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Recap: Distances for Training EBMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contrastive-divergence" class="md-nav__link">
    <span class="md-ellipsis">
      Contrastive Divergence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fisher-divergence-score-matching" class="md-nav__link">
    <span class="md-ellipsis">
      Fisher Divergence (Score Matching)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#noise-contrastive-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      Noise Contrastive Estimation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Noise Contrastive Estimation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimal-discriminator" class="md-nav__link">
    <span class="md-ellipsis">
      Optimal Discriminator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameterizing-the-discriminator-as-an-ebm" class="md-nav__link">
    <span class="md-ellipsis">
      Parameterizing the Discriminator as an EBM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparing-nce-and-gan" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing NCE and GAN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-score-based-models" class="md-nav__link">
    <span class="md-ellipsis">
      Training Score Based Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#denoising-score-matching" class="md-nav__link">
    <span class="md-ellipsis">
      Denoising Score Matching
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_generative_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Generative Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluating_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluating Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_diffusion_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Learning for Computer Vision
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning for Computer Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/image_classification_with_linear_classifiers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Classification with Linear Classifiers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Regularization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/backpropagation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backpropagation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Setting up the Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/neural_networks_setting_up_the_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Setting up the Data
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors, vector addition, and scalar multiplication
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector geometry in Rn and correlation coefficients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/planes_in_r3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Planes in R3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/span_subspaces_and_dimension/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Span, subspaces, and dimension
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/basis_and_orthogonality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basis and orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Applications of projections in Rn- orthogonal bases of planes and linear regression
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/story_proofs_and_axioms_of_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Story Proofs and Axioms of Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/some_famous_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Some famous problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/indicator_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Indicator Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/poisson_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Poisson Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/continuous_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Continuous Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/exponential_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exponential Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/joint_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Joint Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/independence_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Independence of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/multinomial_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multinomial Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/covariance_and_correlation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Covariance and Correlation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/transformations_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformations of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/convolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/beta_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Beta Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/jensens_inequality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jensen's inequality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/central_limit_theorem/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Central Limit Theorem
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/multivariate_normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multivariate Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Productivity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Productivity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    How to Build Your Career in AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            How to Build Your Career in AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Three Steps to Career Growth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_1_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 1- Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_2_projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 2- Projects
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_3_job/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 3- Job
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="score-based-models">Score Based Models</h1>
<h2 id="score-matching">Score Matching</h2>
<p><strong>Energy-Based Model Probability Distribution</strong></p>
<p>In Energy-Based Models, the probability distribution is defined as:</p>
<div class="arithmatex">\[p_\theta(x) = \frac{1}{Z(\theta)} e^{f_\theta(x)}\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(f_\theta(x)\)</span> is the energy function (neural network)</p>
</li>
<li>
<p><span class="arithmatex">\(Z(\theta) = \int e^{f_\theta(x)} dx\)</span> is the partition function (intractable)</p>
</li>
</ul>
<p>Taking the logarithm of the probability distribution:</p>
<div class="arithmatex">\[\log p_\theta(x) = f_\theta(x) - \log Z(\theta)\]</div>
<p>Notice that the partition function <span class="arithmatex">\(Z(\theta)\)</span> appears as a constant term that doesn't depend on <span class="arithmatex">\(x\)</span>.</p>
<p><strong>Stein Score Function</strong></p>
<p>The <strong>Stein score function</strong> <span class="arithmatex">\(s_\theta(x)\)</span> is defined as the gradient of the log probability with respect to <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[s_\theta(x) = \nabla_x \log p_\theta(x)\]</div>
<p>For Energy-Based Models, the score function equals the gradient of the energy function:</p>
<div class="arithmatex">\[s_\theta(x) = \nabla_x \log p_\theta(x) = \nabla_x (f_\theta(x) - \log Z(\theta)) = \nabla_x f_\theta(x)\]</div>
<p>The partition function term <span class="arithmatex">\(\log Z(\theta)\)</span> disappears because it doesn't depend on <span class="arithmatex">\(x\)</span>.</p>
<p><strong>Score as a Vector Field</strong></p>
<p>The score function <span class="arithmatex">\(s_\theta(x)\)</span> is a <strong>vector field</strong> that assigns a vector to each point <span class="arithmatex">\(x\)</span> in the data space. This vector has both:</p>
<ol>
<li>
<p><strong>Magnitude</strong>: How quickly the log probability changes</p>
</li>
<li>
<p><strong>Direction</strong>: The direction of steepest increase in log probability</p>
</li>
</ol>
<p><strong>Intuition</strong>: The score vector points "uphill" in the log probability landscape, indicating the direction where the model assigns higher probability.</p>
<p><strong>Example: Gaussian Distribution</strong></p>
<p>Consider a Gaussian distribution with mean <span class="arithmatex">\(\mu\)</span> and covariance <span class="arithmatex">\(\Sigma\)</span>:</p>
<div class="arithmatex">\[p(x) = \frac{1}{\sqrt{(2\pi)^d |\Sigma|}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu)\right)\]</div>
<p><strong>Log Probability:</strong></p>
<div class="arithmatex">\[\log p(x) = -\frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu) - \frac{1}{2}\log((2\pi)^d |\Sigma|)\]</div>
<p><strong>Score Function:</strong></p>
<div class="arithmatex">\[s(x) = \nabla_x \log p(x) = -\Sigma^{-1}(x - \mu)\]</div>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<p>The score points toward the mean <span class="arithmatex">\(\mu\)</span> (direction of higher probability)</p>
</li>
<li>
<p>The magnitude is proportional to the distance from the mean</p>
</li>
<li>
<p>For isotropic Gaussian (<span class="arithmatex">\(\Sigma = \sigma^2 I\)</span>): <span class="arithmatex">\(s(x) = -\frac{1}{\sigma^2}(x - \mu)\)</span></p>
</li>
</ul>
<p>This example shows how the score function naturally guides samples toward high-probability regions of the distribution.</p>
<h3 id="score-matching-comparing-distributions-via-vector-fields"><strong>Score Matching: Comparing Distributions via Vector Fields</strong></h3>
<p>The core idea of score matching is that we want to compare two probability distributions by comparing their respective vector fields of gradients (score functions).</p>
<p><strong>The Key Insight:</strong></p>
<p>Instead of directly comparing probability densities <span class="arithmatex">\(p_{data}(x)\)</span> and <span class="arithmatex">\(p_\theta(x)\)</span> (which requires computing the intractable partition function), we compare their score functions:</p>
<ul>
<li><strong>Data Score</strong>: <span class="arithmatex">\(s_{data}(x) = \nabla_x \log p_{data}(x)\)</span></li>
<li><strong>Model Score</strong>: <span class="arithmatex">\(s_\theta(x) = \nabla_x \log p_\theta(x) = \nabla_x f_\theta(x)\)</span></li>
</ul>
<p>This measures how different the "pointing directions" are at each location <span class="arithmatex">\(x\)</span>.</p>
<p><strong>L2 Distance Between Score Functions</strong></p>
<p>One way to compare the score functions is to calculate the average L2 distance between the score of <span class="arithmatex">\(p_{data}\)</span> and <span class="arithmatex">\(p_\theta\)</span>:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|s_\theta(x) - s_{data}(x)\|^2 \right]\]</div>
<p><strong>Note</strong>: This loss function is also called the <strong>Fisher divergence</strong> between <span class="arithmatex">\(p_{data}(x)\)</span> and <span class="arithmatex">\(p_\theta(x)\)</span>. The Fisher divergence measures the difference between two probability distributions by comparing their score functions (gradients of log densities) rather than the densities themselves.</p>
<p><strong>Understanding the L2 Distance:</strong></p>
<p>The L2 norm <span class="arithmatex">\(\|s_\theta(x) - s_{data}(x)\|^2\)</span> measures the squared Euclidean distance between two vectors:</p>
<div class="arithmatex">\[\|s_\theta(x) - s_{data}(x)\|^2 = \sum_{i=1}^d (s_\theta(x)_i - s_{data}(x)_i)^2\]</div>
<p>where <span class="arithmatex">\(d\)</span> is the dimension of the data space.</p>
<p><strong>Score matching</strong> is a method for training Energy-Based Models by minimizing the Fisher divergence between the data distribution <span class="arithmatex">\(p_{data}(x)\)</span> and the model distribution <span class="arithmatex">\(p_\theta(x)\)</span>:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|s_\theta(x) - s_{data}(x)\|^2 \right]\]</div>
<p>where <span class="arithmatex">\(s_\theta(x) = \nabla_x \log p_\theta(x)\)</span> and <span class="arithmatex">\(s_{data}(x) = \nabla_x \log p_{data}(x)\)</span> are the score functions of the model and data distributions respectively.</p>
<p>But how do we figure out <span class="arithmatex">\(\nabla_x \log p_{data}(x)\)</span> given only samples?</p>
<p><strong>Score Matching Reformulation (Univariate Case)</strong></p>
<p>For the univariate case where <span class="arithmatex">\(x \in \mathbb{R}\)</span>, we can rewrite the score matching objective to avoid needing the data score. Let's expand the squared difference:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \left(\frac{d}{dx} \log p_\theta(x) - \frac{d}{dx} \log p_{data}(x)\right)^2 \right]\]</div>
<p>Expanding the square:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \left(\frac{d}{dx} \log p_\theta(x)\right)^2 - \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} \log p_{data}(x) + \frac{1}{2} \left(\frac{d}{dx} \log p_{data}(x)\right)^2 \right]\]</div>
<p>The key insight is to use integration by parts on the cross term. For any function <span class="arithmatex">\(f(x)\)</span> and <span class="arithmatex">\(g(x)\)</span>:</p>
<div class="arithmatex">\[\int f(x) \frac{d}{dx} g(x) dx = f(x)g(x) - \int \frac{d}{dx} f(x) \cdot g(x) dx\]</div>
<p>Setting <span class="arithmatex">\(f(x) = \frac{d}{dx} \log p_\theta(x)\)</span> and <span class="arithmatex">\(g(x) = p_{data}(x)\)</span>, we get:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim p_{data}} \left[ \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} \log p_{data}(x) \right] = \int \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} \log p_{data}(x) \cdot p_{data}(x) dx\]</div>
<p>Using the chain rule: <span class="arithmatex">\(\frac{d}{dx} \log p_{data}(x) \cdot p_{data}(x) = \frac{d}{dx} p_{data}(x)\)</span>, we get:</p>
<div class="arithmatex">\[= \int \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} p_{data}(x) dx\]</div>
<p>Using integration by parts:</p>
<div class="arithmatex">\[= \left. \frac{d}{dx} \log p_\theta(x) \cdot p_{data}(x) \right|_{-\infty}^{\infty} - \int \frac{d^2}{dx^2} \log p_\theta(x) \cdot p_{data}(x) dx\]</div>
<p><strong>Why does the boundary term vanish?</strong></p>
<p>The boundary term <span class="arithmatex">\(\left. \frac{d}{dx} \log p_\theta(x) \cdot p_{data}(x) \right|_{-\infty}^{\infty}\)</span> vanishes under reasonable assumptions:</p>
<ol>
<li><strong>Data distribution decay</strong>: <span class="arithmatex">\(p_{data}(x) \to 0\)</span> as <span class="arithmatex">\(|x| \to \infty\)</span> (most real-world distributions have finite support or decay to zero)</li>
<li><strong>Model score boundedness</strong>: <span class="arithmatex">\(\frac{d}{dx} \log p_\theta(x)\)</span> grows at most polynomially as <span class="arithmatex">\(|x| \to \infty\)</span></li>
<li><strong>Product decay</strong>: The product <span class="arithmatex">\(\frac{d}{dx} \log p_\theta(x) \cdot p_{data}(x) \to 0\)</span> as <span class="arithmatex">\(|x| \to \infty\)</span></li>
</ol>
<p>This is a standard assumption in score matching literature and holds for most practical distributions.</p>
<p>Assuming the boundary term vanishes (which is reasonable for well-behaved distributions), we get:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim p_{data}} \left[ \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} \log p_{data}(x) \right] = -\mathbb{E}_{x \sim p_{data}} \left[ \frac{d^2}{dx^2} \log p_\theta(x) \right]\]</div>
<p>Substituting back into the original objective:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \left(\frac{d}{dx} \log p_\theta(x)\right)^2 + \frac{d^2}{dx^2} \log p_\theta(x) \right] + \text{constant}\]</div>
<p>where the constant term <span class="arithmatex">\(\frac{1}{2} \mathbb{E}_{x \sim p_{data}} \left[ \left(\frac{d}{dx} \log p_{data}(x)\right)^2 \right]\)</span> doesn't depend on <span class="arithmatex">\(\theta\)</span> and can be ignored during optimization.</p>
<p><strong>Key Insight</strong>: This reformulation allows us to train the model using only samples from <span class="arithmatex">\(p_{data}(x)\)</span> and the derivatives of our model's log probability, without needing access to the data score function.</p>
<p><strong>Score Matching Reformulation (Multivariate Case)</strong></p>
<p>For the multivariate case where <span class="arithmatex">\(x \in \mathbb{R}^d\)</span>, we can extend the univariate derivation. The score matching objective becomes:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x) - \nabla_x \log p_{data}(x)\|^2 \right]\]</div>
<p>Expanding the squared norm:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x)\|^2 - \nabla_x \log p_\theta(x)^T \nabla_x \log p_{data}(x) + \frac{1}{2} \|\nabla_x \log p_{data}(x)\|^2 \right]\]</div>
<p>The key insight is to use integration by parts on the cross term. For the multivariate case, we need to handle each component separately. Let <span class="arithmatex">\(s_\theta(x)_i\)</span> and <span class="arithmatex">\(s_{data}(x)_i\)</span> denote the <span class="arithmatex">\(i\)</span>-th component of the respective score functions.</p>
<p>For each component <span class="arithmatex">\(i\)</span>, we have:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim p_{data}} \left[ s_\theta(x)_i \cdot s_{data}(x)_i \right] = \int s_\theta(x)_i \cdot s_{data}(x)_i \cdot p_{data}(x) dx\]</div>
<p>Using the chain rule: <span class="arithmatex">\(s_{data}(x)_i \cdot p_{data}(x) = \frac{\partial}{\partial x_i} p_{data}(x)\)</span>, we get:</p>
<div class="arithmatex">\[= \int s_\theta(x)_i \cdot \frac{\partial}{\partial x_i} p_{data}(x) dx\]</div>
<p>Using integration by parts (assuming boundary terms vanish):</p>
<div class="arithmatex">\[= -\int \frac{\partial}{\partial x_i} s_\theta(x)_i \cdot p_{data}(x) dx = -\mathbb{E}_{x \sim p_{data}} \left[ \frac{\partial}{\partial x_i} s_\theta(x)_i \right]\]</div>
<p><strong>Why do the boundary terms vanish in the multivariate case?</strong></p>
<p>For each component <span class="arithmatex">\(i\)</span>, the boundary term is:</p>
<div class="arithmatex">\[\left. s_\theta(x)_i \cdot p_{data}(x) \right|_{x_i = -\infty}^{x_i = \infty}\]</div>
<p>This vanishes under similar assumptions as the univariate case:</p>
<ol>
<li><strong>Data distribution decay</strong>: <span class="arithmatex">\(p_{data}(x) \to 0\)</span> as <span class="arithmatex">\(\|x\| \to \infty\)</span> in any direction</li>
<li><strong>Model score boundedness</strong>: Each component <span class="arithmatex">\(s_\theta(x)_i\)</span> grows at most polynomially as <span class="arithmatex">\(\|x\| \to \infty\)</span></li>
<li><strong>Product decay</strong>: The product <span class="arithmatex">\(s_\theta(x)_i \cdot p_{data}(x) \to 0\)</span> as <span class="arithmatex">\(\|x\| \to \infty\)</span> for each component</li>
</ol>
<p>These assumptions ensure that the boundary terms vanish for all components, allowing us to apply integration by parts component-wise.</p>
<p>Summing over all components:</p>
<div class="arithmatex">\[\sum_{i=1}^d \mathbb{E}_{x \sim p_{data}} \left[ s_\theta(x)_i \cdot s_{data}(x)_i \right] = -\sum_{i=1}^d \mathbb{E}_{x \sim p_{data}} \left[ \frac{\partial}{\partial x_i} s_\theta(x)_i \right] = -\mathbb{E}_{x \sim p_{data}} \left[ \text{tr}(\nabla_x s_\theta(x)) \right]\]</div>
<p>where <span class="arithmatex">\(\text{tr}(\nabla_x s_\theta(x)) = \sum_{i=1}^d \frac{\partial}{\partial x_i} s_\theta(x)_i\)</span> is the trace of the Jacobian matrix of the score function.</p>
<p>Substituting back into the original objective:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x)\|^2 + \text{tr}(\nabla_x \nabla_x \log p_\theta(x)) \right] + \text{constant}\]</div>
<p>where the constant term <span class="arithmatex">\(\frac{1}{2} \mathbb{E}_{x \sim p_{data}} \left[ \|\nabla_x \log p_{data}(x)\|^2 \right]\)</span> doesn't depend on <span class="arithmatex">\(\theta\)</span> and can be ignored during optimization.</p>
<p><strong>Key Insight</strong>: The multivariate case introduces the trace of the Hessian matrix <span class="arithmatex">\(\text{tr}(\nabla_x \nabla_x \log p_\theta(x))\)</span>.</p>
<h3 id="score-matching-algorithm">Score Matching Algorithm</h3>
<p>The score matching algorithm follows these steps:</p>
<p><strong>Sample a mini-batch of datapoints</strong>: <span class="arithmatex">\(\{x_1, x_2, \ldots, x_n\} \sim p_{data}(x)\)</span></p>
<p><strong>Estimate the score matching loss with the empirical mean</strong>: </p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) \approx \frac{1}{n} \sum_{i=1}^n \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x_i)\|^2 + \text{tr}(\nabla_x \nabla_x \log p_\theta(x_i)) \right]\]</div>
<p><strong>Stochastic gradient descent</strong>: Update parameters using gradients of the estimated loss</p>
<p><strong>Advantages:</strong>
* <strong>No need to sample from EBM</strong>: Unlike other training methods for energy-based models, score matching doesn't require generating samples from the model during training. This avoids the computational expense and potential instability of MCMC sampling.
* <strong>Direct optimization</strong>: The objective directly measures how well the model's score function matches the data distribution's score function.
* <strong>Theoretically sound</strong>: Score matching provides a consistent estimator under mild conditions.</p>
<p><strong>Disadvantages:</strong>
* <strong>Computing the Hessian is expensive</strong>: The term <span class="arithmatex">\(\text{tr}(\nabla_x \nabla_x \log p_\theta(x))\)</span> requires computing second derivatives, which scales quadratically with the input dimension and can be computationally prohibitive for large models.
* <strong>Memory requirements</strong>: Storing and computing Hessians for large neural networks requires significant memory.
* <strong>Numerical instability</strong>: Second derivatives can be numerically unstable, especially for deep networks.</p>
<p><strong>Computational Complexity:</strong>
For a model with <span class="arithmatex">\(d\)</span> input dimensions and <span class="arithmatex">\(m\)</span> parameters, computing the Hessian trace requires <span class="arithmatex">\(O(d^2 \cdot m)\)</span> operations, making it impractical for high-dimensional data like images.</p>
<h2 id="recap-distances-for-training-ebms">Recap: Distances for Training EBMs</h2>
<p>When training Energy-Based Models, we need to measure how close our model distribution <span class="arithmatex">\(p_\theta(x)\)</span> is to the data distribution <span class="arithmatex">\(p_{data}(x)\)</span>. Here are the main approaches:</p>
<h3 id="contrastive-divergence">Contrastive Divergence</h3>
<p>Contrastive divergence measures the difference between the data distribution and the model distribution using KL divergence:</p>
<div class="arithmatex">\[\mathcal{L}_{CD}(\theta) = D_{KL}(p_{data}(x) \| p_\theta(x)) - D_{KL}(p_\theta(x) \| p_{data}(x))\]</div>
<p><strong>Key insight</strong>: This objective encourages the model to match the data distribution while preventing mode collapse.</p>
<p><strong>Challenge</strong>: Computing the KL divergence requires sampling from the model distribution <span class="arithmatex">\(p_\theta(x)\)</span>, which is typically done using MCMC methods like Langevin dynamics or Hamiltonian Monte Carlo.</p>
<h3 id="fisher-divergence-score-matching">Fisher Divergence (Score Matching)</h3>
<p>Fisher divergence measures the difference between the score functions (gradients of log densities) of the two distributions:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x) - \nabla_x \log p_{data}(x)\|^2 \right]\]</div>
<p><strong>Key insight</strong>: Instead of comparing probability densities directly, we compare their gradients, which avoids the need to compute the intractable partition function.</p>
<p><strong>Advantage</strong>: No need to sample from the model during training, making it computationally more efficient than contrastive divergence.</p>
<p><strong>Challenge</strong>: Requires computing second derivatives (Hessian) of the log probability, which can be expensive for high-dimensional data.</p>
<h2 id="noise-contrastive-estimation">Noise Contrastive Estimation</h2>
<p><strong>Learning an EBM by contrasting it with a noise distribution.</strong></p>
<p>We have the data distribution <span class="arithmatex">\(p_{data}(x)\)</span>. We have the noise distribution <span class="arithmatex">\(p_n(x)\)</span> which should be analytically tractable and easy to sample from. We can train a discriminator <span class="arithmatex">\(D(x) \in [0, 1]\)</span> to distinguish between data samples and noise samples.</p>
<h3 id="optimal-discriminator">Optimal Discriminator</h3>
<p>The optimal discriminator <span class="arithmatex">\(D^*(x)\)</span> that maximizes this objective is given by:</p>
<div class="arithmatex">\[D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\]</div>
<h3 id="parameterizing-the-discriminator-as-an-ebm">Parameterizing the Discriminator as an EBM</h3>
<p><strong>Key Insight</strong>: Instead of training a separate discriminator, we can parameterize it directly in terms of an Energy-Based Model.</p>
<p>Let's define a parameterized version of the discriminator as:</p>
<div class="arithmatex">\[D_\theta(x) = \frac{p_\theta(x)}{p_\theta(x) + p_n(x)}\]</div>
<p>where <span class="arithmatex">\(p_\theta(x) = \frac{1}{Z(\theta)} e^{f_\theta(x)}\)</span> is our Energy-Based Model.</p>
<p><strong>Implicit Learning of the Data Distribution</strong></p>
<p>By training the discriminator <span class="arithmatex">\(D_\theta(x)\)</span> to distinguish between data samples and noise samples, we are implicitly learning the Energy-Based Model <span class="arithmatex">\(p_\theta(x)\)</span> to approximate the true data distribution <span class="arithmatex">\(p_{data}(x)\)</span>.</p>
<p><strong>Why This Works:</strong></p>
<p>Recall that the optimal discriminator (when trained to perfection) satisfies:</p>
<div class="arithmatex">\[D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\]</div>
<p>But we've parameterized our discriminator as:</p>
<div class="arithmatex">\[D_\theta(x) = \frac{p_\theta(x)}{p_\theta(x) + p_n(x)}\]</div>
<p><strong>The Key Insight</strong>: When we train <span class="arithmatex">\(D_\theta(x)\)</span> to match the optimal discriminator <span class="arithmatex">\(D^*(x)\)</span>, we're essentially forcing:</p>
<div class="arithmatex">\[\frac{p_\theta(x)}{p_\theta(x) + p_n(x)} \approx \frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\]</div>
<p>This equality holds if and only if <span class="arithmatex">\(p_\theta(x) \approx p_{data}(x)\)</span> (assuming <span class="arithmatex">\(p_n(x) &gt; 0\)</span> everywhere).</p>
<p><strong>Mathematical Justification:</strong></p>
<p>If <span class="arithmatex">\(\frac{p_\theta(x)}{p_\theta(x) + p_n(x)} = \frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\)</span>, then:</p>
<div class="arithmatex">\[p_\theta(x) \cdot (p_{data}(x) + p_n(x)) = p_{data}(x) \cdot (p_\theta(x) + p_n(x))\]</div>
<div class="arithmatex">\[p_\theta(x) \cdot p_{data}(x) + p_\theta(x) \cdot p_n(x) = p_{data}(x) \cdot p_\theta(x) + p_{data}(x) \cdot p_n(x)\]</div>
<div class="arithmatex">\[p_\theta(x) \cdot p_n(x) = p_{data}(x) \cdot p_n(x)\]</div>
<p>Since <span class="arithmatex">\(p_n(x) &gt; 0\)</span>, we can divide both sides to get:</p>
<div class="arithmatex">\[p_\theta(x) = p_{data}(x)\]</div>
<p><strong>Modeling the Partition Function as a Trainable Parameter</strong></p>
<p><strong>The EBM Equation:</strong></p>
<p>Our Energy-Based Model is defined as:</p>
<div class="arithmatex">\[p_\theta(x) = \frac{1}{Z(\theta)} e^{f_\theta(x)}\]</div>
<p>where <span class="arithmatex">\(f_\theta(x)\)</span> is the energy function (neural network) and <span class="arithmatex">\(Z(\theta) = \int e^{f_\theta(x)} dx\)</span> is the partition function.</p>
<p><strong>The Partition Function Constraint Problem:</strong></p>
<p>The constraint <span class="arithmatex">\(Z(\theta) = \int e^{f_\theta(x)} dx\)</span> is computationally intractable to satisfy exactly because:</p>
<ol>
<li><strong>High-dimensional integration</strong>: Computing <span class="arithmatex">\(\int e^{f_\theta(x)} dx\)</span> over high-dimensional spaces is extremely expensive</li>
<li><strong>No closed form</strong>: For complex energy functions, there's no analytical solution</li>
<li><strong>Dynamic updates</strong>: The integral changes every time we update the energy function parameters</li>
</ol>
<p><strong>Solution: Treat Z as a Trainable Parameter</strong></p>
<p>Instead of enforcing the constraint, we model <span class="arithmatex">\(Z(\theta)\)</span> as an additional trainable parameter <span class="arithmatex">\(Z\)</span> that is not explicitly constrained to satisfy <span class="arithmatex">\(Z = \int e^{f_\theta(x)} dx\)</span>.</p>
<p>This gives us the modified EBM:</p>
<div class="arithmatex">\[p_{\theta, Z}(x) = \frac{e^{f_\theta(x)}}{Z}\]</div>
<p><strong>Why Z Converges to the Correct Partition Function:</strong></p>
<p>As we train <span class="arithmatex">\(p_{\theta, Z}(x)\)</span> to approximate <span class="arithmatex">\(p_{data}(x)\)</span>, the parameter <span class="arithmatex">\(Z\)</span> automatically converges to the correct partition function value.</p>
<p><strong>Mathematical Justification:</strong></p>
<p>When training converges, we have <span class="arithmatex">\(p_{\theta, Z}(x) \approx p_{data}(x)\)</span>. This means:</p>
<div class="arithmatex">\[\frac{e^{f_\theta(x)}}{Z} \approx p_{data}(x)\]</div>
<p>A direct argument comes from the fact that <span class="arithmatex">\(p_{\theta, Z}(x)\)</span> must approximate <span class="arithmatex">\(p_{data}(x)\)</span>, which must integrate to 1:</p>
<div class="arithmatex">\[\int p_{\theta, Z}(x) dx = \int \frac{e^{f_\theta(x)}}{Z} dx \approx 1\]</div>
<p>This immediately gives us:</p>
<div class="arithmatex">\[Z \approx \int e^{f_\theta(x)} dx\]</div>
<p><strong>Deriving the Discriminator for the Modified EBM</strong></p>
<p>Now let's derive the discriminator <span class="arithmatex">\(D_{\theta, Z}(x)\)</span> for our modified EBM <span class="arithmatex">\(p_{\theta, Z}(x) = \frac{e^{f_\theta(x)}}{Z}\)</span>.</p>
<p>Starting with the discriminator definition:</p>
<div class="arithmatex">\[D_{\theta, Z}(x) = \frac{p_{\theta, Z}(x)}{p_{\theta, Z}(x) + p_n(x)}\]</div>
<p>Substituting our modified EBM:</p>
<div class="arithmatex">\[D_{\theta, Z}(x) = \frac{\frac{e^{f_\theta(x)}}{Z}}{\frac{e^{f_\theta(x)}}{Z} + p_n(x)}\]</div>
<div class="arithmatex">\[D_{\theta, Z}(x) = \frac{e^{f_\theta(x)}}{e^{f_\theta(x)} + Z \cdot p_n(x)}\]</div>
<p><strong>Noise Contrastive Estimation Training Objective</strong></p>
<p>The NCE objective maximizes the log-likelihood of correctly classifying data vs noise samples:</p>
<div class="arithmatex">\[\mathcal{L}_{NCE}(\theta, Z) = \mathbb{E}_{x \sim p_{data}} \left[ \log D_{\theta, Z}(x) \right] + \mathbb{E}_{x \sim p_n} \left[ \log(1 - D_{\theta, Z}(x)) \right]\]</div>
<p>In theory, we could have any noise distribution to make this work. But in pratice, a noise distribution that similar (if we can manage) to the data distribution works very well. At the end of the day you learn an EBM and you learn a partition function. In the limit of infinite data and perfect optimization, the EBM matches the data distribution and Z matches the true partition function of the EBM.</p>
<p>There is no evolving Generator like we had in GAN. The generator here is fixed, which is the noise distribution. We are training a special Discriminator.</p>
<p>Note: The NCE objective function does not guide us to sample any data. We still need to use something like MCMC (e.g., Langevin dynamics, Hamiltonian Monte Carlo) to generate samples from the trained EBM. NCE only provides a way to train the energy function and partition function without computing the intractable partition function integral.</p>
<p>Substituting our discriminator:</p>
<div class="arithmatex">\[\mathcal{L}_{NCE}(\theta, Z) = \mathbb{E}_{x \sim p_{data}} \left[ \log \frac{e^{f_\theta(x)}}{e^{f_\theta(x)} + Z \cdot p_n(x)} \right] + \mathbb{E}_{x \sim p_n} \left[ \log \frac{Z \cdot p_n(x)}{e^{f_\theta(x)} + Z \cdot p_n(x)} \right]\]</div>
<p>Using the sigmoid formulation with <span class="arithmatex">\(h_{\theta, Z}(x) = f_\theta(x) - \log p_n(x) - \log Z\)</span>:</p>
<div class="arithmatex">\[\mathcal{L}_{NCE}(\theta, Z) = \mathbb{E}_{x \sim p_{data}} \left[ \log \sigma(h_{\theta, Z}(x)) \right] + \mathbb{E}_{x \sim p_n} \left[ \log(1 - \sigma(h_{\theta, Z}(x))) \right]\]</div>
<p>This is exactly the binary cross-entropy loss for a binary classifier that distinguishes between data and noise samples.</p>
<p><strong>Training Algorithm</strong>:</p>
<ol>
<li>
<p>Sample data batch: <span class="arithmatex">\(\{x_1, x_2, \ldots, x_n\} \sim p_{data}(x)\)</span></p>
</li>
<li>
<p>Sample noise batch: <span class="arithmatex">\(\{\tilde{x}_1, \tilde{x}_2, \ldots, \tilde{x}_n\} \sim p_n(x)\)</span></p>
</li>
<li>
<p>Compute logits: <span class="arithmatex">\(h_{\theta, Z}(x_i)\)</span> and <span class="arithmatex">\(h_{\theta, Z}(\tilde{x}_i)\)</span></p>
</li>
<li>
<p>Compute binary cross-entropy loss</p>
</li>
<li>
<p>Update both <span class="arithmatex">\(\theta\)</span> (energy function parameters) and <span class="arithmatex">\(Z\)</span> (partition function parameter) via gradient descent</p>
</li>
</ol>
<p><strong>Key Advantage</strong>: Unlike other EBM training methods (like contrastive divergence), NCE does not require sampling from the EBM during the training process. This eliminates the computational expense and potential instability of MCMC sampling during training, making NCE much more efficient and stable.</p>
<h2 id="comparing-nce-and-gan">Comparing NCE and GAN</h2>
<p>Both NCE and GANs use binary classification objectives to train generative models, but they differ significantly in their approach and properties.</p>
<p><strong>Similarities</strong></p>
<ol>
<li><strong>Binary Classification Objective</strong>: Both use binary cross-entropy loss to distinguish between real and fake samples</li>
<li><strong>No Likelihood Computation</strong>: Neither requires computing or maximizing explicit likelihood</li>
<li><strong>Stable Training</strong>: Both avoid the computational challenges of direct likelihood-based training</li>
</ol>
<p><strong>Key Differences</strong></p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>NCE</th>
<th>GAN</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Generator</strong></td>
<td>Fixed noise distribution <span class="arithmatex">\(p_n(x)\)</span></td>
<td>Learnable generator network <span class="arithmatex">\(G_\phi(z)\)</span></td>
</tr>
<tr>
<td><strong>Discriminator</strong></td>
<td>Parameterized as EBM: <span class="arithmatex">\(D_{\theta,Z}(x) = \frac{e^{f_\theta(x)}}{e^{f_\theta(x)} + Z \cdot p_n(x)}\)</span></td>
<td>Separate neural network <span class="arithmatex">\(D_\theta(x)\)</span></td>
</tr>
<tr>
<td><strong>Training</strong></td>
<td>Single objective: <span class="arithmatex">\(\mathcal{L}_{NCE}(\theta, Z)\)</span></td>
<td>Min-max game: <span class="arithmatex">\(\min_G \max_D \mathcal{L}_{GAN}(G, D)\)</span></td>
</tr>
<tr>
<td><strong>Sampling</strong></td>
<td>Requires MCMC after training</td>
<td>Direct sampling via generator</td>
</tr>
<tr>
<td><strong>Mode Coverage</strong></td>
<td>Depends on noise distribution choice</td>
<td>Can adapt to cover all data modes</td>
</tr>
<tr>
<td><strong>Convergence</strong></td>
<td>Single optimization problem</td>
<td>Requires careful balance between generator and discriminator</td>
</tr>
</tbody>
</table>
<p><strong>When to Use Each</strong></p>
<p><strong>Use NCE when:</strong>
- You need interpretable energy functions
- Training stability is crucial
- You want theoretical guarantees
- You can afford MCMC sampling at inference time</p>
<p><strong>Use GAN when:</strong>
- Fast sampling is required
- You need high-quality, diverse samples
- You have computational resources for adversarial training
- You want to avoid MCMC entirely</p>
<h2 id="training-score-based-models">Training Score Based Models</h2>
<p><strong>Is Score Matching Limited to EBMs?</strong></p>
<p>No, score matching is not limited to Energy-Based Models. We can use score matching for other generative model types as well:</p>
<ul>
<li><strong>Autoregressive Models</strong>: Can be trained using score matching</li>
<li><strong>Normalizing Flow Models</strong>: Can also be trained using score matching</li>
<li><strong>Variational Autoencoders</strong>: Score matching can be applied to VAEs</li>
</ul>
<p><strong>But what's the point since likelihoods are tractable?</strong></p>
<p>For models like autoregressive models and normalizing flows, the likelihood is indeed tractable, so we could use maximum likelihood estimation (MLE) instead of score matching. However, in principle, we could still train these models using score matching.</p>
<p><strong>Practical Considerations:</strong>
<strong>MLE is often preferred</strong> when likelihood is tractable because it's more direct and efficient. <strong>Score matching might be useful</strong> when the likelihood computation is numerically unstable</p>
<p><strong>The core ddea of Score-Based Models</strong></p>
<p>The fundamental insight behind score-based models is that instead of modeling the energy function or probability density directly, we model the <strong>score function</strong> <span class="arithmatex">\(s_\theta(x)\)</span>.</p>
<p><strong>What is the Score Function?</strong></p>
<p>The score function is the gradient of the log probability density:</p>
<div class="arithmatex">\[s_\theta(x) = \nabla_x \log p_\theta(x)\]</div>
<p><strong>Direct Modeling Approach:</strong></p>
<p>Instead of learning an energy function <span class="arithmatex">\(f_\theta(x)\)</span> and computing <span class="arithmatex">\(s_\theta(x) = \nabla_x f_\theta(x)\)</span>, we directly model:</p>
<div class="arithmatex">\[s_\theta(x): \mathbb{R}^d \rightarrow \mathbb{R}^d\]</div>
<p>This is a <strong>vector-valued function</strong> that maps from the data space to the same space, representing the gradient field.</p>
<p><strong>Key Properties:</strong></p>
<ol>
<li><strong>Vector Field</strong>: <span class="arithmatex">\(s_\theta(x)\)</span> is a vector field that assigns a gradient vector to each point <span class="arithmatex">\(x\)</span> in the data space</li>
<li><strong>No Partition Function</strong>: We don't need to compute or approximate the partition function <span class="arithmatex">\(Z(\theta)\)</span></li>
<li><strong>Direct Approximation</strong>: <span class="arithmatex">\(s_\theta(x) \approx \nabla_x \log p_{data}(x)\)</span></li>
</ol>
<p><img alt="Score Based Models" src="../score_based_models.png" /></p>
<p><strong>Deriving the Score Matching Objective</strong></p>
<p><strong>Fisher Divergence objective:</strong></p>
<p>We want to minimize the Fisher divergence between the data distribution and the distribution induced by our score function:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|s_\theta(x) - s_{data}(x)\|^2 \right]\]</div>
<p>This measures how well our learned score function <span class="arithmatex">\(s_\theta(x)\)</span> approximates the true score function <span class="arithmatex">\(s_{data}(x) = \nabla_x \log p_{data}(x)\)</span>.</p>
<p><strong>The Challenge:</strong></p>
<p>We don't have access to <span class="arithmatex">\(s_{data}(x) = \nabla_x \log p_{data}(x)\)</span> since we only have samples from <span class="arithmatex">\(p_{data}(x)\)</span>, not its analytical form.</p>
<p>We can rewrite the Fisher divergence to avoid needing the true score function. Let's expand the squared norm:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|s_\theta(x)\|^2 - s_\theta(x)^T s_{data}(x) + \frac{1}{2} \|s_{data}(x)\|^2 \right]\]</div>
<p>The key insight is to handle the cross term <span class="arithmatex">\(s_\theta(x)^T s_{data}(x)\)</span> using integration by parts.</p>
<p>For the univariate case (<span class="arithmatex">\(x \in \mathbb{R}\)</span>), we have:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim p_{data}} \left[ s_\theta(x) \cdot s_{data}(x) \right] = \int s_\theta(x) \cdot \frac{d}{dx} \log p_{data}(x) \cdot p_{data}(x) dx\]</div>
<div class="arithmatex">\[= \int s_\theta(x) \cdot \frac{d}{dx} p_{data}(x) dx\]</div>
<p>Using integration by parts: <span class="arithmatex">\(\int u \cdot \frac{d}{dx} v \, dx = u \cdot v - \int \frac{d}{dx} u \cdot v \, dx\)</span></p>
<p>Setting <span class="arithmatex">\(u = s_\theta(x)\)</span> and <span class="arithmatex">\(v = p_{data}(x)\)</span>:</p>
<div class="arithmatex">\[= \left. s_\theta(x) \cdot p_{data}(x) \right|_{-\infty}^{\infty} - \int \frac{d}{dx} s_\theta(x) \cdot p_{data}(x) dx\]</div>
<p>Assuming the boundary term vanishes (reasonable for well-behaved distributions):</p>
<div class="arithmatex">\[= -\mathbb{E}_{x \sim p_{data}} \left[ \frac{d}{dx} s_\theta(x) \right]\]</div>
<p><strong>Multivariate Case:</strong></p>
<p>For <span class="arithmatex">\(x \in \mathbb{R}^d\)</span>, we apply integration by parts component-wise:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim p_{data}} \left[ s_\theta(x)^T s_{data}(x) \right] = \sum_{i=1}^d \mathbb{E}_{x \sim p_{data}} \left[ s_\theta(x)_i \cdot s_{data}(x)_i \right]\]</div>
<div class="arithmatex">\[= -\sum_{i=1}^d \mathbb{E}_{x \sim p_{data}} \left[ \frac{\partial}{\partial x_i} s_\theta(x)_i \right]\]</div>
<div class="arithmatex">\[= -\mathbb{E}_{x \sim p_{data}} \left[ \text{tr}(\nabla_x s_\theta(x)) \right]\]</div>
<p>where <span class="arithmatex">\(\text{tr}(\nabla_x s_\theta(x)) = \sum_{i=1}^d \frac{\partial}{\partial x_i} s_\theta(x)_i\)</span> is the trace of the Jacobian matrix.</p>
<p><strong>Final Score Matching Objective:</strong></p>
<p>Substituting back into the Fisher divergence:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|s_\theta(x)\|^2 + \text{tr}(\nabla_x s_\theta(x)) \right] + \text{constant}\]</div>
<p>where the constant term <span class="arithmatex">\(\frac{1}{2} \mathbb{E}_{x \sim p_{data}} \left[ \|s_{data}(x)\|^2 \right]\)</span> doesn't depend on <span class="arithmatex">\(\theta\)</span> and can be ignored during optimization.</p>
<p><strong>Key Insight:</strong></p>
<p>This reformulation allows us to train the score function using only samples from <span class="arithmatex">\(p_{data}(x)\)</span> and the derivatives of our score model, without needing access to the true score function <span class="arithmatex">\(s_{data}(x)\)</span>.</p>
<p>The computational cost of the second term makes score matching challenging for high-dimensional data, which motivates alternative approaches like denoising score matching and sliced score matching.</p>
<h2 id="denoising-score-matching">Denoising Score Matching</h2>
<p>Denoising score matching addresses the computational challenges of standard score matching by adding noise to the data.</p>
<p><strong>The Key Idea:</strong></p>
<p>Instead of trying to learn the score function of the original data distribution <span class="arithmatex">\(p_{data}(x)\)</span>, we learn the score function of a noisy version of the data.</p>
<p><strong>Noise Distribution:</strong></p>
<p>We define a noise distribution <span class="arithmatex">\(q_\sigma(\tilde{x} | x)\)</span> that adds noise to clean data points. A common choice is Gaussian noise:</p>
<div class="arithmatex">\[q_\sigma(\tilde{x} | x) = \mathcal{N}(\tilde{x}; x, \sigma^2 I)\]</div>
<p>This means: <span class="arithmatex">\(\tilde{x} = x + \epsilon\)</span> where <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\)</span></p>
<p><strong>Noisy Data Distribution:</strong></p>
<p>The noisy data distribution is the convolution of the original data distribution with the noise:</p>
<div class="arithmatex">\[q_\sigma(\tilde{x}) = \int q_\sigma(\tilde{x} | x) p_{data}(x) dx\]</div>
<p>The denoising score matching objective minimizes the Fisher divergence between the noise perturbed data distribution <span class="arithmatex">\(q_\sigma(\tilde{x})\)</span> and our score model <span class="arithmatex">\(s_\theta(\tilde{x})\)</span>:</p>
<div class="arithmatex">\[\mathcal{L}_{DSM}(\theta) = \mathbb{E}_{\tilde{x} \sim q_\sigma(\tilde{x})} \left[ \frac{1}{2} \|s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x})\|^2 \right]\]</div>
<div class="arithmatex">\[\mathcal{L}_{DSM}(\theta) = \int q_\sigma(\tilde{x}) \left[ \frac{1}{2} \|s_\theta(\tilde{x})\|^2 - s_\theta(\tilde{x})^T \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) + \frac{1}{2} \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})\|^2 \right] d\tilde{x}\]</div>
<p><strong>Focusing on the Cross Term:</strong></p>
<p>The cross term <span class="arithmatex">\(-s_\theta(\tilde{x})^T \nabla_{\tilde{x}} \log q_\sigma(\tilde{x})\)</span> is the challenging part. First, let's write the cross term as an integral:</p>
<div class="arithmatex">\[\int q_\sigma(\tilde{x}) \left[ -s_\theta(\tilde{x})^T \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \right] d\tilde{x}\]</div>
<p>Using the chain rule: <span class="arithmatex">\(\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \cdot q_\sigma(\tilde{x}) = \nabla_{\tilde{x}} q_\sigma(\tilde{x})\)</span>, we get:</p>
<div class="arithmatex">\[= -\int s_\theta(\tilde{x})^T \nabla_{\tilde{x}} q_\sigma(\tilde{x}) d\tilde{x}\]</div>
<p><strong>Substituting the noisy data distribution:</strong></p>
<p>Recall that <span class="arithmatex">\(q_\sigma(\tilde{x}) = \int q_\sigma(\tilde{x} | x) p_{data}(x) dx\)</span>. Substituting this into the integral:</p>
<div class="arithmatex">\[= -\int s_\theta(\tilde{x})^T \nabla_{\tilde{x}} \left[ \int q_\sigma(\tilde{x} | x) p_{data}(x) dx \right] d\tilde{x}\]</div>
<p>Since the gradient operator <span class="arithmatex">\(\nabla_{\tilde{x}}\)</span> acts only on <span class="arithmatex">\(\tilde{x}\)</span> and not on <span class="arithmatex">\(x\)</span>, we can interchange the gradient and the integral over <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[= -\int s_\theta(\tilde{x})^T \left[ \int \nabla_{\tilde{x}} q_\sigma(\tilde{x} | x) p_{data}(x) dx \right] d\tilde{x}\]</div>
<p>We can rearrange this as a double integral:</p>
<div class="arithmatex">\[= -\iint s_\theta(\tilde{x})^T \nabla_{\tilde{x}} q_\sigma(\tilde{x} | x) \cdot p_{data}(x) \, dx \, d\tilde{x}\]</div>
<p>Now we can use the chain rule in reverse: <span class="arithmatex">\(\nabla_{\tilde{x}} q_\sigma(\tilde{x} | x) = \nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) \cdot q_\sigma(\tilde{x} | x)\)</span></p>
<div class="arithmatex">\[= -\iint s_\theta(\tilde{x})^T \nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) \cdot q_\sigma(\tilde{x} | x) \cdot p_{data}(x) \, dx \, d\tilde{x}\]</div>
<p><strong>Final Expression:</strong></p>
<p>This can be written as an expectation:</p>
<div class="arithmatex">\[= -\mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ s_\theta(\tilde{x})^T \nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) \right]\]</div>
<p>Or equivalently:</p>
<div class="arithmatex">\[= -\mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)^T s_\theta(\tilde{x}) \right]\]</div>
<p><strong>Completing the Denoising Score Matching Objective:</strong></p>
<p>Now let's bring this back to the complete objective function. Recall that we started with:</p>
<div class="arithmatex">\[\mathcal{L}_{DSM}(\theta) = \int q_\sigma(\tilde{x}) \left[ \frac{1}{2} \|s_\theta(\tilde{x})\|^2 - s_\theta(\tilde{x})^T \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) + \frac{1}{2} \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})\|^2 \right] d\tilde{x}\]</div>
<p>We've derived the cross term. Now let's handle all three terms:</p>
<p><strong>Term 1: <span class="arithmatex">\(\frac{1}{2} \|s_\theta(\tilde{x})\|^2\)</span></strong></p>
<div class="arithmatex">\[\int q_\sigma(\tilde{x}) \cdot \frac{1}{2} \|s_\theta(\tilde{x})\|^2 d\tilde{x} = \frac{1}{2} \mathbb{E}_{\tilde{x} \sim q_\sigma(\tilde{x})} \left[ \|s_\theta(\tilde{x})\|^2 \right]\]</div>
<p>Substituting <span class="arithmatex">\(q_\sigma(\tilde{x}) = \int q_\sigma(\tilde{x} | x) p_{data}(x) dx\)</span>:</p>
<div class="arithmatex">\[= \frac{1}{2} \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \|s_\theta(\tilde{x})\|^2 \right]\]</div>
<p><strong>Term 2: Cross term (already derived)</strong></p>
<div class="arithmatex">\[- \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)^T s_\theta(\tilde{x}) \right]\]</div>
<p><strong>Term 3: <span class="arithmatex">\(\frac{1}{2} \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})\|^2\)</span></strong></p>
<div class="arithmatex">\[\int q_\sigma(\tilde{x}) \cdot \frac{1}{2} \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})\|^2 d\tilde{x} = \frac{1}{2} \mathbb{E}_{\tilde{x} \sim q_\sigma(\tilde{x})} \left[ \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})\|^2 \right]\]</div>
<p>This term is a constant with respect to <span class="arithmatex">\(\theta\)</span> and can be ignored during optimization.</p>
<p><strong>Combining all terms:</strong></p>
<div class="arithmatex">\[\mathcal{L}_{DSM}(\theta) = \frac{1}{2} \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \|s_\theta(\tilde{x})\|^2 \right] - \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)^T s_\theta(\tilde{x}) \right] + \text{const}\]</div>
<p><strong>Simplifying to the final form:</strong></p>
<div class="arithmatex">\[\mathcal{L}_{DSM}(\theta) = \frac{1}{2} \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \|s_\theta(\tilde{x})\|^2 \right] - \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)^T s_\theta(\tilde{x}) \right] + \text{const}\]</div>
<p>To see how this becomes the final form, let's expand the squared difference:</p>
<div class="arithmatex">\[\|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) - s_\theta(\tilde{x})\|^2 = \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)\|^2 - 2 \nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)^T s_\theta(\tilde{x}) + \|s_\theta(\tilde{x})\|^2\]</div>
<p>Taking the expectation and multiplying by <span class="arithmatex">\(\frac{1}{2}\)</span>:</p>
<div class="arithmatex">\[\frac{1}{2} \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) - s_\theta(\tilde{x})\|^2 \right] = \frac{1}{2} \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \|s_\theta(\tilde{x})\|^2 \right] - \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)^T s_\theta(\tilde{x}) \right] + \frac{1}{2} \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)\|^2 \right]\]</div>
<p>The last term <span class="arithmatex">\(\frac{1}{2} \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)\|^2 \right]\)</span> is a constant with respect to <span class="arithmatex">\(\theta\)</span> and can be absorbed into the constant term.</p>
<p>This can be written as:</p>
<div class="arithmatex">\[\mathcal{L}_{DSM}(\theta) = \frac{1}{2} \mathbb{E}_{x \sim p_{data}, \tilde{x} \sim q_\sigma(\tilde{x} | x)} \left[ \|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) - s_\theta(\tilde{x})\|^2 \right] + \text{const}\]</div>
<p><strong>Key Advantage: easy computation of the Target Score Function</strong></p>
<p>The major advantage of denoising score matching is that <span class="arithmatex">\(\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)\)</span> is <strong>easy to compute analytically</strong>, unlike the true data score function <span class="arithmatex">\(\nabla_x \log p_{data}(x)\)</span>.</p>
<p><strong>For Gaussian Noise:</strong></p>
<p>The most common choice is Gaussian noise: <span class="arithmatex">\(q_\sigma(\tilde{x} | x) = \mathcal{N}(\tilde{x}; x, \sigma^2 I)\)</span></p>
<p>The log probability is:</p>
<div class="arithmatex">\[\log q_\sigma(\tilde{x} | x) = -\frac{1}{2\sigma^2} \|\tilde{x} - x\|^2 - \frac{d}{2} \log(2\pi\sigma^2)\]</div>
<p>Taking the gradient with respect to <span class="arithmatex">\(\tilde{x}\)</span>:</p>
<div class="arithmatex">\[\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) = -\frac{1}{\sigma^2} (\tilde{x} - x)\]</div>
<p>This gradient is <strong>analytically tractable</strong> and <strong>computationally cheap</strong>.</p>
<p><strong>Comparison with Standard Score Matching:</strong></p>
<p>In standard score matching, we need to compute:</p>
<ul>
<li>
<p><span class="arithmatex">\(\nabla_x \log p_\theta(x)\)</span> (our model's score function)</p>
</li>
<li>
<p><span class="arithmatex">\(\text{tr}(\nabla_x \nabla_x \log p_\theta(x))\)</span> (Hessian trace - expensive!)</p>
</li>
</ul>
<p>In denoising score matching, we only need:</p>
<ul>
<li>
<p><span class="arithmatex">\(\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) = -\frac{1}{\sigma^2} (\tilde{x} - x)\)</span> (known analytically)</p>
</li>
<li>
<p><span class="arithmatex">\(s_\theta(\tilde{x})\)</span> (our model's score function)</p>
</li>
</ul>
<p><strong>Training Algorithm:</strong></p>
<ol>
<li>Sample clean data: <span class="arithmatex">\(x \sim p_{data}(x)\)</span></li>
<li>Add noise: <span class="arithmatex">\(\tilde{x} = x + \epsilon\)</span> where <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\)</span></li>
<li>Compute target: <span class="arithmatex">\(\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) = -\frac{1}{\sigma^2} (\tilde{x} - x) = -\frac{1}{\sigma^2} \epsilon\)</span></li>
<li>Compute prediction: <span class="arithmatex">\(s_\theta(\tilde{x})\)</span></li>
<li>Minimize: <span class="arithmatex">\(\|\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) - s_\theta(\tilde{x})\|^2\)</span></li>
</ol>
<p><strong>Monte Carlo Estimation:</strong></p>
<p>For a batch of <span class="arithmatex">\(N\)</span> samples <span class="arithmatex">\(\{x_1, x_2, \ldots, x_N\}\)</span>, the Monte Carlo estimate of the loss is:</p>
<div class="arithmatex">\[\mathcal{L}_{DSM}(\theta) \approx \frac{1}{2N} \sum_{i=1}^N \|\nabla_{\tilde{x}_i} \log q_\sigma(\tilde{x}_i | x_i) - s_\theta(\tilde{x}_i)\|^2\]</div>
<p>where <span class="arithmatex">\(\tilde{x}_i = x_i + \epsilon_i\)</span> with <span class="arithmatex">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2 I)\)</span>.</p>
<p><strong>Practical Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="c1"># For a batch of data</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="k">def</span><span class="w"> </span><span class="nf">denoising_score_matching_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_batch</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="c1"># Add noise to data</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">data_batch</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">noisy_data</span> <span class="o">=</span> <span class="n">data_batch</span> <span class="o">+</span> <span class="n">noise</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="c1"># Compute target score (gradient of log noise distribution)</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">target_score</span> <span class="o">=</span> <span class="o">-</span><span class="n">noise</span> <span class="o">/</span> <span class="p">(</span><span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="c1"># Compute model prediction</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span class="n">predicted_score</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">noisy_data</span><span class="p">)</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    <span class="c1"># Compute loss</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">target_score</span> <span class="o">-</span> <span class="n">predicted_score</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div>
<p><strong>Intuition behind the loss function</strong></p>
<p>The denoising score matching objective has a beautiful intuition: <strong>we're teaching our model to estimate the score function of noisy data, and when the noise is very small, this approximates the score function of the clean data distribution.</strong></p>
<p><strong>The Core Idea:</strong></p>
<ol>
<li>
<p><strong>Learning Noisy Data Structure</strong>: Instead of trying to learn the score function of the complex, unknown data distribution <span class="arithmatex">\(p_{data}(x)\)</span>, we learn the score function of a simpler, known noisy distribution <span class="arithmatex">\(q_\sigma(\tilde{x})\)</span>.</p>
</li>
<li>
<p><strong>Denoising as Learning</strong>: By learning to predict <span class="arithmatex">\(\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) = -\frac{1}{\sigma^2} (\tilde{x} - x)\)</span>, our model learns to "point" from noisy points <span class="arithmatex">\(\tilde{x}\)</span> back toward their clean counterparts <span class="arithmatex">\(x\)</span>.</p>
</li>
</ol>
<p><strong>What does "point" mean?</strong></p>
<p>The score function <span class="arithmatex">\(\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x)\)</span> is a <strong>vector field</strong> that assigns a direction vector to each point <span class="arithmatex">\(\tilde{x}\)</span> in the data space. This vector "points" in the direction of steepest increase in the log probability.</p>
<p>For Gaussian noise, this vector is:</p>
<div class="arithmatex">\[\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) = -\frac{1}{\sigma^2} (\tilde{x} - x)\]</div>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<p><strong>Direction</strong>: The vector points from the noisy point <span class="arithmatex">\(\tilde{x}\)</span> toward the clean point <span class="arithmatex">\(x\)</span></p>
</li>
<li>
<p><strong>Magnitude</strong>: The length of the vector is proportional to the distance between <span class="arithmatex">\(\tilde{x}\)</span> and <span class="arithmatex">\(x\)</span>, scaled by <span class="arithmatex">\(\frac{1}{\sigma^2}\)</span></p>
</li>
<li>
<p><strong>Purpose</strong>: This vector tells us "if you want to increase the probability of this noisy point, move in this direction"</p>
</li>
</ul>
<p><strong>Visual Example:</strong>
Imagine a 2D space where:</p>
<ul>
<li>
<p>Clean point <span class="arithmatex">\(x = (0, 0)\)</span></p>
</li>
<li>
<p>Noisy point <span class="arithmatex">\(\tilde{x} = (1, 1)\)</span> </p>
</li>
<li>
<p>Noise level <span class="arithmatex">\(\sigma = 1\)</span></p>
</li>
</ul>
<p>The score vector at <span class="arithmatex">\(\tilde{x}\)</span> is:</p>
<div class="arithmatex">\[\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) = -(1, 1)\]</div>
<p>This vector points from <span class="arithmatex">\((1, 1)\)</span> toward <span class="arithmatex">\((0, 0)\)</span>, indicating the direction to move to increase the probability of the noisy point under the noise distribution.</p>
<p><strong>Why this matters:</strong>
When our model learns to predict this vector, it's learning to identify the direction that leads back to the clean data. This implicitly teaches it about the local structure of the data manifold - where the "good" data points are located relative to any given noisy point.</p>
<p><strong>Why this is a denoiser:</strong></p>
<p>The score function <span class="arithmatex">\(\nabla_{\tilde{x}} \log q_\sigma(\tilde{x} | x) = -\frac{1}{\sigma^2} (\tilde{x} - x)\)</span> acts as a <strong>denoiser</strong> because it provides the exact direction and magnitude needed to remove the noise from a noisy data point.</p>
<p>When our model learns to predict this score function, it's learning to:</p>
<ul>
<li>
<p><strong>Identify noise</strong>: Recognize what part of the data is noise</p>
</li>
<li>
<p><strong>Compute denoising direction</strong>: Determine which direction to move to remove the noise</p>
</li>
<li>
<p><strong>Estimate noise magnitude</strong>: Understand how much to move in that direction</p>
</li>
</ul>
<p>This is why denoising score matching is so powerful - by learning to denoise, the model implicitly learns the structure of the clean data distribution, even though it never directly sees the clean data score function.</p>
<p><strong>Con:</strong>
Estimates the score of the noise-perturbed data, not the score of the actual data. We have shifted the goal post basically.</p>
<p><strong>Generating Samples with MCMC:</strong></p>
<p>Once we've trained our score function <span class="arithmatex">\(s_\theta(x)\)</span>, we can generate samples using <strong>Markov Chain Monte Carlo (MCMC)</strong> methods, typically <strong>Langevin dynamics</strong>.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>