
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/AI/deep_generative_models/score_based_models/">
      
      
        <link rel="prev" href="../energy_based_models/">
      
      
        <link rel="next" href="../../../math/probability/probability_and_counting/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Score Based Models - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#score-based-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Score Based Models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/probability/probability_and_counting/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#score-matching" class="md-nav__link">
    <span class="md-ellipsis">
      Score Matching
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Score Matching">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#score-matching-comparing-distributions-via-vector-fields" class="md-nav__link">
    <span class="md-ellipsis">
      Score Matching: Comparing Distributions via Vector Fields
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#score-matching-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Score Matching Algorithm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recap-distances-for-training-ebms" class="md-nav__link">
    <span class="md-ellipsis">
      Recap: Distances for Training EBMs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Recap: Distances for Training EBMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contrastive-divergence" class="md-nav__link">
    <span class="md-ellipsis">
      Contrastive Divergence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fisher-divergence-score-matching" class="md-nav__link">
    <span class="md-ellipsis">
      Fisher Divergence (Score Matching)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#noise-contrastive-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      Noise Contrastive Estimation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Noise Contrastive Estimation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimal-discriminator" class="md-nav__link">
    <span class="md-ellipsis">
      Optimal Discriminator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameterizing-the-discriminator-as-an-ebm" class="md-nav__link">
    <span class="md-ellipsis">
      Parameterizing the Discriminator as an EBM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparing-nce-and-gan" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing NCE and GAN
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="score-based-models">Score Based Models</h1>
<h2 id="score-matching">Score Matching</h2>
<p><strong>Energy-Based Model Probability Distribution</strong></p>
<p>In Energy-Based Models, the probability distribution is defined as:</p>
<div class="arithmatex">\[p_\theta(x) = \frac{1}{Z(\theta)} e^{f_\theta(x)}\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(f_\theta(x)\)</span> is the energy function (neural network)</p>
</li>
<li>
<p><span class="arithmatex">\(Z(\theta) = \int e^{f_\theta(x)} dx\)</span> is the partition function (intractable)</p>
</li>
</ul>
<p>Taking the logarithm of the probability distribution:</p>
<div class="arithmatex">\[\log p_\theta(x) = f_\theta(x) - \log Z(\theta)\]</div>
<p>Notice that the partition function <span class="arithmatex">\(Z(\theta)\)</span> appears as a constant term that doesn't depend on <span class="arithmatex">\(x\)</span>.</p>
<p><strong>Stein Score Function</strong></p>
<p>The <strong>Stein score function</strong> <span class="arithmatex">\(s_\theta(x)\)</span> is defined as the gradient of the log probability with respect to <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[s_\theta(x) = \nabla_x \log p_\theta(x)\]</div>
<p>For Energy-Based Models, the score function equals the gradient of the energy function:</p>
<div class="arithmatex">\[s_\theta(x) = \nabla_x \log p_\theta(x) = \nabla_x (f_\theta(x) - \log Z(\theta)) = \nabla_x f_\theta(x)\]</div>
<p>The partition function term <span class="arithmatex">\(\log Z(\theta)\)</span> disappears because it doesn't depend on <span class="arithmatex">\(x\)</span>.</p>
<p><strong>Score as a Vector Field</strong></p>
<p>The score function <span class="arithmatex">\(s_\theta(x)\)</span> is a <strong>vector field</strong> that assigns a vector to each point <span class="arithmatex">\(x\)</span> in the data space. This vector has both:</p>
<ol>
<li>
<p><strong>Magnitude</strong>: How quickly the log probability changes</p>
</li>
<li>
<p><strong>Direction</strong>: The direction of steepest increase in log probability</p>
</li>
</ol>
<p><strong>Intuition</strong>: The score vector points "uphill" in the log probability landscape, indicating the direction where the model assigns higher probability.</p>
<p><strong>Example: Gaussian Distribution</strong></p>
<p>Consider a Gaussian distribution with mean <span class="arithmatex">\(\mu\)</span> and covariance <span class="arithmatex">\(\Sigma\)</span>:</p>
<div class="arithmatex">\[p(x) = \frac{1}{\sqrt{(2\pi)^d |\Sigma|}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu)\right)\]</div>
<p><strong>Log Probability:</strong></p>
<div class="arithmatex">\[\log p(x) = -\frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu) - \frac{1}{2}\log((2\pi)^d |\Sigma|)\]</div>
<p><strong>Score Function:</strong></p>
<div class="arithmatex">\[s(x) = \nabla_x \log p(x) = -\Sigma^{-1}(x - \mu)\]</div>
<p><strong>Interpretation:</strong></p>
<ul>
<li>
<p>The score points toward the mean <span class="arithmatex">\(\mu\)</span> (direction of higher probability)</p>
</li>
<li>
<p>The magnitude is proportional to the distance from the mean</p>
</li>
<li>
<p>For isotropic Gaussian (<span class="arithmatex">\(\Sigma = \sigma^2 I\)</span>): <span class="arithmatex">\(s(x) = -\frac{1}{\sigma^2}(x - \mu)\)</span></p>
</li>
</ul>
<p>This example shows how the score function naturally guides samples toward high-probability regions of the distribution.</p>
<h3 id="score-matching-comparing-distributions-via-vector-fields"><strong>Score Matching: Comparing Distributions via Vector Fields</strong></h3>
<p>The core idea of score matching is that we want to compare two probability distributions by comparing their respective vector fields of gradients (score functions).</p>
<p><strong>The Key Insight:</strong></p>
<p>Instead of directly comparing probability densities <span class="arithmatex">\(p_{data}(x)\)</span> and <span class="arithmatex">\(p_\theta(x)\)</span> (which requires computing the intractable partition function), we compare their score functions:</p>
<ul>
<li><strong>Data Score</strong>: <span class="arithmatex">\(s_{data}(x) = \nabla_x \log p_{data}(x)\)</span></li>
<li><strong>Model Score</strong>: <span class="arithmatex">\(s_\theta(x) = \nabla_x \log p_\theta(x) = \nabla_x f_\theta(x)\)</span></li>
</ul>
<p>This measures how different the "pointing directions" are at each location <span class="arithmatex">\(x\)</span>.</p>
<p><strong>L2 Distance Between Score Functions</strong></p>
<p>One way to compare the score functions is to calculate the average L2 distance between the score of <span class="arithmatex">\(p_{data}\)</span> and <span class="arithmatex">\(p_\theta\)</span>:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|s_\theta(x) - s_{data}(x)\|^2 \right]\]</div>
<p><strong>Note</strong>: This loss function is also called the <strong>Fisher divergence</strong> between <span class="arithmatex">\(p_{data}(x)\)</span> and <span class="arithmatex">\(p_\theta(x)\)</span>. The Fisher divergence measures the difference between two probability distributions by comparing their score functions (gradients of log densities) rather than the densities themselves.</p>
<p><strong>Understanding the L2 Distance:</strong></p>
<p>The L2 norm <span class="arithmatex">\(\|s_\theta(x) - s_{data}(x)\|^2\)</span> measures the squared Euclidean distance between two vectors:</p>
<div class="arithmatex">\[\|s_\theta(x) - s_{data}(x)\|^2 = \sum_{i=1}^d (s_\theta(x)_i - s_{data}(x)_i)^2\]</div>
<p>where <span class="arithmatex">\(d\)</span> is the dimension of the data space.</p>
<p><strong>Score matching</strong> is a method for training Energy-Based Models by minimizing the Fisher divergence between the data distribution <span class="arithmatex">\(p_{data}(x)\)</span> and the model distribution <span class="arithmatex">\(p_\theta(x)\)</span>:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|s_\theta(x) - s_{data}(x)\|^2 \right]\]</div>
<p>where <span class="arithmatex">\(s_\theta(x) = \nabla_x \log p_\theta(x)\)</span> and <span class="arithmatex">\(s_{data}(x) = \nabla_x \log p_{data}(x)\)</span> are the score functions of the model and data distributions respectively.</p>
<p>But how do we figure out <span class="arithmatex">\(\nabla_x \log p_{data}(x)\)</span> given only samples?</p>
<p><strong>Score Matching Reformulation (Univariate Case)</strong></p>
<p>For the univariate case where <span class="arithmatex">\(x \in \mathbb{R}\)</span>, we can rewrite the score matching objective to avoid needing the data score. Let's expand the squared difference:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \left(\frac{d}{dx} \log p_\theta(x) - \frac{d}{dx} \log p_{data}(x)\right)^2 \right]\]</div>
<p>Expanding the square:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \left(\frac{d}{dx} \log p_\theta(x)\right)^2 - \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} \log p_{data}(x) + \frac{1}{2} \left(\frac{d}{dx} \log p_{data}(x)\right)^2 \right]\]</div>
<p>The key insight is to use integration by parts on the cross term. For any function <span class="arithmatex">\(f(x)\)</span> and <span class="arithmatex">\(g(x)\)</span>:</p>
<div class="arithmatex">\[\int f(x) \frac{d}{dx} g(x) dx = f(x)g(x) - \int \frac{d}{dx} f(x) \cdot g(x) dx\]</div>
<p>Setting <span class="arithmatex">\(f(x) = \frac{d}{dx} \log p_\theta(x)\)</span> and <span class="arithmatex">\(g(x) = p_{data}(x)\)</span>, we get:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim p_{data}} \left[ \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} \log p_{data}(x) \right] = \int \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} \log p_{data}(x) \cdot p_{data}(x) dx\]</div>
<p>Using the chain rule: <span class="arithmatex">\(\frac{d}{dx} \log p_{data}(x) \cdot p_{data}(x) = \frac{d}{dx} p_{data}(x)\)</span>, we get:</p>
<div class="arithmatex">\[= \int \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} p_{data}(x) dx\]</div>
<p>Using integration by parts:</p>
<div class="arithmatex">\[= \left. \frac{d}{dx} \log p_\theta(x) \cdot p_{data}(x) \right|_{-\infty}^{\infty} - \int \frac{d^2}{dx^2} \log p_\theta(x) \cdot p_{data}(x) dx\]</div>
<p><strong>Why does the boundary term vanish?</strong></p>
<p>The boundary term <span class="arithmatex">\(\left. \frac{d}{dx} \log p_\theta(x) \cdot p_{data}(x) \right|_{-\infty}^{\infty}\)</span> vanishes under reasonable assumptions:</p>
<ol>
<li><strong>Data distribution decay</strong>: <span class="arithmatex">\(p_{data}(x) \to 0\)</span> as <span class="arithmatex">\(|x| \to \infty\)</span> (most real-world distributions have finite support or decay to zero)</li>
<li><strong>Model score boundedness</strong>: <span class="arithmatex">\(\frac{d}{dx} \log p_\theta(x)\)</span> grows at most polynomially as <span class="arithmatex">\(|x| \to \infty\)</span></li>
<li><strong>Product decay</strong>: The product <span class="arithmatex">\(\frac{d}{dx} \log p_\theta(x) \cdot p_{data}(x) \to 0\)</span> as <span class="arithmatex">\(|x| \to \infty\)</span></li>
</ol>
<p>This is a standard assumption in score matching literature and holds for most practical distributions.</p>
<p>Assuming the boundary term vanishes (which is reasonable for well-behaved distributions), we get:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim p_{data}} \left[ \frac{d}{dx} \log p_\theta(x) \cdot \frac{d}{dx} \log p_{data}(x) \right] = -\mathbb{E}_{x \sim p_{data}} \left[ \frac{d^2}{dx^2} \log p_\theta(x) \right]\]</div>
<p>Substituting back into the original objective:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \left(\frac{d}{dx} \log p_\theta(x)\right)^2 + \frac{d^2}{dx^2} \log p_\theta(x) \right] + \text{constant}\]</div>
<p>where the constant term <span class="arithmatex">\(\frac{1}{2} \mathbb{E}_{x \sim p_{data}} \left[ \left(\frac{d}{dx} \log p_{data}(x)\right)^2 \right]\)</span> doesn't depend on <span class="arithmatex">\(\theta\)</span> and can be ignored during optimization.</p>
<p><strong>Key Insight</strong>: This reformulation allows us to train the model using only samples from <span class="arithmatex">\(p_{data}(x)\)</span> and the derivatives of our model's log probability, without needing access to the data score function.</p>
<p><strong>Score Matching Reformulation (Multivariate Case)</strong></p>
<p>For the multivariate case where <span class="arithmatex">\(x \in \mathbb{R}^d\)</span>, we can extend the univariate derivation. The score matching objective becomes:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x) - \nabla_x \log p_{data}(x)\|^2 \right]\]</div>
<p>Expanding the squared norm:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x)\|^2 - \nabla_x \log p_\theta(x)^T \nabla_x \log p_{data}(x) + \frac{1}{2} \|\nabla_x \log p_{data}(x)\|^2 \right]\]</div>
<p>The key insight is to use integration by parts on the cross term. For the multivariate case, we need to handle each component separately. Let <span class="arithmatex">\(s_\theta(x)_i\)</span> and <span class="arithmatex">\(s_{data}(x)_i\)</span> denote the <span class="arithmatex">\(i\)</span>-th component of the respective score functions.</p>
<p>For each component <span class="arithmatex">\(i\)</span>, we have:</p>
<div class="arithmatex">\[\mathbb{E}_{x \sim p_{data}} \left[ s_\theta(x)_i \cdot s_{data}(x)_i \right] = \int s_\theta(x)_i \cdot s_{data}(x)_i \cdot p_{data}(x) dx\]</div>
<p>Using the chain rule: <span class="arithmatex">\(s_{data}(x)_i \cdot p_{data}(x) = \frac{\partial}{\partial x_i} p_{data}(x)\)</span>, we get:</p>
<div class="arithmatex">\[= \int s_\theta(x)_i \cdot \frac{\partial}{\partial x_i} p_{data}(x) dx\]</div>
<p>Using integration by parts (assuming boundary terms vanish):</p>
<div class="arithmatex">\[= -\int \frac{\partial}{\partial x_i} s_\theta(x)_i \cdot p_{data}(x) dx = -\mathbb{E}_{x \sim p_{data}} \left[ \frac{\partial}{\partial x_i} s_\theta(x)_i \right]\]</div>
<p><strong>Why do the boundary terms vanish in the multivariate case?</strong></p>
<p>For each component <span class="arithmatex">\(i\)</span>, the boundary term is:</p>
<div class="arithmatex">\[\left. s_\theta(x)_i \cdot p_{data}(x) \right|_{x_i = -\infty}^{x_i = \infty}\]</div>
<p>This vanishes under similar assumptions as the univariate case:</p>
<ol>
<li><strong>Data distribution decay</strong>: <span class="arithmatex">\(p_{data}(x) \to 0\)</span> as <span class="arithmatex">\(\|x\| \to \infty\)</span> in any direction</li>
<li><strong>Model score boundedness</strong>: Each component <span class="arithmatex">\(s_\theta(x)_i\)</span> grows at most polynomially as <span class="arithmatex">\(\|x\| \to \infty\)</span></li>
<li><strong>Product decay</strong>: The product <span class="arithmatex">\(s_\theta(x)_i \cdot p_{data}(x) \to 0\)</span> as <span class="arithmatex">\(\|x\| \to \infty\)</span> for each component</li>
</ol>
<p>These assumptions ensure that the boundary terms vanish for all components, allowing us to apply integration by parts component-wise.</p>
<p>Summing over all components:</p>
<div class="arithmatex">\[\sum_{i=1}^d \mathbb{E}_{x \sim p_{data}} \left[ s_\theta(x)_i \cdot s_{data}(x)_i \right] = -\sum_{i=1}^d \mathbb{E}_{x \sim p_{data}} \left[ \frac{\partial}{\partial x_i} s_\theta(x)_i \right] = -\mathbb{E}_{x \sim p_{data}} \left[ \text{tr}(\nabla_x s_\theta(x)) \right]\]</div>
<p>where <span class="arithmatex">\(\text{tr}(\nabla_x s_\theta(x)) = \sum_{i=1}^d \frac{\partial}{\partial x_i} s_\theta(x)_i\)</span> is the trace of the Jacobian matrix of the score function.</p>
<p>Substituting back into the original objective:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x)\|^2 + \text{tr}(\nabla_x \nabla_x \log p_\theta(x)) \right] + \text{constant}\]</div>
<p>where the constant term <span class="arithmatex">\(\frac{1}{2} \mathbb{E}_{x \sim p_{data}} \left[ \|\nabla_x \log p_{data}(x)\|^2 \right]\)</span> doesn't depend on <span class="arithmatex">\(\theta\)</span> and can be ignored during optimization.</p>
<p><strong>Key Insight</strong>: The multivariate case introduces the trace of the Hessian matrix <span class="arithmatex">\(\text{tr}(\nabla_x \nabla_x \log p_\theta(x))\)</span>.</p>
<h3 id="score-matching-algorithm">Score Matching Algorithm</h3>
<p>The score matching algorithm follows these steps:</p>
<p><strong>Sample a mini-batch of datapoints</strong>: <span class="arithmatex">\(\{x_1, x_2, \ldots, x_n\} \sim p_{data}(x)\)</span></p>
<p><strong>Estimate the score matching loss with the empirical mean</strong>: </p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) \approx \frac{1}{n} \sum_{i=1}^n \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x_i)\|^2 + \text{tr}(\nabla_x \nabla_x \log p_\theta(x_i)) \right]\]</div>
<p><strong>Stochastic gradient descent</strong>: Update parameters using gradients of the estimated loss</p>
<p><strong>Advantages:</strong>
* <strong>No need to sample from EBM</strong>: Unlike other training methods for energy-based models, score matching doesn't require generating samples from the model during training. This avoids the computational expense and potential instability of MCMC sampling.
* <strong>Direct optimization</strong>: The objective directly measures how well the model's score function matches the data distribution's score function.
* <strong>Theoretically sound</strong>: Score matching provides a consistent estimator under mild conditions.</p>
<p><strong>Disadvantages:</strong>
* <strong>Computing the Hessian is expensive</strong>: The term <span class="arithmatex">\(\text{tr}(\nabla_x \nabla_x \log p_\theta(x))\)</span> requires computing second derivatives, which scales quadratically with the input dimension and can be computationally prohibitive for large models.
* <strong>Memory requirements</strong>: Storing and computing Hessians for large neural networks requires significant memory.
* <strong>Numerical instability</strong>: Second derivatives can be numerically unstable, especially for deep networks.</p>
<p><strong>Computational Complexity:</strong>
For a model with <span class="arithmatex">\(d\)</span> input dimensions and <span class="arithmatex">\(m\)</span> parameters, computing the Hessian trace requires <span class="arithmatex">\(O(d^2 \cdot m)\)</span> operations, making it impractical for high-dimensional data like images.</p>
<h2 id="recap-distances-for-training-ebms">Recap: Distances for Training EBMs</h2>
<p>When training Energy-Based Models, we need to measure how close our model distribution <span class="arithmatex">\(p_\theta(x)\)</span> is to the data distribution <span class="arithmatex">\(p_{data}(x)\)</span>. Here are the main approaches:</p>
<h3 id="contrastive-divergence">Contrastive Divergence</h3>
<p>Contrastive divergence measures the difference between the data distribution and the model distribution using KL divergence:</p>
<div class="arithmatex">\[\mathcal{L}_{CD}(\theta) = D_{KL}(p_{data}(x) \| p_\theta(x)) - D_{KL}(p_\theta(x) \| p_{data}(x))\]</div>
<p><strong>Key insight</strong>: This objective encourages the model to match the data distribution while preventing mode collapse.</p>
<p><strong>Challenge</strong>: Computing the KL divergence requires sampling from the model distribution <span class="arithmatex">\(p_\theta(x)\)</span>, which is typically done using MCMC methods like Langevin dynamics or Hamiltonian Monte Carlo.</p>
<h3 id="fisher-divergence-score-matching">Fisher Divergence (Score Matching)</h3>
<p>Fisher divergence measures the difference between the score functions (gradients of log densities) of the two distributions:</p>
<div class="arithmatex">\[\mathcal{L}_{SM}(\theta) = \mathbb{E}_{x \sim p_{data}} \left[ \frac{1}{2} \|\nabla_x \log p_\theta(x) - \nabla_x \log p_{data}(x)\|^2 \right]\]</div>
<p><strong>Key insight</strong>: Instead of comparing probability densities directly, we compare their gradients, which avoids the need to compute the intractable partition function.</p>
<p><strong>Advantage</strong>: No need to sample from the model during training, making it computationally more efficient than contrastive divergence.</p>
<p><strong>Challenge</strong>: Requires computing second derivatives (Hessian) of the log probability, which can be expensive for high-dimensional data.</p>
<h2 id="noise-contrastive-estimation">Noise Contrastive Estimation</h2>
<p><strong>Learning an EBM by contrasting it with a noise distribution.</strong></p>
<p>We have the data distribution <span class="arithmatex">\(p_{data}(x)\)</span>. We have the noise distribution <span class="arithmatex">\(p_n(x)\)</span> which should be analytically tractable and easy to sample from. We can train a discriminator <span class="arithmatex">\(D(x) \in [0, 1]\)</span> to distinguish between data samples and noise samples.</p>
<h3 id="optimal-discriminator">Optimal Discriminator</h3>
<p>The optimal discriminator <span class="arithmatex">\(D^*(x)\)</span> that maximizes this objective is given by:</p>
<div class="arithmatex">\[D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\]</div>
<h3 id="parameterizing-the-discriminator-as-an-ebm">Parameterizing the Discriminator as an EBM</h3>
<p><strong>Key Insight</strong>: Instead of training a separate discriminator, we can parameterize it directly in terms of an Energy-Based Model.</p>
<p>Let's define a parameterized version of the discriminator as:</p>
<div class="arithmatex">\[D_\theta(x) = \frac{p_\theta(x)}{p_\theta(x) + p_n(x)}\]</div>
<p>where <span class="arithmatex">\(p_\theta(x) = \frac{1}{Z(\theta)} e^{f_\theta(x)}\)</span> is our Energy-Based Model.</p>
<p><strong>Implicit Learning of the Data Distribution</strong></p>
<p>By training the discriminator <span class="arithmatex">\(D_\theta(x)\)</span> to distinguish between data samples and noise samples, we are implicitly learning the Energy-Based Model <span class="arithmatex">\(p_\theta(x)\)</span> to approximate the true data distribution <span class="arithmatex">\(p_{data}(x)\)</span>.</p>
<p><strong>Why This Works:</strong></p>
<p>Recall that the optimal discriminator (when trained to perfection) satisfies:</p>
<div class="arithmatex">\[D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\]</div>
<p>But we've parameterized our discriminator as:</p>
<div class="arithmatex">\[D_\theta(x) = \frac{p_\theta(x)}{p_\theta(x) + p_n(x)}\]</div>
<p><strong>The Key Insight</strong>: When we train <span class="arithmatex">\(D_\theta(x)\)</span> to match the optimal discriminator <span class="arithmatex">\(D^*(x)\)</span>, we're essentially forcing:</p>
<div class="arithmatex">\[\frac{p_\theta(x)}{p_\theta(x) + p_n(x)} \approx \frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\]</div>
<p>This equality holds if and only if <span class="arithmatex">\(p_\theta(x) \approx p_{data}(x)\)</span> (assuming <span class="arithmatex">\(p_n(x) &gt; 0\)</span> everywhere).</p>
<p><strong>Mathematical Justification:</strong></p>
<p>If <span class="arithmatex">\(\frac{p_\theta(x)}{p_\theta(x) + p_n(x)} = \frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\)</span>, then:</p>
<div class="arithmatex">\[p_\theta(x) \cdot (p_{data}(x) + p_n(x)) = p_{data}(x) \cdot (p_\theta(x) + p_n(x))\]</div>
<div class="arithmatex">\[p_\theta(x) \cdot p_{data}(x) + p_\theta(x) \cdot p_n(x) = p_{data}(x) \cdot p_\theta(x) + p_{data}(x) \cdot p_n(x)\]</div>
<div class="arithmatex">\[p_\theta(x) \cdot p_n(x) = p_{data}(x) \cdot p_n(x)\]</div>
<p>Since <span class="arithmatex">\(p_n(x) &gt; 0\)</span>, we can divide both sides to get:</p>
<div class="arithmatex">\[p_\theta(x) = p_{data}(x)\]</div>
<p><strong>Modeling the Partition Function as a Trainable Parameter</strong></p>
<p><strong>The EBM Equation:</strong></p>
<p>Our Energy-Based Model is defined as:</p>
<div class="arithmatex">\[p_\theta(x) = \frac{1}{Z(\theta)} e^{f_\theta(x)}\]</div>
<p>where <span class="arithmatex">\(f_\theta(x)\)</span> is the energy function (neural network) and <span class="arithmatex">\(Z(\theta) = \int e^{f_\theta(x)} dx\)</span> is the partition function.</p>
<p><strong>The Partition Function Constraint Problem:</strong></p>
<p>The constraint <span class="arithmatex">\(Z(\theta) = \int e^{f_\theta(x)} dx\)</span> is computationally intractable to satisfy exactly because:</p>
<ol>
<li><strong>High-dimensional integration</strong>: Computing <span class="arithmatex">\(\int e^{f_\theta(x)} dx\)</span> over high-dimensional spaces is extremely expensive</li>
<li><strong>No closed form</strong>: For complex energy functions, there's no analytical solution</li>
<li><strong>Dynamic updates</strong>: The integral changes every time we update the energy function parameters</li>
</ol>
<p><strong>Solution: Treat Z as a Trainable Parameter</strong></p>
<p>Instead of enforcing the constraint, we model <span class="arithmatex">\(Z(\theta)\)</span> as an additional trainable parameter <span class="arithmatex">\(Z\)</span> that is not explicitly constrained to satisfy <span class="arithmatex">\(Z = \int e^{f_\theta(x)} dx\)</span>.</p>
<p>This gives us the modified EBM:</p>
<div class="arithmatex">\[p_{\theta, Z}(x) = \frac{e^{f_\theta(x)}}{Z}\]</div>
<p><strong>Why Z Converges to the Correct Partition Function:</strong></p>
<p>As we train <span class="arithmatex">\(p_{\theta, Z}(x)\)</span> to approximate <span class="arithmatex">\(p_{data}(x)\)</span>, the parameter <span class="arithmatex">\(Z\)</span> automatically converges to the correct partition function value.</p>
<p><strong>Mathematical Justification:</strong></p>
<p>When training converges, we have <span class="arithmatex">\(p_{\theta, Z}(x) \approx p_{data}(x)\)</span>. This means:</p>
<div class="arithmatex">\[\frac{e^{f_\theta(x)}}{Z} \approx p_{data}(x)\]</div>
<p>A direct argument comes from the fact that <span class="arithmatex">\(p_{\theta, Z}(x)\)</span> must approximate <span class="arithmatex">\(p_{data}(x)\)</span>, which must integrate to 1:</p>
<div class="arithmatex">\[\int p_{\theta, Z}(x) dx = \int \frac{e^{f_\theta(x)}}{Z} dx \approx 1\]</div>
<p>This immediately gives us:</p>
<div class="arithmatex">\[Z \approx \int e^{f_\theta(x)} dx\]</div>
<p><strong>Deriving the Discriminator for the Modified EBM</strong></p>
<p>Now let's derive the discriminator <span class="arithmatex">\(D_{\theta, Z}(x)\)</span> for our modified EBM <span class="arithmatex">\(p_{\theta, Z}(x) = \frac{e^{f_\theta(x)}}{Z}\)</span>.</p>
<p>Starting with the discriminator definition:</p>
<div class="arithmatex">\[D_{\theta, Z}(x) = \frac{p_{\theta, Z}(x)}{p_{\theta, Z}(x) + p_n(x)}\]</div>
<p>Substituting our modified EBM:</p>
<div class="arithmatex">\[D_{\theta, Z}(x) = \frac{\frac{e^{f_\theta(x)}}{Z}}{\frac{e^{f_\theta(x)}}{Z} + p_n(x)}\]</div>
<div class="arithmatex">\[D_{\theta, Z}(x) = \frac{e^{f_\theta(x)}}{e^{f_\theta(x)} + Z \cdot p_n(x)}\]</div>
<p><strong>Noise Contrastive Estimation Training Objective</strong></p>
<p>The NCE objective maximizes the log-likelihood of correctly classifying data vs noise samples:</p>
<div class="arithmatex">\[\mathcal{L}_{NCE}(\theta, Z) = \mathbb{E}_{x \sim p_{data}} \left[ \log D_{\theta, Z}(x) \right] + \mathbb{E}_{x \sim p_n} \left[ \log(1 - D_{\theta, Z}(x)) \right]\]</div>
<p>In theory, we could have any noise distribution to make this work. But in pratice, a noise distribution that similar (if we can manage) to the data distribution works very well. At the end of the day you learn an EBM and you learn a partition function. In the limit of infinite data and perfect optimization, the EBM matches the data distribution and Z matches the true partition function of the EBM.</p>
<p>There is no evolving Generator like we had in GAN. The generator here is fixed, which is the noise distribution. We are training a special Discriminator.</p>
<p>Note: The NCE objective function does not guide us to sample any data. We still need to use something like MCMC (e.g., Langevin dynamics, Hamiltonian Monte Carlo) to generate samples from the trained EBM. NCE only provides a way to train the energy function and partition function without computing the intractable partition function integral.</p>
<p>Substituting our discriminator:</p>
<div class="arithmatex">\[\mathcal{L}_{NCE}(\theta, Z) = \mathbb{E}_{x \sim p_{data}} \left[ \log \frac{e^{f_\theta(x)}}{e^{f_\theta(x)} + Z \cdot p_n(x)} \right] + \mathbb{E}_{x \sim p_n} \left[ \log \frac{Z \cdot p_n(x)}{e^{f_\theta(x)} + Z \cdot p_n(x)} \right]\]</div>
<p>Using the sigmoid formulation with <span class="arithmatex">\(h_{\theta, Z}(x) = f_\theta(x) - \log p_n(x) - \log Z\)</span>:</p>
<div class="arithmatex">\[\mathcal{L}_{NCE}(\theta, Z) = \mathbb{E}_{x \sim p_{data}} \left[ \log \sigma(h_{\theta, Z}(x)) \right] + \mathbb{E}_{x \sim p_n} \left[ \log(1 - \sigma(h_{\theta, Z}(x))) \right]\]</div>
<p>This is exactly the binary cross-entropy loss for a binary classifier that distinguishes between data and noise samples.</p>
<p><strong>Training Algorithm</strong>:</p>
<ol>
<li>
<p>Sample data batch: <span class="arithmatex">\(\{x_1, x_2, \ldots, x_n\} \sim p_{data}(x)\)</span></p>
</li>
<li>
<p>Sample noise batch: <span class="arithmatex">\(\{\tilde{x}_1, \tilde{x}_2, \ldots, \tilde{x}_n\} \sim p_n(x)\)</span></p>
</li>
<li>
<p>Compute logits: <span class="arithmatex">\(h_{\theta, Z}(x_i)\)</span> and <span class="arithmatex">\(h_{\theta, Z}(\tilde{x}_i)\)</span></p>
</li>
<li>
<p>Compute binary cross-entropy loss</p>
</li>
<li>
<p>Update both <span class="arithmatex">\(\theta\)</span> (energy function parameters) and <span class="arithmatex">\(Z\)</span> (partition function parameter) via gradient descent</p>
</li>
</ol>
<p><strong>Key Advantage</strong>: Unlike other EBM training methods (like contrastive divergence), NCE does not require sampling from the EBM during the training process. This eliminates the computational expense and potential instability of MCMC sampling during training, making NCE much more efficient and stable.</p>
<h2 id="comparing-nce-and-gan">Comparing NCE and GAN</h2>
<p>Both NCE and GANs use binary classification objectives to train generative models, but they differ significantly in their approach and properties.</p>
<p><strong>Similarities</strong></p>
<ol>
<li><strong>Binary Classification Objective</strong>: Both use binary cross-entropy loss to distinguish between real and fake samples</li>
<li><strong>No Likelihood Computation</strong>: Neither requires computing or maximizing explicit likelihood</li>
<li><strong>Stable Training</strong>: Both avoid the computational challenges of direct likelihood-based training</li>
</ol>
<p><strong>Key Differences</strong></p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>NCE</th>
<th>GAN</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Generator</strong></td>
<td>Fixed noise distribution <span class="arithmatex">\(p_n(x)\)</span></td>
<td>Learnable generator network <span class="arithmatex">\(G_\phi(z)\)</span></td>
</tr>
<tr>
<td><strong>Discriminator</strong></td>
<td>Parameterized as EBM: <span class="arithmatex">\(D_{\theta,Z}(x) = \frac{e^{f_\theta(x)}}{e^{f_\theta(x)} + Z \cdot p_n(x)}\)</span></td>
<td>Separate neural network <span class="arithmatex">\(D_\theta(x)\)</span></td>
</tr>
<tr>
<td><strong>Training</strong></td>
<td>Single objective: <span class="arithmatex">\(\mathcal{L}_{NCE}(\theta, Z)\)</span></td>
<td>Min-max game: <span class="arithmatex">\(\min_G \max_D \mathcal{L}_{GAN}(G, D)\)</span></td>
</tr>
<tr>
<td><strong>Sampling</strong></td>
<td>Requires MCMC after training</td>
<td>Direct sampling via generator</td>
</tr>
<tr>
<td><strong>Mode Coverage</strong></td>
<td>Depends on noise distribution choice</td>
<td>Can adapt to cover all data modes</td>
</tr>
<tr>
<td><strong>Convergence</strong></td>
<td>Single optimization problem</td>
<td>Requires careful balance between generator and discriminator</td>
</tr>
</tbody>
</table>
<p><strong>When to Use Each</strong></p>
<p><strong>Use NCE when:</strong>
- You need interpretable energy functions
- Training stability is crucial
- You want theoretical guarantees
- You can afford MCMC sampling at inference time</p>
<p><strong>Use GAN when:</strong>
- Fast sampling is required
- You need high-quality, diverse samples
- You have computational resources for adversarial training
- You want to avoid MCMC entirely</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>