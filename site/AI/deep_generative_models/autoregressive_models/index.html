
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/ai/deep_generative_models/autoregressive_models/">
      
      
        <link rel="prev" href="../introduction/">
      
      
        <link rel="next" href="../variational_autoencoders/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Autoregressive Models - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#autoregressive-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Autoregressive Models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-tabs__link">
          
  
  
  Productivity

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#representation" class="md-nav__link">
    <span class="md-ellipsis">
      Representation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-and-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Learning and inference
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_generative_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Generative Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluating_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluating Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_diffusion_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors, vector addition, and scalar multiplication
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector geometry in Rn and correlation coefficients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/planes_in_r3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Planes in R3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/span_subspaces_and_dimension/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Span, subspaces, and dimension
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/basis_and_orthogonality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basis and orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Applications of projections in Rn- orthogonal bases of planes and linear regression
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/story_proofs_and_axioms_of_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Story Proofs and Axioms of Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/monty_hall_problem_and_simpsons_paradox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Monty Hall Problem and Simpson's Paradox
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Productivity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Productivity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    How to Build Your Career in AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            How to Build Your Career in AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Three Steps to Career Growth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_1_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 1- Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_2_projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 2- Projects
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_3_job/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 3- Job
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="autoregressive-models">Autoregressive models</h1>
<p>We assume we are given access to a dataset:
$$
\mathcal{D} = { \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_m }
$$
where each datapoint is n-dimensional. For simplicity, we assume the datapoints are binary.
$$
x_i \in {0,1}^n
$$</p>
<h2 id="representation">Representation</h2>
<p>If you have n random variables:
$$
X_1, X_2, \dots, X_n
$$
then their joint probability can be written as a product of conditional probabilities:
$$
P(X_1, X_2, \dots, X_n) = P(X_1) \cdot P(X_2 \mid X_1) \cdot P(X_3 \mid X_1, X_2) \cdot \dots \cdot P(X_n \mid X_1, X_2, \dots, X_{n-1})
$$
In words:</p>
<blockquote>
<p>The probability of all n variables taking particular values equals:
→ the probability of the <strong>first variable</strong>,<br />
→ times the probability of the <strong>second variable given the first</strong>,<br />
→ times the probability of the <strong>third variable given the first two</strong>,<br />
→ and so on, until the n-th variable.</p>
</blockquote>
<p>By this chain rule of probability, we can factorize the joint distribution over the n-dimensions as:</p>
<div class="arithmatex">\[
p(\mathbf{x}) = \prod_{i=1}^n p(x_i \mid x_1, x_2, \dots, x_{i-1}) = \prod_{i=1}^n p(x_i \mid x_{&lt;i})
\]</div>
<p>where</p>
<div class="arithmatex">\[
x_{&lt;i} = [x_1, x_2, \dots, x_{i-1}]
\]</div>
<p>denotes the vector of random variables with index less than i.</p>
<p>The chain rule factorization can be expressed graphically as a Bayesian network.</p>
<p><img alt="Bayesian network with no conditional independence assumptions" src="../no_assumption_bn.png" /></p>
<p>Such a Bayesian network that makes no conditional independence assumptions is said to obey the <em>autoregressive</em> property. The term <em>autoregressive</em> originates from the literature on time-series models where observations from the previous time-steps are used to predict the value at the current time step. Here, we fix an ordering of the variables x1, x2, …, xn and the distribution for the i-th random variable depends on the values of all the preceding random variables in the chosen ordering x1, x2, …, xi−1.</p>
<p>If we allow for every conditional p(xi|x&lt;i) to be specified in a tabular form, then such a representation is fully general and can represent any possible distribution over n random variables. However, the space complexity for such a representation grows exponentially with n.</p>
<p>To see why, let us consider the conditional for the last dimension, given by p(xn|x&lt;n). In order to fully specify this conditional, we need to specify a probability for 2^(n−1) configurations of the variables x1, x2, …, xn−1. Since the probabilities should sum to 1, the total number of parameters for specifying this conditional is given by 2^(n−1)−1. Hence, a tabular representation for the conditionals is impractical for learning the joint distribution factorized via chain rule.</p>
<p>In an autoregressive generative model, the conditionals are specified as parameterized functions with a fixed number of parameters. Specifically, we assume that each conditional distribution corresponds to a Bernoulli random variable. We then learn a function that maps the preceding random variables to the parameter (mean) of this Bernoulli distribution. Hence, we have:</p>
<div class="arithmatex">\[
p_{\theta_i}(x_i \mid x_{&lt;i}) = \text{Bern} \left( f_i(x_1, x_2, \dots, x_{i-1}) \right)
\]</div>
<p>where the function is defined as:</p>
<div class="arithmatex">\[
f_i : \{0,1\}^{i-1} \to [0,1]
\]</div>
<p>and theta_i denotes the set of parameters used to specify this function.
This function takes in a vector of size (i-1) where each element is a 0 or a 1, and outputs a scalar bit.</p>
<p>The total number of parameters in an autoregressive generative model is given by:</p>
<div class="arithmatex">\[
\sum_{i=1}^n \left| \theta_i \right|
\]</div>
<p>In the simplest case, we can specify the function as a linear combination of the input elements followed by a sigmoid non-linearity (to restrict the output to lie between 0 and 1). This gives us the formulation of a fully-visible sigmoid belief network (FVSBN).</p>
<div class="arithmatex">\[
f_i(x_1, x_2, \dots, x_{i-1}) = \sigma(\alpha^{(i)}_0 + \alpha^{(i)}_1 x_1 + \dots + \alpha^{(i)}_{i-1} x_{i-1})
\]</div>
<p>where <span class="arithmatex">\(\sigma\)</span> denotes the sigmoid function and <span class="arithmatex">\(\theta_i = \{\alpha^{(i)}_0, \alpha^{(i)}_1, \dots, \alpha^{(i)}_{i-1}\}\)</span> denote the parameters of the mean function. The conditional for variable <span class="arithmatex">\(i\)</span> requires <span class="arithmatex">\(i\)</span> parameters, and hence the total number of parameters in the model is given by <span class="arithmatex">\(\sum_{i=1}^n i = O(n^2)\)</span>. Note that the number of parameters are much fewer than the exponential complexity of the tabular case.</p>
<p>A natural way to increase the expressiveness of an autoregressive generative model is to use more flexible parameterizations for the mean function e.g., multi-layer perceptrons (MLP). For example, consider the case of a neural network with 1 hidden layer. The mean function for variable <span class="arithmatex">\(i\)</span> can be expressed as</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{h}_i &amp;= \sigma(\mathbf{A}_i \mathbf{x}_{&lt;i} + \mathbf{c}_i) \\
f_i(x_1, x_2, \dots, x_{i-1}) &amp;= \sigma(\boldsymbol{\alpha}^{(i)} \mathbf{h}_i + b_i)
\end{align}
\]</div>
<p>where <span class="arithmatex">\(\mathbf{h}_i \in \mathbb{R}^d\)</span> denotes the hidden layer activations for the MLP and <span class="arithmatex">\(\theta_i = \{\mathbf{A}_i \in \mathbb{R}^{d \times (i-1)}, \mathbf{c}_i \in \mathbb{R}^d, \boldsymbol{\alpha}^{(i)} \in \mathbb{R}^d, b_i \in \mathbb{R}\}\)</span> are the set of parameters for the mean function <span class="arithmatex">\(\mu_i(\cdot)\)</span>. The total number of parameters in this model is dominated by the matrices <span class="arithmatex">\(\mathbf{A}_i\)</span> and given by <span class="arithmatex">\(O(n^2d)\)</span>.</p>
<blockquote>
<p><strong>Note</strong>: The term "mean function" here refers to the function that determines the mean (expected value) of the Bernoulli distribution for each variable. Since we're modeling binary variables, the mean of the Bernoulli distribution is the probability of the variable being 1. The sigmoid function <span class="arithmatex">\(\sigma\)</span> ensures that this probability lies between 0 and 1.</p>
</blockquote>
<p>For a Bernoulli random variable <span class="arithmatex">\(X\)</span> with parameter <span class="arithmatex">\(p\)</span>, the expectation (mean) is given by:</p>
<div class="arithmatex">\[
\mathbb{E}[X] = 1 \cdot p + 0 \cdot (1-p) = p
\]</div>
<p>This is because:
- <span class="arithmatex">\(X\)</span> takes value 1 with probability <span class="arithmatex">\(p\)</span>
- <span class="arithmatex">\(X\)</span> takes value 0 with probability <span class="arithmatex">\((1-p)\)</span>
- The expectation is the weighted sum of all possible values, where the weights are their respective probabilities</p>
<p>Therefore, when we say the mean function determines the mean of the Bernoulli distribution, we're saying it determines the probability <span class="arithmatex">\(p\)</span> of the variable being 1.</p>
<p>The Neural Autoregressive Density Estimator (NADE) provides an alternate MLP-based parameterization that is more statistically and computationally efficient than the vanilla approach. In NADE, parameters are shared across the functions used for evaluating the conditionals. In particular, the hidden layer activations are specified as</p>
<div class="arithmatex">\[
\begin{align}
\mathbf{h}_i &amp;= \sigma(\mathbf{W}_{.,&lt;i} \mathbf{x}_{&lt;i} + \mathbf{c}) \\
f_i(x_1, x_2, \dots, x_{i-1}) &amp;= \sigma(\boldsymbol{\alpha}^{(i)} \mathbf{h}_i + b_i)
\end{align}
\]</div>
<p>where <span class="arithmatex">\(\theta = \{\mathbf{W} \in \mathbb{R}^{d \times n}, \mathbf{c} \in \mathbb{R}^d, \{\boldsymbol{\alpha}^{(i)} \in \mathbb{R}^d\}_{i=1}^n, \{b_i \in \mathbb{R}\}_{i=1}^n\}\)</span> is the full set of parameters for the mean functions <span class="arithmatex">\(f_1(\cdot), f_2(\cdot), \dots, f_n(\cdot)\)</span>. The weight matrix <span class="arithmatex">\(\mathbf{W}\)</span> and the bias vector <span class="arithmatex">\(\mathbf{c}\)</span> are shared across the conditionals. Sharing parameters offers two benefits:</p>
<ol>
<li>
<p>The total number of parameters gets reduced from <span class="arithmatex">\(O(n^2d)\)</span> to <span class="arithmatex">\(O(nd)\)</span>.</p>
</li>
<li>
<p>The hidden unit activations can be evaluated in <span class="arithmatex">\(O(nd)\)</span> time via the following recursive strategy:</p>
</li>
</ol>
<div class="arithmatex">\[
\begin{align}
\mathbf{h}_i &amp;= \sigma(\mathbf{a}_i) \\
\mathbf{a}_{i+1} &amp;= \mathbf{a}_i + \mathbf{W}_{[.,i]} x_i
\end{align}
\]</div>
<p>with the base case given by <span class="arithmatex">\(\mathbf{a}_1 = \mathbf{c}\)</span>.</p>
<p>The RNADE algorithm extends NADE to learn generative models over real-valued data. Here, the conditionals are modeled via a continuous distribution such as a equi-weighted mixture of <span class="arithmatex">\(K\)</span> Gaussians. Instead of learning a mean function, we now learn the means <span class="arithmatex">\(\mu_{i,1}, \mu_{i,2}, \dots, \mu_{i,K}\)</span> and variances <span class="arithmatex">\(\Sigma_{i,1}, \Sigma_{i,2}, \dots, \Sigma_{i,K}\)</span> of the <span class="arithmatex">\(K\)</span> Gaussians for every conditional. For statistical and computational efficiency, a single function <span class="arithmatex">\(g_i: \mathbb{R}^{i-1} \to \mathbb{R}^{2K}\)</span> outputs all the means and variances of the <span class="arithmatex">\(K\)</span> Gaussians for the <span class="arithmatex">\(i\)</span>-th conditional distribution.</p>
<p>The conditional distribution <span class="arithmatex">\(p_{\theta_i}(x_i \mid \mathbf{x}_{&lt;i})\)</span> in RNADE is given by:</p>
<div class="arithmatex">\[
p_{\theta_i}(x_i \mid \mathbf{x}_{&lt;i}) = \frac{1}{K} \sum_{k=1}^K \mathcal{N}(x_i; \mu_{i,k}, \Sigma_{i,k})
\]</div>
<p>where <span class="arithmatex">\(\mathcal{N}(x; \mu, \Sigma)\)</span> denotes the probability density of a Gaussian distribution with mean <span class="arithmatex">\(\mu\)</span> and variance <span class="arithmatex">\(\Sigma\)</span> evaluated at <span class="arithmatex">\(x\)</span>. The parameters <span class="arithmatex">\(\{\mu_{i,k}, \Sigma_{i,k}\}_{k=1}^K\)</span> are the outputs of the function <span class="arithmatex">\(g_i(\mathbf{x}_{&lt;i})\)</span>.</p>
<p>This is how RNADE is autoregressive. Example sequence showing autoregressive dependencies:</p>
<p><span class="arithmatex">\(x_1\)</span>: 
  - Input to <span class="arithmatex">\(g_1\)</span>: <span class="arithmatex">\(\mathbf{x}_{&lt;1} = []\)</span> (empty)
  - Output: <span class="arithmatex">\(\{\mu_{1,k}, \Sigma_{1,k}\}_{k=1}^K\)</span> for <span class="arithmatex">\(p(x_1)\)</span></p>
<p><span class="arithmatex">\(x_2\)</span>: 
  - Input to <span class="arithmatex">\(g_2\)</span>: <span class="arithmatex">\(\mathbf{x}_{&lt;2} = [x_1]\)</span>
  - Output: <span class="arithmatex">\(\{\mu_{2,k}, \Sigma_{2,k}\}_{k=1}^K\)</span> for <span class="arithmatex">\(p(x_2 \mid x_1)\)</span></p>
<p><span class="arithmatex">\(x_3\)</span>: 
  - Input to <span class="arithmatex">\(g_3\)</span>: <span class="arithmatex">\(\mathbf{x}_{&lt;3} = [x_1, x_2]\)</span>
  - Output: <span class="arithmatex">\(\{\mu_{3,k}, \Sigma_{3,k}\}_{k=1}^K\)</span> for <span class="arithmatex">\(p(x_3 \mid x_1, x_2)\)</span></p>
<p><span class="arithmatex">\(x_4\)</span>: 
  - Input to <span class="arithmatex">\(g_4\)</span>: <span class="arithmatex">\(\mathbf{x}_{&lt;4} = [x_1, x_2, x_3]\)</span>
  - Output: <span class="arithmatex">\(\{\mu_{4,k}, \Sigma_{4,k}\}_{k=1}^K\)</span> for <span class="arithmatex">\(p(x_4 \mid x_1, x_2, x_3)\)</span></p>
<p>This sequential, conditional generation process is what makes RNADE an autoregressive model. The mixture of Gaussians is just the form of the conditional distribution, but the autoregressive property comes from how these distributions are parameterized based on previous variables.</p>
<h2 id="learning-and-inference">Learning and inference</h2>
<p>Recall that learning a generative model involves optimizing the closeness between the data and model distributions. One commonly used notion of closeness is the KL divergence between the data and the model distributions:</p>
<div class="arithmatex">\[
\min_{\theta \in \Theta} d_{KL}(p_{data}, p_{\theta}) = \min_{\theta \in \Theta} \mathbb{E}_{x \sim p_{data}}[\log p_{data}(x) - \log p_{\theta}(x)]
\]</div>
<p>where:
- <span class="arithmatex">\(p_{data}\)</span> is the true data distribution
- <span class="arithmatex">\(p_{\theta}\)</span> is our model distribution parameterized by <span class="arithmatex">\(\theta\)</span>
- <span class="arithmatex">\(\Theta\)</span> is the set of all possible parameter values
- <span class="arithmatex">\(d_{KL}\)</span> is the Kullback-Leibler divergence</p>
<p>Let's break down how this minimization works:</p>
<ol>
<li>For a fixed value of <span class="arithmatex">\(\theta\)</span>, we compute:</li>
<li>The expectation over all possible data points <span class="arithmatex">\(x\)</span> from <span class="arithmatex">\(p_{data}\)</span></li>
<li>For each <span class="arithmatex">\(x\)</span>, we compute <span class="arithmatex">\(\log p_{data}(x) - \log p_{\theta}(x)\)</span></li>
<li>
<p>This gives us a single scalar value for this particular <span class="arithmatex">\(\theta\)</span></p>
</li>
<li>
<p>The minimization operator <span class="arithmatex">\(\min_{\theta \in \Theta}\)</span> then:</p>
</li>
<li>Tries different values of <span class="arithmatex">\(\theta\)</span> in the parameter space <span class="arithmatex">\(\Theta\)</span></li>
<li>
<p>Finds the <span class="arithmatex">\(\theta\)</span> that gives the smallest expected value</p>
</li>
<li>
<p>Since <span class="arithmatex">\(p_{data}\)</span> is constant with respect to <span class="arithmatex">\(\theta\)</span>, minimizing the KL divergence is equivalent to maximizing the expected log-likelihood of the data under our model:</p>
</li>
</ol>
<div class="arithmatex">\[
\max_{\theta \in \Theta} \mathbb{E}_{x \sim p_{data}}[\log p_{\theta}(x)]
\]</div>
<p>This is because <span class="arithmatex">\(\log p_{data}(x)\)</span> doesn't depend on <span class="arithmatex">\(\theta\)</span>, so it can be treated as a constant. Minimizing <span class="arithmatex">\(-\log p_{\theta}(x)\)</span> is the same as maximizing <span class="arithmatex">\(\log p_{\theta}(x)\)</span></p>
<p>To approximate the expectation over the unknown <span class="arithmatex">\(p_{data}\)</span>, we make an assumption: points in the dataset <span class="arithmatex">\(\mathcal{D}\)</span> are sampled i.i.d. from <span class="arithmatex">\(p_{data}\)</span>. This allows us to obtain an unbiased Monte Carlo estimate of the objective as:</p>
<div class="arithmatex">\[
\max_{\theta \in \Theta} \frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} \log p_{\theta}(x) = \mathcal{L}(\theta | \mathcal{D})
\]</div>
<p>The maximum likelihood estimation (MLE) objective has an intuitive interpretation: pick the model parameters <span class="arithmatex">\(\theta \in \Theta\)</span> that maximize the log-probability of the observed datapoints in <span class="arithmatex">\(\mathcal{D}\)</span>.</p>
<p>In practice, we optimize the MLE objective using mini-batch gradient ascent. The algorithm operates in iterations. At every iteration <span class="arithmatex">\(t\)</span>, we sample a mini-batch <span class="arithmatex">\(\mathcal{B}_t\)</span> of datapoints sampled randomly from the dataset (<span class="arithmatex">\(|\mathcal{B}_t| &lt; |\mathcal{D}|\)</span>) and compute gradients of the objective evaluated for the mini-batch. These parameters at iteration <span class="arithmatex">\(t+1\)</span> are then given via the following update rule:</p>
<div class="arithmatex">\[
\theta^{(t+1)} = \theta^{(t)} + r_t \nabla_{\theta} \mathcal{L}(\theta^{(t)} | \mathcal{B}_t)
\]</div>
<p>where <span class="arithmatex">\(\theta^{(t+1)}\)</span> and <span class="arithmatex">\(\theta^{(t)}\)</span> are the parameters at iterations <span class="arithmatex">\(t+1\)</span> and <span class="arithmatex">\(t\)</span> respectively, and <span class="arithmatex">\(r_t\)</span> is the learning rate at iteration <span class="arithmatex">\(t\)</span>. Typically, we only specify the initial learning rate <span class="arithmatex">\(r_1\)</span> and update the rate based on a schedule.</p>
<p>Now that we have a well-defined objective and optimization procedure, the only remaining task is to evaluate the objective in the context of an autoregressive generative model. To this end, we first write the MLE objective in terms of the joint probability:</p>
<div class="arithmatex">\[
\max_{\theta \in \Theta} \frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} \log p_{\theta}(x)
\]</div>
<p>Then, we substitute the factorized joint distribution of an autoregressive model. Since <span class="arithmatex">\(p_{\theta}(x) = \prod_{i=1}^n p_{\theta_i}(x_i | x_{&lt;i})\)</span>, we have:</p>
<div class="arithmatex">\[
\log p_{\theta}(x) = \log \prod_{i=1}^n p_{\theta_i}(x_i | x_{&lt;i}) = \sum_{i=1}^n \log p_{\theta_i}(x_i | x_{&lt;i})
\]</div>
<p>Substituting this into the MLE objective, we get:</p>
<div class="arithmatex">\[
\max_{\theta \in \Theta} \frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} \sum_{i=1}^n \log p_{\theta_i}(x_i | x_{&lt;i})
\]</div>
<p>where <span class="arithmatex">\(\theta = \{\theta_1, \theta_2, \dots, \theta_n\}\)</span> now denotes the collective set of parameters for the conditionals.</p>
<p>Inference in an autoregressive model is straightforward. For density estimation of an arbitrary point <span class="arithmatex">\(x\)</span>, we simply evaluate the log-conditionals <span class="arithmatex">\(\log p_{\theta_i}(x_i | x_{&lt;i})\)</span> for each <span class="arithmatex">\(i\)</span> and add these up to obtain the log-likelihood assigned by the model to <span class="arithmatex">\(x\)</span>. Since we have the complete vector <span class="arithmatex">\(x = [x_1, x_2, \dots, x_n]\)</span>, we know all the values needed for each conditional <span class="arithmatex">\(x_{&lt;i}\)</span>, so each of the conditionals can be evaluated in parallel. Hence, density estimation is efficient on modern hardware.</p>
<p>For example, given a 4-dimensional vector <span class="arithmatex">\(x = [x_1, x_2, x_3, x_4]\)</span>, we can compute all conditionals in parallel:</p>
<ul>
<li><span class="arithmatex">\(\log p_{\theta_1}(x_1)\)</span> (no conditioning needed)</li>
<li><span class="arithmatex">\(\log p_{\theta_2}(x_2 | x_1)\)</span> (using known <span class="arithmatex">\(x_1\)</span>)</li>
<li><span class="arithmatex">\(\log p_{\theta_3}(x_3 | x_1, x_2)\)</span> (using known <span class="arithmatex">\(x_1, x_2\)</span>)</li>
<li><span class="arithmatex">\(\log p_{\theta_4}(x_4 | x_1, x_2, x_3)\)</span> (using known <span class="arithmatex">\(x_1, x_2, x_3\)</span>)</li>
</ul>
<p>Then sum them to get the total log-likelihood: <span class="arithmatex">\(\log p_{\theta}(x) = \sum_{i=1}^4 \log p_{\theta_i}(x_i | x_{&lt;i})\)</span></p>
<p>Sampling from an autoregressive model is a sequential procedure. Here, we first sample <span class="arithmatex">\(x_1\)</span>, then we sample <span class="arithmatex">\(x_2\)</span> conditioned on the sampled <span class="arithmatex">\(x_1\)</span>, followed by <span class="arithmatex">\(x_3\)</span> conditioned on both <span class="arithmatex">\(x_1\)</span> and <span class="arithmatex">\(x_2\)</span> and so on until we sample <span class="arithmatex">\(x_n\)</span> conditioned on the previously sampled <span class="arithmatex">\(x_{&lt;n}\)</span>. For applications requiring real-time generation of high-dimensional data such as audio synthesis, the sequential sampling can be an expensive process.</p>
<p>Finally, an autoregressive model does not directly learn unsupervised representations of the data. This is because:</p>
<ol>
<li>The model directly models the data distribution <span class="arithmatex">\(p(x)\)</span> through a sequence of conditional distributions <span class="arithmatex">\(p(x_i | x_{&lt;i})\)</span></li>
<li>There is no explicit latent space or bottleneck that forces the model to learn a compressed representation</li>
<li>Each variable is modeled based on previous variables, but there's no mechanism to learn a global, compressed representation of the entire data point</li>
<li>The model's parameters <span class="arithmatex">\(\theta_i\)</span> are specific to each conditional distribution and don't encode a meaningful representation of the data</li>
</ol>
<p>In contrast, latent variable models like variational autoencoders explicitly learn a compressed representation by:
1. Introducing a latent space <span class="arithmatex">\(z\)</span> that captures the essential features of the data
2. Learning an encoder that maps data to this latent space
3. Learning a decoder that reconstructs data from the latent space
4. Using a bottleneck that forces the model to learn meaningful representations</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>