
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/ai/deep_generative_models/normalizing_flow_models/">
      
      
        <link rel="prev" href="../variational_autoencoders/">
      
      
        <link rel="next" href="../recap_at_this_point/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Normalizing flow models - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#normalizing-flow-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Normalizing flow models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-tabs__link">
          
  
  
  Productivity

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#change-of-variables-formula" class="md-nav__link">
    <span class="md-ellipsis">
      Change of Variables Formula
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Change of Variables Formula">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#univariate-case" class="md-nav__link">
    <span class="md-ellipsis">
      Univariate Case
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multivariate-case" class="md-nav__link">
    <span class="md-ellipsis">
      Multivariate Case
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalizing-flow-models-deep-dive" class="md-nav__link">
    <span class="md-ellipsis">
      Normalizing Flow Models deep dive
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Normalizing Flow Models deep dive">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#planar-flow" class="md-nav__link">
    <span class="md-ellipsis">
      Planar Flow
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nice-and-realnvp" class="md-nav__link">
    <span class="md-ellipsis">
      NICE and RealNVP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#autoregressive-flow-models" class="md-nav__link">
    <span class="md-ellipsis">
      Autoregressive Flow Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Autoregressive Flow Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#masked-autoregressive-flow-maf" class="md-nav__link">
    <span class="md-ellipsis">
      Masked Autoregressive Flow (MAF)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#made-blocks" class="md-nav__link">
    <span class="md-ellipsis">
      MADE Blocks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detailed-maf-implementation-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Detailed MAF Implementation Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inverse-autoregressive-flow-iaf" class="md-nav__link">
    <span class="md-ellipsis">
      Inverse Autoregressive Flow (IAF)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_generative_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Generative Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluating_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluating Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_diffusion_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Learning for Computer Vision
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning for Computer Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/image_classification_with_linear_classifiers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Classification with Linear Classifiers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors, vector addition, and scalar multiplication
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector geometry in Rn and correlation coefficients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/planes_in_r3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Planes in R3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/span_subspaces_and_dimension/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Span, subspaces, and dimension
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/basis_and_orthogonality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basis and orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Applications of projections in Rn- orthogonal bases of planes and linear regression
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/story_proofs_and_axioms_of_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Story Proofs and Axioms of Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/some_famous_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Some famous problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/indicator_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Indicator Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/poisson_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Poisson Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/continuous_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Continuous Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/exponential_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exponential Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/joint_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Joint Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/independence_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Independence of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/multinomial_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multinomial Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/covariance_and_correlation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Covariance and Correlation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/transformations_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformations of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/convolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolution
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Productivity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Productivity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    How to Build Your Career in AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            How to Build Your Career in AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Three Steps to Career Growth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_1_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 1- Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_2_projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 2- Projects
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_3_job/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 3- Job
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="normalizing-flow-models">Normalizing Flow Models</h1>
<p>So far we have learned two types of likelihood based generative models:</p>
<p><strong>Autoregressive Models</strong>: <span class="arithmatex">\(p_\theta(x) = \prod_{i=1}^N p_\theta(x_i|x_{&lt;i})\)</span></p>
<p><strong>Variational autoencoders</strong>: <span class="arithmatex">\(p_\theta(x) = \int p_\theta(x,z)dz\)</span></p>
<p>The two methods have relative strengths and weaknesses. Autoregressive models provide tractable likelihoods but no direct mechanism for learning features, whereas variational autoencoders can learn feature representations but have intractable marginal likelihoods.</p>
<h2 id="change-of-variables-formula">Change of Variables Formula</h2>
<p>In normalizing flows, we wish to map simple distributions (easy to sample and evaluate densities) to complex ones (learned via data). The change of variables formula describes how to evaluate densities of a random variable that is a deterministic transformation from another variable.</p>
<p>Let's start with the univariate case and then generalize to multivariate random variables.</p>
<h3 id="univariate-case">Univariate Case</h3>
<p>Consider two random variables <span class="arithmatex">\(Z\)</span> and <span class="arithmatex">\(X\)</span> related by a strictly monotonic function <span class="arithmatex">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span> such that <span class="arithmatex">\(X = f(Z)\)</span>. We want to find the probability density function of <span class="arithmatex">\(X\)</span> in terms of the density of <span class="arithmatex">\(Z\)</span>.</p>
<p>The key insight comes from the fact that probabilities must be preserved under the transformation. For any interval <span class="arithmatex">\([a, b]\)</span> in the <span class="arithmatex">\(X\)</span> space:</p>
<div class="arithmatex">\[P(a \leq X \leq b) = P(f^{-1}(a) \leq Z \leq f^{-1}(b))\]</div>
<p>This can be written as:</p>
<div class="arithmatex">\[\int_a^b p_X(x) dx = \int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz\]</div>
<p>To perform the substitution <span class="arithmatex">\(z = f^{-1}(x)\)</span>, we need to express <span class="arithmatex">\(dz\)</span> in terms of <span class="arithmatex">\(dx\)</span>. Since <span class="arithmatex">\(z = f^{-1}(x)\)</span>, we can use the chain rule to find:</p>
<div class="arithmatex">\[\frac{dz}{dx} = \frac{d}{dx}f^{-1}(x) = \frac{1}{f'(f^{-1}(x))}\]</div>
<p>This follows from the inverse function theorem: if <span class="arithmatex">\(y = f(x)\)</span>, then <span class="arithmatex">\(\frac{dx}{dy} = \frac{1}{f'(x)}\)</span>.</p>
<p>Therefore, <span class="arithmatex">\(dz = \frac{1}{f'(f^{-1}(x))} dx\)</span>. However, we need to take the absolute value because probability densities must be non-negative. If <span class="arithmatex">\(f'(f^{-1}(x)) &lt; 0\)</span> (meaning <span class="arithmatex">\(f\)</span> is decreasing), then <span class="arithmatex">\(\frac{1}{f'(f^{-1}(x))} &lt; 0\)</span>, which would make the density negative. Therefore, we use:</p>
<div class="arithmatex">\[dz = \frac{1}{|f'(f^{-1}(x))|} dx\]</div>
<p>Substituting this into our integral:</p>
<div class="arithmatex">\[\int_a^b p_X(x) dx = \int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz = \int_a^b p_Z(f^{-1}(x)) \cdot \frac{1}{|f'(f^{-1}(x))|} dx\]</div>
<p>Since this equality must hold for all intervals <span class="arithmatex">\([a, b]\)</span>, the integrands must be equal:</p>
<div class="arithmatex">\[p_X(x) = p_Z(f^{-1}(x)) \cdot \frac{1}{|f'(f^{-1}(x))|}\]</div>
<p>This is the univariate change of variables formula. The factor <span class="arithmatex">\(\frac{1}{|f'(f^{-1}(x))|}\)</span> accounts for how the transformation stretches or compresses the probability mass.</p>
<p><strong>Why should <span class="arithmatex">\(f\)</span> be monotonic?</strong> The monotonicity requirement ensures that <span class="arithmatex">\(f\)</span> is invertible (one-to-one), which is crucial for the change of variables formula to work correctly. If <span class="arithmatex">\(f\)</span> were not monotonic, there could be multiple values of <span class="arithmatex">\(z\)</span> that map to the same value of <span class="arithmatex">\(x\)</span>, making the inverse function <span class="arithmatex">\(f^{-1}\)</span> ill-defined. This would violate the fundamental assumption that we can uniquely determine the original variable <span class="arithmatex">\(z\)</span> from the transformed variable <span class="arithmatex">\(x\)</span>.</p>
<p>For example, if <span class="arithmatex">\(f(z) = z^2\)</span> (which is not monotonic on <span class="arithmatex">\(\mathbb{R}\)</span>), then both <span class="arithmatex">\(z = 2\)</span> and <span class="arithmatex">\(z = -2\)</span> map to <span class="arithmatex">\(x = 4\)</span>. This creates ambiguity in the inverse mapping and would require special handling to account for multiple pre-images.</p>
<h3 id="multivariate-case">Multivariate Case</h3>
<p>For the multivariate case, we have random variables <span class="arithmatex">\(\mathbf{Z}\)</span> and <span class="arithmatex">\(\mathbf{X}\)</span> related by a bijective function <span class="arithmatex">\(f: \mathbb{R}^n \rightarrow \mathbb{R}^n\)</span> such that <span class="arithmatex">\(\mathbf{X} = f(\mathbf{Z})\)</span>.</p>
<p>The key insight is that the probability mass in any region must be preserved under the transformation. For any region <span class="arithmatex">\(A\)</span> in the <span class="arithmatex">\(\mathbf{X}\)</span> space:</p>
<div class="arithmatex">\[P(\mathbf{X} \in A) = P(\mathbf{Z} \in f^{-1}(A))\]</div>
<p>This can be written as:</p>
<div class="arithmatex">\[\int_A p_X(\mathbf{x}) d\mathbf{x} = \int_{f^{-1}(A)} p_Z(\mathbf{z}) d\mathbf{z}\]</div>
<p>To perform the multivariate substitution <span class="arithmatex">\(\mathbf{z} = f^{-1}(\mathbf{x})\)</span>, we need to understand how the volume element <span class="arithmatex">\(d\mathbf{z}\)</span> transforms. The Jacobian matrix <span class="arithmatex">\(\frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}}\)</span> is an <span class="arithmatex">\(n \times n\)</span> matrix where:</p>
<div class="arithmatex">\[\left[\frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}}\right]_{ij} = \frac{\partial f^{-1}_i(\mathbf{x})}{\partial x_j}\]</div>
<p>This matrix describes how small changes in <span class="arithmatex">\(\mathbf{x}\)</span> correspond to changes in <span class="arithmatex">\(\mathbf{z}\)</span>. In multivariate calculus, when we perform a change of variables <span class="arithmatex">\(\mathbf{z} = f^{-1}(\mathbf{x})\)</span>, the volume element transforms as:</p>
<div class="arithmatex">\[d\mathbf{z} = \left|\det\left(\frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}}\right)\right| d\mathbf{x}\]</div>
<p>This is the multivariate generalization of the univariate substitution <span class="arithmatex">\(dz = \frac{1}{|f'(f^{-1}(x))|} dx\)</span>. The determinant of the Jacobian matrix measures how the transformation affects the volume of a small region:
- If <span class="arithmatex">\(|\det(J)| &gt; 1\)</span>, the transformation expands volume
- If <span class="arithmatex">\(|\det(J)| &lt; 1\)</span>, the transformation contracts volume<br />
- If <span class="arithmatex">\(|\det(J)| = 1\)</span>, the transformation preserves volume</p>
<p>Substituting this into our integral:</p>
<div class="arithmatex">\[\int_A p_X(\mathbf{x}) d\mathbf{x} = \int_{f^{-1}(A)} p_Z(\mathbf{z}) d\mathbf{z} = \int_A p_Z(f^{-1}(\mathbf{x})) \left|\det\left(\frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}}\right)\right| d\mathbf{x}\]</div>
<p>Since this equality must hold for all regions <span class="arithmatex">\(A\)</span>, the integrands must be equal:</p>
<div class="arithmatex">\[p_X(\mathbf{x}) = p_Z(f^{-1}(\mathbf{x})) \left|\det\left(\frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}}\right)\right|\]</div>
<p>This is the multivariate change of variables formula. The determinant of the Jacobian matrix accounts for how the transformation affects the volume of probability mass.</p>
<p><strong>Alternative Form Using Forward Mapping:</strong>
Using the property that <span class="arithmatex">\(\det(A^{-1}) = \det(A)^{-1}\)</span> for any invertible matrix <span class="arithmatex">\(A\)</span>, we can rewrite this as:</p>
<div class="arithmatex">\[p_X(\mathbf{x}) = p_Z(\mathbf{z}) \left|\det\left(\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}}\right)\right|^{-1}\]</div>
<p>This form is often more convenient in practice because it uses the forward mapping <span class="arithmatex">\(f\)</span> rather than the inverse mapping <span class="arithmatex">\(f^{-1}\)</span>.</p>
<p><strong>Final result</strong>: Let <span class="arithmatex">\(Z\)</span> and <span class="arithmatex">\(X\)</span> be random variables which are related by a mapping <span class="arithmatex">\(f: \mathbb{R}^n \rightarrow \mathbb{R}^n\)</span> such that <span class="arithmatex">\(X = f(Z)\)</span> and <span class="arithmatex">\(Z = f^{-1}(X)\)</span>. Then</p>
<div class="arithmatex">\[p_X(\mathbf{x}) = p_Z(f^{-1}(\mathbf{x})) \left|\det\left(\frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}}\right)\right|\]</div>
<p>There are several things to note here:</p>
<ul>
<li><span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{z}\)</span> need to be continuous and have the same dimension.</li>
<li><span class="arithmatex">\(\frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}}\)</span> is a matrix of dimension <span class="arithmatex">\(n \times n\)</span>, where each entry at location <span class="arithmatex">\((i,j)\)</span> is defined as <span class="arithmatex">\(\frac{\partial f^{-1}(\mathbf{x})_i}{\partial x_j}\)</span>. This matrix is also known as the Jacobian matrix.</li>
<li><span class="arithmatex">\(\det(A)\)</span> denotes the determinant of a square matrix <span class="arithmatex">\(A\)</span>.</li>
</ul>
<p>For any invertible matrix <span class="arithmatex">\(A\)</span>, <span class="arithmatex">\(\det(A^{-1}) = \det(A)^{-1}\)</span>, so for <span class="arithmatex">\(\mathbf{z} = f^{-1}(\mathbf{x})\)</span> we have</p>
<div class="arithmatex">\[p_X(\mathbf{x}) = p_Z(\mathbf{z}) \left|\det\left(\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}}\right)\right|^{-1}\]</div>
<p>If <span class="arithmatex">\(\left|\det\left(\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}}\right)\right| = 1\)</span>, then the mapping is volume preserving, which means that the transformed distribution <span class="arithmatex">\(p_X\)</span> will have the same "volume" compared to the original one <span class="arithmatex">\(p_Z\)</span>.</p>
<h2 id="normalizing-flow-models-deep-dive">Normalizing Flow Models deep dive</h2>
<p>Let us consider a directed, latent-variable model over observed variables <span class="arithmatex">\(X\)</span> and latent variables <span class="arithmatex">\(Z\)</span>. In a normalizing flow model, the mapping between <span class="arithmatex">\(Z\)</span> and <span class="arithmatex">\(X\)</span>, given by <span class="arithmatex">\(f_\theta: \mathbb{R}^n \rightarrow \mathbb{R}^n\)</span>, is deterministic and invertible such that <span class="arithmatex">\(X = f_\theta(Z)\)</span> and <span class="arithmatex">\(Z = f^{-1}_\theta(X)\)</span>.</p>
<p>Using change of variables, the marginal likelihood <span class="arithmatex">\(p(x)\)</span> is given by</p>
<div class="arithmatex">\[p_X(\mathbf{x}; \theta) = p_Z(f^{-1}_\theta(\mathbf{x})) \left|\det\left(\frac{\partial f^{-1}_\theta(\mathbf{x})}{\partial \mathbf{x}}\right)\right|\]</div>
<p>The name "normalizing flow" can be interpreted as the following:</p>
<ul>
<li>
<p><strong>"Normalizing"</strong> means that the change of variables gives a normalized density after applying an invertible transformation. When we transform a random variable through an invertible function, the resulting density automatically integrates to 1 (is normalized) because the change of variables formula preserves the total probability mass. This is different from other methods where we might need to explicitly normalize or approximate the density.</p>
</li>
<li>
<p><strong>"Flow"</strong> means that the invertible transformations can be composed with each other to create more complex invertible transformations. If we have two invertible functions <span class="arithmatex">\(f_1\)</span> and <span class="arithmatex">\(f_2\)</span>, then their composition <span class="arithmatex">\(f_2 \circ f_1\)</span> is also invertible. This allows us to build complex transformations by chaining simpler ones, creating a "flow" of transformations.</p>
</li>
</ul>
<p>Different from autoregressive models and variational autoencoders, deep normalizing flow models require specific architectural structures:</p>
<ol>
<li>
<p><strong>The input and output dimensions must be the same</strong> - This is necessary for the transformation to be invertible. If the dimensions don't match, we can't uniquely map back and forth between the spaces.</p>
</li>
<li>
<p><strong>The transformation must be invertible</strong> - This is fundamental to the change of variables formula and allows us to compute both the forward transformation (for sampling) and the inverse transformation (for density evaluation).</p>
</li>
<li>
<p><strong>Computing the determinant of the Jacobian needs to be efficient (and differentiable)</strong> - The change of variables formula requires computing the determinant of the Jacobian matrix. For high-dimensional spaces, this can be computationally expensive, so we need architectures that make this computation tractable.</p>
</li>
</ol>
<p>Next, we introduce several popular forms of flow models that satisfy these properties.</p>
<h3 id="planar-flow">Planar Flow</h3>
<p>The Planar Flow introduces the following invertible transformation:</p>
<div class="arithmatex">\[\mathbf{x} = f_\theta(\mathbf{z}) = \mathbf{z} + \mathbf{u}h(\mathbf{w}^\top\mathbf{z} + b)\]</div>
<p>where <span class="arithmatex">\(\mathbf{u}, \mathbf{w}, b\)</span> are parameters.</p>
<p>The absolute value of the determinant of the Jacobian is given by:</p>
<div class="arithmatex">\[\left|\det\left(\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}}\right)\right| = |1 + h'(\mathbf{w}^\top\mathbf{z} + b)\mathbf{u}^\top\mathbf{w}|\]</div>
<p>However, <span class="arithmatex">\(\mathbf{u}, \mathbf{w}, b, h(\cdot)\)</span> need to be restricted in order to be invertible. For example, <span class="arithmatex">\(h = \tanh\)</span> and <span class="arithmatex">\(h'(\mathbf{w}^\top\mathbf{z} + b)\mathbf{u}^\top\mathbf{w} \geq -1\)</span>. Note that while <span class="arithmatex">\(f_\theta(\mathbf{z})\)</span> is invertible, computing <span class="arithmatex">\(f^{-1}_\theta(\mathbf{z})\)</span> could be difficult analytically. The following models address this problem, where both <span class="arithmatex">\(f_\theta\)</span> and <span class="arithmatex">\(f^{-1}_\theta\)</span> have simple analytical forms.</p>
<h3 id="nice-and-realnvp">NICE and RealNVP</h3>
<p>The Nonlinear Independent Components Estimation (NICE) model and Real Non-Volume Preserving (RealNVP) model compose two kinds of invertible transformations: additive coupling layers and rescaling layers. The coupling layer in NICE partitions a variable <span class="arithmatex">\(\mathbf{z}\)</span> into two disjoint subsets, say <span class="arithmatex">\(\mathbf{z}_1\)</span> and <span class="arithmatex">\(\mathbf{z}_2\)</span>. Then it applies the following transformation:</p>
<p><strong>Forward mapping <span class="arithmatex">\(\mathbf{z} \rightarrow \mathbf{x}\)</span>:</strong></p>
<ul>
<li><span class="arithmatex">\(\mathbf{x}_1 = \mathbf{z}_1\)</span>, which is an identity mapping.</li>
<li><span class="arithmatex">\(\mathbf{x}_2 = \mathbf{z}_2 + m_\theta(\mathbf{z}_1)\)</span>, where <span class="arithmatex">\(m_\theta\)</span> is a neural network.</li>
</ul>
<p><strong>Inverse mapping <span class="arithmatex">\(\mathbf{x} \rightarrow \mathbf{z}\)</span>:</strong></p>
<ul>
<li><span class="arithmatex">\(\mathbf{z}_1 = \mathbf{x}_1\)</span>, which is an identity mapping.</li>
<li><span class="arithmatex">\(\mathbf{z}_2 = \mathbf{x}_2 - m_\theta(\mathbf{x}_1)\)</span>, which is the inverse of the forward transformation.</li>
</ul>
<p>Therefore, the Jacobian of the forward mapping is lower triangular, whose determinant is simply the product of the elements on the diagonal, which is 1. Therefore, this defines a volume preserving transformation. RealNVP adds scaling factors to the transformation:</p>
<div class="arithmatex">\[\mathbf{x}_2 = \exp(s_\theta(\mathbf{z}_1)) \odot \mathbf{z}_2 + m_\theta(\mathbf{z}_1)\]</div>
<p>where <span class="arithmatex">\(\odot\)</span> denotes elementwise product. This results in a non-volume preserving transformation.</p>
<h3 id="autoregressive-flow-models">Autoregressive Flow Models</h3>
<p>Some autoregressive models can also be interpreted as flow models. For a Gaussian autoregressive model, one receives some Gaussian noise for each dimension of <span class="arithmatex">\(\mathbf{x}\)</span>, which can be treated as the latent variables <span class="arithmatex">\(\mathbf{z}\)</span>. Such transformations are also invertible, meaning that given <span class="arithmatex">\(\mathbf{x}\)</span> and the model parameters, we can obtain <span class="arithmatex">\(\mathbf{z}\)</span> exactly.</p>
<h4 id="masked-autoregressive-flow-maf">Masked Autoregressive Flow (MAF)</h4>
<p><strong>Masked Autoregressive Flow (MAF)</strong> uses this interpretation, where the forward mapping is an autoregressive model. However, sampling is sequential and slow, in <span class="arithmatex">\(O(n)\)</span> time where <span class="arithmatex">\(n\)</span> is the dimension of the samples.</p>
<p><strong>MAF Architecture and Mathematical Formulation:</strong></p>
<p>The MAF is comprised of <strong>Masked Autoencoder for Distribution Estimation (MADE)</strong> blocks, which has a special masking scheme at each layer such that the autoregressive property is preserved. In particular, we consider a Gaussian autoregressive model:</p>
<div class="arithmatex">\[p(\mathbf{x}) = \prod_{i=1}^n p(x_i | \mathbf{x}_{&lt;i})\]</div>
<p>such that the conditional Gaussians <span class="arithmatex">\(p(x_i | \mathbf{x}_{&lt;i}) = \mathcal{N}(x_i | \mu_i, (\exp(\alpha_i))^2)\)</span> are parameterized by neural networks <span class="arithmatex">\(\mu_i = f_{\mu_i}(\mathbf{x}_{&lt;i})\)</span> and <span class="arithmatex">\(\alpha_i = f_{\alpha_i}(\mathbf{x}_{&lt;i})\)</span>. Note that <span class="arithmatex">\(\alpha_i\)</span> denotes the log standard deviation of the Gaussian <span class="arithmatex">\(p(x_i | \mathbf{x}_{&lt;i})\)</span>.</p>
<p>As seen in the change of variables formula, a normalizing flow uses a series of deterministic and invertible mappings <span class="arithmatex">\(f: \mathbb{R}^n \rightarrow \mathbb{R}^n\)</span> such that <span class="arithmatex">\(\mathbf{x} = f(\mathbf{z})\)</span> and <span class="arithmatex">\(\mathbf{z} = f^{-1}(\mathbf{x})\)</span> to transform a simple prior distribution <span class="arithmatex">\(p_z\)</span> (e.g. isotropic Gaussian) into a more expressive one. In particular, a normalizing flow which composes <span class="arithmatex">\(k\)</span> invertible transformations <span class="arithmatex">\(\{f_j\}_{j=1}^k\)</span> such that <span class="arithmatex">\(\mathbf{x} = f_k \circ f_{k-1} \circ \cdots \circ f_1(\mathbf{z}_0)\)</span> takes advantage of the change-of-variables property:</p>
<div class="arithmatex">\[\log p(\mathbf{x}) = \log p_z(f^{-1}(\mathbf{x})) + \sum_{j=1}^k \log \left|\det\left(\frac{\partial f_j^{-1}(\mathbf{x}_j)}{\partial \mathbf{x}_j}\right)\right|\]</div>
<p>In MAF, the forward mapping is: <span class="arithmatex">\(x_i = \mu_i + z_i \cdot \exp(\alpha_i)\)</span>, and the inverse mapping is: <span class="arithmatex">\(z_i = (x_i - \mu_i)/\exp(\alpha_i)\)</span>. The log of the absolute value of the determinant of the Jacobian is:</p>
<div class="arithmatex">\[\log \left|\det\left(\frac{\partial f^{-1}}{\partial \mathbf{x}}\right)\right| = -\sum_{i=1}^n \alpha_i\]</div>
<p>where <span class="arithmatex">\(\mu_i\)</span> and <span class="arithmatex">\(\alpha_i\)</span> are as defined above.</p>
<p><strong>Connection between <span class="arithmatex">\(p(\mathbf{x})\)</span> and <span class="arithmatex">\(\log p(\mathbf{x})\)</span> formulations:</strong></p>
<p>The two formulations are equivalent but serve different purposes:</p>
<ol>
<li><strong><span class="arithmatex">\(p(\mathbf{x})\)</span> formulation (autoregressive view)</strong>:</li>
</ol>
<div class="arithmatex">\[p(\mathbf{x}) = \prod_{i=1}^n p(x_i | \mathbf{x}_{&lt;i}) = \prod_{i=1}^n \mathcal{N}(x_i | \mu_i, (\exp(\alpha_i))^2)\]</div>
<ol>
<li><strong><span class="arithmatex">\(\log p(\mathbf{x})\)</span> formulation (flow view)</strong>:</li>
</ol>
<div class="arithmatex">\[\log p(\mathbf{x}) = \log p_z(f^{-1}(\mathbf{x})) + \sum_{j=1}^k \log \left|\det\left(\frac{\partial f_j^{-1}(\mathbf{x}_j)}{\partial \mathbf{x}_j}\right)\right|\]</div>
<p><strong>How they relate:</strong></p>
<p>Taking the logarithm of the autoregressive formulation:</p>
<div class="arithmatex">\[\log p(\mathbf{x}) = \sum_{i=1}^n \log p(x_i | \mathbf{x}_{&lt;i}) = \sum_{i=1}^n \log \mathcal{N}(x_i | \mu_i, (\exp(\alpha_i))^2)\]</div>
<p>For a Gaussian distribution <span class="arithmatex">\(\mathcal{N}(x | \mu, \sigma^2)\)</span>, we have:</p>
<div class="arithmatex">\[\log \mathcal{N}(x | \mu, \sigma^2) = -\frac{1}{2}\log(2\pi) - \log(\sigma) - \frac{(x-\mu)^2}{2\sigma^2}\]</div>
<p>Substituting <span class="arithmatex">\(\sigma = \exp(\alpha_i)\)</span> and using the inverse mapping <span class="arithmatex">\(z_i = (x_i - \mu_i)/\exp(\alpha_i)\)</span>:</p>
<div class="arithmatex">\[\log p(\mathbf{x}) = \sum_{i=1}^n \left[-\frac{1}{2}\log(2\pi) - \alpha_i - \frac{z_i^2}{2}\right] = \sum_{i=1}^n \log \mathcal{N}(z_i | 0, 1) - \sum_{i=1}^n \alpha_i\]</div>
<p>This shows that the autoregressive formulation (using conditional Gaussians) is equivalent to the flow formulation (using change of variables with a standard normal prior and the Jacobian determinant term <span class="arithmatex">\(-\sum_{i=1}^n \alpha_i\)</span>).</p>
<p><strong>Key insight:</strong> The <span class="arithmatex">\(\alpha_i\)</span> terms serve dual purposes - they parameterize the conditional standard deviations in the autoregressive view, and they contribute to the Jacobian determinant in the flow view.</p>
<p><strong>What are <span class="arithmatex">\(\mu_1\)</span> and <span class="arithmatex">\(\alpha_1\)</span> in MAF?</strong></p>
<p>In MAF, for the first dimension (<span class="arithmatex">\(i=1\)</span>):</p>
<ul>
<li>
<p><strong><span class="arithmatex">\(\mu_1\)</span></strong>: This is the <strong>mean</strong> of the first conditional distribution <span class="arithmatex">\(p(x_1)\)</span>. Since <span class="arithmatex">\(x_1\)</span> has no previous dimensions to condition on (<span class="arithmatex">\(\mathbf{x}_{&lt;1}\)</span> is empty), <span class="arithmatex">\(\mu_1\)</span> is typically a learned constant parameter or computed from a bias term in the neural network.</p>
</li>
<li>
<p><strong><span class="arithmatex">\(\alpha_1\)</span></strong>: This is the <strong>log standard deviation</strong> of the first conditional distribution <span class="arithmatex">\(p(x_1)\)</span>. The actual standard deviation is <span class="arithmatex">\(\exp(\alpha_1)\)</span>, and <span class="arithmatex">\(\alpha_1\)</span> is also typically a learned constant parameter.</p>
</li>
</ul>
<p>This makes sense because the first dimension has no autoregressive dependencies - it's the starting point of the autoregressive chain.</p>
<h4 id="made-blocks">MADE Blocks</h4>
<p><strong>MADE (Masked Autoencoder for Distribution Estimation)</strong> is a key architectural component that enables efficient autoregressive modeling. MADE uses a special masking scheme to ensure that the autoregressive property is preserved while allowing for efficient parallel computation of all conditional parameters.</p>
<p><strong>How MADE Works:</strong></p>
<ol>
<li>
<p><strong>Masking Scheme</strong>: Each layer in the neural network has a mask that ensures each output unit only depends on a subset of input units, maintaining the autoregressive ordering.</p>
</li>
<li>
<p><strong>Autoregressive Property</strong>: For dimension <span class="arithmatex">\(i\)</span>, the network can only access inputs <span class="arithmatex">\(x_j\)</span> where <span class="arithmatex">\(j &lt; i\)</span>, ensuring that <span class="arithmatex">\(p(x_i | \mathbf{x}_{&lt;i})\)</span> only depends on previous dimensions.</p>
</li>
<li>
<p><strong>Parallel Parameter Computation</strong>: Despite the autoregressive constraints, MADE can compute all <span class="arithmatex">\(\mu_i\)</span> and <span class="arithmatex">\(\alpha_i\)</span> parameters in parallel during training, making it much more efficient than sequential autoregressive models.</p>
</li>
</ol>
<p><strong>Mathematical Implementation:</strong></p>
<p>The masking is implemented by multiplying the weight matrices with binary masks:</p>
<div class="arithmatex">\[W_{masked} = W \odot M\]</div>
<p>where <span class="arithmatex">\(M\)</span> is a binary mask matrix that enforces the autoregressive dependencies. The mask ensures that:
- Output <span class="arithmatex">\(i\)</span> can only depend on inputs <span class="arithmatex">\(j &lt; i\)</span>
- This creates a lower triangular dependency structure</p>
<p><strong>Connection to MAF:</strong>
MAF uses MADE blocks as its core building blocks, allowing it to efficiently compute all the conditional parameters <span class="arithmatex">\(\mu_i\)</span> and <span class="arithmatex">\(\alpha_i\)</span> while maintaining the autoregressive structure required for the flow transformation.</p>
<h4 id="detailed-maf-implementation-analysis">Detailed MAF Implementation Analysis</h4>
<p>Let's analyze a complete MAF implementation that demonstrates the concepts discussed above:</p>
<p><strong>Core Components:</strong></p>
<ol>
<li><strong>MaskedLinear</strong>: Implements the masking mechanism for autoregressive dependencies</li>
<li><strong>PermuteLayer</strong>: Reorders dimensions between flow layers</li>
<li><strong>MADE</strong>: Single MADE block with forward and inverse transformations</li>
<li><strong>MAF</strong>: Complete model with multiple MADE blocks</li>
</ol>
<p><strong>1. MaskedLinear Layer:</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">MaskedLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</code></pre></div>
<p><strong>Key Features:</strong>
- <strong>Masking</strong>: The mask is a binary matrix that enforces autoregressive dependencies
- <strong>Element-wise Multiplication</strong>: <code>self.mask * self.weight</code> zeros out forbidden connections
- <strong>Autoregressive Property</strong>: Ensures output <span class="arithmatex">\(i\)</span> only depends on inputs <span class="arithmatex">\(j &lt; i\)</span></p>
<p><strong>2. PermuteLayer:</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">PermuteLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">):</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>        <span class="k">return</span> <span class="n">inputs</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">perm</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">inverse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>        <span class="k">return</span> <span class="n">inputs</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">perm</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>
<p><strong>Purpose:</strong>
- <strong>Dimension Reordering</strong>: Reverses the order of dimensions between flow layers
- <strong>Expressiveness</strong>: Allows different autoregressive orderings across layers
- <strong>Jacobian</strong>: Since it's just a permutation, the Jacobian determinant is 1 (log_det = 0)</p>
<p><strong>3. MADE Block Implementation:</strong></p>
<p><strong>Forward Method (z → x):</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>    <span class="n">log_det</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">):</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># MADE network with masking</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>        <span class="n">mean</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Split into mean and log_std</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>        <span class="n">x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">z</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">alpha</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>  <span class="c1"># Transform</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>        <span class="k">if</span> <span class="n">log_det</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>            <span class="n">log_det</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>            <span class="n">log_det</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">log_det</span><span class="p">,</span> <span class="n">alpha</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>    <span class="n">log_det</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_det</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Negative sum for change of variables</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">log_det</span>
</code></pre></div></p>
<p><strong>Key Implementation Details:</strong>
- <strong>Sequential Processing</strong>: Each dimension is processed one by one
- <strong>Autoregressive Access</strong>: The MADE network can only access previously computed <span class="arithmatex">\(x\)</span> values
- <strong>Transformation</strong>: <span class="arithmatex">\(x_i = \mu_i + z_i \cdot \exp(\alpha_i)\)</span>
- <strong>Log Determinant</strong>: Accumulates <span class="arithmatex">\(\alpha_i\)</span> values and takes negative sum</p>
<p><strong>Inverse Method (x → z):</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">inverse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># MADE network with masking</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>    <span class="n">mean</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Split into mean and log_std</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span>  <span class="c1"># Inverse transform</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>    <span class="n">log_det</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Negative sum for change of variables</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">log_det</span>
</code></pre></div></p>
<p><strong>Key Implementation Details:</strong>
- <strong>Parallel Processing</strong>: All dimensions can be processed simultaneously
- <strong>Autoregressive Masking</strong>: The masking ensures proper dependencies
- <strong>Inverse Transformation</strong>: <span class="arithmatex">\(z_i = (x_i - \mu_i) / \exp(\alpha_i)\)</span>
- <strong>Log Determinant</strong>: Same formula as forward, but computed in parallel</p>
<p><strong>4. Complete MAF Model:</strong></p>
<p><strong>Architecture:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_flows</span><span class="p">):</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>    <span class="n">nf_blocks</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_flows</span><span class="p">):</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>        <span class="n">nf_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MADE</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>        <span class="n">nf_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">PermuteLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">))</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">nf</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">nf_blocks</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Structure:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>Input → MADE₁ → Permute₁ → MADE₂ → Permute₂ → ... → MADEₖ → Permuteₖ → Output
</code></pre></div></p>
<p><strong>Log Probability Computation:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">log_probs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>    <span class="n">log_det_list</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>    <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">:</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>        <span class="n">x</span><span class="p">,</span> <span class="n">log_det</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Transform x → z</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>        <span class="n">log_det_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_det</span><span class="p">)</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>    <span class="n">sum_log_det</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_det_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># Final z after all transformations</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>    <span class="n">p_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Prior log probability</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a>    <span class="n">log_prob</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_z</span> <span class="o">+</span> <span class="n">sum_log_det</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># Change of variables formula</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>    <span class="k">return</span> <span class="n">log_prob</span>
</code></pre></div></p>
<p><strong>Sampling Process:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>    <span class="n">x_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># Sample from prior</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>    <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>  <span class="c1"># Reverse order for sampling</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a>        <span class="n">x_sample</span><span class="p">,</span> <span class="n">log_det</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x_sample</span><span class="p">)</span>  <span class="c1"># Transform z → x</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a>    <span class="k">return</span> <span class="n">x_sample</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></p>
<p><strong>Understanding the Flow Methods:</strong></p>
<ol>
<li>
<p><strong>During Training (likelihood computation):</strong>
   <div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="c1"># We have x, want to compute log p(x)</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">:</span>  <span class="c1"># Forward order</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>    <span class="n">x</span><span class="p">,</span> <span class="n">log_det</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># x → z (inverse of this flow)</span>
</code></pre></div></p>
</li>
<li>
<p><strong>During Sampling:</strong>
   <div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="c1"># We have z, want to get x</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>  <span class="c1"># Reverse order</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>    <span class="n">x_sample</span><span class="p">,</span> <span class="n">log_det</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x_sample</span><span class="p">)</span>  <span class="c1"># z → x (forward of this flow)</span>
</code></pre></div></p>
</li>
</ol>
<p><strong>In other words:</strong></p>
<ul>
<li><strong>Each flow's <code>forward()</code> method</strong>: Transforms <span class="arithmatex">\(\mathbf{z} \rightarrow \mathbf{x}\)</span> for that specific flow</li>
<li><strong>Each flow's <code>inverse()</code> method</strong>: Transforms <span class="arithmatex">\(\mathbf{x} \rightarrow \mathbf{z}\)</span> for that specific flow</li>
<li><strong>During training</strong>: We use <code>inverse()</code> to go from data space to latent space</li>
<li><strong>During sampling</strong>: We use <code>forward()</code> to go from latent space to data space</li>
</ul>
<p><strong>Mathematical Perspective:</strong>
Let <span class="arithmatex">\(f_i\)</span> denote the forward transformation of the <span class="arithmatex">\(i\)</span>-th flow (from <span class="arithmatex">\(\mathbf{z}\)</span> to <span class="arithmatex">\(\mathbf{x}\)</span>), and <span class="arithmatex">\(f_i^{-1}\)</span> denote its inverse transformation (from <span class="arithmatex">\(\mathbf{x}\)</span> to <span class="arithmatex">\(\mathbf{z}\)</span>).</p>
<ul>
<li><strong>Training</strong>: <span class="arithmatex">\(f_k^{-1} \circ f_{k-1}^{-1} \circ \cdots \circ f_1^{-1}(\mathbf{x}) = \mathbf{z}\)</span> (using <code>inverse()</code> methods)</li>
<li><strong>Sampling</strong>: <span class="arithmatex">\(f_1 \circ f_2 \circ \cdots \circ f_k(\mathbf{z}) = \mathbf{x}\)</span> (using <code>forward()</code> methods)</li>
</ul>
<p><strong>What is <span class="arithmatex">\(k\)</span>?</strong></p>
<p>The parameter <span class="arithmatex">\(k\)</span> represents the <strong>total number of flow layers</strong> in the MAF model. In the implementation, this corresponds to <code>n_flows</code> in the MAF constructor.</p>
<p><strong>In the MAF Architecture:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_flows</span><span class="p">):</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>    <span class="c1"># n_flows = k (total number of flow layers)</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_flows</span><span class="p">):</span>  <span class="c1"># i goes from 0 to k-1</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>        <span class="n">nf_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MADE</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>        <span class="n">nf_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">PermuteLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">))</span>
</code></pre></div></p>
<p><strong>Flow Composition Structure:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>Input → MADE₁ → Permute₁ → MADE₂ → Permute₂ → ... → MADEₖ → Permuteₖ → Output
</code></pre></div></p>
<p>Where:
- <strong><span class="arithmatex">\(f_1\)</span></strong>: First MADE block (MADE₁)
- <strong><span class="arithmatex">\(f_2\)</span></strong>: Second MADE block (MADE₂)
- <strong>...</strong>
- <strong><span class="arithmatex">\(f_k\)</span></strong>: Last MADE block (MADEₖ)</p>
<p><strong>Example with <span class="arithmatex">\(k = 3\)</span>:</strong>
- <strong>Training</strong>: <span class="arithmatex">\(f_3^{-1} \circ f_2^{-1} \circ f_1^{-1}(\mathbf{x}) = \mathbf{z}\)</span>
- <strong>Sampling</strong>: <span class="arithmatex">\(f_1 \circ f_2 \circ f_3(\mathbf{z}) = \mathbf{x}\)</span></p>
<p><strong>Key Insight:</strong> The <code>forward()</code> method of each flow is designed to be the inverse transformation for the overall model's training direction. This is why we use <code>forward()</code> during sampling in reverse order.</p>
<p>This implementation demonstrates how the theoretical concepts of MAF translate into practical code, showing the interplay between autoregressive structure, masking, and flow transformations.</p>
<h4 id="inverse-autoregressive-flow-iaf">Inverse Autoregressive Flow (IAF)</h4>
<p>To address the sampling problem (sequential) in MAF, the <strong>Inverse Autoregressive Flow (IAF)</strong> simply inverts the generating process. In this case, the sampling (generation), is still parallelized. However, computing the likelihood of new data points is slow.</p>
<p><strong>Forward mapping from <span class="arithmatex">\(\mathbf{z} \rightarrow \mathbf{x}\)</span> (parallel):</strong></p>
<ol>
<li>
<p>Sample <span class="arithmatex">\(z_i \sim \mathcal{N}(0,1)\)</span> for <span class="arithmatex">\(i = 1, \ldots, n\)</span></p>
</li>
<li>
<p>Compute all <span class="arithmatex">\(\mu_i, \alpha_i\)</span> (can be done in parallel)</p>
</li>
<li>
<p>Let <span class="arithmatex">\(x_1 = \exp(\alpha_1)z_1 + \mu_1\)</span></p>
</li>
<li>
<p>Let <span class="arithmatex">\(x_2 = \exp(\alpha_2)z_2 + \mu_2\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\ldots\)</span></p>
</li>
</ol>
<p><strong>Inverse mapping from <span class="arithmatex">\(\mathbf{x} \rightarrow \mathbf{z}\)</span> (sequential):</strong></p>
<ol>
<li>
<p>Let <span class="arithmatex">\(z_1 = (x_1 - \mu_1)/\exp(\alpha_1)\)</span></p>
</li>
<li>
<p>Compute <span class="arithmatex">\(\mu_2(z_1), \alpha_2(z_1)\)</span></p>
</li>
<li>
<p>Let <span class="arithmatex">\(z_2 = (x_2 - \mu_2)/\exp(\alpha_2)\)</span></p>
</li>
<li>
<p>Compute <span class="arithmatex">\(\mu_3(z_1,z_2), \alpha_3(z_1,z_2)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\ldots\)</span></p>
</li>
</ol>
<p><strong>Key insight:</strong> Fast to sample from, slow to evaluate likelihoods of data points (train).</p>
<p><strong>Efficient Likelihood for Generated Points:</strong>
However, for generated points the likelihood can be computed efficiently (since the noise are already obtained). When we generate samples using IAF, we start with known noise values <span class="arithmatex">\(\mathbf{z}\)</span> and transform them to get <span class="arithmatex">\(\mathbf{x}\)</span>. Since we already have the noise values, we don't need to perform the expensive sequential inverse mapping to recover them. We can directly compute the likelihood using the change of variables formula:</p>
<div class="arithmatex">\[\log p(\mathbf{x}) = \log p(\mathbf{z}) - \sum_{i=1}^n \alpha_i\]</div>
<p>where we already know all the <span class="arithmatex">\(\alpha_i\)</span> values from the forward pass. This is much faster than the <span class="arithmatex">\(O(n)\)</span> sequential computation required for arbitrary data points.</p>
<p><strong>Derivation of the Change of Variables Formula for IAF:</strong></p>
<p>Let's derive how we get this formula. Starting with the general change of variables formula:</p>
<div class="arithmatex">\[\log p(\mathbf{x}) = \log p(\mathbf{z}) + \log \left|\det\left(\frac{\partial \mathbf{z}}{\partial \mathbf{x}}\right)\right|\]</div>
<p>For IAF, the forward transformation is:</p>
<div class="arithmatex">\[x_i = \exp(\alpha_i)z_i + \mu_i\]</div>
<p>The inverse transformation is:</p>
<div class="arithmatex">\[z_i = \frac{x_i - \mu_i}{\exp(\alpha_i)}\]</div>
<p>The Jacobian matrix <span class="arithmatex">\(\frac{\partial \mathbf{z}}{\partial \mathbf{x}}\)</span> is diagonal because each <span class="arithmatex">\(z_i\)</span> only depends on <span class="arithmatex">\(x_i\)</span>:</p>
<div class="arithmatex">\[\frac{\partial z_i}{\partial x_j} = \begin{cases} 
\frac{1}{\exp(\alpha_i)} &amp; \text{if } i = j \\
0 &amp; \text{if } i \neq j
\end{cases}\]</div>
<p>Therefore, the determinant is the product of the diagonal elements:</p>
<div class="arithmatex">\[\det\left(\frac{\partial \mathbf{z}}{\partial \mathbf{x}}\right) = \prod_{i=1}^n \frac{1}{\exp(\alpha_i)} = \exp\left(-\sum_{i=1}^n \alpha_i\right)\]</div>
<p>Taking the absolute value and logarithm:</p>
<div class="arithmatex">\[\log \left|\det\left(\frac{\partial \mathbf{z}}{\partial \mathbf{x}}\right)\right| = \log \exp\left(-\sum_{i=1}^n \alpha_i\right) = -\sum_{i=1}^n \alpha_i\]</div>
<p>Substituting back into the change of variables formula:</p>
<div class="arithmatex">\[\log p(\mathbf{x}) = \log p(\mathbf{z}) - \sum_{i=1}^n \alpha_i\]</div>
<p>This derivation shows why the likelihood computation is efficient for generated samples - we already have all the <span class="arithmatex">\(\alpha_i\)</span> values from the forward pass, so we just need to sum them up.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>