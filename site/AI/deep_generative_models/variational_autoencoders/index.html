
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/AI/deep_generative_models/variational_autoencoders/">
      
      
        <link rel="prev" href="../introduction/">
      
      
        <link rel="next" href="../../../math/probability/probability_and_counting/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Variational autoencoders - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#variational-autoencoders" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Variational autoencoders
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Welcome to my wiki!

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../autoregressive_models/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/probability/probability_and_counting/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Welcome to my wiki!
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep generative models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep generative models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Variational autoencoders
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Variational autoencoders
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#representation" class="md-nav__link">
    <span class="md-ellipsis">
      Representation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-directed-latent-variable-models" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Directed Latent Variable Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#black-box-variational-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Black-Box Variational Inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Estimation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parameterizing-distributions-via-deep-neural-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Parameterizing Distributions via Deep Neural Networks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#amortized-variational-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Amortized Variational Inference
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Amortized Variational Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#steps-of-amortized-variational-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Steps of Amortized Variational Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and counting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="variational-autoencoders">Variational autoencoders</h1>
<h2 id="representation">Representation</h2>
<p>Consider a directed, latent variable model as shown below.</p>
<p><img alt="Graphical model for a directed, latent variable model." src="../vae_zx.png" /></p>
<p>In the model above, <span class="arithmatex">\(z\)</span> and <span class="arithmatex">\(x\)</span> denote the latent and observed variables respectively. The joint distribution expressed by this model is given as</p>
<div class="arithmatex">\[p(x,z) = p(x|z)p(z).\]</div>
<p>From a generative modeling perspective, this model describes a generative process for the observed data <span class="arithmatex">\(x\)</span> using the following procedure:</p>
<div class="arithmatex">\[z \sim p(z)\]</div>
<div class="arithmatex">\[x \sim p(x|z)\]</div>
<p>If one adopts the belief that the latent variables <span class="arithmatex">\(z\)</span> somehow encode semantically meaningful information about <span class="arithmatex">\(x\)</span>, it is natural to view this generative process as first generating the "high-level" semantic information about <span class="arithmatex">\(x\)</span> first before fully generating <span class="arithmatex">\(x\)</span>.</p>
<p>We now consider a family of distributions <span class="arithmatex">\(\mathcal{Z}\)</span> where <span class="arithmatex">\(p(z) \in \mathcal{Z}\)</span> describes a probability distribution over <span class="arithmatex">\(z\)</span>. Next, consider a family of conditional distributions <span class="arithmatex">\(\mathcal{X|Z}\)</span> where <span class="arithmatex">\(p(x|z) \in \mathcal{X|Z}\)</span> describes a conditional probability distribution over <span class="arithmatex">\(x\)</span> given <span class="arithmatex">\(z\)</span>. Then our hypothesis class of generative models is the set of all possible combinations</p>
<div class="arithmatex">\[\mathcal{X,Z} = \{p(x,z) \mid p(z) \in \mathcal{Z}, p(x|z) \in \mathcal{X|Z}\}.\]</div>
<p>Given a dataset <span class="arithmatex">\(\mathcal{D} = \{x^{(1)}, \ldots, x^{(n)}\}\)</span>, we are interested in the following learning and inference tasks:</p>
<ol>
<li>Selecting <span class="arithmatex">\(p \in \mathcal{X,Z}\)</span> that "best" fits <span class="arithmatex">\(\mathcal{D}\)</span>.</li>
<li>Given a sample <span class="arithmatex">\(x\)</span> and a model <span class="arithmatex">\(p \in \mathcal{X,Z}\)</span>, what is the posterior distribution over the latent variables <span class="arithmatex">\(z\)</span>?</li>
</ol>
<p>The posterior distribution <span class="arithmatex">\(p(z|x)\)</span> represents our updated beliefs about the latent variables <span class="arithmatex">\(z\)</span> after observing the data <span class="arithmatex">\(x\)</span>. In other words, it tells us what values of <span class="arithmatex">\(z\)</span> are most likely to have generated the observed <span class="arithmatex">\(x\)</span>. This is particularly useful for tasks like feature extraction, where we want to understand what latent factors might have generated our observed data.</p>
<h2 id="learning-directed-latent-variable-models">Learning Directed Latent Variable Models</h2>
<p>One way to measure how closely <span class="arithmatex">\(p(x,z)\)</span> fits the observed dataset <span class="arithmatex">\(\mathcal{D}\)</span> is to measure the Kullback-Leibler (KL) divergence between the data distribution (which we denote as <span class="arithmatex">\(p_{data}(x)\)</span>) and the model's marginal distribution <span class="arithmatex">\(p(x) = \int p(x,z)dz\)</span>. The distribution that "best" fits the data is thus obtained by minimizing the KL divergence.</p>
<div class="arithmatex">\[\min_{p \in \mathcal{X,Z}} D_{KL}(p_{data}(x) \| p(x)).\]</div>
<p>As we have seen previously, optimizing an empirical estimate of the KL divergence is equivalent to maximizing the marginal log-likelihood <span class="arithmatex">\(\log p(x)\)</span> over <span class="arithmatex">\(\mathcal{D}\)</span>:</p>
<div class="arithmatex">\[\max_{p \in \mathcal{X,Z}} \sum_{x \in \mathcal{D}} \log p(x) = \max_{p \in \mathcal{X,Z}} \sum_{x \in \mathcal{D}} \log \int p(x,z)dz.\]</div>
<p>However, it turns out this problem is generally intractable for high-dimensional <span class="arithmatex">\(z\)</span> as it involves an integration (or sums in the case <span class="arithmatex">\(z\)</span> is discrete) over all the possible latent sources of variation <span class="arithmatex">\(z\)</span>. This intractability arises from several challenges:</p>
<ol>
<li>
<p><strong>Computational Complexity</strong>: The integral <span class="arithmatex">\(\int p(x,z)dz\)</span> requires evaluating the joint distribution <span class="arithmatex">\(p(x,z)\)</span> for all possible values of <span class="arithmatex">\(z\)</span>. In high-dimensional spaces, this becomes computationally prohibitive as the number of points to evaluate grows exponentially with the dimension of <span class="arithmatex">\(z\)</span>.</p>
</li>
<li>
<p><strong>Numerical Integration</strong>: Even if we could evaluate the integrand at all points, computing the integral numerically becomes increasingly difficult as the dimension of <span class="arithmatex">\(z\)</span> grows. Traditional numerical integration methods like quadrature become impractical in high dimensions.</p>
</li>
<li>
<p><strong>Posterior Inference</strong>: The intractability of the marginal likelihood also makes it difficult to compute the posterior distribution <span class="arithmatex">\(p(z|x)\)</span>, which is crucial for tasks like feature extraction and data generation.</p>
</li>
</ol>
<p>This intractability motivates the need for approximate inference methods, such as variational inference. One option is to estimate the objective via Monte Carlo. For any given datapoint <span class="arithmatex">\(x\)</span>, we can obtain the following estimate for its marginal log-likelihood:</p>
<div class="arithmatex">\[\log p(x) \approx \log \frac{1}{k} \sum_{i=1}^k p(x|z^{(i)}), \text{ where } z^{(i)} \sim p(z)\]</div>
<p>This Monte Carlo estimate is derived as follows:</p>
<p>First, recall that the marginal likelihood <span class="arithmatex">\(p(x)\)</span> can be written as an expectation:</p>
<div class="arithmatex">\[p(x) = \int p(x|z)p(z)dz = \mathbb{E}_{z \sim p(z)}[p(x|z)]\]</div>
<p>The Monte Carlo method approximates this expectation by drawing <span class="arithmatex">\(k\)</span> samples from <span class="arithmatex">\(p(z)\)</span> and computing their average:</p>
<div class="arithmatex">\[\mathbb{E}_{z \sim p(z)}[p(x|z)] \approx \frac{1}{k} \sum_{i=1}^k p(x|z^{(i)}), \text{ where } z^{(i)} \sim p(z)\]</div>
<p>Taking the logarithm of both sides gives us our final estimate:</p>
<div class="arithmatex">\[\log p(x) \approx \log \frac{1}{k} \sum_{i=1}^k p(x|z^{(i)}), \text{ where } z^{(i)} \sim p(z)\]</div>
<p>This approximation becomes more accurate as <span class="arithmatex">\(k\)</span> increases, but at the cost of more computational resources. The key insight is that we're using random sampling to approximate the intractable integral, trading exact computation for statistical estimation.</p>
<p>Rather than maximizing the log-likelihood directly, an alternate is to instead construct a lower bound that is more amenable to optimization. To do so, we note that evaluating the marginal likelihood <span class="arithmatex">\(p(x)\)</span> is at least as difficult as as evaluating the posterior <span class="arithmatex">\(p(z|x)\)</span> for any latent vector <span class="arithmatex">\(z\)</span> since by definition <span class="arithmatex">\(p(z|x) = p(x,z)/p(x)\)</span>.</p>
<p>Next, we introduce a variational family <span class="arithmatex">\(\mathcal{Q}\)</span> of distributions that approximate the true, but intractable posterior <span class="arithmatex">\(p(z|x)\)</span>. Further henceforth, we will assume a parameteric setting where any distribution in the model family <span class="arithmatex">\(\mathcal{X,Z}\)</span> is specified via a set of parameters <span class="arithmatex">\(\theta \in \Theta\)</span> and distributions in the variational family <span class="arithmatex">\(\mathcal{Q}\)</span> are specified via a set of parameters <span class="arithmatex">\(\lambda \in \Lambda\)</span>.</p>
<p>Given <span class="arithmatex">\(\mathcal{X,Z}\)</span> and <span class="arithmatex">\(\mathcal{Q}\)</span>, we note that the following relationships hold true for any <span class="arithmatex">\(x\)</span> and all variational distributions <span class="arithmatex">\(q_\lambda(z) \in \mathcal{Q}\)</span>:</p>
<div class="arithmatex">\[\log p_\theta(x) = \log \int p_\theta(x,z)dz = \log \int \frac{q_\lambda(z)}{q_\lambda(z)}p_\theta(x,z)dz \geq \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right] := \text{ELBO}(x;\theta,\lambda)\]</div>
<p>where we have used Jensen's inequality in the final step. The key insight here is that since the logarithm function is concave, Jensen's inequality tells us that for any random variable <span class="arithmatex">\(X\)</span> and concave function <span class="arithmatex">\(f\)</span>, we have <span class="arithmatex">\(\mathbb{E}[f(X)] \leq f(\mathbb{E}[X])\)</span>. In our case:</p>
<p>We first multiply and divide by <span class="arithmatex">\(q_\lambda(z)\)</span> inside the integral to get:</p>
<div class="arithmatex">\[\log \int \frac{q_\lambda(z)}{q_\lambda(z)}p_\theta(x,z)dz = \log \int q_\lambda(z)\frac{p_\theta(x,z)}{q_\lambda(z)}dz\]</div>
<p>The integral <span class="arithmatex">\(\int q_\lambda(z)\frac{p_\theta(x,z)}{q_\lambda(z)}dz\)</span> can be seen as an expectation <span class="arithmatex">\(\mathbb{E}_{q_\lambda(z)}\left[\frac{p_\theta(x,z)}{q_\lambda(z)}\right]\)</span></p>
<p>Since <span class="arithmatex">\(\log\)</span> is a concave function, Jensen's inequality gives us:</p>
<div class="arithmatex">\[\log \mathbb{E}_{q_\lambda(z)}\left[\frac{p_\theta(x,z)}{q_\lambda(z)}\right] \geq \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right]\]</div>
<p>This inequality is what allows us to obtain a lower bound on the log-likelihood, which we call the Evidence Lower BOund (ELBO). The ELBO admits a tractable unbiased Monte Carlo estimator</p>
<div class="arithmatex">\[\frac{1}{k}\sum_{i=1}^k \log\frac{p_\theta(x,z^{(i)})}{q_\lambda(z^{(i)})}, \text{ where } z^{(i)} \sim q_\lambda(z),\]</div>
<p>so long as it is easy to sample from and evaluate densities for <span class="arithmatex">\(q_\lambda(z)\)</span>.</p>
<p>In summary, we can learn a latent variable model by maximizing the ELBO with respect to both the model parameters <span class="arithmatex">\(\theta\)</span> and the variational parameters <span class="arithmatex">\(\lambda\)</span> for any given datapoint <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[\max_\theta \sum_{x \in \mathcal{D}} \max_\lambda \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right].\]</div>
<p>This optimization objective can be broken down into two parts:</p>
<ol>
<li><strong>Inner Optimization</strong>: For each datapoint <span class="arithmatex">\(x\)</span>, we find the best variational parameters <span class="arithmatex">\(\lambda\)</span> that make <span class="arithmatex">\(q_\lambda(z)\)</span> as close as possible to the true posterior <span class="arithmatex">\(p(z|x)\)</span>. This is done by maximizing the ELBO with respect to <span class="arithmatex">\(\lambda\)</span>. </li>
</ol>
<p>Why do we need <span class="arithmatex">\(q_\lambda(z)\)</span> to approximate <span class="arithmatex">\(p(z|x)\)</span>? Since <span class="arithmatex">\(p(x) = p(x,z)/p(z|x)\)</span>, as <span class="arithmatex">\(q_\lambda(z)\)</span> tends to <span class="arithmatex">\(p(z|x)\)</span>, the ratio <span class="arithmatex">\(p(x,z)/q_\lambda(z)\)</span> tends to <span class="arithmatex">\(p(x)\)</span>. This means that by making our variational approximation closer to the true posterior, we get a better estimate of the marginal likelihood <span class="arithmatex">\(p(x)\)</span>.</p>
<ol>
<li><strong>Outer Optimization</strong>: Across all datapoints in the dataset <span class="arithmatex">\(\mathcal{D}\)</span>, we find the best model parameters <span class="arithmatex">\(\theta\)</span> that maximize the average ELBO. This improves the generative model's ability to explain the data.</li>
</ol>
<p>The outer sum <span class="arithmatex">\(\sum_{x \in \mathcal{D}}\)</span> is necessary because we want to learn a model that works well for all datapoints in our dataset, not just a single example. This is equivalent to maximizing the average ELBO across all datapoints.</p>
<h2 id="black-box-variational-inference">Black-Box Variational Inference</h2>
<p>We shall focus on first-order stochastic gradient methods for optimizing the ELBO.
This inspires Black-Box Variational Inference (BBVI), a general-purpose Expectation-Maximization-like algorithm for variational learning of latent variable models, where, for each mini-batch <span class="arithmatex">\(\mathcal{B} = \{x^{(1)}, \ldots, x^{(m)}\}\)</span>, the following two steps are performed.</p>
<p><strong>Step 1</strong></p>
<p>We first do per-sample optimization of <span class="arithmatex">\(q\)</span> by iteratively applying the update</p>
<div class="arithmatex">\[\lambda^{(i)} \leftarrow \lambda^{(i)} + \tilde{\nabla}_\lambda \text{ELBO}(x^{(i)}; \theta, \lambda^{(i)}),\]</div>
<p>where <span class="arithmatex">\(\text{ELBO}(x; \theta, \lambda) = \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right]\)</span>, and <span class="arithmatex">\(\tilde{\nabla}_\lambda\)</span> denotes an unbiased estimate of the ELBO gradient. This step seeks to approximate the log-likelihood <span class="arithmatex">\(\log p_\theta(x^{(i)})\)</span>.</p>
<p><strong>Step 2</strong></p>
<p>We then perform a single update step based on the mini-batch</p>
<div class="arithmatex">\[\theta \leftarrow \theta + \tilde{\nabla}_\theta \sum_i \text{ELBO}(x^{(i)}; \theta, \lambda^{(i)}),\]</div>
<p>which corresponds to the step that hopefully moves <span class="arithmatex">\(p_\theta\)</span> closer to <span class="arithmatex">\(p_{data}\)</span>.</p>
<h2 id="gradient-estimation">Gradient Estimation</h2>
<p>The gradients <span class="arithmatex">\(\nabla_\lambda \text{ELBO}\)</span> and <span class="arithmatex">\(\nabla_\theta \text{ELBO}\)</span> can be estimated via Monte Carlo sampling. While it is straightforward to construct an unbiased estimate of <span class="arithmatex">\(\nabla_\theta \text{ELBO}\)</span> by simply pushing <span class="arithmatex">\(\nabla_\theta\)</span> through the expectation operator, the same cannot be said for <span class="arithmatex">\(\nabla_\lambda\)</span>. Instead, we see that</p>
<div class="arithmatex">\[\nabla_\lambda \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right] = \mathbb{E}_{q_\lambda(z)}\left[\left(\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right) \cdot \nabla_\lambda \log q_\lambda(z)\right].\]</div>
<p>This equality follows from the log-derivative trick (also commonly referred to as the REINFORCE trick). To derive this, we start with the gradient of the expectation:</p>
<div class="arithmatex">\[\nabla_\lambda \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right] = \nabla_\lambda \int q_\lambda(z) \log\frac{p_\theta(x,z)}{q_\lambda(z)} dz\]</div>
<p>Using the product rule and chain rule:</p>
<div class="arithmatex">\[= \int \nabla_\lambda q_\lambda(z) \cdot \log\frac{p_\theta(x,z)}{q_\lambda(z)} + q_\lambda(z) \cdot \nabla_\lambda \log\frac{p_\theta(x,z)}{q_\lambda(z)} dz\]</div>
<p>The second term vanishes because:
<span class="arithmatex">\(\nabla_\lambda \log\frac{p_\theta(x,z)}{q_\lambda(z)} = \nabla_\lambda [\log p_\theta(x,z) - \log q_\lambda(z)]\)</span>.
Since <span class="arithmatex">\(p_\theta(x,z)\)</span> doesn't depend on <span class="arithmatex">\(\lambda\)</span>, <span class="arithmatex">\(\nabla_\lambda \log p_\theta(x,z) = 0\)</span>. Therefore, <span class="arithmatex">\(\nabla_\lambda \log\frac{p_\theta(x,z)}{q_\lambda(z)} = -\nabla_\lambda \log q_\lambda(z)\)</span>. 
When we multiply by <span class="arithmatex">\(q_\lambda(z)\)</span> and integrate, we get:</p>
<div class="arithmatex">\[\int q_\lambda(z) \cdot (-\nabla_\lambda \log q_\lambda(z)) dz = -\int \nabla_\lambda q_\lambda(z) dz = -\nabla_\lambda \int q_\lambda(z) dz = -\nabla_\lambda 1 = 0\]</div>
<p>where we used the fact that <span class="arithmatex">\(\int q_\lambda(z) dz = 1\)</span> for any valid probability distribution.</p>
<p>For the first term, we use the identity <span class="arithmatex">\(\nabla_\lambda q_\lambda(z) = q_\lambda(z) \nabla_\lambda \log q_\lambda(z)\)</span>:</p>
<div class="arithmatex">\[= \int q_\lambda(z) \nabla_\lambda \log q_\lambda(z) \cdot \log\frac{p_\theta(x,z)}{q_\lambda(z)} dz\]</div>
<p>This can be rewritten as an expectation:</p>
<div class="arithmatex">\[= \mathbb{E}_{q_\lambda(z)}\left[\left(\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right) \cdot \nabla_\lambda \log q_\lambda(z)\right]\]</div>
<p>The gradient estimator <span class="arithmatex">\(\tilde{\nabla}_\lambda \text{ELBO}\)</span> is thus</p>
<div class="arithmatex">\[\frac{1}{k}\sum_{i=1}^k \left[\left(\log\frac{p_\theta(x,z^{(i)})}{q_\lambda(z^{(i)})}\right) \cdot \nabla_\lambda \log q_\lambda(z^{(i)})\right], \text{ where } z^{(i)} \sim q_\lambda(z).\]</div>
<p>However, it is often noted that this estimator suffers from high variance. One of the key contributions of the variational autoencoder paper is the reparameterization trick, which introduces a fixed, auxiliary distribution <span class="arithmatex">\(p(\epsilon)\)</span> and a differentiable function <span class="arithmatex">\(T(\epsilon; \lambda)\)</span> such that the procedure</p>
<div class="arithmatex">\[\epsilon \sim p(\epsilon)\]</div>
<div class="arithmatex">\[z \leftarrow T(\epsilon; \lambda),\]</div>
<p>is equivalent to sampling from <span class="arithmatex">\(q_\lambda(z)\)</span>. This two-step procedure works as follows:</p>
<ol>
<li>First, we sample <span class="arithmatex">\(\epsilon\)</span> from a fixed distribution <span class="arithmatex">\(p(\epsilon)\)</span> that doesn't depend on <span class="arithmatex">\(\lambda\)</span> (e.g., standard normal)</li>
<li>Then, we transform this sample using a deterministic function <span class="arithmatex">\(T(\epsilon; \lambda)\)</span> that depends on <span class="arithmatex">\(\lambda\)</span></li>
</ol>
<p>The key insight is that if we choose <span class="arithmatex">\(T\)</span> appropriately, the distribution of <span class="arithmatex">\(z = T(\epsilon; \lambda)\)</span> will be exactly <span class="arithmatex">\(q_\lambda(z)\)</span>. For example, if <span class="arithmatex">\(q_\lambda(z)\)</span> is a normal distribution with mean <span class="arithmatex">\(\mu_\lambda\)</span> and standard deviation <span class="arithmatex">\(\sigma_\lambda\)</span>, we can use:</p>
<p><span class="arithmatex">\(p(\epsilon) = \mathcal{N}(0, 1)\)</span></p>
<p><span class="arithmatex">\(T(\epsilon; \lambda) = \mu_\lambda + \sigma_\lambda \cdot \epsilon\)</span></p>
<p>By the Law of the Unconscious Statistician, we can see that</p>
<div class="arithmatex">\[\nabla_\lambda \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right] = \mathbb{E}_{p(\epsilon)}\left[\nabla_\lambda \log\frac{p_\theta(x,T(\epsilon; \lambda))}{q_\lambda(T(\epsilon; \lambda))}\right].\]</div>
<p>In contrast to the REINFORCE trick, the reparameterization trick is often noted empirically to have lower variance and thus results in more stable training.</p>
<h2 id="parameterizing-distributions-via-deep-neural-networks">Parameterizing Distributions via Deep Neural Networks</h2>
<p>So far, we have described <span class="arithmatex">\(p_\theta(x,z)\)</span> and <span class="arithmatex">\(q_\lambda(z)\)</span> in the abstract. To instantiate these objects, we consider choices of parametric distributions for <span class="arithmatex">\(p_\theta(z)\)</span>, <span class="arithmatex">\(p_\theta(x|z)\)</span>, and <span class="arithmatex">\(q_\lambda(z)\)</span>. A popular choice for <span class="arithmatex">\(p_\theta(z)\)</span> is the unit Gaussian</p>
<div class="arithmatex">\[p_\theta(z) = \mathcal{N}(z|0,I),\]</div>
<p>in which case <span class="arithmatex">\(\theta\)</span> is simply the empty set since the prior is a fixed distribution.</p>
<p>In the case where <span class="arithmatex">\(p_\theta(x|z)\)</span> is a Gaussian distribution, we can thus represent it as</p>
<div class="arithmatex">\[p_\theta(x|z) = \mathcal{N}(x|\mu_\theta(z), \Sigma_\theta(z)),\]</div>
<p>where <span class="arithmatex">\(\mu_\theta(z)\)</span> and <span class="arithmatex">\(\Sigma_\theta(z)\)</span> are neural networks that specify the mean and covariance matrix for the Gaussian distribution over <span class="arithmatex">\(x\)</span> when conditioned on <span class="arithmatex">\(z\)</span>.</p>
<p>Finally, the variational family for the proposal distribution <span class="arithmatex">\(q_\lambda(z)\)</span> needs to be chosen judiciously so that the reparameterization trick is possible. Many continuous distributions in the location-scale family can be reparameterized. In practice, a popular choice is again the Gaussian distribution, where</p>
<div class="arithmatex">\[\begin{align*}
\lambda &amp;= (\mu, \Sigma) \\
q_\lambda(z) &amp;= \mathcal{N}(z|\mu, \Sigma) \\
p(\varepsilon) &amp;= \mathcal{N}(z|0,I) \\
T(\varepsilon; \lambda) &amp;= \mu + \Sigma^{1/2}\varepsilon,
\end{align*}\]</div>
<p>where <span class="arithmatex">\(\Sigma^{1/2}\)</span> is the Cholesky decomposition of <span class="arithmatex">\(\Sigma\)</span>. For simplicity, practitioners often restrict <span class="arithmatex">\(\Sigma\)</span> to be a diagonal matrix (which restricts the distribution family to that of factorized Gaussians).</p>
<p>The reparameterization trick consists of four key steps:</p>
<ol>
<li>
<p><strong>Parameter Definition</strong>: We define the variational parameters <span class="arithmatex">\(\lambda\)</span> as a tuple containing the mean vector <span class="arithmatex">\(\mu\)</span> and covariance matrix <span class="arithmatex">\(\Sigma\)</span> of our Gaussian distribution. These parameters will be learned during training.</p>
</li>
<li>
<p><strong>Variational Distribution</strong>: We specify that our variational distribution <span class="arithmatex">\(q_\lambda(z)\)</span> is a Gaussian distribution parameterized by <span class="arithmatex">\(\mu\)</span> and <span class="arithmatex">\(\Sigma\)</span>. This is the distribution we ideally want to sample from.</p>
</li>
<li>
<p><strong>Auxiliary Distribution</strong>: Instead of sampling directly from <span class="arithmatex">\(q_\lambda(z)\)</span>, we introduce a fixed auxiliary distribution <span class="arithmatex">\(p(\varepsilon)\)</span> which is a standard normal distribution (mean 0, identity covariance). This distribution doesn't depend on our parameters <span class="arithmatex">\(\lambda\)</span>.</p>
</li>
<li>
<p><strong>Transformation Function</strong>: We define a deterministic function <span class="arithmatex">\(T(\varepsilon; \lambda)\)</span> that transforms samples from the auxiliary distribution into samples from our variational distribution. The transformation is given by <span class="arithmatex">\(\mu + \Sigma^{1/2}\varepsilon\)</span>, where <span class="arithmatex">\(\Sigma^{1/2}\)</span> is the Cholesky decomposition of <span class="arithmatex">\(\Sigma\)</span>.</p>
</li>
</ol>
<p>The key insight is that instead of sampling directly from <span class="arithmatex">\(q_\lambda(z)\)</span>, we can:
1. Sample <span class="arithmatex">\(\varepsilon\)</span> from the standard normal distribution <span class="arithmatex">\(p(\varepsilon)\)</span>
2. Transform it using <span class="arithmatex">\(T(\varepsilon; \lambda)\)</span> to make it seem like we're getting a sample from <span class="arithmatex">\(q_\lambda(z)\)</span></p>
<p>This trick is crucial because it allows us to compute gradients with respect to <span class="arithmatex">\(\lambda\)</span> through the sampling process. Since the transformation <span class="arithmatex">\(T\)</span> is differentiable, we can backpropagate through it to update the parameters <span class="arithmatex">\(\lambda\)</span> during training. This is why the reparameterization trick often leads to lower variance in gradient estimates compared to the REINFORCE trick.</p>
<h2 id="amortized-variational-inference">Amortized Variational Inference</h2>
<p>A noticeable limitation of black-box variational inference is that Step 1 executes an optimization subroutine that is computationally expensive. Recall that the goal of Step 1 is to find</p>
<div class="arithmatex">\[\lambda^* = \arg\max_{\lambda \in \Lambda} \text{ELBO}(x; \theta, \lambda).\]</div>
<p>For a given choice of <span class="arithmatex">\(\theta\)</span>, there is a well-defined mapping from <span class="arithmatex">\(x \mapsto \lambda^*\)</span>. A key realization is that this mapping can be learned. In particular, one can train an encoding function (parameterized by <span class="arithmatex">\(\phi\)</span>) <span class="arithmatex">\(f_\phi: \mathcal{X} \to \Lambda\)</span> (where <span class="arithmatex">\(\Lambda\)</span> is the space of <span class="arithmatex">\(\lambda\)</span> parameters) on the following objective</p>
<div class="arithmatex">\[\max_\phi \sum_{x \in \mathcal{D}} \text{ELBO}(x; \theta, f_\phi(x)).\]</div>
<p>It is worth noting at this point that <span class="arithmatex">\(f_\phi(x)\)</span> can be interpreted as defining the conditional distribution <span class="arithmatex">\(q_\phi(z|x)\)</span>. With a slight abuse of notation, we define</p>
<div class="arithmatex">\[\text{ELBO}(x; \theta, \phi) = \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\right],\]</div>
<p>and rewrite the optimization problem as</p>
<div class="arithmatex">\[\max_\phi \sum_{x \in \mathcal{D}} \text{ELBO}(x; \theta, \phi).\]</div>
<p>It is also worth noting that optimizing <span class="arithmatex">\(\phi\)</span> over the entire dataset as a subroutine every time we sample a new mini-batch is clearly not reasonable. However, if we believe that <span class="arithmatex">\(f_\phi\)</span> is capable of quickly adapting to a close-enough approximation of <span class="arithmatex">\(\lambda^*\)</span> given the current choice of <span class="arithmatex">\(\theta\)</span>, then we can interleave the optimization of <span class="arithmatex">\(\phi\)</span> and <span class="arithmatex">\(\theta\)</span>. This yields the following procedure, where for each mini-batch <span class="arithmatex">\(\mathcal{B} = \{x^{(1)}, \ldots, x^{(m)}\}\)</span>, we perform the following two updates jointly:</p>
<div class="arithmatex">\[\begin{align*}
\phi &amp;\leftarrow \phi + \tilde{\nabla}_\phi \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi) \\
\theta &amp;\leftarrow \theta + \tilde{\nabla}_\theta \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi),
\end{align*}\]</div>
<p>rather than running BBVI's Step 1 as a subroutine. By leveraging the learnability of <span class="arithmatex">\(x \mapsto \lambda^*\)</span>, this optimization procedure amortizes the cost of variational inference. If one further chooses to define <span class="arithmatex">\(f_\phi\)</span> as a neural network, the result is the variational autoencoder.</p>
<h3 id="steps-of-amortized-variational-inference">Steps of Amortized Variational Inference</h3>
<p>Let's break down the amortized variational inference procedure in detail:</p>
<ol>
<li><strong>Initial Setup</strong>:</li>
<li>We have a dataset <span class="arithmatex">\(\mathcal{D} = \{x^{(1)}, \ldots, x^{(n)}\}\)</span></li>
<li>We have a generative model <span class="arithmatex">\(p_\theta(x,z)\)</span> with parameters <span class="arithmatex">\(\theta\)</span></li>
<li>
<p>We want to learn both the model parameters <span class="arithmatex">\(\theta\)</span> and the variational parameters <span class="arithmatex">\(\lambda\)</span> for each datapoint</p>
</li>
<li>
<p><strong>Traditional BBVI Approach</strong>:</p>
</li>
<li>For each datapoint <span class="arithmatex">\(x\)</span>, we would need to run an optimization to find:</li>
</ol>
<div class="arithmatex">\[\lambda^* = \arg\max_{\lambda \in \Lambda} \text{ELBO}(x; \theta, \lambda)\]</div>
<ul>
<li>
<p>This is computationally expensive as it requires running an optimization subroutine for each datapoint</p>
</li>
<li>
<p><strong>Key Insight - Learnable Mapping</strong>:</p>
</li>
<li>Instead of optimizing <span class="arithmatex">\(\lambda\)</span> separately for each <span class="arithmatex">\(x\)</span>, we realize that there's a mapping from <span class="arithmatex">\(x\)</span> to <span class="arithmatex">\(\lambda^*\)</span></li>
<li>This mapping can be learned using a function <span class="arithmatex">\(f_\phi: \mathcal{X} \to \Lambda\)</span> parameterized by <span class="arithmatex">\(\phi\)</span></li>
<li>
<p>The function <span class="arithmatex">\(f_\phi\)</span> takes a datapoint <span class="arithmatex">\(x\)</span> and outputs the variational parameters <span class="arithmatex">\(\lambda\)</span></p>
</li>
<li>
<p><strong>Training the Encoder</strong>:</p>
</li>
<li>We train <span class="arithmatex">\(f_\phi\)</span> to maximize the ELBO across all datapoints:</li>
</ul>
<div class="arithmatex">\[\max_\phi \sum_{x \in \mathcal{D}} \text{ELBO}(x; \theta, f_\phi(x))\]</div>
<ul>
<li>
<p>This is equivalent to learning a conditional distribution <span class="arithmatex">\(q_\phi(z|x)\)</span></p>
</li>
<li>
<p><strong>Joint Optimization</strong>:</p>
</li>
<li>Instead of running BBVI's Step 1 as a subroutine, we interleave the optimization of <span class="arithmatex">\(\phi\)</span> and <span class="arithmatex">\(\theta\)</span></li>
<li>For each mini-batch <span class="arithmatex">\(\mathcal{B} = \{x^{(1)}, \ldots, x^{(m)}\}\)</span>, we perform two updates:</li>
</ul>
<div class="arithmatex">\[\begin{align*}
\phi &amp;\leftarrow \phi + \tilde{\nabla}_\phi \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi) \\
\theta &amp;\leftarrow \theta + \tilde{\nabla}_\theta \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi)
\end{align*}\]</div>
<ol>
<li><strong>Practical Implementation</strong>:</li>
<li>When <span class="arithmatex">\(f_\phi\)</span> is implemented as a neural network, we get a variational autoencoder</li>
<li>The encoder network <span class="arithmatex">\(f_\phi\)</span> maps inputs <span class="arithmatex">\(x\)</span> to variational parameters</li>
<li>The decoder network maps latent variables <span class="arithmatex">\(z\)</span> to reconstructed inputs</li>
<li>Both networks are trained end-to-end using the ELBO objective</li>
</ol>
<p>The key advantage of this approach is that it amortizes the cost of variational inference by learning a single function <span class="arithmatex">\(f_\phi\)</span> that can quickly approximate the optimal variational parameters for any input <span class="arithmatex">\(x\)</span>, rather than running a separate optimization for each datapoint.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>