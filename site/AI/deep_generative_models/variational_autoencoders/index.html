
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/ai/deep_generative_models/variational_autoencoders/">
      
      
        <link rel="prev" href="../autoregressive_models/">
      
      
        <link rel="next" href="../normalizing_flow_models/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Variational Autoencoders - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#variational-autoencoders" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Variational Autoencoders
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-tabs__link">
          
  
  
  Productivity

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#representation" class="md-nav__link">
    <span class="md-ellipsis">
      Representation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-directed-latent-variable-models" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Directed Latent Variable Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#black-box-variational-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Black-Box Variational Inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Estimation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parameterizing-distributions-via-deep-neural-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Parameterizing Distributions via Deep Neural Networks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#amortized-variational-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Amortized Variational Inference
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Amortized Variational Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#steps-of-amortized-variational-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Steps of Amortized Variational Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#decomposition-of-the-negative-elbo" class="md-nav__link">
    <span class="md-ellipsis">
      Decomposition of the Negative ELBO
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#practical-implementation-of-elbo-computation" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Implementation of ELBO Computation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#-vae" class="md-nav__link">
    <span class="md-ellipsis">
      Î²-VAE
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#importance-weighted-autoencoder-iwae" class="md-nav__link">
    <span class="md-ellipsis">
      Importance Weighted Autoencoder (IWAE)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gaussian-mixture-vae-gmvae" class="md-nav__link">
    <span class="md-ellipsis">
      Gaussian Mixture VAE (GMVAE)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-semi-supervised-vae-ssvae" class="md-nav__link">
    <span class="md-ellipsis">
      The Semi-Supervised VAE (SSVAE)
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_generative_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Generative Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluating_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluating Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_diffusion_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Learning for Computer Vision
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning for Computer Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/image_classification_with_linear_classifiers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Classification with Linear Classifiers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors, vector addition, and scalar multiplication
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector geometry in Rn and correlation coefficients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/planes_in_r3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Planes in R3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/span_subspaces_and_dimension/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Span, subspaces, and dimension
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/basis_and_orthogonality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basis and orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Applications of projections in Rn- orthogonal bases of planes and linear regression
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/story_proofs_and_axioms_of_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Story Proofs and Axioms of Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/some_famous_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Some famous problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/indicator_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Indicator Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/poisson_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Poisson Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/continuous_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Continuous Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/exponential_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exponential Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/joint_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Joint Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/independence_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Independence of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/multinomial_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multinomial Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/covariance_and_correlation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Covariance and Correlation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/transformations_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformations of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/convolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolution
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Productivity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Productivity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    How to Build Your Career in AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            How to Build Your Career in AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Three Steps to Career Growth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_1_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 1- Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_2_projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 2- Projects
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_3_job/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 3- Job
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="variational-autoencoders">Variational autoencoders</h1>
<h2 id="representation">Representation</h2>
<p>Consider a directed, latent variable model as shown below.</p>
<p><img alt="Graphical model for a directed, latent variable model." src="../vae_zx.png" /></p>
<p>In the model above, <span class="arithmatex">\(z\)</span> and <span class="arithmatex">\(x\)</span> denote the latent and observed variables respectively. The joint distribution expressed by this model is given as</p>
<div class="arithmatex">\[p(x,z) = p(x|z)p(z).\]</div>
<p>From a generative modeling perspective, this model describes a generative process for the observed data <span class="arithmatex">\(x\)</span> using the following procedure:</p>
<div class="arithmatex">\[z \sim p(z)\]</div>
<div class="arithmatex">\[x \sim p(x|z)\]</div>
<p>If one adopts the belief that the latent variables <span class="arithmatex">\(z\)</span> somehow encode semantically meaningful information about <span class="arithmatex">\(x\)</span>, it is natural to view this generative process as first generating the "high-level" semantic information about <span class="arithmatex">\(x\)</span> first before fully generating <span class="arithmatex">\(x\)</span>.</p>
<p>We now consider a family of distributions <span class="arithmatex">\(\mathcal{Z}\)</span> where <span class="arithmatex">\(p(z) \in \mathcal{Z}\)</span> describes a probability distribution over <span class="arithmatex">\(z\)</span>. Next, consider a family of conditional distributions <span class="arithmatex">\(\mathcal{X|Z}\)</span> where <span class="arithmatex">\(p(x|z) \in \mathcal{X|Z}\)</span> describes a conditional probability distribution over <span class="arithmatex">\(x\)</span> given <span class="arithmatex">\(z\)</span>. Then our hypothesis class of generative models is the set of all possible combinations</p>
<div class="arithmatex">\[\mathcal{X,Z} = \{p(x,z) \mid p(z) \in \mathcal{Z}, p(x|z) \in \mathcal{X|Z}\}.\]</div>
<p>Given a dataset <span class="arithmatex">\(\mathcal{D} = \{x^{(1)}, \ldots, x^{(n)}\}\)</span>, we are interested in the following learning and inference tasks:</p>
<ol>
<li>Selecting <span class="arithmatex">\(p \in \mathcal{X,Z}\)</span> that "best" fits <span class="arithmatex">\(\mathcal{D}\)</span>.</li>
<li>Given a sample <span class="arithmatex">\(x\)</span> and a model <span class="arithmatex">\(p \in \mathcal{X,Z}\)</span>, what is the posterior distribution over the latent variables <span class="arithmatex">\(z\)</span>?</li>
</ol>
<p>The posterior distribution <span class="arithmatex">\(p(z|x)\)</span> represents our updated beliefs about the latent variables <span class="arithmatex">\(z\)</span> after observing the data <span class="arithmatex">\(x\)</span>. In other words, it tells us what values of <span class="arithmatex">\(z\)</span> are most likely to have generated the observed <span class="arithmatex">\(x\)</span>. This is particularly useful for tasks like feature extraction, where we want to understand what latent factors might have generated our observed data.</p>
<h2 id="learning-directed-latent-variable-models">Learning Directed Latent Variable Models</h2>
<p>One way to measure how closely <span class="arithmatex">\(p(x,z)\)</span> fits the observed dataset <span class="arithmatex">\(\mathcal{D}\)</span> is to measure the Kullback-Leibler (KL) divergence between the data distribution (which we denote as <span class="arithmatex">\(p_{data}(x)\)</span>) and the model's marginal distribution <span class="arithmatex">\(p(x) = \int p(x,z)dz\)</span>. The distribution that "best" fits the data is thus obtained by minimizing the KL divergence.</p>
<div class="arithmatex">\[\min_{p \in \mathcal{X,Z}} D_{KL}(p_{data}(x) \| p(x)).\]</div>
<p>As we have seen previously, optimizing an empirical estimate of the KL divergence is equivalent to maximizing the marginal log-likelihood <span class="arithmatex">\(\log p(x)\)</span> over <span class="arithmatex">\(\mathcal{D}\)</span>:</p>
<div class="arithmatex">\[\max_{p \in \mathcal{X,Z}} \sum_{x \in \mathcal{D}} \log p(x) = \max_{p \in \mathcal{X,Z}} \sum_{x \in \mathcal{D}} \log \int p(x,z)dz.\]</div>
<p>However, it turns out this problem is generally intractable for high-dimensional <span class="arithmatex">\(z\)</span> as it involves an integration (or sums in the case <span class="arithmatex">\(z\)</span> is discrete) over all the possible latent sources of variation <span class="arithmatex">\(z\)</span>. This intractability arises from several challenges:</p>
<ol>
<li>
<p><strong>Computational Complexity</strong>: The integral <span class="arithmatex">\(\int p(x,z)dz\)</span> requires evaluating the joint distribution <span class="arithmatex">\(p(x,z)\)</span> for all possible values of <span class="arithmatex">\(z\)</span>. In high-dimensional spaces, this becomes computationally prohibitive as the number of points to evaluate grows exponentially with the dimension of <span class="arithmatex">\(z\)</span>.</p>
</li>
<li>
<p><strong>Numerical Integration</strong>: Even if we could evaluate the integrand at all points, computing the integral numerically becomes increasingly difficult as the dimension of <span class="arithmatex">\(z\)</span> grows. Traditional numerical integration methods like quadrature become impractical in high dimensions.</p>
</li>
<li>
<p><strong>Posterior Inference</strong>: The intractability of the marginal likelihood also makes it difficult to compute the posterior distribution <span class="arithmatex">\(p(z|x)\)</span>, which is crucial for tasks like feature extraction and data generation.</p>
</li>
</ol>
<p>This intractability motivates the need for approximate inference methods, such as variational inference. One option is to estimate the objective via Monte Carlo. For any given datapoint <span class="arithmatex">\(x\)</span>, we can obtain the following estimate for its marginal log-likelihood:</p>
<div class="arithmatex">\[\log p(x) \approx \log \frac{1}{k} \sum_{i=1}^k p(x|z^{(i)}), \text{ where } z^{(i)} \sim p(z)\]</div>
<p>This Monte Carlo estimate is derived as follows:</p>
<p>First, recall that the marginal likelihood <span class="arithmatex">\(p(x)\)</span> can be written as an expectation:</p>
<div class="arithmatex">\[p(x) = \int p(x|z)p(z)dz = \mathbb{E}_{z \sim p(z)}[p(x|z)]\]</div>
<p>The Monte Carlo method approximates this expectation by drawing <span class="arithmatex">\(k\)</span> samples from <span class="arithmatex">\(p(z)\)</span> and computing their average:</p>
<div class="arithmatex">\[\mathbb{E}_{z \sim p(z)}[p(x|z)] \approx \frac{1}{k} \sum_{i=1}^k p(x|z^{(i)}), \text{ where } z^{(i)} \sim p(z)\]</div>
<p>Taking the logarithm of both sides gives us our final estimate:</p>
<div class="arithmatex">\[\log p(x) \approx \log \frac{1}{k} \sum_{i=1}^k p(x|z^{(i)}), \text{ where } z^{(i)} \sim p(z)\]</div>
<p>This approximation becomes more accurate as <span class="arithmatex">\(k\)</span> increases, but at the cost of more computational resources. The key insight is that we're using random sampling to approximate the intractable integral, trading exact computation for statistical estimation.</p>
<p>Rather than maximizing the log-likelihood directly, an alternate is to instead construct a lower bound that is more amenable to optimization. To do so, we note that evaluating the marginal likelihood <span class="arithmatex">\(p(x)\)</span> is at least as difficult as as evaluating the posterior <span class="arithmatex">\(p(z|x)\)</span> for any latent vector <span class="arithmatex">\(z\)</span> since by definition <span class="arithmatex">\(p(z|x) = p(x,z)/p(x)\)</span>.</p>
<p>Next, we introduce a variational family <span class="arithmatex">\(\mathcal{Q}\)</span> of distributions that approximate the true, but intractable posterior <span class="arithmatex">\(p(z|x)\)</span>. Further henceforth, we will assume a parameteric setting where any distribution in the model family <span class="arithmatex">\(\mathcal{X,Z}\)</span> is specified via a set of parameters <span class="arithmatex">\(\theta \in \Theta\)</span> and distributions in the variational family <span class="arithmatex">\(\mathcal{Q}\)</span> are specified via a set of parameters <span class="arithmatex">\(\lambda \in \Lambda\)</span>.</p>
<p>Given <span class="arithmatex">\(\mathcal{X,Z}\)</span> and <span class="arithmatex">\(\mathcal{Q}\)</span>, we note that the following relationships hold true for any <span class="arithmatex">\(x\)</span> and all variational distributions <span class="arithmatex">\(q_\lambda(z) \in \mathcal{Q}\)</span>:</p>
<div class="arithmatex">\[\log p_\theta(x) = \log \int p_\theta(x,z)dz = \log \int \frac{q_\lambda(z)}{q_\lambda(z)}p_\theta(x,z)dz \geq \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right] := \text{ELBO}(x;\theta,\lambda)\]</div>
<p>where we have used Jensen's inequality in the final step. The key insight here is that since the logarithm function is concave, Jensen's inequality tells us that for any random variable <span class="arithmatex">\(X\)</span> and concave function <span class="arithmatex">\(f\)</span>, we have <span class="arithmatex">\(\mathbb{E}[f(X)] \leq f(\mathbb{E}[X])\)</span>. In our case:</p>
<p>We first multiply and divide by <span class="arithmatex">\(q_\lambda(z)\)</span> inside the integral to get:</p>
<div class="arithmatex">\[\log \int \frac{q_\lambda(z)}{q_\lambda(z)}p_\theta(x,z)dz = \log \int q_\lambda(z)\frac{p_\theta(x,z)}{q_\lambda(z)}dz\]</div>
<p>The integral <span class="arithmatex">\(\int q_\lambda(z)\frac{p_\theta(x,z)}{q_\lambda(z)}dz\)</span> can be seen as an expectation <span class="arithmatex">\(\mathbb{E}_{q_\lambda(z)}\left[\frac{p_\theta(x,z)}{q_\lambda(z)}\right]\)</span></p>
<p>Since <span class="arithmatex">\(\log\)</span> is a concave function, Jensen's inequality gives us:</p>
<div class="arithmatex">\[\log \mathbb{E}_{q_\lambda(z)}\left[\frac{p_\theta(x,z)}{q_\lambda(z)}\right] \geq \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right]\]</div>
<p>This inequality is what allows us to obtain a lower bound on the log-likelihood, which we call the Evidence Lower BOund (ELBO). The ELBO admits a tractable unbiased Monte Carlo estimator</p>
<div class="arithmatex">\[\frac{1}{k}\sum_{i=1}^k \log\frac{p_\theta(x,z^{(i)})}{q_\lambda(z^{(i)})}, \text{ where } z^{(i)} \sim q_\lambda(z),\]</div>
<p>so long as it is easy to sample from and evaluate densities for <span class="arithmatex">\(q_\lambda(z)\)</span>.</p>
<p>In summary, we can learn a latent variable model by maximizing the ELBO with respect to both the model parameters <span class="arithmatex">\(\theta\)</span> and the variational parameters <span class="arithmatex">\(\lambda\)</span> for any given datapoint <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[\max_\theta \sum_{x \in \mathcal{D}} \max_\lambda \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right].\]</div>
<p>This optimization objective can be broken down into two parts:</p>
<ol>
<li><strong>Inner Optimization</strong>: For each datapoint <span class="arithmatex">\(x\)</span>, we find the best variational parameters <span class="arithmatex">\(\lambda\)</span> that make <span class="arithmatex">\(q_\lambda(z)\)</span> as close as possible to the true posterior <span class="arithmatex">\(p(z|x)\)</span>. This is done by maximizing the ELBO with respect to <span class="arithmatex">\(\lambda\)</span>. </li>
</ol>
<p>Why do we need <span class="arithmatex">\(q_\lambda(z)\)</span> to approximate <span class="arithmatex">\(p(z|x)\)</span>? Since <span class="arithmatex">\(p(x) = p(x,z)/p(z|x)\)</span>, as <span class="arithmatex">\(q_\lambda(z)\)</span> tends to <span class="arithmatex">\(p(z|x)\)</span>, the ratio <span class="arithmatex">\(p(x,z)/q_\lambda(z)\)</span> tends to <span class="arithmatex">\(p(x)\)</span>. This means that by making our variational approximation closer to the true posterior, we get a better estimate of the marginal likelihood <span class="arithmatex">\(p(x)\)</span>.</p>
<ol>
<li><strong>Outer Optimization</strong>: Across all datapoints in the dataset <span class="arithmatex">\(\mathcal{D}\)</span>, we find the best model parameters <span class="arithmatex">\(\theta\)</span> that maximize the average ELBO. This improves the generative model's ability to explain the data.</li>
</ol>
<p>The outer sum <span class="arithmatex">\(\sum_{x \in \mathcal{D}}\)</span> is necessary because we want to learn a model that works well for all datapoints in our dataset, not just a single example. This is equivalent to maximizing the average ELBO across all datapoints.</p>
<h2 id="black-box-variational-inference">Black-Box Variational Inference</h2>
<p>We shall focus on first-order stochastic gradient methods for optimizing the ELBO.
This inspires Black-Box Variational Inference (BBVI), a general-purpose Expectation-Maximization-like algorithm for variational learning of latent variable models, where, for each mini-batch <span class="arithmatex">\(\mathcal{B} = \{x^{(1)}, \ldots, x^{(m)}\}\)</span>, the following two steps are performed.</p>
<p><strong>Step 1</strong></p>
<p>We first do per-sample optimization of <span class="arithmatex">\(q\)</span> by iteratively applying the update</p>
<div class="arithmatex">\[\lambda^{(i)} \leftarrow \lambda^{(i)} + \tilde{\nabla}_\lambda \text{ELBO}(x^{(i)}; \theta, \lambda^{(i)}),\]</div>
<p>where <span class="arithmatex">\(\text{ELBO}(x; \theta, \lambda) = \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right]\)</span>, and <span class="arithmatex">\(\tilde{\nabla}_\lambda\)</span> denotes an unbiased estimate of the ELBO gradient. This step seeks to approximate the log-likelihood <span class="arithmatex">\(\log p_\theta(x^{(i)})\)</span>.</p>
<p><strong>Step 2</strong></p>
<p>We then perform a single update step based on the mini-batch</p>
<div class="arithmatex">\[\theta \leftarrow \theta + \tilde{\nabla}_\theta \sum_i \text{ELBO}(x^{(i)}; \theta, \lambda^{(i)}),\]</div>
<p>which corresponds to the step that hopefully moves <span class="arithmatex">\(p_\theta\)</span> closer to <span class="arithmatex">\(p_{data}\)</span>.</p>
<h2 id="gradient-estimation">Gradient Estimation</h2>
<p>The gradients <span class="arithmatex">\(\nabla_\lambda \text{ELBO}\)</span> and <span class="arithmatex">\(\nabla_\theta \text{ELBO}\)</span> can be estimated via Monte Carlo sampling. While it is straightforward to construct an unbiased estimate of <span class="arithmatex">\(\nabla_\theta \text{ELBO}\)</span> by simply pushing <span class="arithmatex">\(\nabla_\theta\)</span> through the expectation operator, the same cannot be said for <span class="arithmatex">\(\nabla_\lambda\)</span>. Instead, we see that</p>
<div class="arithmatex">\[\nabla_\lambda \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right] = \mathbb{E}_{q_\lambda(z)}\left[\left(\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right) \cdot \nabla_\lambda \log q_\lambda(z)\right].\]</div>
<p>This equality follows from the log-derivative trick (also commonly referred to as the REINFORCE trick). To derive this, we start with the gradient of the expectation:</p>
<div class="arithmatex">\[\nabla_\lambda \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right] = \nabla_\lambda \int q_\lambda(z) \log\frac{p_\theta(x,z)}{q_\lambda(z)} dz\]</div>
<p>Using the product rule and chain rule:</p>
<div class="arithmatex">\[= \int \nabla_\lambda q_\lambda(z) \cdot \log\frac{p_\theta(x,z)}{q_\lambda(z)} + q_\lambda(z) \cdot \nabla_\lambda \log\frac{p_\theta(x,z)}{q_\lambda(z)} dz\]</div>
<p>The second term vanishes because:
<span class="arithmatex">\(\nabla_\lambda \log\frac{p_\theta(x,z)}{q_\lambda(z)} = \nabla_\lambda [\log p_\theta(x,z) - \log q_\lambda(z)]\)</span>.
Since <span class="arithmatex">\(p_\theta(x,z)\)</span> doesn't depend on <span class="arithmatex">\(\lambda\)</span>, <span class="arithmatex">\(\nabla_\lambda \log p_\theta(x,z) = 0\)</span>. Therefore, <span class="arithmatex">\(\nabla_\lambda \log\frac{p_\theta(x,z)}{q_\lambda(z)} = -\nabla_\lambda \log q_\lambda(z)\)</span>. 
When we multiply by <span class="arithmatex">\(q_\lambda(z)\)</span> and integrate, we get:</p>
<div class="arithmatex">\[\int q_\lambda(z) \cdot (-\nabla_\lambda \log q_\lambda(z)) dz = -\int \nabla_\lambda q_\lambda(z) dz = -\nabla_\lambda \int q_\lambda(z) dz = -\nabla_\lambda 1 = 0\]</div>
<p>where we used the fact that <span class="arithmatex">\(\int q_\lambda(z) dz = 1\)</span> for any valid probability distribution.</p>
<p>For the first term, we use the identity <span class="arithmatex">\(\nabla_\lambda q_\lambda(z) = q_\lambda(z) \nabla_\lambda \log q_\lambda(z)\)</span>:</p>
<div class="arithmatex">\[= \int q_\lambda(z) \nabla_\lambda \log q_\lambda(z) \cdot \log\frac{p_\theta(x,z)}{q_\lambda(z)} dz\]</div>
<p>This can be rewritten as an expectation:</p>
<div class="arithmatex">\[= \mathbb{E}_{q_\lambda(z)}\left[\left(\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right) \cdot \nabla_\lambda \log q_\lambda(z)\right]\]</div>
<p>The gradient estimator <span class="arithmatex">\(\tilde{\nabla}_\lambda \text{ELBO}\)</span> is thus</p>
<div class="arithmatex">\[\frac{1}{k}\sum_{i=1}^k \left[\left(\log\frac{p_\theta(x,z^{(i)})}{q_\lambda(z^{(i)})}\right) \cdot \nabla_\lambda \log q_\lambda(z^{(i)})\right], \text{ where } z^{(i)} \sim q_\lambda(z).\]</div>
<p>However, it is often noted that this estimator suffers from high variance. One of the key contributions of the variational autoencoder paper is the reparameterization trick, which introduces a fixed, auxiliary distribution <span class="arithmatex">\(p(\epsilon)\)</span> and a differentiable function <span class="arithmatex">\(T(\epsilon; \lambda)\)</span> such that the procedure</p>
<div class="arithmatex">\[\epsilon \sim p(\epsilon)\]</div>
<div class="arithmatex">\[z \leftarrow T(\epsilon; \lambda),\]</div>
<p>is equivalent to sampling from <span class="arithmatex">\(q_\lambda(z)\)</span>. This two-step procedure works as follows:</p>
<ol>
<li>First, we sample <span class="arithmatex">\(\epsilon\)</span> from a fixed distribution <span class="arithmatex">\(p(\epsilon)\)</span> that doesn't depend on <span class="arithmatex">\(\lambda\)</span> (e.g., standard normal)</li>
<li>Then, we transform this sample using a deterministic function <span class="arithmatex">\(T(\epsilon; \lambda)\)</span> that depends on <span class="arithmatex">\(\lambda\)</span></li>
</ol>
<p>The key insight is that if we choose <span class="arithmatex">\(T\)</span> appropriately, the distribution of <span class="arithmatex">\(z = T(\epsilon; \lambda)\)</span> will be exactly <span class="arithmatex">\(q_\lambda(z)\)</span>. For example, if <span class="arithmatex">\(q_\lambda(z)\)</span> is a normal distribution with mean <span class="arithmatex">\(\mu_\lambda\)</span> and standard deviation <span class="arithmatex">\(\sigma_\lambda\)</span>, we can use:</p>
<p><span class="arithmatex">\(p(\epsilon) = \mathcal{N}(0, 1)\)</span></p>
<p><span class="arithmatex">\(T(\epsilon; \lambda) = \mu_\lambda + \sigma_\lambda \cdot \epsilon\)</span></p>
<p>By the Law of the Unconscious Statistician, we can see that</p>
<div class="arithmatex">\[\nabla_\lambda \mathbb{E}_{q_\lambda(z)}\left[\log\frac{p_\theta(x,z)}{q_\lambda(z)}\right] = \mathbb{E}_{p(\epsilon)}\left[\nabla_\lambda \log\frac{p_\theta(x,T(\epsilon; \lambda))}{q_\lambda(T(\epsilon; \lambda))}\right].\]</div>
<p>In contrast to the REINFORCE trick, the reparameterization trick is often noted empirically to have lower variance and thus results in more stable training.</p>
<h2 id="parameterizing-distributions-via-deep-neural-networks">Parameterizing Distributions via Deep Neural Networks</h2>
<p>So far, we have described <span class="arithmatex">\(p_\theta(x,z)\)</span> and <span class="arithmatex">\(q_\lambda(z)\)</span> in the abstract. To instantiate these objects, we consider choices of parametric distributions for <span class="arithmatex">\(p_\theta(z)\)</span>, <span class="arithmatex">\(p_\theta(x|z)\)</span>, and <span class="arithmatex">\(q_\lambda(z)\)</span>. A popular choice for <span class="arithmatex">\(p_\theta(z)\)</span> is the unit Gaussian</p>
<div class="arithmatex">\[p_\theta(z) = \mathcal{N}(z|0,I),\]</div>
<p>in which case <span class="arithmatex">\(\theta\)</span> is simply the empty set since the prior is a fixed distribution.</p>
<p>In the case where <span class="arithmatex">\(p_\theta(x|z)\)</span> is a Gaussian distribution, we can thus represent it as</p>
<div class="arithmatex">\[p_\theta(x|z) = \mathcal{N}(x|\mu_\theta(z), \Sigma_\theta(z)),\]</div>
<p>where <span class="arithmatex">\(\mu_\theta(z)\)</span> and <span class="arithmatex">\(\Sigma_\theta(z)\)</span> are neural networks that specify the mean and covariance matrix for the Gaussian distribution over <span class="arithmatex">\(x\)</span> when conditioned on <span class="arithmatex">\(z\)</span>.</p>
<p>Finally, the variational family for the proposal distribution <span class="arithmatex">\(q_\lambda(z)\)</span> needs to be chosen judiciously so that the reparameterization trick is possible. Many continuous distributions in the location-scale family can be reparameterized. In practice, a popular choice is again the Gaussian distribution, where</p>
<div class="arithmatex">\[\begin{align*}
\lambda &amp;= (\mu, \Sigma) \\
q_\lambda(z) &amp;= \mathcal{N}(z|\mu, \Sigma) \\
p(\varepsilon) &amp;= \mathcal{N}(z|0,I) \\
T(\varepsilon; \lambda) &amp;= \mu + \Sigma^{1/2}\varepsilon,
\end{align*}\]</div>
<p>where <span class="arithmatex">\(\Sigma^{1/2}\)</span> is the Cholesky decomposition of <span class="arithmatex">\(\Sigma\)</span>. For simplicity, practitioners often restrict <span class="arithmatex">\(\Sigma\)</span> to be a diagonal matrix (which restricts the distribution family to that of factorized Gaussians).</p>
<p>The reparameterization trick consists of four key steps:</p>
<ol>
<li>
<p><strong>Parameter Definition</strong>: We define the variational parameters <span class="arithmatex">\(\lambda\)</span> as a tuple containing the mean vector <span class="arithmatex">\(\mu\)</span> and covariance matrix <span class="arithmatex">\(\Sigma\)</span> of our Gaussian distribution. These parameters will be learned during training.</p>
</li>
<li>
<p><strong>Variational Distribution</strong>: We specify that our variational distribution <span class="arithmatex">\(q_\lambda(z)\)</span> is a Gaussian distribution parameterized by <span class="arithmatex">\(\mu\)</span> and <span class="arithmatex">\(\Sigma\)</span>. This is the distribution we ideally want to sample from.</p>
</li>
<li>
<p><strong>Auxiliary Distribution</strong>: Instead of sampling directly from <span class="arithmatex">\(q_\lambda(z)\)</span>, we introduce a fixed auxiliary distribution <span class="arithmatex">\(p(\varepsilon)\)</span> which is a standard normal distribution (mean 0, identity covariance). This distribution doesn't depend on our parameters <span class="arithmatex">\(\lambda\)</span>.</p>
</li>
<li>
<p><strong>Transformation Function</strong>: We define a deterministic function <span class="arithmatex">\(T(\varepsilon; \lambda)\)</span> that transforms samples from the auxiliary distribution into samples from our variational distribution. The transformation is given by <span class="arithmatex">\(\mu + \Sigma^{1/2}\varepsilon\)</span>, where <span class="arithmatex">\(\Sigma^{1/2}\)</span> is the Cholesky decomposition of <span class="arithmatex">\(\Sigma\)</span>.</p>
</li>
</ol>
<p>The key insight is that instead of sampling directly from <span class="arithmatex">\(q_\lambda(z)\)</span>, we can:
1. Sample <span class="arithmatex">\(\varepsilon\)</span> from the standard normal distribution <span class="arithmatex">\(p(\varepsilon)\)</span>
2. Transform it using <span class="arithmatex">\(T(\varepsilon; \lambda)\)</span> to make it seem like we're getting a sample from <span class="arithmatex">\(q_\lambda(z)\)</span></p>
<p>This trick is crucial because it allows us to compute gradients with respect to <span class="arithmatex">\(\lambda\)</span> through the sampling process. Since the transformation <span class="arithmatex">\(T\)</span> is differentiable, we can backpropagate through it to update the parameters <span class="arithmatex">\(\lambda\)</span> during training. This is why the reparameterization trick often leads to lower variance in gradient estimates compared to the REINFORCE trick.</p>
<h2 id="amortized-variational-inference">Amortized Variational Inference</h2>
<p>A noticeable limitation of black-box variational inference is that Step 1 executes an optimization subroutine that is computationally expensive. Recall that the goal of Step 1 is to find</p>
<div class="arithmatex">\[\lambda^* = \arg\max_{\lambda \in \Lambda} \text{ELBO}(x; \theta, \lambda).\]</div>
<p>For a given choice of <span class="arithmatex">\(\theta\)</span>, there is a well-defined mapping from <span class="arithmatex">\(x \mapsto \lambda^*\)</span>. A key realization is that this mapping can be learned. In particular, one can train an encoding function (parameterized by <span class="arithmatex">\(\phi\)</span>) <span class="arithmatex">\(f_\phi: \mathcal{X} \to \Lambda\)</span> (where <span class="arithmatex">\(\Lambda\)</span> is the space of <span class="arithmatex">\(\lambda\)</span> parameters) on the following objective</p>
<div class="arithmatex">\[\max_\phi \sum_{x \in \mathcal{D}} \text{ELBO}(x; \theta, f_\phi(x)).\]</div>
<p>It is worth noting at this point that <span class="arithmatex">\(f_\phi(x)\)</span> can be interpreted as defining the conditional distribution <span class="arithmatex">\(q_\phi(z|x)\)</span>. With a slight abuse of notation, we define</p>
<div class="arithmatex">\[\text{ELBO}(x; \theta, \phi) = \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\right],\]</div>
<p>and rewrite the optimization problem as</p>
<div class="arithmatex">\[\max_\phi \sum_{x \in \mathcal{D}} \text{ELBO}(x; \theta, \phi).\]</div>
<p>It is also worth noting that optimizing <span class="arithmatex">\(\phi\)</span> over the entire dataset as a subroutine every time we sample a new mini-batch is clearly not reasonable. However, if we believe that <span class="arithmatex">\(f_\phi\)</span> is capable of quickly adapting to a close-enough approximation of <span class="arithmatex">\(\lambda^*\)</span> given the current choice of <span class="arithmatex">\(\theta\)</span>, then we can interleave the optimization of <span class="arithmatex">\(\phi\)</span> and <span class="arithmatex">\(\theta\)</span>. This yields the following procedure, where for each mini-batch <span class="arithmatex">\(\mathcal{B} = \{x^{(1)}, \ldots, x^{(m)}\}\)</span>, we perform the following two updates jointly:</p>
<div class="arithmatex">\[\begin{align*}
\phi &amp;\leftarrow \phi + \tilde{\nabla}_\phi \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi) \\
\theta &amp;\leftarrow \theta + \tilde{\nabla}_\theta \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi),
\end{align*}\]</div>
<p>rather than running BBVI's Step 1 as a subroutine. By leveraging the learnability of <span class="arithmatex">\(x \mapsto \lambda^*\)</span>, this optimization procedure amortizes the cost of variational inference. If one further chooses to define <span class="arithmatex">\(f_\phi\)</span> as a neural network, the result is the variational autoencoder.</p>
<h3 id="steps-of-amortized-variational-inference">Steps of Amortized Variational Inference</h3>
<p>Let's break down the amortized variational inference procedure in detail:</p>
<ol>
<li><strong>Initial Setup</strong>:</li>
<li>We have a dataset <span class="arithmatex">\(\mathcal{D} = \{x^{(1)}, \ldots, x^{(n)}\}\)</span></li>
<li>We have a generative model <span class="arithmatex">\(p_\theta(x,z)\)</span> with parameters <span class="arithmatex">\(\theta\)</span></li>
<li>
<p>We want to learn both the model parameters <span class="arithmatex">\(\theta\)</span> and the variational parameters <span class="arithmatex">\(\lambda\)</span> for each datapoint</p>
</li>
<li>
<p><strong>Traditional BBVI Approach</strong>:</p>
</li>
<li>For each datapoint <span class="arithmatex">\(x\)</span>, we would need to run an optimization to find:</li>
</ol>
<div class="arithmatex">\[\lambda^* = \arg\max_{\lambda \in \Lambda} \text{ELBO}(x; \theta, \lambda)\]</div>
<ul>
<li>
<p>This is computationally expensive as it requires running an optimization subroutine for each datapoint</p>
</li>
<li>
<p><strong>Key Insight - Learnable Mapping</strong>:</p>
</li>
<li>Instead of optimizing <span class="arithmatex">\(\lambda\)</span> separately for each <span class="arithmatex">\(x\)</span>, we realize that there's a mapping from <span class="arithmatex">\(x\)</span> to <span class="arithmatex">\(\lambda^*\)</span></li>
<li>This mapping can be learned using a function <span class="arithmatex">\(f_\phi: \mathcal{X} \to \Lambda\)</span> parameterized by <span class="arithmatex">\(\phi\)</span></li>
<li>
<p>The function <span class="arithmatex">\(f_\phi\)</span> takes a datapoint <span class="arithmatex">\(x\)</span> and outputs the variational parameters <span class="arithmatex">\(\lambda\)</span></p>
</li>
<li>
<p><strong>Training the Encoder</strong>:</p>
</li>
<li>We train <span class="arithmatex">\(f_\phi\)</span> to maximize the ELBO across all datapoints:</li>
</ul>
<div class="arithmatex">\[\max_\phi \sum_{x \in \mathcal{D}} \text{ELBO}(x; \theta, f_\phi(x))\]</div>
<ul>
<li>
<p>This is equivalent to learning a conditional distribution <span class="arithmatex">\(q_\phi(z|x)\)</span></p>
</li>
<li>
<p><strong>Joint Optimization</strong>:</p>
</li>
<li>Instead of running BBVI's Step 1 as a subroutine, we interleave the optimization of <span class="arithmatex">\(\phi\)</span> and <span class="arithmatex">\(\theta\)</span></li>
<li>For each mini-batch <span class="arithmatex">\(\mathcal{B} = \{x^{(1)}, \ldots, x^{(m)}\}\)</span>, we perform two updates:</li>
</ul>
<div class="arithmatex">\[\begin{align*}
\phi &amp;\leftarrow \phi + \tilde{\nabla}_\phi \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi) \\
\theta &amp;\leftarrow \theta + \tilde{\nabla}_\theta \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi)
\end{align*}\]</div>
<ol>
<li><strong>Practical Implementation</strong>:</li>
<li>When <span class="arithmatex">\(f_\phi\)</span> is implemented as a neural network, we get a variational autoencoder</li>
<li>The encoder network <span class="arithmatex">\(f_\phi\)</span> maps inputs <span class="arithmatex">\(x\)</span> to variational parameters</li>
<li>The decoder network maps latent variables <span class="arithmatex">\(z\)</span> to reconstructed inputs</li>
<li>Both networks are trained end-to-end using the ELBO objective</li>
</ol>
<p>In practice, the encoder neural network <span class="arithmatex">\(f_\phi\)</span> outputs the parameters of a diagonal Gaussian distribution:</p>
<div class="arithmatex">\[q_\phi(z|x) = \mathcal{N}(z|\mu_\phi(x), \text{diag}(\sigma^2_\phi(x)))\]</div>
<p>where <span class="arithmatex">\(\mu_\phi(x)\)</span> and <span class="arithmatex">\(\sigma^2_\phi(x)\)</span> are the mean and variance vectors output by the encoder network. To sample from this distribution during training, we use the reparameterization trick:</p>
<div class="arithmatex">\[z = \mu_\phi(x) + \sigma_\phi(x) \odot \varepsilon, \quad \varepsilon \sim \mathcal{N}(0,I)\]</div>
<p>where <span class="arithmatex">\(\odot\)</span> denotes element-wise multiplication. This allows us to backpropagate through the sampling process and train the encoder network end-to-end.</p>
<p>The key advantage of this approach is that it amortizes the cost of variational inference by learning a single function <span class="arithmatex">\(f_\phi\)</span> that can quickly approximate the optimal variational parameters for any input <span class="arithmatex">\(x\)</span>, rather than running a separate optimization for each datapoint.</p>
<h2 id="decomposition-of-the-negative-elbo">Decomposition of the Negative ELBO</h2>
<p>Starting with the definition of the ELBO:</p>
<div class="arithmatex">\[\text{ELBO}(x; \theta, \phi) = \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\right]\]</div>
<p>We can expand the joint distribution <span class="arithmatex">\(p_\theta(x,z)\)</span> using the chain rule of probability:</p>
<div class="arithmatex">\[p_\theta(x,z) = p_\theta(x|z)p_\theta(z)\]</div>
<p>Substituting this into the ELBO:</p>
<div class="arithmatex">\[\text{ELBO}(x; \theta, \phi) = \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p_\theta(x|z)p_\theta(z)}{q_\phi(z|x)}\right]\]</div>
<p>Using the properties of logarithms, we can split this into three terms:</p>
<div class="arithmatex">\[\text{ELBO}(x; \theta, \phi) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)]\]</div>
<p>The second and third terms can be combined to form the KL divergence between <span class="arithmatex">\(q_\phi(z|x)\)</span> and <span class="arithmatex">\(p_\theta(z)\)</span>:</p>
<div class="arithmatex">\[\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)] = -\mathbb{E}_{q_\phi(z|x)}\left[\log\frac{q_\phi(z|x)}{p_\theta(z)}\right] = -D_{KL}(q_\phi(z|x) \| p_\theta(z))\]</div>
<p>Therefore, the ELBO can be written as:</p>
<div class="arithmatex">\[\text{ELBO}(x; \theta, \phi) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p_\theta(z))\]</div>
<p>It is insightful to note that the negative ELBO can be decomposed into two terms:</p>
<div class="arithmatex">\[-\text{ELBO}(x; \theta, \phi) = \underbrace{-\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction Loss}} + \underbrace{D_{KL}(q_\phi(z|x) \| p_\theta(z))}_{\text{KL Divergence}}\]</div>
<p>This decomposition reveals two key components of the training objective:</p>
<ol>
<li><strong>Reconstruction Loss</strong>: <span class="arithmatex">\(-\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span></li>
<li>This term measures how well the model can reconstruct the input <span class="arithmatex">\(x\)</span> from its latent representation <span class="arithmatex">\(z\)</span></li>
<li>It encourages the encoder to produce latent codes that preserve the essential information about the input</li>
<li>
<p>In practice, this is often implemented as the mean squared error or binary cross-entropy between the input and its reconstruction</p>
</li>
<li>
<p><strong>KL Divergence</strong>: <span class="arithmatex">\(D_{KL}(q_\phi(z|x) \| p_\theta(z))\)</span></p>
</li>
<li>This term measures how far the approximate posterior <span class="arithmatex">\(q_\phi(z|x)\)</span> is from the prior <span class="arithmatex">\(p_\theta(z)\)</span></li>
<li>It encourages the latent space to follow the prior distribution (typically a standard normal distribution)</li>
</ol>
<h2 id="practical-implementation-of-elbo-computation">Practical Implementation of ELBO Computation</h2>
<p>Let's look at how the ELBO is actually computed in practice. Here's a detailed implementation with explanations:</p>
<p>We implement the (rec+kl) decomposed form for practicality and clarity because:</p>
<ul>
<li>KL has a closed form (for two Gaussians <span class="arithmatex">\(q_\phi(z|x) \sim \mathcal{N}(\mu,\sigma^2)\)</span>, and <span class="arithmatex">\(p(z) \sim \mathcal{N}(0,I)\)</span>, the KL term can be computed analytically). A closed form means we can compute the exact value using a finite number of standard operations (addition, multiplication, logarithms, etc.) without needing numerical integration or approximation. This closed form is derived as follows:</li>
</ul>
<p>For two multivariate Gaussians <span class="arithmatex">\(q_\phi(z|x) = \mathcal{N}(\mu,\Sigma)\)</span> and <span class="arithmatex">\(p(z) = \mathcal{N}(0,I)\)</span>, the KL divergence is:</p>
<div class="arithmatex">\[D_{KL}(q_\phi(z|x) \| p(z)) = \frac{1}{2}\left[\text{tr}(\Sigma) + \mu^T\mu - d - \log|\Sigma|\right]\]</div>
<p>where <span class="arithmatex">\(\text{tr}(\Sigma)\)</span> is the trace of the covariance matrix <span class="arithmatex">\(\Sigma\)</span> (the sum of its diagonal elements), <span class="arithmatex">\(\mu^T\mu\)</span> is the squared L2 norm of the mean vector, <span class="arithmatex">\(d\)</span> is the dimension of the latent space, and <span class="arithmatex">\(|\Sigma|\)</span> is the determinant of <span class="arithmatex">\(\Sigma\)</span>. For diagonal covariance matrices <span class="arithmatex">\(\Sigma = \text{diag}(\sigma^2)\)</span>, this simplifies to:</p>
<div class="arithmatex">\[D_{KL}(q_\phi(z|x) \| p(z)) = \frac{1}{2}\sum_{i=1}^d (\mu_i^2 + \sigma_i^2 - \log(\sigma_i^2) - 1)\]</div>
<p>This analytical solution is not only computationally efficient but also provides exact gradients, unlike Monte Carlo estimates which would require sampling.</p>
<ul>
<li>
<p>The analytical KL avoids noisy gradients that arise from computing KL via sampling so the decomposition makes training more stable. When using Monte Carlo estimation, the gradients can have high variance due to the randomness in sampling. The analytical form provides deterministic gradients, which leads to more stable optimization. This is particularly important because the KL term acts as a regularizer, and having stable gradients for this term helps prevent the model from either collapsing to a degenerate solution (where the KL term becomes too small) or failing to learn meaningful representations (where the KL term dominates).</p>
</li>
<li>
<p>The decomposed form allows you to monitor reconstruction loss and KL separately which is very helpful in debugging and understanding model behavior</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">negative_elbo_bound</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="sd">    Computes the Evidence Lower Bound, KL and, Reconstruction costs</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="sd">        x: tensor: (batch, dim): Observations</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="sd">        nelbo: tensor: (): Negative evidence lower bound</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="sd">        kl: tensor: (): ELBO KL divergence to prior</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="sd">        rec: tensor: (): ELBO Reconstruction term</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    <span class="c1"># Step 1: Get the parameters of the approximate posterior q_phi(z|x)</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    <span class="n">q_phi_z_given_x_m</span><span class="p">,</span> <span class="n">q_phi_z_given_x_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="c1"># Step 2: Compute the KL divergence term</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    <span class="c1"># This computes D_KL(q_phi(z|x) || p_theta(z))</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    <span class="n">kl</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">kl_normal</span><span class="p">(</span><span class="n">q_phi_z_given_x_m</span><span class="p">,</span> <span class="n">q_phi_z_given_x_v</span><span class="p">,</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>                      <span class="bp">self</span><span class="o">.</span><span class="n">z_prior_m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_prior_v</span><span class="p">)</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>    <span class="c1"># Step 3: Take m samples from the approximate posterior using reparameterization</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    <span class="c1"># This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>    <span class="n">z_samples</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">sample_gaussian</span><span class="p">(</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>        <span class="n">q_phi_z_given_x_m</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">),</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>        <span class="n">q_phi_z_given_x_v</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">))</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>    <span class="c1"># Step 4: Get the decoder outputs (logits)</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>    <span class="c1"># These parameterize the Bernoulli distributions for reconstruction</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>    <span class="n">f_theta_of_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec</span><span class="p">(</span><span class="n">z_samples</span><span class="p">)</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>    <span class="c1"># Step 5: Compute the reconstruction term</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>    <span class="c1"># This computes -E_q[log p_theta(x|z)] using binary cross-entropy</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>    <span class="n">rec</span> <span class="o">=</span> <span class="o">-</span><span class="n">ut</span><span class="o">.</span><span class="n">log_bernoulli_with_logits</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f_theta_of_z</span><span class="p">)</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>    <span class="c1"># Step 6: Combine terms to get the negative ELBO</span>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a>    <span class="n">nelbo</span> <span class="o">=</span> <span class="n">kl</span> <span class="o">+</span> <span class="n">rec</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>    <span class="c1"># Step 7: Average over the batch</span>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>    <span class="n">nelbo_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">nelbo</span><span class="p">)</span>
<a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a>    <span class="n">kl_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kl</span><span class="p">)</span>
<a id="__codelineno-0-41" name="__codelineno-0-41" href="#__codelineno-0-41"></a>    <span class="n">rec_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rec</span><span class="p">)</span>
<a id="__codelineno-0-42" name="__codelineno-0-42" href="#__codelineno-0-42"></a>
<a id="__codelineno-0-43" name="__codelineno-0-43" href="#__codelineno-0-43"></a>    <span class="k">return</span> <span class="n">nelbo_avg</span><span class="p">,</span> <span class="n">kl_avg</span><span class="p">,</span> <span class="n">rec_avg</span>
</code></pre></div>
<p>Let's break down each step:</p>
<ol>
<li><strong>Encoder Output</strong>: </li>
<li>The encoder network takes input <span class="arithmatex">\(x\)</span> and outputs the parameters of the approximate posterior <span class="arithmatex">\(q_\phi(z|x)\)</span></li>
<li>
<p>These parameters are the mean (<span class="arithmatex">\(\mu_\phi(x)\)</span>) and variance (<span class="arithmatex">\(\sigma^2_\phi(x)\)</span>) of a diagonal Gaussian</p>
</li>
<li>
<p><strong>KL Divergence</strong>:</p>
</li>
<li>Computes <span class="arithmatex">\(D_{KL}(q_\phi(z|x) \| p_\theta(z))\)</span></li>
<li>For diagonal Gaussians, this has a closed-form solution</li>
<li>
<p>The prior <span class="arithmatex">\(p_\theta(z)\)</span> is typically a standard normal distribution</p>
</li>
<li>
<p><strong>Sampling</strong>:</p>
</li>
<li>Uses the reparameterization trick to sample from <span class="arithmatex">\(q_\phi(z|x)\)</span></li>
<li>Implements <span class="arithmatex">\(z = \mu_\phi(x) + \sigma_\phi(x) \odot \varepsilon\)</span> where <span class="arithmatex">\(\varepsilon \sim \mathcal{N}(0,I)\)</span></li>
<li>
<p>The samples are used to estimate the reconstruction term</p>
</li>
<li>
<p><strong>Decoder Output</strong>:</p>
</li>
<li>The decoder network takes the sampled <span class="arithmatex">\(z\)</span> and outputs logits</li>
<li>
<p>These logits parameterize Bernoulli distributions for each element of <span class="arithmatex">\(x\)</span></p>
</li>
<li>
<p><strong>Reconstruction Term</strong>:</p>
</li>
<li>Computes <span class="arithmatex">\(-\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span></li>
<li>Uses binary cross-entropy loss which takes logits directly</li>
<li>
<p>The sigmoid function is incorporated into the loss computation</p>
</li>
<li>
<p><strong>Final ELBO</strong>:</p>
</li>
<li>Combines the KL divergence and reconstruction terms</li>
<li>
<p>The negative ELBO is what we minimize during training</p>
</li>
<li>
<p><strong>Batch Averaging</strong>:</p>
</li>
<li>Averages the losses over the batch</li>
<li>This gives us the final training objective</li>
</ol>
<p>This implementation shows how the theoretical ELBO decomposition we discussed earlier is actually computed in practice, with all the necessary components for training a VAE on binary data.</p>
<p><strong>Note on Sampling from <span class="arithmatex">\(q_\phi(z|x)\)</span></strong>: The sampling step in the implementation is crucial for two reasons:</p>
<p><strong>Monte Carlo Estimation</strong>: The reconstruction term <span class="arithmatex">\(-\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span> involves an expectation over <span class="arithmatex">\(q_\phi(z|x)\)</span>. We estimate this expectation using Monte Carlo sampling:</p>
<div class="arithmatex">\[-\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] \approx -\frac{1}{K}\sum_{k=1}^K \log p_\theta(x|z^{(k)})\]</div>
<p>where <span class="arithmatex">\(z^{(k)} \sim q_\phi(z|x)\)</span>. In practice, we often use <span class="arithmatex">\(K=1\)</span> (a single sample) as it works well and is computationally efficient.</p>
<p><strong>Gradient Estimation</strong>: We need to compute gradients of this expectation with respect to both <span class="arithmatex">\(\phi\)</span> (encoder parameters) and <span class="arithmatex">\(\theta\)</span> (decoder parameters). The reparameterization trick allows us to:
- Sample from a fixed distribution <span class="arithmatex">\(p(\varepsilon)\)</span> that doesn't depend on <span class="arithmatex">\(\phi\)</span>
- Transform these samples using a deterministic function that depends on <span class="arithmatex">\(\phi\)</span>
- Backpropagate through this transformation to compute gradients
- This results in lower variance gradient estimates compared to the REINFORCE trick</p>
<p>The sampling step is therefore essential for both estimating the ELBO and computing its gradients during training.</p>
<h2 id="-vae">Î²-VAE</h2>
<p>A popular variation of the normal VAE is called the Î²-VAE. The Î²-VAE optimizes the following objective:</p>
<div class="arithmatex">\[
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta D_{KL}(q_\phi(z|x) || p(z))
\]</div>
<p>Here, Î² is a positive real number. From a training objective, we want to decrease the negative of ELBO, also called NELBO:</p>
<div class="arithmatex">\[
\text{NELBO} = \text{Reconstruction Loss} + \beta D_{KL}(q_\phi(z|x) \| p(z))
\]</div>
<p>We see that the second term acts as a regularization term. Î² can be thought of as a hyperparameter that adjusts how much we want to regularize. Greater the Î², more is the training optimized to reduce KL divergence, and a higher possibility of overfitting (and also more the <span class="arithmatex">\(q_\phi(z|x)\)</span> closely approximates <span class="arithmatex">\(p(z)\)</span>). Lesser the Î², optimization is geared towards increasing the KL divergence, leading to a more general model. When Î² is set to 1 however, we get the standard VAE.</p>
<h2 id="importance-weighted-autoencoder-iwae">Importance Weighted Autoencoder (IWAE)</h2>
<p>While the ELBO serves as a lower bound to the true marginal log-likelihood, it may be loose if the variational posterior <span class="arithmatex">\(q_\phi(z|x)\)</span> is a poor approximation to the true posterior <span class="arithmatex">\(p_\theta(z|x)\)</span>. The key idea behind IWAE is to use <span class="arithmatex">\(m &gt; 1\)</span> samples from the approximate posterior <span class="arithmatex">\(q_\phi(z|x)\)</span> to obtain the following IWAE bound:</p>
<div class="arithmatex">\[
\mathcal{L}_m(x; \theta,\phi) = \mathbb{E}_{z^{(1)},...,z^{(m)} \text{ i.i.d.} \sim q_\phi(z|x)} \log \frac{1}{m}\sum_{i=1}^m \frac{p_\theta(x,z^{(i)})}{q_\phi(z^{(i)}|x)}
\]</div>
<p>Notice that for the special case of <span class="arithmatex">\(m=1\)</span>, the IWAE objective <span class="arithmatex">\(\mathcal{L}_m\)</span> reduces to the standard ELBO <span class="arithmatex">\(\mathcal{L}_1 = \mathbb{E}_{z\sim q_\phi(z|x)} \log \frac{p_\theta(x,z)}{q_\phi(z|x)}\)</span>.</p>
<p>As a pseudocode, the main modification to the standard VAE would be:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="c1"># Step 3: Take m samples from the approximate posterior using reparameterization</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="c1"># This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="n">z_samples</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">sample_gaussian</span><span class="p">(</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>    <span class="n">q_phi_z_given_x_m</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">),</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>    <span class="n">q_phi_z_given_x_v</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">))</span>
</code></pre></div>
<h2 id="gaussian-mixture-vae-gmvae">Gaussian Mixture VAE (GMVAE)</h2>
<p>The VAE's prior distribution was a parameter-free isotropic Gaussian <span class="arithmatex">\(p_\theta(z) = \mathcal{N}(z|0,I)\)</span>. While this original setup works well, there are settings in which we desire more expressivity to better model our data. Let's look at GMVAE, which has a mixture of Gaussians as the prior distribution.</p>
<div class="arithmatex">\[p_\theta(z) = \sum_{i=1}^k \frac{1}{k}\mathcal{N}(z|\mu_i, \text{diag}(\sigma^2_i))\]</div>
<p>where <span class="arithmatex">\(i \in \{1, ..., k\}\)</span> denotes the <span class="arithmatex">\(i\)</span>th cluster index. For notational simplicity, we shall subsume our mixture of Gaussian parameters <span class="arithmatex">\(\{\mu_i, \sigma_i\}_{i=1}^k\)</span> into our generative model parameters <span class="arithmatex">\(\theta\)</span>. For simplicity, we have also assumed fixed uniform weights <span class="arithmatex">\(1/k\)</span> over the possible different clusters.</p>
<p>Apart from the prior, the GMVAE shares an identical setup as the VAE:</p>
<div class="arithmatex">\[q_\phi(z|x) = \mathcal{N}(z|\mu_\phi(x), \text{diag}(\sigma^2_\phi(x)))\]</div>
<div class="arithmatex">\[p_\theta(x|z) = \text{Bern}(x|f_\theta(z))\]</div>
<p>Although the ELBO for the GMVAE: <span class="arithmatex">\(\mathbb{E}_{q_\phi(z)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p_\theta(z))\)</span> is identical to that of the VAE, we note that the KL term <span class="arithmatex">\(D_{KL}(q_\phi(z|x) \| p_\theta(z))\)</span> cannot be computed analytically between a Gaussian distribution <span class="arithmatex">\(q_\phi(z|x)\)</span> and a mixture of Gaussians <span class="arithmatex">\(p_\theta(z)\)</span>. However, we can obtain its unbiased estimator via Monte Carlo sampling:</p>
<div class="arithmatex">\[D_{KL}(q_\phi(z|x) \| p_\theta(z)) \approx \log q_\phi(z^{(1)}|x) - \log p_\theta(z^{(1)})\]</div>
<div class="arithmatex">\[= \underbrace{\log\mathcal{N}(z^{(1)}|\mu_\phi(x), \text{diag}(\sigma^2_\phi(x)))}_{\text{log normal}} - \underbrace{\log\sum_{i=1}^k \frac{1}{k}\mathcal{N}(z^{(1)}|\mu_i, \text{diag}(\sigma^2_i))}_{\text{log normal mixture}}\]</div>
<p>where <span class="arithmatex">\(z^{(1)} \sim q_\phi(z|x)\)</span> denotes a single sample.</p>
<h2 id="the-semi-supervised-vae-ssvae">The Semi-Supervised VAE (SSVAE)</h2>
<p>The Semi-Supervised VAE (SSVAE) extends the standard VAE to handle both labeled and unlabeled data. In a semi-supervised setting, we have a dataset <span class="arithmatex">\(\mathcal{D}\)</span> that consists of:
- Labeled data: <span class="arithmatex">\(\mathcal{D}_l = \{(x^{(i)}, y^{(i)})\}_{i=1}^{N_l}\)</span>
- Unlabeled data: <span class="arithmatex">\(\mathcal{D}_u = \{x^{(i)}\}_{i=1}^{N_u}\)</span></p>
<p>where <span class="arithmatex">\(y^{(i)}\)</span> represents the class label for the <span class="arithmatex">\(i\)</span>-th labeled example. The SSVAE introduces an additional latent variable <span class="arithmatex">\(y\)</span> to model the class labels, and the joint distribution is factorized as:</p>
<div class="arithmatex">\[
p_\theta(x, y, z) = p_\theta(x|y,z)p_\theta(y|z)p_\theta(z)
\]</div>
<p>This factorization is derived from the chain rule of probability. We first factorize <span class="arithmatex">\(p_\theta(x, y, z)\)</span> as <span class="arithmatex">\(p_\theta(x|y,z)p_\theta(y,z)\)</span>, and then further factorize <span class="arithmatex">\(p_\theta(y,z)\)</span> as <span class="arithmatex">\(p_\theta(y|z)p_\theta(z)\)</span>. This reflects the generative process where:
1. First, we sample <span class="arithmatex">\(z\)</span> from the prior <span class="arithmatex">\(p_\theta(z)\)</span>
2. Then, we sample <span class="arithmatex">\(y\)</span> conditioned on <span class="arithmatex">\(z\)</span> from <span class="arithmatex">\(p_\theta(y|z)\)</span>
3. Finally, we generate <span class="arithmatex">\(x\)</span> conditioned on both <span class="arithmatex">\(y\)</span> and <span class="arithmatex">\(z\)</span> from <span class="arithmatex">\(p_\theta(x|y,z)\)</span></p>
<p>The approximate posterior for labeled data is:</p>
<div class="arithmatex">\[
q_\phi(y,z|x) = q_\phi(z|x,y)q_\phi(y|x)
\]</div>
<p>This factorization is derived from the chain rule of probability for the approximate posterior. The chain rule states that for any random variables <span class="arithmatex">\(A\)</span>, <span class="arithmatex">\(B\)</span>, and <span class="arithmatex">\(C\)</span>, we can write:</p>
<div class="arithmatex">\[p(A,B|C) = p(A|B,C)p(B|C)\]</div>
<p>This equation is derived from the definition of conditional probability. Let's break it down step by step:</p>
<ol>
<li>First, recall that conditional probability is defined as:</li>
</ol>
<div class="arithmatex">\[p(A|B) = \frac{p(A,B)}{p(B)}\]</div>
<ol>
<li>For our case with three variables, we can write:</li>
</ol>
<div class="arithmatex">\[p(A,B|C) = \frac{p(A,B,C)}{p(C)}\]</div>
<ol>
<li>We can also write:</li>
</ol>
<div class="arithmatex">\[p(A|B,C) = \frac{p(A,B,C)}{p(B,C)}\]</div>
<p>and</p>
<div class="arithmatex">\[p(B|C) = \frac{p(B,C)}{p(C)}\]</div>
<ol>
<li>Multiplying these last two equations:</li>
</ol>
<div class="arithmatex">\[p(A|B,C)p(B|C) = \frac{p(A,B,C)}{p(B,C)} \cdot \frac{p(B,C)}{p(C)} = \frac{p(A,B,C)}{p(C)} = p(A,B|C)\]</div>
<p>Therefore, we have proven that:</p>
<div class="arithmatex">\[p(A,B|C) = p(A|B,C)p(B|C)\]</div>
<p>In our case, we can identify:
- <span class="arithmatex">\(A\)</span> as <span class="arithmatex">\(z\)</span> (the latent code)
- <span class="arithmatex">\(B\)</span> as <span class="arithmatex">\(y\)</span> (the label)
- <span class="arithmatex">\(C\)</span> as <span class="arithmatex">\(x\)</span> (the observed data)</p>
<p>Therefore, applying the chain rule:</p>
<div class="arithmatex">\[q_\phi(y,z|x) = q_\phi(z|x,y)q_\phi(y|x)\]</div>
<p>This means:
1. First, we predict the label <span class="arithmatex">\(y\)</span> from <span class="arithmatex">\(x\)</span> using <span class="arithmatex">\(q_\phi(y|x)\)</span>
2. Then, we infer the latent code <span class="arithmatex">\(z\)</span> using both <span class="arithmatex">\(x\)</span> and the predicted <span class="arithmatex">\(y\)</span> through <span class="arithmatex">\(q_\phi(z|x,y)\)</span></p>
<p>and for unlabeled data:</p>
<div class="arithmatex">\[
q_\phi(y,z|x) = q_\phi(z|x,y)q_\phi(y)
\]</div>
<p>For unlabeled data, since we don't know the true label <span class="arithmatex">\(y\)</span>, we use a prior distribution <span class="arithmatex">\(q_\phi(y)\)</span> (typically a uniform distribution over classes) instead of <span class="arithmatex">\(q_\phi(y|x)\)</span>. The factorization reflects that:
1. We sample a label <span class="arithmatex">\(y\)</span> from the prior <span class="arithmatex">\(q_\phi(y)\)</span>
2. Then, we infer the latent code <span class="arithmatex">\(z\)</span> using both <span class="arithmatex">\(x\)</span> and the sampled <span class="arithmatex">\(y\)</span> through <span class="arithmatex">\(q_\phi(z|x,y)\)</span></p>
<p>The training objective for SSVAE combines:
1. The ELBO for labeled data
2. The ELBO for unlabeled data
3. A classification loss for labeled data</p>
<p>This allows the model to learn both the data distribution and the class labels in a semi-supervised manner.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>