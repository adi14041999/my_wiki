
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/ai/deep_generative_models/score_based_diffusion_models/">
      
      
        <link rel="prev" href="../diffusion_models_from_an_image_generation_perspective/">
      
      
        <link rel="next" href="../../../math/foundational_math/the_quadratic_formula/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Score Based Diffusion Models - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#score-based-diffusion-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Score Based Diffusion Models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../deep_learning_for_computer_vision/introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../math/foundational_math/the_quadratic_formula/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-tabs__link">
          
  
  
  Productivity

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Learning for Computer Vision
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning for Computer Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/image_classification_with_linear_classifiers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Classification with Linear Classifiers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Regularization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/backpropagation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backpropagation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/backpropagation_for_a_linear_layer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backpropagation for a Linear Layer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Setting up the Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/neural_networks_setting_up_the_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Setting up the Data
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Learning and Evaluation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Putting it together- Minimal Neural Network Case Study
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolutional Neural Networks- Architectures, Convolution / Pooling Layers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/a_review_of_rnns_and_transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    A review of RNNs and Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/self_supervised_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Self-Supervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deep_learning_for_computer_vision/a_review_of_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    A review of Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Natural Language Processing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Natural Language Processing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/representing_words/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Representing words
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/svd_based_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SVD based methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/distributional_semantics_and_word2vec/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributional semantics and Word2vec
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/language_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/recurrent_neural_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recurrent Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/gated_recurrent_units/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gated Recurrent Units (GRUs)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/long_short_term_memory_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Long-Short Term Memory (LSTM) Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/seq2seq_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Seq2Seq Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/attention_mechanism/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention Mechanism
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/how_large_language_models_work_a_visual_intro_to_transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    How large language models work, a visual intro to transformers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/attention_in_transformers_visually_explained/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention in transformers, visually explained
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/the_basic_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    The basic Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/decoder_only_transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Decoder-only Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/differences_between_the_basic_transformer_and_the_decoder_only_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Differences between the basic Transformer and the Decoder-Only Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../natural_language_processing/self_attention_and_transformers_a_mathematical_approach/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Self-Attention and Transformers, a mathematical approach
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" checked>
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_based_generative_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Generative Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluating_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluating Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../diffusion_models_from_an_image_generation_perspective/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Diffusion Models from an image generation perspective
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#quick-recap-score-based-models" class="md-nav__link">
    <span class="md-ellipsis">
      Quick Recap: Score Based Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#diffusion-models-as-score-based-models-hierarchical-vaes" class="md-nav__link">
    <span class="md-ellipsis">
      Diffusion Models as Score Based Models &amp; Hierarchical VAEs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Diffusion Models as Score Based Models &amp; Hierarchical VAEs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rewriting-the-elbo-for-diffusion-models" class="md-nav__link">
    <span class="md-ellipsis">
      Rewriting the ELBO for Diffusion Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sampling" class="md-nav__link">
    <span class="md-ellipsis">
      Sampling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Foundational Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Foundational Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/foundational_math/the_quadratic_formula/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    The Quadratic Formula
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/foundational_math/trigonometry_fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Trigonometry fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/foundational_math/intro_to_complex_numbers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Intro to Complex Numbers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors, vector addition, and scalar multiplication
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector geometry in Rn and correlation coefficients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/planes_in_r3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Planes in R3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/span_subspaces_and_dimension/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Span, subspaces, and dimension
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/basis_and_orthogonality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basis and orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Applications of projections in Rn- orthogonal bases of planes and linear regression
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/story_proofs_and_axioms_of_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Story Proofs and Axioms of Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/some_famous_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Some famous problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/indicator_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Indicator Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/poisson_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Poisson Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/continuous_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Continuous Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/exponential_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exponential Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/joint_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Joint Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/independence_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Independence of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/multinomial_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multinomial Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/covariance_and_correlation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Covariance and Correlation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/transformations_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformations of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/convolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/beta_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Beta Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/conditional_expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/jensens_inequality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jensen's inequality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/central_limit_theorem/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Central Limit Theorem
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/multivariate_normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multivariate Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/probability/markov_chains/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Markov Chains
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Multivariate Calculus
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Multivariate Calculus
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/multivariate_calculus/the_tl_dr_version_of_derivatives/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    The TL;DR version of Derivatives
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../math/multivariate_calculus/multivariable_functions_level_sets_and_contour_plots/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multivariable functions, level sets, and contour plots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Productivity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Productivity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    How to Build Your Career in AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            How to Build Your Career in AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Three Steps to Career Growth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_1_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 1- Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_2_projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 2- Projects
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_3_job/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 3- Job
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/make_every_day_count/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Make Every Day Count
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="score-based-diffusion-models">Score Based Diffusion Models</h1>
<h2 id="quick-recap-score-based-models">Quick Recap: Score Based Models</h2>
<p>From our exploration of score-based generative modeling, we learned several key concepts:</p>
<p><strong>Score Function</strong>: The gradient of the log probability density, <span class="arithmatex">\(\nabla_x \log p(x)\)</span>, which points "uphill" in the probability landscape toward high-density regions.</p>
<p><strong>Score Matching</strong>: A training objective that learns the score function by minimizing the Fisher divergence between the learned and true score functions.</p>
<p><strong>Score Matching Objective</strong>: The original score matching objective is:</p>
<div class="arithmatex">\[\mathcal{L}(\theta) = \mathbb{E}_{x \sim p_{data}(x)} \left[ \frac{1}{2} \| s_\theta(x) \|_2^2 + \text{tr}(\nabla_x s_\theta(x)) \right]\]</div>
<p>where <span class="arithmatex">\(\text{tr}(\nabla_x s_\theta(x))\)</span> is the trace of the Jacobian of the score function, which is computationally expensive to evaluate.</p>
<p><strong>Denoising Score Matching (DSM)</strong>: A practical variant that trains the score function to predict the direction from noisy to clean data, avoiding the need to compute the true score function.</p>
<p><strong>DSM Objective</strong>: The denoising score matching objective is:</p>
<div class="arithmatex">\[\mathcal{L}(\theta) = \mathbb{E}_{y \sim p_{data}(y)} \mathbb{E}_{x \sim \mathcal{N}(x; y, \sigma^2 I)} \left[ \frac{1}{2} \left\| s_\theta(x) - \frac{y - x}{\sigma^2} \right\|_2^2 \right]\]</div>
<p>where <span class="arithmatex">\(s_\theta(x)\)</span> learns to predict the score function of the noise-perturbed distribution, and <span class="arithmatex">\(\frac{y - x}{\sigma^2}\)</span> is the target score function that points from noisy sample <span class="arithmatex">\(x\)</span> toward clean data <span class="arithmatex">\(y\)</span>.</p>
<p><strong>Langevin Dynamics</strong>: A continuous-time stochastic process that uses the score function to guide sampling:</p>
<div class="arithmatex">\[dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t\]</div>
<p><strong>Discretized Form</strong>: For practical implementation:</p>
<div class="arithmatex">\[x_{t+1} = x_t + \frac{\epsilon}{2} \cdot s_\theta(x_t) + \sqrt{2\epsilon} \cdot \eta_t\]</div>
<p><strong>Mode Collapse</strong>: Standard Langevin dynamics struggles with multi-modal distributions and low-density regions.</p>
<p><strong>Annealed Langevin Dynamics</strong>: Addresses this by using multiple noise scales <span class="arithmatex">\(\sigma_1 &lt; \sigma_2 &lt; \ldots &lt; \sigma_L\)</span>, creating a sequence of increasingly noisy distributions that are easier to sample from.</p>
<p><strong>Stochastic Differential Equations (SDEs)</strong>: General framework for continuous-time stochastic processes:</p>
<div class="arithmatex">\[dx = f(x, t)dt + g(t)dw\]</div>
<p><strong>Reverse SDE</strong>: Any SDE has a corresponding reverse process for sampling:</p>
<div class="arithmatex">\[dx = [f(x, t) - g^2(t)\nabla_x \log p_t(x)]dt + g(t)d\bar{w}\]</div>
<p><strong>Time-Dependent Score Models</strong>: Neural networks that learn <span class="arithmatex">\(s_\theta(x, t) \approx \nabla_x \log p_t(x)\)</span> for continuous-time processes.</p>
<p><strong>Key insights:</strong></p>
<ol>
<li><strong>Score functions act as denoisers</strong>: They point from noisy to clean data</li>
<li><strong>Multiple noise scales help</strong>: Annealing from high to low noise improves sampling</li>
<li><strong>Continuous-time generalizes discrete</strong>: SDEs provide a unified framework</li>
<li><strong>Reverse processes enable generation</strong>: The reverse SDE naturally incorporates the score function for sampling</li>
</ol>
<h2 id="diffusion-models-as-score-based-models-hierarchical-vaes">Diffusion Models as Score Based Models &amp; Hierarchical VAEs</h2>
<p><strong>Iterative Denoising perspective</strong>: In annealed Langevin dynamics with multiple noise scales, the sampling process can be viewed as <strong>iterative denoising</strong>. Starting from high noise levels and gradually reducing noise, each step uses the score function to denoise the sample, progressively refining it from a noisy state toward the clean data distribution.</p>
<p><strong>Training perspective</strong>: The inverse process involves <strong>iteratively adding Gaussian noise</strong> to clean data during training. By corrupting data with increasing levels of noise, the model learns to predict the score function at each noise level, enabling it to reverse the corruption process during sampling.</p>
<p><img alt="Iterative denoising" src="../iter_denoise.png" /></p>
<p><strong>VAE Perspective</strong>: This entire framework can be viewed as a <strong>VAE</strong> where:</p>
<ul>
<li>
<p><strong>Encoder process</strong>: The forward process that converts clean data to noise through iterative corruption</p>
</li>
<li>
<p><strong>Decoder process</strong>: The reverse process that generates samples by iteratively denoising from noise</p>
</li>
</ul>
<p><strong>Noise Perturbation process</strong>: Each <span class="arithmatex">\(x_t\)</span> represents a noise-perturbed density that is obtained by adding Gaussian noise to <span class="arithmatex">\(x_{t-1}\)</span>. This creates a Markov chain where each step adds a small amount of noise to the previous state.</p>
<p>We can write the forward process as a conditional distribution:</p>
<div class="arithmatex">\[q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)\]</div>
<p>where <span class="arithmatex">\(\beta_t\)</span> is the noise schedule that determines how much noise is added at each step.</p>
<p>The joint distribution of the entire forward process is:</p>
<div class="arithmatex">\[q(x_1, x_2, \ldots, x_T | x_0) = \prod_{t=1}^T q(x_t | x_{t-1})\]</div>
<p>This factorization follows from the <strong>chain rule of probability</strong> and the <strong>Markov property</strong> of the forward process:</p>
<p><strong>Chain Rule</strong>: For any joint distribution, we can write:</p>
<div class="arithmatex">\[q(x_1, x_2, \ldots, x_T | x_0) = q(x_1 | x_0) \cdot q(x_2 | x_0, x_1) \cdot q(x_3 | x_0, x_1, x_2) \cdots q(x_T | x_0, x_1, \ldots, x_{T-1})\]</div>
<p><strong>Markov Property</strong>: In the forward process, each <span class="arithmatex">\(x_t\)</span> depends only on <span class="arithmatex">\(x_{t-1}\)</span>, not on earlier states:</p>
<div class="arithmatex">\[q(x_t | x_0, x_1, \ldots, x_{t-1}) = q(x_t | x_{t-1})\]</div>
<p>Substituting the Markov property into the chain rule:</p>
<div class="arithmatex">\[q(x_1, x_2, \ldots, x_T | x_0) = q(x_1 | x_0) \cdot q(x_2 | x_1) \cdot q(x_3 | x_2) \cdots q(x_T | x_{T-1})\]</div>
<p>This can be written compactly as:</p>
<div class="arithmatex">\[q(x_1, x_2, \ldots, x_T | x_0) = \prod_{t=1}^T q(x_t | x_{t-1})\]</div>
<p>This represents the probability of the entire noise corruption sequence, where each step depends only on the previous step (Markov property).</p>
<p><strong>Comparison with VAEs</strong>: In a typical VAE, you would take <span class="arithmatex">\(x_0\)</span> and map it via a neural network to obtain some mean and standard deviation to parameterize the distribution of the latent variable. Here, we obtain the distribution of the latent variables through the <strong>predefined noise corruption procedure</strong> we defined above, rather than learning it with a neural network.</p>
<p><strong>Multistep transitions</strong>: A key advantage of this process is that we can compute transitions between any two time steps efficiently. For example, we can directly compute <span class="arithmatex">\(q(x_t | x_0)\)</span> without going through all intermediate steps.</p>
<p>Starting from <span class="arithmatex">\(x_0\)</span>, we can write:</p>
<div class="arithmatex">\[x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}\]</div>
<p>where <span class="arithmatex">\(\alpha_t = 1 - \beta_t\)</span> and <span class="arithmatex">\(\epsilon_{t-1} \sim \mathcal{N}(0, I)\)</span>.</p>
<p>Recursively substituting:</p>
<div class="arithmatex">\[x_t = \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2}) + \sqrt{1 - \alpha_t} \epsilon_{t-1}\]</div>
<p>Continuing this recursion, we get:</p>
<div class="arithmatex">\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\]</div>
<p>where <span class="arithmatex">\(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\)</span> and <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, I)\)</span>.</p>
<p><strong>Result</strong>: The multistep transition is:</p>
<div class="arithmatex">\[q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\]</div>
<p>This allows us to sample <span class="arithmatex">\(x_t\)</span> directly from <span class="arithmatex">\(x_0\)</span> in a single step, making training much more efficient.</p>
<p><strong>Diffusion analogy</strong>: We can think of this as a <strong>diffusion process</strong>. This is like a diffuser where given an initial state, we keep adding noise at every step. This is analogous to <strong>heat diffusion</strong> in a space- just as heat spreads out and becomes more uniform over time, our data distribution becomes increasingly noisy and uniform Gaussian as we add more noise at each step.</p>
<p>The process gradually "diffuses" the structured information in the data into random noise, creating a smooth transition from the complex data distribution to a simple Gaussian noise distribution.</p>
<p><img alt="Diffusion" src="../diff.png" /></p>
<p>The ideal sampling process would be:</p>
<ol>
<li>Sample <span class="arithmatex">\(x_T\)</span> from <span class="arithmatex">\(\pi(x_T)\)</span>. Start with pure noise from the prior distribution</li>
<li>Iteratively sample from the true denoising distribution <span class="arithmatex">\(q(x_{t-1} | x_t)\)</span>.</li>
</ol>
<p>This would generate samples by following the exact reverse of the forward diffusion process, gradually denoising from pure noise back to clean data.</p>
<p>The challenge however, is that we don't know the true denoising distributions <span class="arithmatex">\(q(x_{t-1} | x_t)\)</span>. While the forward process <span class="arithmatex">\(q(x_t | x_{t-1})\)</span> is predefined and tractable, the reverse process is not.</p>
<p>However, we can learn an approximation <span class="arithmatex">\(p_\theta(x_{t-1} | x_t)\)</span> which is a Gaussian distribution with learned parameters:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\]</div>
<p>where <span class="arithmatex">\(\mu_\theta(x_t, t)\)</span> is a neural network that learns the mean of the denoising distribution, and <span class="arithmatex">\(\sigma_t^2 I\)</span> is the fixed variance schedule.</p>
<p>This is similar to a VAE decoder:</p>
<p><strong>VAE Decoder</strong>:</p>
<div class="arithmatex">\[p_\theta(x | z) = \mathcal{N}(x; \mu_\theta(z), \sigma_\theta^2(z) I)\]</div>
<p><strong>Diffusion reverse process</strong>:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\]</div>
<p>The diffusion decoder <span class="arithmatex">\(p_\theta(x_{t-1} | x_t)\)</span> is trying to learn to approximate the true denoising distributions <span class="arithmatex">\(q(x_{t-1} | x_t)\)</span>.</p>
<p>The joint distribution of the learned reverse process is:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = \prod_{t=1}^T p_\theta(x_{t-1} | x_t)\]</div>
<p>Let's derive the joint distribution of the learned reverse process step by step.</p>
<p>In the general case of <span class="arithmatex">\(n\)</span> random variables <span class="arithmatex">\(X_1, X_2, \ldots, X_n\)</span>, the values of an arbitrary subset of variables can be known and one can ask for the joint probability of all other variables. For example, if the values of <span class="arithmatex">\(X_{k+1}, X_{k+2}, \ldots, X_n\)</span> are known, the probability for <span class="arithmatex">\(X_1, X_2, \ldots, X_k\)</span> given these known values is:</p>
<div class="arithmatex">\[P(X_1, X_2, \ldots, X_k|X_{k+1}, X_{k+2}, \ldots, X_n) = \frac{P(X_1, X_2, \ldots, X_n)}{P(X_{k+1}, X_{k+2}, \ldots, X_n)}\]</div>
<p>This is the fundamental definition of conditional probability for multiple random variables.</p>
<p>For any three events <span class="arithmatex">\(A\)</span>, <span class="arithmatex">\(B\)</span>, and <span class="arithmatex">\(C\)</span>, the joint conditional probability is defined as:</p>
<div class="arithmatex">\[P(A, B|C) = \frac{P(A, B, C)}{P(C)}\]</div>
<p>We can write the joint probability <span class="arithmatex">\(P(A, B, C)\)</span> using the chain rule:</p>
<div class="arithmatex">\[P(A, B, C) = P(A|B, C) \cdot P(B, C)\]</div>
<p>Substituting this into our definition:</p>
<div class="arithmatex">\[P(A, B|C) = \frac{P(A|B, C) \cdot P(B, C)}{P(C)}\]</div>
<p>We can write <span class="arithmatex">\(P(B, C)\)</span> as:</p>
<div class="arithmatex">\[P(B, C) = P(B|C) \cdot P(C)\]</div>
<div class="arithmatex">\[P(A, B|C) = \frac{P(A|B, C) \cdot P(B|C) \cdot P(C)}{P(C)}\]</div>
<p>The <span class="arithmatex">\(P(C)\)</span> terms cancel out:</p>
<div class="arithmatex">\[P(A, B|C) = P(A|B, C) \cdot P(B|C)\]</div>
<p>The learned reverse process consists of a sequence of conditional distributions:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\]</div>
<p>where <span class="arithmatex">\(\mu_\theta(x_t, t)\)</span> is a neural network that learns the mean of the denoising distribution.</p>
<p>For the joint distribution <span class="arithmatex">\(p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T)\)</span>, we can apply the chain rule:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = p_\theta(x_0 | x_1, \ldots, x_T) \cdot p_\theta(x_1 | x_2, \ldots, x_T) \cdots p_\theta(x_{T-1} | x_T)\]</div>
<p>In the reverse process, we assume that each <span class="arithmatex">\(x_{t-1}\)</span> depends only on <span class="arithmatex">\(x_t\)</span>, not on future states. This is the <strong>reverse Markov property</strong>:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t, x_{t+1}, \ldots, x_T) = p_\theta(x_{t-1} | x_t)\]</div>
<p>Substituting the reverse Markov property into the chain rule:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = p_\theta(x_0 | x_1) \cdot p_\theta(x_1 | x_2) \cdots p_\theta(x_{T-1} | x_T)\]</div>
<p>This can be written compactly as:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = \prod_{t=1}^T p_\theta(x_{t-1} | x_t)\]</div>
<p>To get the complete joint distribution, we need to include the prior distribution over <span class="arithmatex">\(x_T\)</span>:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_T) = p(x_T) \cdot p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T)\]</div>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_T) = p(x_T) \cdot \prod_{t=1}^T p_\theta(x_{t-1} | x_t)\]</div>
<p>A crucial aspect of the diffusion process is choosing the values of <span class="arithmatex">\(\bar{\alpha}_t\)</span> such that after many steps, we are left with pure noise. This ensures that the forward process converges to a simple, known distribution.</p>
<p>Common choices for the noise schedule include:</p>
<ol>
<li><strong>Linear Schedule</strong>: <span class="arithmatex">\(\beta_t = \frac{t}{T} \cdot \beta_{\text{max}}\)</span></li>
<li><strong>Cosine Schedule</strong>: <span class="arithmatex">\(\beta_t = \cos\left(\frac{t}{T} \cdot \frac{\pi}{2}\right)\)</span></li>
<li><strong>Quadratic Schedule</strong>: <span class="arithmatex">\(\beta_t = \left(\frac{t}{T}\right)^2 \cdot \beta_{\text{max}}\)</span></li>
</ol>
<p><strong>Example</strong>: For a linear schedule with <span class="arithmatex">\(\beta_{\text{max}} = 0.02\)</span> and <span class="arithmatex">\(T = 1000\)</span>, we get <span class="arithmatex">\(\beta_1 = 0.00002\)</span>, <span class="arithmatex">\(\beta_{500} = 0.01\)</span> and <span class="arithmatex">\(\beta_{1000} = 0.02\)</span>.</p>
<p>Once we have trained the diffusion model and learned the reverse process <span class="arithmatex">\(p_\theta(x_{t-1} | x_t)\)</span>, we can generate new samples by running the reverse process. Here's how sampling works. </p>
<p>Sample <span class="arithmatex">\(x_T\)</span> from the prior distribution <span class="arithmatex">\(x_T \sim \mathcal{N}(x_T; 0, I)\)</span>.</p>
<p>For <span class="arithmatex">\(t = T, T-1, \ldots, 1\)</span>, sample from the learned reverse process <span class="arithmatex">\(x_{t-1} \sim p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\)</span>.</p>
<p>After <span class="arithmatex">\(T\)</span> steps, we obtain <span class="arithmatex">\(x_0\)</span>, which is our generated sample.</p>
<p>This entire diffusion framework can be viewed as a <strong>Variational Autoencoder (VAE)</strong> with a crucial difference: <strong>the encoder is fixed and predefined, while only the decoder is learned</strong>.</p>
<p><strong>Standard VAE Structure</strong>:</p>
<ul>
<li>
<p><strong>Encoder</strong>: <span class="arithmatex">\(q_\phi(z | x) = \mathcal{N}(z; \mu_\phi(x), \sigma_\phi^2(x) I)\)</span></p>
</li>
<li>
<p><strong>Decoder</strong>: <span class="arithmatex">\(p_\theta(x | z) = \mathcal{N}(x; \mu_\theta(z), \sigma_\theta^2(z) I)\)</span></p>
</li>
<li>
<p><strong>Prior</strong>: <span class="arithmatex">\(p(z) = \mathcal{N}(z; 0, I)\)</span></p>
</li>
</ul>
<p><strong>Vanilla VAE ELBO (Non-KL form)</strong>:</p>
<div class="arithmatex">\[ELBO_{\text{VAE}} = \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z|x)} \right]\]</div>
<p><strong>Hierarchical VAE Structure</strong> (z  z  x):</p>
<ul>
<li>
<p><strong>Encoder</strong>: <span class="arithmatex">\(q_\phi(z_1, z_2 | x) = q_\phi(z_1 | x) \cdot q_\phi(z_2 | z_1)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(q_\phi(z_1 | x) = \mathcal{N}(z_1; \mu_\phi(x), \sigma_\phi^2(x) I)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(q_\phi(z_2 | z_1) = \mathcal{N}(z_2; \mu_\phi(z_1), \sigma_\phi^2(z_1) I)\)</span></p>
</li>
<li>
<p><strong>Decoder</strong>: <span class="arithmatex">\(p_\theta(x, z_1 | z_2) = p_\theta(x | z_1) \cdot p_\theta(z_1 | z_2)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(p_\theta(x | z_1) = \mathcal{N}(x; \mu_\theta(z_1), \sigma_\theta^2(z_1) I)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(p_\theta(z_1 | z_2) = \mathcal{N}(z_1; \mu_\theta(z_2), \sigma_\theta^2(z_2) I)\)</span></p>
</li>
<li>
<p><strong>Prior</strong>: <span class="arithmatex">\(p(z_2) = \mathcal{N}(z_2; 0, I)\)</span></p>
</li>
</ul>
<p><strong>Hierarchical VAE ELBO (Non-KL form)</strong>:</p>
<div class="arithmatex">\[ELBO_{\text{HVAE}} = \mathbb{E}_{q_\phi(z_1,z_2|x)} \left[ \log \frac{p_\theta(x, z_1, z_2)}{q_\phi(z_1, z_2|x)} \right]\]</div>
<p>Following the hierarchical VAE formulation, we can write the ELBO for diffusion models. In diffusion models, we have a sequence of latent variables <span class="arithmatex">\(x_1, x_2, \ldots, x_T\)</span> where <span class="arithmatex">\(x_T\)</span> is the most abstract (pure noise) and <span class="arithmatex">\(x_0\)</span> is the data.</p>
<p><strong>Diffusion Model Structure</strong> (x_T  x_{T-1}  ...  x_1  x_0):</p>
<ul>
<li>
<p><strong>Encoder</strong>: <span class="arithmatex">\(q(x_1, x_2, \ldots, x_T | x_0) = \prod_{t=1}^T q(x_t | x_{t-1})\)</span> - Fixed noise corruption process</p>
</li>
<li>
<p><strong>Decoder</strong>: <span class="arithmatex">\(p_\theta(x_0, x_1, \ldots, x_{T-1} | x_T) = \prod_{t=1}^T p_\theta(x_{t-1} | x_t)\)</span> - Learned denoising process</p>
</li>
<li>
<p><strong>Prior</strong>: <span class="arithmatex">\(p(x_T) = \mathcal{N}(x_T; 0, I)\)</span></p>
</li>
</ul>
<p><strong>Diffusion Model ELBO (Non-KL form)</strong>:</p>
<div class="arithmatex">\[ELBO_{\text{Diff}} = \mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ \log \frac{p_\theta(x_0, x_1, \ldots, x_T)}{q(x_1, \ldots, x_T|x_0)} \right]\]</div>
<p>The Negative Evidence Lower BOund (NELBO) is the negative of the ELBO, which is what we actually minimize during training:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = -\mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ \log \frac{p_\theta(x_0, x_1, \ldots, x_T)}{q(x_1, \ldots, x_T|x_0)} \right]\]</div>
<p>This can be rewritten as:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = \mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ -\log \frac{p_\theta(x_0, x_1, \ldots, x_T)}{q(x_1, \ldots, x_T|x_0)} \right]\]</div>
<p>The decoder learns to predict the mean function <span class="arithmatex">\(\mu_\theta(x_t, t)\)</span> for the reverse process. Let's derive how this function is parameterized.</p>
<p>The true reverse process <span class="arithmatex">\(q(x_{t-1} | x_t, x_0)\)</span> can be derived using Bayes' theorem. For Gaussian distributions, this gives us:</p>
<div class="arithmatex">\[q(x_{t-1} | x_t, x_0) = \mathcal{N}(x_{t-1}; \mu_t(x_t, x_0), \sigma_t^2 I)\]</div>
<p>where it can be shown that:</p>
<div class="arithmatex">\[\mu_t(x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon \right)\]</div>
<p>and:</p>
<div class="arithmatex">\[\sigma_t^2 = \frac{\beta_t(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}\]</div>
<p>The learned reverse process is:</p>
<div class="arithmatex">\[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\]</div>
<p>Since we want the learned process to approximate the true reverse process, we parameterize <span class="arithmatex">\(\mu_\theta(x_t, t)\)</span> to match the form of <span class="arithmatex">\(\mu_t(x_t, x_0)\)</span>:</p>
<div class="arithmatex">\[\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right)\]</div>
<p>where <span class="arithmatex">\(\epsilon_\theta(x_t, t)\)</span> is a neural network that predicts the noise <span class="arithmatex">\(\epsilon\)</span> given <span class="arithmatex">\(x_t\)</span> and <span class="arithmatex">\(t\)</span>.</p>
<h3 id="rewriting-the-elbo-for-diffusion-models">Rewriting the ELBO for Diffusion Models</h3>
<p>Let's rewrite the diffusion model ELBO and transform it to resemble denoising score matching.</p>
<p>Starting with the diffusion model ELBO:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = \mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ -\log \frac{p_\theta(x_0, x_1, \ldots, x_T)}{q(x_1, \ldots, x_T|x_0)} \right]\]</div>
<p>We can expand this as:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = \mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ -\log p_\theta(x_0, x_1, \ldots, x_T) + \log q(x_1, \ldots, x_T|x_0) \right]\]</div>
<p>The learned joint distribution is:</p>
<div class="arithmatex">\[p_\theta(x_0, x_1, \ldots, x_T) = p(x_T) \cdot \prod_{t=1}^T p_\theta(x_{t-1} | x_t)\]</div>
<p>The true joint distribution is:</p>
<div class="arithmatex">\[q(x_1, \ldots, x_T | x_0) = \prod_{t=1}^T q(x_t | x_{t-1})\]</div>
<p>Substituting these:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = \mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ -\log p(x_T) - \sum_{t=1}^T \log p_\theta(x_{t-1} | x_t) + \sum_{t=1}^T \log q(x_t | x_{t-1}) \right]\]</div>
<p>For Gaussian distributions, the log-likelihood is:</p>
<div class="arithmatex">\[\log \mathcal{N}(x; \mu, \sigma^2 I) = -\frac{1}{2\sigma^2} \|x - \mu\|^2 + C\]</div>
<p>where <span class="arithmatex">\(C\)</span> is a constant that doesn't depend on the parameters.</p>
<p>For the learned reverse process:</p>
<div class="arithmatex">\[\log p_\theta(x_{t-1} | x_t) = -\frac{1}{2\sigma_t^2} \|x_{t-1} - \mu_\theta(x_t, t)\|^2 + C\]</div>
<p>For the true forward process:</p>
<div class="arithmatex">\[\log q(x_t | x_{t-1}) = -\frac{1}{2\beta_t} \|x_t - \sqrt{1 - \beta_t} x_{t-1}\|^2 + C\]</div>
<p>Using the definition of <span class="arithmatex">\(\mu_\theta(x_t, t)\)</span>:</p>
<div class="arithmatex">\[\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right)\]</div>
<p>The squared error term becomes:</p>
<div class="arithmatex">\[\|x_{t-1} - \mu_\theta(x_t, t)\|^2 = \left\|x_{t-1} - \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right)\right\|^2\]</div>
<p>From the forward process, we know:</p>
<div class="arithmatex">\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\]</div>
<p>And from the multistep transition:</p>
<div class="arithmatex">\[x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_{t-1}\]</div>
<p>Substituting these into the squared error:</p>
<div class="arithmatex">\[\|x_{t-1} - \mu_\theta(x_t, t)\|^2 = \left\|\sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_{t-1} - \frac{1}{\sqrt{\alpha_t}} \left( \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right)\right\|^2\]</div>
<p>Using the relationship <span class="arithmatex">\(\bar{\alpha}_t = \bar{\alpha}_{t-1} \cdot \alpha_t\)</span>, we can simplify:</p>
<div class="arithmatex">\[\|x_{t-1} - \mu_\theta(x_t, t)\|^2 = \left\|\frac{\beta_t}{\sqrt{\alpha_t(1 - \bar{\alpha}_t)}} (\epsilon - \epsilon_\theta(x_t, t))\right\|^2\]</div>
<p>This simplifies to:</p>
<div class="arithmatex">\[\|x_{t-1} - \mu_\theta(x_t, t)\|^2 = \frac{\beta_t^2}{\alpha_t(1 - \bar{\alpha}_t)} \|\epsilon - \epsilon_\theta(x_t, t)\|^2\]</div>
<p>Substituting back into the ELBO:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = \mathbb{E}_{q(x_1,\ldots,x_T|x_0)} \left[ -\log p(x_T) + \sum_{t=1}^T \frac{\beta_t^2}{2\sigma_t^2 \alpha_t(1 - \bar{\alpha}_t)} \|\epsilon - \epsilon_\theta(x_t, t)\|^2 + \sum_{t=1}^T \frac{1}{2\beta_t} \|x_t - \sqrt{1 - \beta_t} x_{t-1}\|^2 \right]\]</div>
<p>The key term in the ELBO is:</p>
<div class="arithmatex">\[\sum_{t=1}^T \frac{\beta_t^2}{2\sigma_t^2 \alpha_t(1 - \bar{\alpha}_t)} \|\epsilon - \epsilon_\theta(x_t, t)\|^2\]</div>
<p>Let's define:</p>
<div class="arithmatex">\[\lambda_t = \frac{\beta_t^2}{2\sigma_t^2 \alpha_t(1 - \bar{\alpha}_t)}\]</div>
<p>From the forward process, we know:</p>
<div class="arithmatex">\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\]</div>
<p>where <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, I)\)</span>.</p>
<p>The expectation <span class="arithmatex">\(\mathbb{E}_{q(x_1,\ldots,x_T|x_0)}\)</span> can be rewritten as:</p>
<div class="arithmatex">\[\mathbb{E}_{x_0 \sim p_{data}(x_0)} \mathbb{E}_{\epsilon_1, \ldots, \epsilon_T \sim \mathcal{N}(0, I)}\]</div>
<p>Since each <span class="arithmatex">\(x_t\)</span> is generated as:</p>
<div class="arithmatex">\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon_t\]</div>
<p>The ELBO can be simplified to:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = \mathbb{E}_{x_0, \epsilon, t} \left[ \lambda_t \|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t)\|^2 \right] + \text{constant terms}\]</div>
<p>where:</p>
<div class="arithmatex">\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon_t\]</div>
<p>This can be written more compactly as:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = \mathbb{E}_{x_0, \epsilon, t} \left[ \lambda_t \|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t)\|^2 \right] + \text{constant terms}\]</div>
<p>where <span class="arithmatex">\(x_0 \sim p_{data}(x_0)\)</span> (clean data), <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, I)\)</span> (noise), <span class="arithmatex">\(t \sim \text{Uniform}(1, T)\)</span> (timestep)</p>
<p>The diffusion model ELBO is equivalent to:</p>
<div class="arithmatex">\[\mathcal{L}_{\text{Diff}} = \mathbb{E}_{x_0, \epsilon, t} \left[ \lambda_t \|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t)\|^2 \right]\]</div>
<p>where <span class="arithmatex">\(\lambda_t = \frac{\beta_t^2}{2\sigma_t^2 \alpha_t(1 - \bar{\alpha}_t)}\)</span> is the weighting factor for each timestep.</p>
<p><span class="arithmatex">\(\epsilon_\theta\)</span> is a neural network model trained to predict the noise <span class="arithmatex">\(\epsilon\)</span> from the input noisy sample <span class="arithmatex">\(x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\)</span>. In other words, <span class="arithmatex">\(\epsilon_\theta\)</span> learns to denoise the input noisy image. It is a <strong>Noise predictor</strong> because <span class="arithmatex">\(\epsilon_\theta(x_t, t) \approx \epsilon\)</span>. </p>
<p><strong>Note:</strong> Predicting the noise is equivalent to predicting the clean sample. Given the forward diffusion equation <span class="arithmatex">\(x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\)</span>, if we predict <span class="arithmatex">\(\hat{\epsilon} = \epsilon_\theta(x_t, t)\)</span>, we can recover the clean sample as:</p>
<div class="arithmatex">\[x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \hat{\epsilon}}{\sqrt{\bar{\alpha}_t}}\]</div>
<p><strong>Note</strong>
In the original ELBO, we had two summation-terms:</p>
<ol>
<li>
<p><span class="arithmatex">\(\sum_{t=1}^T \frac{\beta_t^2}{2\sigma_t^2 \alpha_t(1 - \bar{\alpha}_t)} \|\epsilon - \epsilon_\theta(x_t, t)\|^2\)</span> (noise prediction term)</p>
</li>
<li>
<p><span class="arithmatex">\(\sum_{t=1}^T \frac{1}{2\beta_t} \|x_t - \sqrt{1 - \beta_t} x_{t-1}\|^2\)</span> (forward process term)</p>
</li>
</ol>
<p>The second summation-term <span class="arithmatex">\(\sum_{t=1}^T \frac{1}{2\beta_t} \|x_t - \sqrt{1 - \beta_t} x_{t-1}\|^2\)</span> represents the log-likelihood of the forward process <span class="arithmatex">\(q(x_t | x_{t-1})\)</span>. This summation-term does <strong>not depend on the model parameters <span class="arithmatex">\(\theta\)</span></strong> because the forward process is fixed and predefined. It only depends on the noise schedule <span class="arithmatex">\(\beta_t\)</span> and the data. When we take the gradient with respect to <span class="arithmatex">\(\theta\)</span> to optimize the model, this summation-term vanishes.</p>
<h3 id="sampling">Sampling</h3>
<p>While the ELBO loss <span class="arithmatex">\(\mathcal{L}_{\text{Diff}}\)</span> and the score-based objective are roughly equivalent in terms of what they learn, the <strong>sampling procedures</strong> differ between these two approaches.</p>
<p>In a <strong>Score-Based Model (SBM)</strong>, sampling is performed using <strong>Langevin dynamics</strong>. In a <strong>Diffusion Model (VAE form)</strong>, sampling follows the <strong>learned reverse process</strong>.</p>
<p>The connection between the two approaches comes from the relationship between the score function and the noise predictor:</p>
<p><strong>Score function</strong>: <span class="arithmatex">\(s_\theta(x_t, t) = \nabla_x \log p_t(x_t)\)</span></p>
<p><strong>Noise predictor</strong>: <span class="arithmatex">\(\epsilon_\theta(x_t, t)\)</span> predicts the noise added during the forward process</p>
<p><strong>Relationship</strong>: For Gaussian noise, the score function is proportional to the negative noise.</p>
<p>From the forward process, we have:</p>
<div class="arithmatex">\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\]</div>
<p>where <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, I)\)</span>.</p>
<p>The distribution of <span class="arithmatex">\(x_t\)</span> given <span class="arithmatex">\(x_0\)</span> is:</p>
<div class="arithmatex">\[q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\]</div>
<p>The score function is the gradient of the log probability density:</p>
<div class="arithmatex">\[s(x_t, t) = \nabla_{x_t} \log q(x_t | x_0)\]</div>
<p>For the Gaussian distribution <span class="arithmatex">\(q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\)</span>:</p>
<div class="arithmatex">\[\log q(x_t | x_0) = -\frac{1}{2(1 - \bar{\alpha}_t)} \|x_t - \sqrt{\bar{\alpha}_t} x_0\|^2 + C\]</div>
<p>where <span class="arithmatex">\(C\)</span> is a constant that doesn't depend on <span class="arithmatex">\(x_t\)</span>.</p>
<p>Taking the gradient with respect to <span class="arithmatex">\(x_t\)</span>:</p>
<div class="arithmatex">\[\nabla_{x_t} \log q(x_t | x_0) = -\frac{1}{1 - \bar{\alpha}_t} (x_t - \sqrt{\bar{\alpha}_t} x_0)\]</div>
<p>From the forward process, we can express <span class="arithmatex">\(x_0\)</span> in terms of <span class="arithmatex">\(x_t\)</span> and <span class="arithmatex">\(\epsilon\)</span>:</p>
<div class="arithmatex">\[x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon}{\sqrt{\bar{\alpha}_t}}\]</div>
<div class="arithmatex">\[\nabla_{x_t} \log q(x_t | x_0) = -\frac{1}{1 - \bar{\alpha}_t} \left(x_t - \sqrt{\bar{\alpha}_t} \cdot \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon}{\sqrt{\bar{\alpha}_t}}\right)\]</div>
<p>Simplifying the expression:</p>
<div class="arithmatex">\[\nabla_{x_t} \log q(x_t | x_0) = -\frac{1}{1 - \bar{\alpha}_t} \left(x_t - (x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon)\right)\]</div>
<div class="arithmatex">\[\nabla_{x_t} \log q(x_t | x_0) = -\frac{1}{1 - \bar{\alpha}_t} \cdot \sqrt{1 - \bar{\alpha}_t} \epsilon\]</div>
<div class="arithmatex">\[\nabla_{x_t} \log q(x_t | x_0) = -\frac{\epsilon}{\sqrt{1 - \bar{\alpha}_t}}\]</div>
<p>Therefore, the score function is:</p>
<div class="arithmatex">\[s(x_t, t) = \nabla_{x_t} \log q(x_t | x_0) = -\frac{\epsilon}{\sqrt{1 - \bar{\alpha}_t}}\]</div>
<p>In practice, we learn:</p>
<ul>
<li>
<p><strong>Score function</strong>: <span class="arithmatex">\(s_\theta(x_t, t) \approx \nabla_{x_t} \log q(x_t | x_0)\)</span></p>
</li>
<li>
<p><strong>Noise predictor</strong>: <span class="arithmatex">\(\epsilon_\theta(x_t, t) \approx \epsilon\)</span></p>
</li>
</ul>
<p>Therefore, <span class="arithmatex">\(s_\theta(x_t, t) \approx -\frac{\epsilon_\theta(x_t, t)}{\sqrt{1 - \bar{\alpha}_t}}\)</span>.</p>
<p>Both sampling methods work because they're learning the same underlying structure:</p>
<ol>
<li><strong>Score-based</strong>: Learns the gradient of the log-density at each noise level</li>
<li><strong>Diffusion</strong>: Learns the noise that was added during the forward process</li>
</ol>
<p>Since the score function and noise predictor are mathematically related, both approaches can generate high-quality samples, but they use different sampling algorithms.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>