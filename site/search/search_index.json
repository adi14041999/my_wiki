{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my wiki!","text":""},{"location":"ai/deep_generative_models/autoregressive_models/","title":"Autoregressive models","text":"<p>We assume we are given access to a dataset: $$ \\mathcal{D} = { \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_m } $$ where each datapoint is n-dimensional. For simplicity, we assume the datapoints are binary. $$ x_i \\in {0,1}^n $$</p>"},{"location":"ai/deep_generative_models/autoregressive_models/#representation","title":"Representation","text":"<p>If you have n random variables: $$ X_1, X_2, \\dots, X_n $$ then their joint probability can be written as a product of conditional probabilities: $$ P(X_1, X_2, \\dots, X_n) = P(X_1) \\cdot P(X_2 \\mid X_1) \\cdot P(X_3 \\mid X_1, X_2) \\cdot \\dots \\cdot P(X_n \\mid X_1, X_2, \\dots, X_{n-1}) $$ In words:</p> <p>The probability of all n variables taking particular values equals: \u2192 the probability of the first variable, \u2192 times the probability of the second variable given the first, \u2192 times the probability of the third variable given the first two, \u2192 and so on, until the n-th variable.</p> <p>By this chain rule of probability, we can factorize the joint distribution over the n-dimensions as:</p> \\[ p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i \\mid x_1, x_2, \\dots, x_{i-1}) = \\prod_{i=1}^n p(x_i \\mid x_{&lt;i}) \\] <p>where</p> \\[ x_{&lt;i} = [x_1, x_2, \\dots, x_{i-1}] \\] <p>denotes the vector of random variables with index less than i.</p> <p>The chain rule factorization can be expressed graphically as a Bayesian network.</p> <p></p> <p>Such a Bayesian network that makes no conditional independence assumptions is said to obey the autoregressive property. The term autoregressive originates from the literature on time-series models where observations from the previous time-steps are used to predict the value at the current time step. Here, we fix an ordering of the variables x1, x2, \u2026, xn and the distribution for the i-th random variable depends on the values of all the preceding random variables in the chosen ordering x1, x2, \u2026, xi\u22121.</p> <p>If we allow for every conditional p(xi|x&lt;i) to be specified in a tabular form, then such a representation is fully general and can represent any possible distribution over n random variables. However, the space complexity for such a representation grows exponentially with n.</p> <p>To see why, let us consider the conditional for the last dimension, given by p(xn|x&lt;n). In order to fully specify this conditional, we need to specify a probability for 2^(n\u22121) configurations of the variables x1, x2, \u2026, xn\u22121. Since the probabilities should sum to 1, the total number of parameters for specifying this conditional is given by 2^(n\u22121)\u22121. Hence, a tabular representation for the conditionals is impractical for learning the joint distribution factorized via chain rule.</p> <p>In an autoregressive generative model, the conditionals are specified as parameterized functions with a fixed number of parameters. Specifically, we assume that each conditional distribution corresponds to a Bernoulli random variable. We then learn a function that maps the preceding random variables to the parameter (mean) of this Bernoulli distribution. Hence, we have:</p> \\[ p_{\\theta_i}(x_i \\mid x_{&lt;i}) = \\text{Bern} \\left( f_i(x_1, x_2, \\dots, x_{i-1}) \\right) \\] <p>where the function is defined as:</p> \\[ f_i : \\{0,1\\}^{i-1} \\to [0,1] \\] <p>and theta_i denotes the set of parameters used to specify this function. This function takes in a vector of size (i-1) where each element is a 0 or a 1, and outputs a scalar bit.</p> <p>The total number of parameters in an autoregressive generative model is given by:</p> \\[ \\sum_{i=1}^n \\left| \\theta_i \\right| \\] <p>In the simplest case, we can specify the function as a linear combination of the input elements followed by a sigmoid non-linearity (to restrict the output to lie between 0 and 1). This gives us the formulation of a fully-visible sigmoid belief network (FVSBN).</p> \\[ f_i(x_1, x_2, \\dots, x_{i-1}) = \\sigma(\\alpha^{(i)}_0 + \\alpha^{(i)}_1 x_1 + \\dots + \\alpha^{(i)}_{i-1} x_{i-1}) \\] <p>where \\(\\sigma\\) denotes the sigmoid function and \\(\\theta_i = \\{\\alpha^{(i)}_0, \\alpha^{(i)}_1, \\dots, \\alpha^{(i)}_{i-1}\\}\\) denote the parameters of the mean function. The conditional for variable \\(i\\) requires \\(i\\) parameters, and hence the total number of parameters in the model is given by \\(\\sum_{i=1}^n i = O(n^2)\\). Note that the number of parameters are much fewer than the exponential complexity of the tabular case.</p> <p>A natural way to increase the expressiveness of an autoregressive generative model is to use more flexible parameterizations for the mean function e.g., multi-layer perceptrons (MLP). For example, consider the case of a neural network with 1 hidden layer. The mean function for variable \\(i\\) can be expressed as</p> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{A}_i \\mathbf{x}_{&lt;i} + \\mathbf{c}_i) \\\\ f_i(x_1, x_2, \\dots, x_{i-1}) &amp;= \\sigma(\\boldsymbol{\\alpha}^{(i)} \\mathbf{h}_i + b_i) \\end{align} \\] <p>where \\(\\mathbf{h}_i \\in \\mathbb{R}^d\\) denotes the hidden layer activations for the MLP and \\(\\theta_i = \\{\\mathbf{A}_i \\in \\mathbb{R}^{d \\times (i-1)}, \\mathbf{c}_i \\in \\mathbb{R}^d, \\boldsymbol{\\alpha}^{(i)} \\in \\mathbb{R}^d, b_i \\in \\mathbb{R}\\}\\) are the set of parameters for the mean function \\(\\mu_i(\\cdot)\\). The total number of parameters in this model is dominated by the matrices \\(\\mathbf{A}_i\\) and given by \\(O(n^2d)\\).</p> <p>Note: The term \"mean function\" here refers to the function that determines the mean (expected value) of the Bernoulli distribution for each variable. Since we're modeling binary variables, the mean of the Bernoulli distribution is the probability of the variable being 1. The sigmoid function \\(\\sigma\\) ensures that this probability lies between 0 and 1.</p> <p>For a Bernoulli random variable \\(X\\) with parameter \\(p\\), the expectation (mean) is given by:</p> \\[ \\mathbb{E}[X] = 1 \\cdot p + 0 \\cdot (1-p) = p \\] <p>This is because: - \\(X\\) takes value 1 with probability \\(p\\) - \\(X\\) takes value 0 with probability \\((1-p)\\) - The expectation is the weighted sum of all possible values, where the weights are their respective probabilities</p> <p>Therefore, when we say the mean function determines the mean of the Bernoulli distribution, we're saying it determines the probability \\(p\\) of the variable being 1.</p> <p>The Neural Autoregressive Density Estimator (NADE) provides an alternate MLP-based parameterization that is more statistically and computationally efficient than the vanilla approach. In NADE, parameters are shared across the functions used for evaluating the conditionals. In particular, the hidden layer activations are specified as</p> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{W}_{.,&lt;i} \\mathbf{x}_{&lt;i} + \\mathbf{c}) \\\\ f_i(x_1, x_2, \\dots, x_{i-1}) &amp;= \\sigma(\\boldsymbol{\\alpha}^{(i)} \\mathbf{h}_i + b_i) \\end{align} \\] <p>where \\(\\theta = \\{\\mathbf{W} \\in \\mathbb{R}^{d \\times n}, \\mathbf{c} \\in \\mathbb{R}^d, \\{\\boldsymbol{\\alpha}^{(i)} \\in \\mathbb{R}^d\\}_{i=1}^n, \\{b_i \\in \\mathbb{R}\\}_{i=1}^n\\}\\) is the full set of parameters for the mean functions \\(f_1(\\cdot), f_2(\\cdot), \\dots, f_n(\\cdot)\\). The weight matrix \\(\\mathbf{W}\\) and the bias vector \\(\\mathbf{c}\\) are shared across the conditionals. Sharing parameters offers two benefits:</p> <ol> <li> <p>The total number of parameters gets reduced from \\(O(n^2d)\\) to \\(O(nd)\\).</p> </li> <li> <p>The hidden unit activations can be evaluated in \\(O(nd)\\) time via the following recursive strategy:</p> </li> </ol> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{a}_i) \\\\ \\mathbf{a}_{i+1} &amp;= \\mathbf{a}_i + \\mathbf{W}_{[.,i]} x_i \\end{align} \\] <p>with the base case given by \\(\\mathbf{a}_1 = \\mathbf{c}\\).</p> <p>The RNADE algorithm extends NADE to learn generative models over real-valued data. Here, the conditionals are modeled via a continuous distribution such as a equi-weighted mixture of \\(K\\) Gaussians. Instead of learning a mean function, we now learn the means \\(\\mu_{i,1}, \\mu_{i,2}, \\dots, \\mu_{i,K}\\) and variances \\(\\Sigma_{i,1}, \\Sigma_{i,2}, \\dots, \\Sigma_{i,K}\\) of the \\(K\\) Gaussians for every conditional. For statistical and computational efficiency, a single function \\(g_i: \\mathbb{R}^{i-1} \\to \\mathbb{R}^{2K}\\) outputs all the means and variances of the \\(K\\) Gaussians for the \\(i\\)-th conditional distribution.</p> <p>The conditional distribution \\(p_{\\theta_i}(x_i \\mid \\mathbf{x}_{&lt;i})\\) in RNADE is given by:</p> \\[ p_{\\theta_i}(x_i \\mid \\mathbf{x}_{&lt;i}) = \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(x_i; \\mu_{i,k}, \\Sigma_{i,k}) \\] <p>where \\(\\mathcal{N}(x; \\mu, \\Sigma)\\) denotes the probability density of a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\Sigma\\) evaluated at \\(x\\). The parameters \\(\\{\\mu_{i,k}, \\Sigma_{i,k}\\}_{k=1}^K\\) are the outputs of the function \\(g_i(\\mathbf{x}_{&lt;i})\\).</p> <p>This is how RNADE is autoregressive. Example sequence showing autoregressive dependencies:</p> <p>\\(x_1\\):    - Input to \\(g_1\\): \\(\\mathbf{x}_{&lt;1} = []\\) (empty)   - Output: \\(\\{\\mu_{1,k}, \\Sigma_{1,k}\\}_{k=1}^K\\) for \\(p(x_1)\\)</p> <p>\\(x_2\\):    - Input to \\(g_2\\): \\(\\mathbf{x}_{&lt;2} = [x_1]\\)   - Output: \\(\\{\\mu_{2,k}, \\Sigma_{2,k}\\}_{k=1}^K\\) for \\(p(x_2 \\mid x_1)\\)</p> <p>\\(x_3\\):    - Input to \\(g_3\\): \\(\\mathbf{x}_{&lt;3} = [x_1, x_2]\\)   - Output: \\(\\{\\mu_{3,k}, \\Sigma_{3,k}\\}_{k=1}^K\\) for \\(p(x_3 \\mid x_1, x_2)\\)</p> <p>\\(x_4\\):    - Input to \\(g_4\\): \\(\\mathbf{x}_{&lt;4} = [x_1, x_2, x_3]\\)   - Output: \\(\\{\\mu_{4,k}, \\Sigma_{4,k}\\}_{k=1}^K\\) for \\(p(x_4 \\mid x_1, x_2, x_3)\\)</p> <p>This sequential, conditional generation process is what makes RNADE an autoregressive model. The mixture of Gaussians is just the form of the conditional distribution, but the autoregressive property comes from how these distributions are parameterized based on previous variables.</p>"},{"location":"ai/deep_generative_models/autoregressive_models/#learning-and-inference","title":"Learning and inference","text":"<p>Recall that learning a generative model involves optimizing the closeness between the data and model distributions. One commonly used notion of closeness is the KL divergence between the data and the model distributions:</p> \\[ \\min_{\\theta \\in \\Theta} d_{KL}(p_{data}, p_{\\theta}) = \\min_{\\theta \\in \\Theta} \\mathbb{E}_{x \\sim p_{data}}[\\log p_{data}(x) - \\log p_{\\theta}(x)] \\] <p>where: - \\(p_{data}\\) is the true data distribution - \\(p_{\\theta}\\) is our model distribution parameterized by \\(\\theta\\) - \\(\\Theta\\) is the set of all possible parameter values - \\(d_{KL}\\) is the Kullback-Leibler divergence</p> <p>Let's break down how this minimization works:</p> <ol> <li>For a fixed value of \\(\\theta\\), we compute:</li> <li>The expectation over all possible data points \\(x\\) from \\(p_{data}\\)</li> <li>For each \\(x\\), we compute \\(\\log p_{data}(x) - \\log p_{\\theta}(x)\\)</li> <li> <p>This gives us a single scalar value for this particular \\(\\theta\\)</p> </li> <li> <p>The minimization operator \\(\\min_{\\theta \\in \\Theta}\\) then:</p> </li> <li>Tries different values of \\(\\theta\\) in the parameter space \\(\\Theta\\)</li> <li> <p>Finds the \\(\\theta\\) that gives the smallest expected value</p> </li> <li> <p>Since \\(p_{data}\\) is constant with respect to \\(\\theta\\), minimizing the KL divergence is equivalent to maximizing the expected log-likelihood of the data under our model:</p> </li> </ol> \\[ \\max_{\\theta \\in \\Theta} \\mathbb{E}_{x \\sim p_{data}}[\\log p_{\\theta}(x)] \\] <p>This is because \\(\\log p_{data}(x)\\) doesn't depend on \\(\\theta\\), so it can be treated as a constant. Minimizing \\(-\\log p_{\\theta}(x)\\) is the same as maximizing \\(\\log p_{\\theta}(x)\\)</p> <p>To approximate the expectation over the unknown \\(p_{data}\\), we make an assumption: points in the dataset \\(\\mathcal{D}\\) are sampled i.i.d. from \\(p_{data}\\). This allows us to obtain an unbiased Monte Carlo estimate of the objective as:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\log p_{\\theta}(x) = \\mathcal{L}(\\theta | \\mathcal{D}) \\] <p>The maximum likelihood estimation (MLE) objective has an intuitive interpretation: pick the model parameters \\(\\theta \\in \\Theta\\) that maximize the log-probability of the observed datapoints in \\(\\mathcal{D}\\).</p> <p>In practice, we optimize the MLE objective using mini-batch gradient ascent. The algorithm operates in iterations. At every iteration \\(t\\), we sample a mini-batch \\(\\mathcal{B}_t\\) of datapoints sampled randomly from the dataset (\\(|\\mathcal{B}_t| &lt; |\\mathcal{D}|\\)) and compute gradients of the objective evaluated for the mini-batch. These parameters at iteration \\(t+1\\) are then given via the following update rule:</p> \\[ \\theta^{(t+1)} = \\theta^{(t)} + r_t \\nabla_{\\theta} \\mathcal{L}(\\theta^{(t)} | \\mathcal{B}_t) \\] <p>where \\(\\theta^{(t+1)}\\) and \\(\\theta^{(t)}\\) are the parameters at iterations \\(t+1\\) and \\(t\\) respectively, and \\(r_t\\) is the learning rate at iteration \\(t\\). Typically, we only specify the initial learning rate \\(r_1\\) and update the rate based on a schedule.</p> <p>Now that we have a well-defined objective and optimization procedure, the only remaining task is to evaluate the objective in the context of an autoregressive generative model. To this end, we first write the MLE objective in terms of the joint probability:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\log p_{\\theta}(x) \\] <p>Then, we substitute the factorized joint distribution of an autoregressive model. Since \\(p_{\\theta}(x) = \\prod_{i=1}^n p_{\\theta_i}(x_i | x_{&lt;i})\\), we have:</p> \\[ \\log p_{\\theta}(x) = \\log \\prod_{i=1}^n p_{\\theta_i}(x_i | x_{&lt;i}) = \\sum_{i=1}^n \\log p_{\\theta_i}(x_i | x_{&lt;i}) \\] <p>Substituting this into the MLE objective, we get:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\sum_{i=1}^n \\log p_{\\theta_i}(x_i | x_{&lt;i}) \\] <p>where \\(\\theta = \\{\\theta_1, \\theta_2, \\dots, \\theta_n\\}\\) now denotes the collective set of parameters for the conditionals.</p> <p>Inference in an autoregressive model is straightforward. For density estimation of an arbitrary point \\(x\\), we simply evaluate the log-conditionals \\(\\log p_{\\theta_i}(x_i | x_{&lt;i})\\) for each \\(i\\) and add these up to obtain the log-likelihood assigned by the model to \\(x\\). Since we have the complete vector \\(x = [x_1, x_2, \\dots, x_n]\\), we know all the values needed for each conditional \\(x_{&lt;i}\\), so each of the conditionals can be evaluated in parallel. Hence, density estimation is efficient on modern hardware.</p> <p>For example, given a 4-dimensional vector \\(x = [x_1, x_2, x_3, x_4]\\), we can compute all conditionals in parallel:</p> <ul> <li>\\(\\log p_{\\theta_1}(x_1)\\) (no conditioning needed)</li> <li>\\(\\log p_{\\theta_2}(x_2 | x_1)\\) (using known \\(x_1\\))</li> <li>\\(\\log p_{\\theta_3}(x_3 | x_1, x_2)\\) (using known \\(x_1, x_2\\))</li> <li>\\(\\log p_{\\theta_4}(x_4 | x_1, x_2, x_3)\\) (using known \\(x_1, x_2, x_3\\))</li> </ul> <p>Then sum them to get the total log-likelihood: \\(\\log p_{\\theta}(x) = \\sum_{i=1}^4 \\log p_{\\theta_i}(x_i | x_{&lt;i})\\)</p> <p>Sampling from an autoregressive model is a sequential procedure. Here, we first sample \\(x_1\\), then we sample \\(x_2\\) conditioned on the sampled \\(x_1\\), followed by \\(x_3\\) conditioned on both \\(x_1\\) and \\(x_2\\) and so on until we sample \\(x_n\\) conditioned on the previously sampled \\(x_{&lt;n}\\). For applications requiring real-time generation of high-dimensional data such as audio synthesis, the sequential sampling can be an expensive process.</p> <p>Finally, an autoregressive model does not directly learn unsupervised representations of the data. This is because:</p> <ol> <li>The model directly models the data distribution \\(p(x)\\) through a sequence of conditional distributions \\(p(x_i | x_{&lt;i})\\)</li> <li>There is no explicit latent space or bottleneck that forces the model to learn a compressed representation</li> <li>Each variable is modeled based on previous variables, but there's no mechanism to learn a global, compressed representation of the entire data point</li> <li>The model's parameters \\(\\theta_i\\) are specific to each conditional distribution and don't encode a meaningful representation of the data</li> </ol> <p>In contrast, latent variable models like variational autoencoders explicitly learn a compressed representation by: 1. Introducing a latent space \\(z\\) that captures the essential features of the data 2. Learning an encoder that maps data to this latent space 3. Learning a decoder that reconstructs data from the latent space 4. Using a bottleneck that forces the model to learn meaningful representations</p>"},{"location":"ai/deep_generative_models/energy_based_models/","title":"Energy-Based Models","text":""},{"location":"ai/deep_generative_models/energy_based_models/#parameterizing-probability-distributions","title":"Parameterizing Probability Distributions","text":"<p>Probability distributions \\(p(x)\\) are a key building block in generative modeling. Building a neural network that ensures \\(p(x) \\geq 0\\) is not hard. However, the real challenge lies in ensuring that the distribution satisfies the normalization constraint: for discrete variables, the sum over all possible values of \\(x\\) must equal 1, while for continuous variables, the integral over the entire domain must equal 1.</p> <p>Problem: \\(g_\\theta(x) \\geq 0\\) is easy. But \\(\\sum_x g_\\theta(x) = Z(\\theta) \\neq 1\\) in general, so \\(g_\\theta(x)\\) is not a valid probability mass function. For continuous variables, \\(\\int g_\\theta(x) dx = Z(\\theta) \\neq 1\\) in general, so \\(g_\\theta(x)\\) is not a valid probability density function.</p> <p>Solution:</p> \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} g_\\theta(x) = \\frac{1}{\\int g_\\theta(x) dx} g_\\theta(x) = \\frac{1}{\\text{Volume}(g_\\theta)} g_\\theta(x)\\] <p>Then by definition,</p> \\[\\int p_\\theta(x) dx = \\int \\frac{1}{Z(\\theta)} g_\\theta(x) dx = \\frac{Z(\\theta)}{Z(\\theta)} = 1\\] <p>Here, \\(g_\\theta(x)\\) is the output of the neural network with parameters \\(\\theta\\) at input \\(x\\). The volume of \\(g_\\theta\\), denoted as \\(\\text{Volume}(g_\\theta)\\), is defined as the integral of \\(g_\\theta(x)\\) over the entire domain: \\(\\text{Volume}(g_\\theta) = \\int g_\\theta(x) dx = Z(\\theta)\\). It is a normalizing constant (w.r.t. \\(x\\)) but changes for different \\(\\theta\\). For example, we choose \\(g_\\theta(x)\\) so that we know the volume analytically as a function of \\(\\theta\\).</p> <p>The partition function \\(Z(\\theta)\\) is the normalization constant that ensures a probability distribution integrates (or sums) to 1. It's called a \"partition function\" because it partitions the unnormalized function \\(g_\\theta(x)\\) into a proper probability distribution.</p> <p>Example: \\(g_{(\\mu, \\sigma)}(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)</p> \\[\\text{Volume}(g_{(\\mu, \\sigma)}) = \\int_{-\\infty}^{\\infty} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx = \\sqrt{2\\pi\\sigma^2}\\] <p>Therefore, the normalized probability density function is:</p> \\[p_{(\\mu, \\sigma)}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] <p>This is the standard normal (Gaussian) distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Functional forms \\(g_\\theta(x)\\) need to allow analytical integration. Despite being restrictive, they are very useful as building blocks for more complex distributions.</p> <p>Note: What we've been doing with autoregressive models, flow models, and VAEs are essentially tricks for composing simple functions that are normalized to build more complex probabilistic models that are by construction guaranteed to be normalized. These approaches avoid the intractability of computing the partition function for complex distributions by designing architectures where normalization is preserved through the composition of simple, analytically tractable components.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#energy-based-models_1","title":"Energy Based Models","text":"<p>We are going to formalize EBMs the following way:</p> \\[p_\\theta(x) = \\frac{1}{\\int e^{f_\\theta(x)} dx} \\cdot e^{f_\\theta(x)}\\] \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} \\cdot e^{f_\\theta(x)}\\] <p>Why \\(e^{f_\\theta(x)}\\) and not \\(f_\\theta(x)^2\\)?</p> <p>Both \\(e^{f_\\theta(x)}\\) and \\(f_\\theta(x)^2\\) produce non-negative outputs, but we choose the exponential form for several important reasons:</p> <ol> <li> <p>Additive Energy: The exponential form allows us to work with additive energy functions. If we have \\(f_\\theta(x) = f_1(x) + f_2(x)\\), then \\(e^{f_\\theta(x)} = e^{f_1(x)} \\cdot e^{f_2(x)}\\), which is a natural way to combine energy terms.</p> </li> <li> <p>Log-Probability Interpretation: Taking the logarithm gives us \\(\\log p_\\theta(x) = f_\\theta(x) - \\log Z(\\theta)\\). This means \\(f_\\theta(x)\\) directly represents the unnormalized log-probability, making it easier to work with in practice.</p> </li> <li> <p>Gradient Properties: The exponential function has the property that \\(\\frac{d}{dx}e^{f(x)} = e^{f(x)} \\cdot f'(x)\\). This makes gradient-based learning more stable and interpretable.</p> </li> <li> <p>Numerical Stability: The exponential function grows more smoothly than quadratic functions, which can lead to better numerical stability during training.</p> </li> <li> <p>Dynamic Range: The exponential function can capture much larger variations in probability compared to quadratic functions. While \\(f_\\theta(x)^2\\) is bounded by the square of the function's range, \\(e^{f_\\theta(x)}\\) can represent probabilities that vary by many orders of magnitude.</p> </li> <li> <p>Statistical Mechanics Connection: The exponential form follows from the Boltzmann distribution in statistical mechanics, where \\(p(x) \\propto e^{-E(x)/kT}\\), where \\(-E(x)\\) is the energy of state \\(x\\). Hence the name.</p> </li> </ol> <p>Pros: Very flexible, can use any \\(f_\\theta(x)\\)</p> <p>Cons: \\(Z(\\theta)\\) is intractable, so no access to likelihood. Thus, evaluating and optimizing likelihood \\(p_\\theta(x)\\) is hard (learning is hard). Also, sampling from \\(p_\\theta(x)\\) is hard. Another con is there is no feature learning (but can add latent variables). EBMs also suffer from the curse of dimensionality - as the dimension of \\(x\\) increases, the volume of the space grows exponentially, making it increasingly difficult to learn meaningful energy functions and sample efficiently.</p> <p>Given two points \\(x_1\\) and \\(x_2\\), evaluating \\(p_\\theta(x_1)\\) or \\(p_\\theta(x_2)\\) requires calculating \\(Z(\\theta)\\). However, their ratio does not involve calculating \\(Z(\\theta)\\).</p> \\[\\frac{p_\\theta(x_1)}{p_\\theta(x_2)} = \\frac{\\frac{1}{Z(\\theta)} \\cdot e^{f_\\theta(x_1)}}{\\frac{1}{Z(\\theta)} \\cdot e^{f_\\theta(x_2)}} = \\frac{e^{f_\\theta(x_1)}}{e^{f_\\theta(x_2)}} = e^{f_\\theta(x_1) - f_\\theta(x_2)}\\] <p>The partition function \\(Z(\\theta)\\) cancels out in the ratio, so we only need to evaluate the energy function at the two points and take their difference. This means we can determine which of \\(x_1\\) or \\(x_2\\) is more likely under our model without needing to compute the intractable partition function.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#training-ebms-with-contrastive-divergence","title":"Training EBMs with Contrastive Divergence","text":"<p>Let's assume we want to maximize \\(\\frac{\\exp(f_\\theta(x_{train}))}{Z(\\theta)}\\). \\(x_{train}\\) is the 'correct answer'- we want to increase the probability of this under the model. Let's also assume we have a 'wrong answer'. The objective is to not just maximize \\(\\exp(f_\\theta(x_{train}))\\) but also minimize \\(Z(\\theta)\\) because that's going to result in the 'wrong' answer being pushed down.</p> <p>Instead of evaluating \\(Z(\\theta)\\) exactly, we use a Monte Carlo estimate.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#contrastive-divergence-algorithm","title":"Contrastive Divergence Algorithm","text":""},{"location":"ai/deep_generative_models/energy_based_models/#high-level-idea","title":"High-Level Idea","text":"<p>The contrastive divergence algorithm works as follows:</p> <p>Algorithm:</p> <ol> <li> <p>Assuming we can sample from the model, sample \\(x_{sample} \\sim p_\\theta\\)</p> </li> <li> <p>Take a step on the gradient: \\(\\nabla_\\theta(f_\\theta(x_{train}) - f_\\theta(x_{sample}))\\)</p> </li> <li> <p>Keep repeating this to make the training data more likely than typical samples from the model</p> </li> </ol>"},{"location":"ai/deep_generative_models/energy_based_models/#why-does-this-work","title":"Why Does This Work?","text":"<p>We want to maximize the log-likelihood: \\(\\max_\\theta(f_\\theta(x_{train}) - \\log Z(\\theta))\\)</p> <p>Mathematical Derivation:</p> <p>The gradient of the log-likelihood is:</p> \\[\\nabla_\\theta \\log p_\\theta(x_{train}) = \\nabla_\\theta(f_\\theta(x_{train}) - \\log Z(\\theta))\\] <p>Let's split the terms and take the derivative:</p> \\[\\nabla_\\theta \\log p_\\theta(x_{train}) = \\nabla_\\theta f_\\theta(x_{train}) - \\nabla_\\theta \\log Z(\\theta)\\] <p>Now we need to compute \\(\\nabla_\\theta \\log Z(\\theta)\\). Let's expand this:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\nabla_\\theta \\log \\int e^{f_\\theta(x)} dx\\] <p>Using the chain rule and the fact that \\(\\nabla \\log f(x) = \\frac{\\nabla f(x)}{f(x)}\\):</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\nabla_\\theta \\int e^{f_\\theta(x)} dx\\] <p>Since the integral and derivative can be exchanged:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\int \\nabla_\\theta e^{f_\\theta(x)} dx\\] <p>Using the chain rule again:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\int e^{f_\\theta(x)} \\nabla_\\theta f_\\theta(x) dx\\] <p>Notice that \\(\\frac{e^{f_\\theta(x)}}{Z(\\theta)} = p_\\theta(x)\\), so:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\int p_\\theta(x) \\nabla_\\theta f_\\theta(x) dx = \\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)]\\] <p>Final Result:</p> <p>Putting it all together:</p> \\[\\nabla_\\theta \\log p_\\theta(x_{train}) = \\nabla_\\theta f_\\theta(x_{train}) - \\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)]\\] <p>The Key Insight:</p> <p>The second term \\(\\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)]\\) is an expectation over the model distribution. We approximate (Monte Carlo approximation) this expectation using samples from the model:</p> \\[\\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)] \\approx \\nabla_\\theta f_\\theta(x_{sample})\\] <p>where \\(x_{sample} \\sim p_\\theta\\) is a sample from our model.</p> <p>Important note on sampling:</p> <p>Unlike autoregressive models or normalizing flow models, Energy-Based Models do not provide a direct way to sample from \\(p_\\theta(x)\\). In autoregressive models, we can sample sequentially by conditioning on previous values. In flow models, we can sample from a simple base distribution and transform it through invertible functions. However, in EBMs, we need to use approximate sampling methods like:</p> <ul> <li>Langevin Dynamics: Gradient-based sampling with noise</li> <li>Gibbs Sampling: For discrete variables, updating one variable at a time</li> <li>Metropolis-Hastings: Markov chain Monte Carlo methods</li> <li>Hamiltonian Monte Carlo: More sophisticated MCMC methods</li> </ul> <p>This sampling challenge is one of the main difficulties in training EBMs, as we need to run these sampling procedures every time we want to estimate the gradient.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#sampling-from-ebms-with-markov-monte-carlo-methods","title":"Sampling from EBMs with Markov Monte Carlo Methods","text":""},{"location":"ai/deep_generative_models/energy_based_models/#metropolis-hastings-algorithm","title":"Metropolis-Hastings Algorithm","text":"<p>Metropolis-Hastings (MH) is a general-purpose Markov Chain Monte Carlo (MCMC) method for sampling from complex probability distributions. It's particularly useful for Energy-Based Models where direct sampling is not possible.</p> <p>The Algorithm</p> <p>Step 1: Initialize Start with an initial sample \\(x^{(0)}\\) (could be random or from training data)</p> <p>Step 2: Propose a New Sample For each iteration \\(t\\):</p> <ul> <li> <p>Generate a proposal \\(x^*\\) from a proposal distribution \\(q(x^* | x^{(t)})\\)</p> </li> <li> <p>The proposal distribution should be easy to sample from (e.g., Gaussian centered at current point)</p> </li> </ul> <p>Step 3: Accept or Reject</p> <p>Compute the acceptance probability:</p> \\[\\alpha = \\min\\left(1, \\frac{e^{f_\\theta(x^*)} \\cdot q(x^{(t)} | x^*)}{e^{f_\\theta(x^{(t)})} \\cdot q(x^* | x^{(t)})}\\right)\\] <p>The <code>min(1, ...)</code> ensures the acceptance probability is between 0 and 1. When the ratio is &gt; 1, we always accept (probability = 1). When the ratio is \u2264 1, we accept with probability equal to the ratio.</p> <p>Step 4: Update With probability \\(\\alpha\\), accept the proposal: \\(x^{(t+1)} = x^*\\). With probability \\(1-\\alpha\\), reject and keep current: \\(x^{(t+1)} = x^{(t)}\\)</p> <p>Step 5: Repeat Continue for many iterations until convergence</p> <p>This algorithm provides a robust foundation for sampling from Energy-Based Models, though it may require careful tuning and monitoring for optimal performance.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#unadjusted-langevin-mcmc","title":"Unadjusted Langevin MCMC","text":"<p>Unadjusted Langevin MCMC (ULMCMC) is another popular method for sampling from Energy-Based Models. Unlike Metropolis-Hastings, it doesn't use an accept/reject step, making it computationally more efficient.</p> <p>The Algorithm</p> <p>Step 1: Initialize Start with an initial sample \\(x^{(0)}\\) (could be random or from training data)</p> <p>Step 2: Langevin Dynamics Update For each iteration \\(t\\):</p> \\[x^{(t+1)} = x^{(t)} + \\epsilon \\nabla_x f_\\theta(x^{(t)}) + \\sqrt{2\\epsilon} \\eta_t\\] <p>where:</p> <ul> <li> <p>\\(\\epsilon\\) is the step size (learning rate)</p> </li> <li> <p>\\(\\nabla_x f_\\theta(x^{(t)})\\) is the gradient of the energy function</p> </li> <li> <p>\\(\\eta_t \\sim \\mathcal{N}(0, I)\\) is Gaussian noise</p> </li> </ul> <p>Step 3: Repeat Continue for many iterations until convergence</p> <p>Intuition</p> <p>The update rule can be understood as:</p> <ol> <li> <p>Gradient Ascent: \\(\\epsilon \\nabla_x f_\\theta(x^{(t)})\\) moves the sample toward higher energy regions</p> </li> <li> <p>Noise Injection: \\(\\sqrt{2\\epsilon} \\eta_t\\) adds randomness to prevent getting stuck in local optima</p> </li> <li> <p>Balance: The step size \\(\\epsilon\\) controls the trade-off between exploration and exploitation</p> </li> </ol> <p>High-Dimensional Expense</p> <p>In high dimensions, gradient computation becomes expensive, and the noise term \\(\\sqrt{2\\epsilon} \\eta_t\\) scales with dimension, making each step computationally costly. This computational burden is particularly problematic when training Energy-Based Models using Contrastive Divergence.</p> <p>The Training Bottleneck:</p> <p>Each training step in Contrastive Divergence requires sampling from the model distribution \\(p_\\theta(x)\\). This sampling process itself is computationally expensive:</p> <ol> <li>Single Sampling Step: Each Langevin step requires computing gradients and adding noise, both of which scale with dimension</li> <li>Multiple Sampling Steps: To get a good sample, we typically need hundreds or thousands of Langevin steps</li> <li>Per Training Step: Each gradient update of the model parameters requires multiple samples</li> </ol> <p>Computational Complexity:</p> <ul> <li>Gradient Computation: \\(O(d)\\) where \\(d\\) is the dimension</li> <li>Noise Generation: \\(O(d)\\) for generating \\(\\eta_t \\sim \\mathcal{N}(0, I)\\)</li> <li>Per Langevin Step: \\(O(d)\\) total cost</li> <li>Sampling Process: \\(O(k \\cdot d)\\) where \\(k\\) is the number of Langevin steps (typically 100-1000)</li> <li>Per Training Step: \\(O(n \\cdot k \\cdot d)\\) where \\(n\\) is the number of samples needed</li> </ul> <p>Practical Impact:</p> <p>This means that training an EBM using Contrastive Divergence with Langevin sampling can be extremely slow, especially for high-dimensional data like images. The sampling process becomes the computational bottleneck, making it difficult to scale EBMs to large datasets or high-dimensional problems.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/","title":"Evaluating Generative Models","text":"<p>In any research field, evaluation drives progress. How do we evaluate generative models? The evaluation of discriminative models (classification, regression, etc.) is well understood because:</p> <p>Clear ground truth: For discriminative tasks, we have access to labeled data that serves as ground truth. We can directly compare the model's predictions with the true labels.</p> <p>Simple Metrics: Evaluation metrics are straightforward and interpretable:</p> <ul> <li> <p>Classification: Accuracy, precision, recall, F1-score, ROC-AUC</p> </li> <li> <p>Regression: Mean squared error (MSE), mean absolute error (MAE), R\u00b2</p> </li> <li> <p>Ranking: NDCG, MAP, MRR</p> </li> </ul> <p>Domain-Agnostic: These metrics work across different domains (computer vision, NLP, etc.) with minimal adaptation.</p> <p>Example: For a binary classifier, we can compute accuracy as \\(\\frac{\\text{correct predictions}}{\\text{total predictions}}\\) and immediately understand how well the model performs.</p> <p>Evaluating generative models is highly non-trivial.  Key question: What is the task you care about? Density estimation- do you care about evaluating probabilities of images? Compression? Pure sampling/generation? Representation learning from unlabelled data? More than one task?</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#density-estimation-or-compression","title":"Density Estimation or Compression","text":"<p>Likelihood as a metric is pretty good for Density Estimation.</p> <ul> <li> <p>Split data into train, validation and test sets.</p> </li> <li> <p>Learn model \\(p_{\\theta}\\) using the train set.</p> </li> <li> <p>Tune hyperparameters on the validation set.</p> </li> <li> <p>Evaluate generalization with likelihood on test set: \\(\\mathbb{E}_{p_{data}}[\\log p_\\theta]\\)</p> </li> </ul> <p>Note: This is the same as compression because, by Shannon's source coding theorem, the optimal code length for encoding data from distribution \\(p_{data}\\) using model \\(p_\\theta\\) is \\(-\\log p_\\theta(x)\\). The average number of bits needed to encode data from \\(p_{data}\\) using model \\(p_\\theta\\) is:</p> \\[\\text{Average Code Length} = \\mathbb{E}_{p_{data}}[-\\log p_\\theta(x)] = -\\mathbb{E}_{p_{data}}[\\log p_\\theta(x)]\\] <p>Therefore, maximizing \\(\\mathbb{E}_{p_{data}}[\\log p_\\theta]\\) is equivalent to minimizing the expected code length, which is the goal of compression. The intuition is that we assign short codes to frequent data points.</p> <p>Perplexity: Another common metric for evaluating generative models is perplexity, defined as:</p> \\[\\text{Perplexity} = 2^{-\\frac{1}{D}\\mathbb{E}_{p_{data}}[\\log p_\\theta(x)]}\\] <p>where \\(D\\) is the dimension of the data. This normalizes the log-likelihood by the data dimension, making perplexity comparable across different dimensionalities.</p> <p>Perplexity measures how \"surprised\" the model is by the data. Lower perplexity indicates better performance. For language models, perplexity represents the average number of choices the model has at each step when predicting the next token.</p> <p>Not all generative models have tractable likelihoods. For models where exact likelihood computation is intractable, we need alternative evaluation approaches:</p> <p>VAEs: We can compare models using the Evidence Lower BOund (ELBO):</p> \\[\\text{ELBO} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p(z))\\] <p>While ELBO is a lower bound on the true likelihood, it provides a reasonable proxy for model comparison within the VAE framework.</p> <p>GANs: GANs pose a unique challenge because they don't provide explicit likelihood estimates.</p> <p>In general, unbiased estimation of probability density functions from samples is impossible.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#sample-quality","title":"Sample Quality","text":"<p>Human evaluations are the gold standard.</p> <p>HYPE_time: A metric that measures the minimum time it takes for a human to distinguish between real and generated samples. Higher HYPE_time indicates better sample quality, as it takes humans longer to detect that samples are fake.</p> <p>HYPE_infinity: The percentage of samples that deceive people under unlimited time. The larger the better.</p> <p>Key Insight: HYPE metrics provide a human-centric evaluation of generative models, measuring how convincingly the model can fool human evaluators. This is particularly relevant for applications where human perception is the ultimate judge of quality.</p> <p>Human evaluations are expensive, biased and hard to reproduce.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#inception-score","title":"Inception Score","text":"<p>The Inception Score measures the quality and diversity of generated samples using a pre-trained classifier (typically Inception-v3 for images). It is based on two key principles:</p> <ol> <li>Sharpness: Generated samples should be easily classifiable (high confidence predictions)</li> <li>Diversity: The model should generate samples from different classes</li> </ol>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#frechet-inception-distance-fid","title":"Fr\u00e9chet Inception Distance (FID)","text":"<p>The Fr\u00e9chet Inception Distance (FID) measures similarities in the feature representations for datapoints sampled from \\(p_{\\theta}\\) and the test dataset.</p> <p>How FID is Computed:</p> <p>Feature Extraction: Use a pre-trained Inception network (typically Inception-v3) to extract features from both real and generated samples. Let \\(f_r(x)\\) and \\(f_g(x)\\) be the feature extractors for real and generated samples respectively.</p> <p>Distribution Modeling: Model the feature distributions as multivariate Gaussians.</p> <ul> <li> <p>For real data: \\(\\mathcal{N}(\\mu_r, \\Sigma_r)\\) where:</p> <ul> <li> <p>\\(\\mu_r = \\mathbb{E}_{x \\sim p_{data}}[f_r(x)]\\) (mean of real features)</p> </li> <li> <p>\\(\\Sigma_r = \\mathbb{E}_{x \\sim p_{data}}[(f_r(x) - \\mu_r)(f_r(x) - \\mu_r)^T]\\) (covariance of real features)</p> </li> </ul> </li> <li> <p>For generated data: \\(\\mathcal{N}(\\mu_g, \\Sigma_g)\\) where:</p> <ul> <li> <p>\\(\\mu_g = \\mathbb{E}_{x \\sim p_\\theta}[f_g(x)]\\) (mean of generated features)</p> </li> <li> <p>\\(\\Sigma_g = \\mathbb{E}_{x \\sim p_\\theta}[(f_g(x) - \\mu_g)(f_g(x) - \\mu_g)^T]\\) (covariance of generated features)</p> </li> </ul> </li> </ul> <p>Fr\u00e9chet Distance Calculation: Compute the Fr\u00e9chet distance between the two Gaussian distributions:</p> \\[\\text{FID} = \\|\\mu_r - \\mu_g\\|^2 + \\text{tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})\\] <p>where:</p> <ul> <li> <p>\\(\\|\\mu_r - \\mu_g\\|^2\\) is the squared Euclidean distance between means</p> </li> <li> <p>\\(\\text{tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})\\) is the trace of the covariance difference term</p> </li> <li> <p>The matrix square root \\((\\Sigma_r \\Sigma_g)^{1/2}\\) is computed using eigendecomposition</p> </li> </ul> <p>Note: Check this resource on evaluating Text-To-Image Models: HEIM</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#evaluating-latent-representations-and-prompting","title":"Evaluating Latent Representations and Prompting","text":""},{"location":"ai/deep_generative_models/evaluating_generative_models/#clustering","title":"Clustering","text":"<p>Clustering is a powerful method for evaluating the quality and structure of latent representations learned by generative models. It provides insights into how well the model organizes and separates different concepts in its latent space. Clusters can be obtained by applying k-means or any other algorithm in the latent space of the generative model.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#lossy-compression-or-reconstruction","title":"Lossy Compression or Reconstruction","text":"<p>Latent representations can be evaluated based on the maximum compression they can achieve without significant loss in reconstruction accuracy. This involves measuring the trade-off between the dimensionality of the latent space and the quality of reconstructed samples. A good latent representation should maintain high reconstruction fidelity while using a compact, low-dimensional encoding that captures the essential features of the data. There are many quantitative evaluation metrics for this.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#disentanglement","title":"Disentanglement","text":"<p>Intuitively, we want representations that disentangle independent and interpretable attributes of the observed data. Disentanglement means that different dimensions of the latent space should correspond to distinct, meaningful factors of variation in the data. For example, in face generation, one latent dimension might control facial expression while another controls hair color, allowing for independent manipulation of these attributes. There are many quantitative evaluation metrics for this.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/","title":"Generative Adversarial Networks","text":""},{"location":"ai/deep_generative_models/generative_adversarial_networks/#introduction-gans-as-a-paradigm-shift","title":"Introduction: GANs as a Paradigm Shift","text":"<p>GANs are unique from all the other model families that we have seen so far, such as autoregressive models, VAEs, and normalizing flow models, because we do not train them using maximum likelihood.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#the-traditional-likelihood-based-paradigm","title":"The Traditional Likelihood-Based Paradigm","text":"<p>All the generative models we've explored so far follow a similar training paradigm:</p> <ol> <li>Autoregressive Models: Maximize \\(\\log p_\\theta(x) = \\sum_{i=1}^N \\log p_\\theta(x_i|x_{&lt;i})\\)</li> <li>Variational Autoencoders: Maximize the ELBO \\(\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z))\\)</li> <li>Normalizing Flow Models: Maximize \\(\\log p_\\theta(x) = \\log p_z(f^{-1}_\\theta(x)) + \\log |\\det(\\frac{\\partial f^{-1}_\\theta(x)}{\\partial x})|\\)</li> </ol> <p>Common Theme: All these models are trained by maximizing some form of likelihood or likelihood approximation.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#gans-a-different-approach","title":"GANs: A Different Approach","text":"<p>GANs break away from this paradigm entirely. Instead of maximizing likelihood, GANs use adversarial training - a fundamentally different approach to generative modeling. We'll get to what a Generator and a Discriminator are in a bit but here is a quick table showing how GAN is different.</p> <p>Key Differences:</p> Aspect Likelihood-Based Models GANs Training Objective Maximize likelihood/ELBO Minimax game between generator and discriminator Loss Function \\(\\mathcal{L} = -\\log p_\\theta(x)\\) \\(\\mathcal{L}_G = -\\log D(G(z))\\), \\(\\mathcal{L}_D = -\\log D(x) - \\log(1-D(G(z)))\\) Model Evaluation Direct likelihood computation No explicit likelihood computation Training Stability Generally stable Can be unstable, requires careful tuning Sample Quality May produce blurry samples Often produces sharp, realistic samples"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#likelihood-free-learning","title":"Likelihood-Free Learning","text":"<p>Why not use maximum likelihood? In fact, it is not so clear that better likelihood numbers necessarily correspond to higher sample quality. We know that the optimal generative model will give us the best sample quality and highest test log-likelihood. However, models with high test log-likelihoods can still yield poor samples, and vice versa.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#the-likelihood-vs-sample-quality-disconnect","title":"The Likelihood vs. Sample Quality Disconnect","text":"<p>To see why, consider pathological cases in which our model is comprised almost entirely of noise, or our model simply memorizes the training set:</p> <ol> <li> <p>Noise Model: A model that outputs pure noise might assign some probability to real data points, leading to a non-zero (though poor) likelihood, but produces completely useless samples.</p> </li> <li> <p>Memorization Model: A model that perfectly memorizes the training set will have very high likelihood on training data but will only reproduce exact training examples, lacking generalization and diversity.</p> </li> </ol> <p>Therefore, we turn to likelihood-free training with the hope that optimizing a different objective will allow us to disentangle our desiderata of obtaining high likelihoods as well as high-quality samples.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#the-two-sample-test-framework","title":"The Two-Sample Test Framework","text":"<p>Recall that maximum likelihood required us to evaluate the likelihood of the data under our model \\(p_\\theta\\). A natural way to set up a likelihood-free objective is to consider the two-sample test, a statistical test that determines whether or not a finite set of samples from two distributions are from the same distribution using only samples from \\(P\\) and \\(Q\\).</p> <p>Concretely, given \\(S_1 = \\{x \\sim P\\}\\) and \\(S_2 = \\{x \\sim Q\\}\\), we compute a test statistic \\(T\\) according to the difference in \\(S_1\\) and \\(S_2\\) that, when less than a threshold \\(\\alpha\\), accepts the null hypothesis that \\(P = Q\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#application-to-generative-modeling","title":"Application to Generative Modeling","text":"<p>Analogously, we have in our generative modeling setup access to our training set \\(S_1 = \\{x \\sim p_{data}\\}\\) and \\(S_2 = \\{x \\sim p_\\theta\\}\\). The key idea is to train the model to minimize a two-sample test objective between \\(S_1\\) and \\(S_2\\).</p> <p>However, this objective becomes extremely difficult to work with in high dimensions, so we choose to optimize a surrogate objective that instead maximizes some distance between \\(S_1\\) and \\(S_2\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#why-this-approach-makes-sense","title":"Why this approach makes sense","text":"<p>1. Avoiding pathological cases: - The two-sample test framework naturally avoids the noise and memorization problems - It forces the model to learn the true underlying distribution structure</p> <p>3. Flexibility: - We can choose different distance metrics or test statistics - This allows us to focus on different aspects of sample quality</p> <p>Key Insight: GANs implement likelihood-free learning by using a neural network (the discriminator) to learn an optimal test statistic for distinguishing between real and generated data, and then training the generator to minimize this learned distance.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#gan-objective","title":"GAN Objective","text":"<p>We thus arrive at the generative adversarial network formulation. There are two components in a GAN: (1) a generator and (2) a discriminator. The generator \\(G_\\theta\\) is a directed latent variable model that deterministically generates samples \\(x\\) from \\(z\\), and the discriminator \\(D_\\phi\\) is a function whose job is to distinguish samples from the real dataset and the generator.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#components","title":"Components","text":"<ul> <li>Generator \\(G_\\theta\\): A neural network that transforms noise \\(z \\sim p(z)\\) to samples \\(G_\\theta(z)\\)</li> <li>Discriminator \\(D_\\phi\\): A neural network that outputs a probability \\(D_\\phi(x) \\in [0,1]\\) indicating whether \\(x\\) is real or generated</li> </ul>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#the-minimax-game","title":"The Minimax Game","text":"<p>The generator and discriminator both play a two player minimax game, where: - Generator: Minimizes a two-sample test objective (\\(p_{data} = p_\\theta\\)) - Discriminator: Maximizes the objective (\\(p_{data} \\neq p_\\theta\\))</p> <p>Intuitively, the generator tries to fool the discriminator to the best of its ability by generating samples that look indistinguishable from \\(p_{data}\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#formal-objective","title":"Formal Objective","text":"<p>The GAN objective can be written as:</p> \\[\\min_\\theta \\max_\\phi V(G_\\theta, D_\\phi) = \\mathbb{E}_{x \\sim p_{data}}[\\log D_\\phi(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1-D_\\phi(G_\\theta(z)))]\\]"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#understanding-the-objective","title":"Understanding the Objective","text":"<p>Let's unpack this expression:</p> <p>For the Discriminator (maximizing with respect to \\(\\phi\\)): - Given a fixed generator \\(G_\\theta\\), the discriminator performs binary classification - It tries to assign probability 1 to data points from the training set \\(x \\sim p_{data}\\) - It tries to assign probability 0 to generated samples \\(x \\sim p_G\\)</p> <p>For the Generator (minimizing with respect to \\(\\theta\\)): - Given a fixed discriminator \\(D_\\phi\\), the generator tries to maximize \\(D_\\phi(G_\\theta(z))\\) - This is equivalent to minimizing \\(\\log(1-D_\\phi(G_\\theta(z)))\\)</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#optimal-discriminator","title":"Optimal Discriminator","text":"<p>In this setup, the optimal discriminator is:</p> \\[D^*_G(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_G(x)}\\] <p>Derivation: The discriminator's objective is to maximize:</p> \\[\\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{x \\sim p_G}[\\log(1-D(x))]\\] <p>This is maximized when:</p> \\[D(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_G(x)}\\] <p>On the other hand, the generator minimizes this objective for a fixed discriminator \\(D_\\phi\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#connection-to-jensen-shannon-divergence","title":"Connection to Jensen-Shannon Divergence","text":"<p>After performing some algebra, plugging in the optimal discriminator \\(D^*_G(\\cdot)\\) into the overall objective \\(V(G_\\theta, D^*_G(x))\\) gives us:</p> \\[2D_{JSD}[p_{data}, p_G] - \\log 4\\] <p>The \\(D_{JSD}\\) term is the Jensen-Shannon Divergence, which is also known as the symmetric form of the KL divergence:</p> \\[D_{JSD}[p,q] = \\frac{1}{2}\\left(D_{KL}\\left[p, \\frac{p+q}{2}\\right] + D_{KL}\\left[q, \\frac{p+q}{2}\\right]\\right)\\]"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#properties-of-jsd","title":"Properties of JSD","text":"<p>The JSD satisfies all properties of the KL divergence, and has the additional perk that \\(D_{JSD}[p,q] = D_{JSD}[q,p]\\) (symmetry).</p> <p>Key Properties:</p> <ol> <li> <p>Non-negative: \\(D_{JSD}[p,q] \\geq 0\\)</p> </li> <li> <p>Symmetric: \\(D_{JSD}[p,q] = D_{JSD}[q,p]\\)</p> </li> <li> <p>Zero iff equal: \\(D_{JSD}[p,q] = 0\\) if and only if \\(p = q\\)</p> </li> <li> <p>Bounded: \\(D_{JSD}[p,q] \\leq \\log 2\\)</p> </li> </ol>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#optimal-solution","title":"Optimal Solution","text":"<p>With this distance metric, the optimal generator for the GAN objective becomes \\(p_G = p_{data}\\), and the optimal objective value that we can achieve with optimal generators and discriminators \\(G^*(\\cdot)\\) and \\(D^*_{G^*}(x)\\) is \\(-\\log 4\\).</p> <p>Why \\(-\\log 4\\)? - When \\(p_G = p_{data}\\), we have \\(D_{JSD}[p_{data}, p_G] = 0\\) - Therefore, \\(V(G^*, D^*) = 2 \\cdot 0 - \\log 4 = -\\log 4\\)</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#gan-training-algorithm","title":"GAN Training Algorithm","text":"<p>Thus, the way in which we train a GAN is as follows:</p> <p>For epochs \\(1, \\ldots, N\\) do:</p> <ol> <li>Sample minibatch of size \\(m\\) from data: \\(x^{(1)}, \\ldots, x^{(m)} \\sim p_{data}\\)</li> <li>Sample minibatch of size \\(m\\) of noise: \\(z^{(1)}, \\ldots, z^{(m)} \\sim p_z\\)</li> <li>Take a gradient descent step on the generator parameters \\(\\theta\\):</li> </ol> \\[\\nabla_\\theta V(G_\\theta, D_\\phi) = \\frac{1}{m}\\nabla_\\theta \\sum_{i=1}^m \\log(1-D_\\phi(G_\\theta(z^{(i)})))\\] <ol> <li>Take a gradient ascent step on the discriminator parameters \\(\\phi\\):</li> </ol> \\[\\nabla_\\phi V(G_\\theta, D_\\phi) = \\frac{1}{m}\\nabla_\\phi \\sum_{i=1}^m [\\log D_\\phi(x^{(i)}) + \\log(1-D_\\phi(G_\\theta(z^{(i)})))]\\] <p>Key Points:</p> <ol> <li>Alternating Updates: We update the generator and discriminator in alternating steps</li> <li>Minibatch Training: We use minibatches of both real data and noise samples</li> <li>Generator Update: Minimizes the probability that the discriminator correctly identifies generated samples</li> <li>Discriminator Update: Maximizes the probability of correctly classifying real vs. generated samples</li> </ol> <p>Practical Considerations:</p> <ul> <li>Learning Rate Balance: The learning rates for generator and discriminator must be carefully balanced</li> <li>Update Frequency: Often the discriminator is updated multiple times per generator update</li> <li>Convergence Monitoring: Training progress is monitored through discriminator accuracy and sample quality</li> <li>Early Stopping: Training may be stopped when the discriminator can no longer distinguish real from fake</li> </ul> <p>This formulation shows that GANs are essentially implementing an adaptive two-sample test, where the discriminator learns the optimal way to distinguish between real and generated data, and the generator learns to minimize this learned distance.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#challenges","title":"Challenges","text":"<p>Although GANs have been successfully applied to several domains and tasks, working with them in practice is challenging because of their: (1) unstable optimization procedure, (2) potential for mode collapse, (3) difficulty in evaluation.</p> <p>1. Unstable Optimization Procedure</p> <p>During optimization, the generator and discriminator loss often continue to oscillate without converging to a clear stopping point. Due to the lack of a robust stopping criteria, it is difficult to know when exactly the GAN has finished training.</p> <p>Causes of Instability: - Minimax Nature: The adversarial game creates competing objectives - Gradient Issues: Vanishing/exploding gradients can occur - Learning Rate Sensitivity: Small changes in learning rates can cause divergence - Network Capacity Imbalance: If one network becomes too powerful, training collapses</p> <p>Symptoms: - Oscillating loss curves - No clear convergence pattern - Sudden collapse of training - Generator or discriminator loss going to zero/infinity</p> <p>2. Mode Collapse</p> <p>The generator of a GAN can often get stuck producing one of a few types of samples over and over again (mode collapse). This occurs when the generator finds a few \"safe\" modes that consistently fool the discriminator and stops exploring the full data distribution.</p> <p>What is Mode Collapse: - Definition: Generator only produces samples from a subset of the true distribution modes - Example: In image generation, only producing images of one type (e.g., only front-facing faces) - Problem: Lack of diversity in generated samples</p> <p>Causes: - Discriminator Overfitting: Discriminator becomes too good at detecting certain types of fake samples - Generator Optimization: Generator finds local optima that work well against current discriminator - Training Imbalance: One network becomes too powerful relative to the other</p> <p>3. Difficulty in Evaluation</p> <p>Unlike likelihood-based models, GANs don't provide explicit likelihood values, making evaluation challenging.</p> <p>Evaluation Challenges: - No Likelihood: Can't use traditional metrics like log-likelihood - Subjective Quality: Sample quality is often subjective and domain-specific - Diversity vs. Quality Trade-off: Hard to balance sample quality with diversity - Mode Coverage: Difficult to measure if all modes of the data distribution are captured</p> <p>Addressing the Challenges:</p> <p>Most fixes to these challenges are empirically driven, and there has been a significant amount of work put into developing new architectures, regularization schemes, and noise perturbations in an attempt to circumvent these issues.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#selected-gans","title":"Selected GANs","text":"<p>Next, we focus our attention to a few select types of GAN architectures and explore them in more detail.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#f-gan","title":"f-GAN","text":"<p>The f-GAN optimizes the variant of the two-sample test objective that we have discussed so far, but using a very general notion of distance: the f-divergence. Given two densities \\(p\\) and \\(q\\), the f-divergence can be written as:</p> \\[D_f(p,q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>where \\(f\\) is any convex, lower-semicontinuous function with \\(f(1) = 0\\). Several of the distance \"metrics\" that we have seen so far fall under the class of f-divergences, such as KL and Jensen-Shannon.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#understanding-the-requirements","title":"Understanding the Requirements","text":"<p>What is a convex function? A function \\(f\\) is convex if for any two points \\(x, y\\) and any \\(\\lambda \\in [0,1]\\), we have:</p> \\[f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\\] <p>This means that the line segment between any two points on the function lies above or on the function itself. </p> <p>Understanding the Line Segment Property:</p> <p>Let's break down what this means geometrically:</p> <p>Two Points: Consider any two points \\((x, f(x))\\) and \\((y, f(y))\\) on the graph of the function \\(f\\)</p> <p>Line Segment: The line segment connecting these points consists of all points of the form:</p> \\[(\\lambda x + (1-\\lambda)y, \\lambda f(x) + (1-\\lambda)f(y))\\] <p>where \\(\\lambda \\in [0,1]\\)</p> <p>Function Value: At the same \\(x\\)-coordinate \\(\\lambda x + (1-\\lambda)y\\), the function value is:</p> \\[f(\\lambda x + (1-\\lambda)y)\\] <p>Convexity Condition: The inequality \\(f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\\) means that the function value at any point on the line segment is less than or equal to the corresponding point on the straight line connecting \\((x, f(x))\\) and \\((y, f(y))\\)</p> <p>Visual Interpretation: If you draw a straight line between any two points on a convex function's graph. The entire function between those points must lie on or below that straight line.</p> <p>Convex functions have important properties:</p> <ul> <li> <p>Single minimum: If a minimum exists, it's global</p> </li> <li> <p>Well-behaved gradients: Useful for optimization</p> </li> <li> <p>Jensen's inequality: \\(\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X])\\) for convex \\(f\\)</p> </li> </ul> <p>What is a lower-semicontinuous function? A function \\(f\\) is lower-semicontinuous at a point \\(x_0\\) if:</p> \\[\\liminf_{x \\to x_0} f(x) \\geq f(x_0)\\] <p>Understanding \\(\\liminf\\) and the Infimum:</p> <p>The notation \\(\\liminf_{x \\to x_0} f(x)\\) involves two concepts:</p> <ol> <li> <p>Infimum (inf): The infimum of a set is the greatest lower bound. For a set \\(S\\), \\(\\inf S\\) is the largest number that is less than or equal to all elements in \\(S\\).</p> </li> <li> <p>Limit Inferior: \\(\\liminf_{x \\to x_0} f(x)\\) is the infimum of all limit points of \\(f(x)\\) as \\(x\\) approaches \\(x_0\\).</p> </li> </ol> <p>How \\(\\liminf\\) works:</p> <p>Consider all sequences \\(\\{x_n\\}\\) that converge to \\(x_0\\). For each sequence, we look at the limit of \\(f(x_n)\\) (if it exists). The \\(\\liminf\\) is the infimum of all these possible limit values.</p> <p>Mathematical Definition:</p> \\[\\liminf_{x \\to x_0} f(x) = \\inf \\left\\{ \\lim_{n \\to \\infty} f(x_n) : x_n \\to x_0 \\text{ and } \\lim_{n \\to \\infty} f(x_n) \\text{ exists} \\right\\}\\] <p>Why consider Sequences even for Continuous Functions?</p> <p>You might wonder: \"If \\(f\\) is continuous, why do we need sequences? Can't we just use the regular limit?\"</p> <p>Key Insight: Lower-semicontinuity is a weaker condition than continuity. A function can be lower-semicontinuous without being continuous.</p> <p>The Relationship:</p> <ol> <li>Continuous functions are always lower-semicontinuous</li> <li>Lower-semicontinuous functions may have discontinuities (but only \"jumps up\")</li> </ol> <p>Example of Lower-Semicontinuous but NOT Continuous: Consider \\(f(x) = \\begin{cases} 0 &amp; \\text{if } x &lt; 0 \\\\ 1 &amp; \\text{if } x \\geq 0 \\end{cases}\\). This function is lower-semicontinuous at \\(x = 0\\) (no \"jump down\"). But it's NOT continuous at \\(x = 0\\) (there's a \"jump up\")</p> <p>Example of NOT Lower-Semicontinuous: Consider \\(f(x) = \\begin{cases} 0 &amp; \\text{if } x &lt; 0 \\\\ 1 &amp; \\text{if } x = 0 \\\\ 0 &amp; \\text{if } x &gt; 0 \\end{cases}\\) at \\(x_0 = 0\\):</p> <p>\\(\\liminf_{x \\to 0^-} f(x) = 0\\) (approaching from left)</p> <p>\\(\\liminf_{x \\to 0^+} f(x) = 0\\) (approaching from right)</p> <p>\\(\\liminf_{x \\to 0} f(x) = 0\\) (overall limit inferior)</p> <p>Since \\(f(0) = 1\\) and \\(0 \\not\\geq 1\\), this function is NOT lower-semicontinuous at \\(x = 0\\)</p> <p>Why the Sequence definition is uiversal:</p> <p>The sequence-based definition works for ALL functions, whether they're: - Continuous everywhere - Lower-semicontinuous but not continuous - Neither continuous nor lower-semicontinuous</p> <p>For Continuous Functions: If \\(f\\) is continuous at \\(x_0\\), then:</p> \\[\\liminf_{x \\to x_0} f(x) = \\lim_{x \\to x_0} f(x) = f(x_0)\\] <p>So the sequence definition \"reduces\" to the regular limit, but it's still the same mathematical concept.</p> <p>Why this matters for f-Divergences: The f-divergence framework needs to work with functions that might not be continuous everywhere, so we need the more general sequence-based definition.</p> <p>Why do we need \\(f(1) = 0\\)? This requirement ensures that the f-divergence has the correct properties for a distance measure:</p> <p>Zero when distributions are equal: When \\(p = q\\), we have \\(\\frac{p(x)}{q(x)} = 1\\) everywhere, so:</p> \\[D_f(p,p) = \\mathbb{E}_{x \\sim p}[f(1)] = \\mathbb{E}_{x \\sim p}[0] = 0\\] <p>Distance-like behavior: This property ensures that the f-divergence behaves like a proper distance measure, being zero only when the distributions are identical</p> <p>Example: For KL divergence, \\(f(u) = u \\log u\\) satisfies \\(f(1) = 1 \\cdot \\log 1 = 0\\), making it a valid choice for an f-divergence.</p> <p>Important Clarification: KL Divergence and Convexity</p> <p>You might be wondering: \"But \\(\\log u\\) is concave, so how can KL divergence be an f-divergence?\" This is a great observation! The key is that the \\(f\\) function for KL divergence is \\(f(u) = u \\log u\\), not just \\(\\log u\\).</p> <p>The KL Divergence Formula: The KL divergence between distributions \\(p\\) and \\(q\\) is:</p> \\[D_{KL}(p||q) = \\mathbb{E}_{x \\sim p}\\left[\\log\\frac{p(x)}{q(x)}\\right] = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)}\\log\\frac{p(x)}{q(x)}\\right]\\] <p>Notice that the second form matches the f-divergence formula with \\(f(u) = u \\log u\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#f-divergence-examples","title":"f-Divergence Examples","text":"<p>Common f-divergences:</p> <ol> <li>KL Divergence: \\(f(u) = u \\log u\\)</li> <li>Reverse KL: \\(f(u) = -\\log u\\)</li> <li>Jensen-Shannon: \\(f(u) = u \\log u - (u+1) \\log \\frac{u+1}{2}\\)</li> <li>Total Variation: \\(f(u) = \\frac{1}{2}|u-1|\\)</li> <li>Pearson \u03c7\u00b2: \\(f(u) = (u-1)^2\\)</li> </ol>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#setting-up-the-f-gan-objective","title":"Setting up the f-GAN Objective","text":"<p>To set up the f-GAN objective, we borrow two commonly used tools from convex optimization: the Fenchel conjugate and duality. Specifically, we obtain a lower bound to any f-divergence via its Fenchel conjugate:</p> \\[D_f(p,q) \\geq \\sup_{T \\in \\mathcal{T}} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>Where \\(f^*\\) is the Fenchel conjugate of \\(f\\):</p> \\[f^*(t) = \\sup_{u \\in \\text{dom}(f)} (tu - f(u))\\] <p>What is the Fenchel Conjugate?</p> <p>The Fenchel conjugate \\(f^*\\) of a function \\(f\\) is defined as:</p> \\[f^*(t) = \\sup_{u \\in \\text{dom}(f)} (tu - f(u))\\] <p>Economic Intuition for the Fenchel Conjugate:</p> <p>One of the most intuitive ways to understand the convex conjugate function \\(f^*(t)\\) is through an economic lens. Imagine \\(f(u)\\) as the cost function representing the total expense incurred to produce a quantity \\(u\\) of a certain product. The variable \\(y\\) corresponds to the market price per unit of that product.</p> <p>In this context, the product \\(xy\\) represents the revenue generated by selling \\(x\\) units at price \\(t\\). The term \\(f(u)\\), as mentioned, is the cost of producing those units. Therefore, the expression \\(tu - f(u)\\) represents the profit earned by producing and selling \\(u\\) units at price \\(t\\).</p> <p>The convex conjugate \\(f^*(t)\\) is defined as the supremum (or maximum) of this profit over all possible production quantities \\(u\\):</p> \\[f^*(t) = \\sup_u (tu - f(u))\\] <p>Thus, \\(f^*(t)\\) gives the optimal profit achievable at the market price \\(t\\), assuming the producer chooses the best production quantity \\(u\\) to maximize profit.</p> <p>Geometric Interpretation in Economics:</p> <p>Now, consider the graph of the cost function \\(f(u)\\). Assume \\(f\\) is convex, continuous, and differentiable, which is a reasonable assumption for many cost functions in economics.</p> <p>The slope of the cost curve at any point \\(u\\) is given by the derivative \\(f'(u)\\). This derivative represents the marginal cost \u2014 the additional cost to produce one more unit at quantity \\(u\\).</p> <p>The condition for optimal production quantity \\(u\\) at price \\(t\\) arises from maximizing profit:</p> \\[\\max_u \\{tu - f(u)\\}\\] <p>Taking the derivative with respect to \\(u\\) and setting it to zero for an optimum:</p> \\[t - f'(u) = 0 \\implies t = f'(u)\\] <p>This means the optimal production quantity \\(u\\) is found where the price \\(t\\) equals the marginal cost \\(f'(u)\\).</p> <p>Geometrically, this corresponds to finding a tangent line to the graph of \\(f(u)\\) that has slope \\(t\\). Using a ruler, you can \"slide\" the line around until it just touches the cost curve without crossing it. The point of tangency corresponds to the optimal \\(u\\).</p> <p>Importantly, the vertical intercept of this tangent line relates directly to the optimal profit. The tangent line can be expressed as:</p> \\[\\ell(u) = f(u_0) + f'(u_0)(u - u_0)\\] <p>At \\(u = 0\\), the intercept is:</p> \\[\\ell(0) = f(u_0) - u_0 f'(u_0)\\] <p>Notice that:</p> \\[-(u_0 t - f(u_0)) = f(u_0) - u_0 t\\] <p>Since \\(t = f'(u_0)\\), the intercept equals the negative of the optimal profit. Therefore, the intercept of the tangent line with slope \\(t\\) gives \\(-f^*(t)\\).</p> <p></p> <p>In the above diagram, for a given \\(y\\), we are trying to maximize the difference between the line \\(xy\\) and \\(f(x)\\). For that given \\(y\\), it turns out that the maximum value that \\(xy - f(x)\\) occurs when we draw a tangent to \\(f(x)\\) with slope \\(y\\). The point at which the tangent occurs is the optimum \\(x\\). It also turns out that the vertical intercept is \\(-f^*(y)\\) = maximum value that \\(xy - f(x)\\) occurs for the given \\(y\\). For a different \\(y\\), there will be a different \\(x\\) where a line parallel to the line \\(xy\\) becomes tangent. The graph of \\(f^*(y)\\) is essentially how the negative of the vertical intercept varies over the domain \\(y\\). The graphs of \\(f(x)\\) and \\(f^*(y)\\) live in different dual spaces. \\(f(x)\\) is a function of \"quantities\", while \\(f^*(y)\\) is a function of \"prices\" or \"slopes\". </p> <p>Key Properties: 1. Convexity: If \\(f\\) is convex, then \\(f^*\\) is also convex 2. Duality: \\((f^*)^* = f\\) (the conjugate of the conjugate is the original function) 3. Domain: The domain of \\(f^*\\) depends on the behavior of \\(f\\)</p> <p>Examples of Fenchel Conjugates:</p> <p>Example 1: KL Divergence</p> <p>For \\(f(u) = u \\log u\\):</p> \\[f^*(t) = \\sup_{u &gt; 0} (tu - u \\log u)\\] <p>To find this, we set the derivative to zero:</p> \\[\\frac{d}{du}(tu - u \\log u) = t - \\log u - 1 = 0\\] \\[\\log u = t - 1\\] \\[u = e^{t-1}\\] <p>Substituting back:</p> \\[f^*(t) = te^{t-1} - e^{t-1}(t-1) = e^{t-1}\\] <p>Example 2: Reverse KL</p> <p>For \\(f(u) = -\\log u\\):</p> \\[f^*(t) = \\sup_{u &gt; 0} (tu + \\log u)\\] <p>Setting derivative to zero:</p> \\[\\frac{d}{du}(tu + \\log u) = t + \\frac{1}{u} = 0\\] \\[u = -\\frac{1}{t}\\] <p>Substituting back:</p> \\[f^*(t) = t(-\\frac{1}{t}) + \\log(-\\frac{1}{t}) = -1 + \\log(-\\frac{1}{t}) = -1 - \\log(-t)\\] <p>Example 3: Total Variation</p> <p>For \\(f(u) = \\frac{1}{2}|u-1|\\):</p> \\[f^*(t) = \\sup_{u} (tu - \\frac{1}{2}|u-1|)\\] <p>This gives:</p> \\[f^*(t) = \\begin{cases} t &amp; \\text{if } |t| \\leq \\frac{1}{2} \\\\ +\\infty &amp; \\text{otherwise} \\end{cases}\\] <p>The Duality Principle:</p> <p>The Fenchel conjugate provides a way to transform optimization problems. The key insight is that:</p> <p>Primal Problem: \\(\\inf_{u} f(u)\\)</p> <p>Dual Problem: \\(\\sup_{t} -f^*(t)\\)</p> <p>The Primal-Dual Relationship:</p> <p>The f-divergence can be expressed in both forms:</p> \\[D_f(p,q) = \\mathbb{E}_{x \\sim q}\\left[f\\left(\\frac{p(x)}{q(x)}\\right)\\right] = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)]- \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>Where:</p> <ul> <li> <p>Primal form: \\(\\mathbb{E}_{x \\sim q}\\left[f\\left(\\frac{p(x)}{q(x)}\\right)\\right]\\) (direct computation)</p> </li> <li> <p>Dual form: \\(\\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\) (optimization problem)</p> </li> </ul> <p>Derivation: From Primal to Dual Form</p> <p>Let's walk through the step-by-step derivation of how we transform the primal form into the dual form:</p> <p>Step 1: Start with the Primal Form</p> <p>The f-divergence is defined as:</p> \\[D_f(p,q) = \\mathbb{E}_{x \\sim q}\\left[f\\left(\\frac{p(x)}{q(x)}\\right)\\right]\\] <p>This is the \"primal form\" - it directly computes the divergence by evaluating the function \\(f\\) at the ratio \\(\\frac{p(x)}{q(x)}\\).</p> <p>Step 2: Apply the Fenchel Conjugate Identity</p> <p>The key insight comes from the Fenchel conjugate identity. For any convex function \\(f\\) and any point \\(u\\), we have:</p> \\[f(u) = \\sup_{t} (tu - f^*(t))\\] <p>This is a fundamental result in convex analysis known as the Fenchel-Moreau theorem. It states that a convex function can be recovered from its conjugate.</p> <p>Step 3: Substitute the Identity</p> <p>We substitute this identity into our primal form:</p> \\[D_f(p,q) = \\mathbb{E}_{x \\sim q}\\left[\\sup_{t} \\left(t \\cdot \\frac{p(x)}{q(x)} - f^*(t)\\right)\\right]\\] <p>Step 4: Exchange Supremum and Expectation</p> <p>This is the crucial step. We can exchange the supremum and expectation under certain conditions (satisfied for convex \\(f\\)):</p> \\[D_f(p,q) = \\sup_{T} \\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)} - f^*(T(x))\\right]\\] <p>Here, we've replaced the variable \\(t\\) with a function \\(T(x)\\) that can depend on \\(x\\).</p> <p>Step 5: Simplify the Expression</p> <p>We can rewrite the expectation:</p> \\[\\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)} - f^*(T(x))\\right] = \\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)}\\right] - \\mathbb{E}_{x \\sim q}[f^*(T(x))]\\] <p>The first term can be simplified using the definition of expectation:</p> \\[\\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)}\\right] = \\int T(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x) dx = \\int T(x) \\cdot p(x) dx = \\mathbb{E}_{x \\sim p}[T(x)]\\] <p>Step 6: Arrive at the Dual Form</p> <p>Putting it all together:</p> \\[D_f(p,q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x \\sim q}[f^*(T(x))]\\right)\\] <p>In other words (to distinguish the two different inputs to the Discriminator):</p> \\[D_f(p,q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>This is the dual form of the f-divergence.</p> <p>Why This Derivation Works:</p> <ol> <li>Convexity: The convexity of \\(f\\) ensures that the Fenchel conjugate identity holds</li> <li>Exchange of Supremum and Expectation: This is valid because we're optimizing over a convex set of functions</li> <li>Duality Gap: Under certain conditions, there is no duality gap, meaning the primal and dual forms give the same value</li> </ol> <p>Key Insights from the Derivation:</p> <ol> <li> <p>From Direct Computation to Optimization: The primal form requires direct computation of \\(f(\\frac{p(x)}{q(x)})\\), while the dual form transforms this into an optimization problem over functions \\(T\\).</p> </li> <li> <p>Role of the Fenchel Conjugate: The conjugate \\(f^*\\) appears naturally in the dual form.</p> </li> <li> <p>Connection to GANs: The dual form is perfect for GANs because we can parameterize \\(T\\) as a neural network (the discriminator). The optimization becomes a minimax game. We can use gradient-based optimization.</p> </li> </ol> <p>Example: KL Divergence Derivation</p> <p>Let's see this in action for KL divergence where \\(f(u) = u \\log u\\):</p> <p>Primal Form:</p> \\[D_{KL}(p||q) = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)} \\log \\frac{p(x)}{q(x)}\\right]\\] <p>Step 1: Use the Fenchel conjugate identity for \\(f(u) = u \\log u\\)</p> <p>We know that \\(f^*(t) = e^{t-1}\\) (from our earlier examples)</p> <p>Step 2: Substitute:</p> \\[D_{KL}(p||q) = \\mathbb{E}_{x \\sim q}\\left[\\sup_{t} \\left(t \\cdot \\frac{p(x)}{q(x)} - e^{t-1}\\right)\\right]\\] <p>Step 3: Exchange supremum and expectation:</p> \\[D_{KL}(p||q) = \\sup_{T} \\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)} - e^{T(x)-1}\\right]\\] <p>Step 4: Simplify:</p> \\[D_{KL}(p||q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x \\sim q}[e^{T(x)-1}]\\right)\\] <p>This gives us the dual form for KL divergence, which can be used in f-GAN training.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#steps-in-f-gan","title":"Steps in f-GAN","text":"<p>Step 1: Choose an f-divergence Select a convex function \\(f\\) with \\(f(1) = 0\\) (e.g., KL divergence, Jensen-Shannon, etc.)</p> <p>Step 2: Compute the Fenchel conjugate Find \\(f^*\\) analytically or numerically</p> <p>Step 3: Parameterize the dual variable Replace \\(T\\) with a neural network \\(T_\\phi\\) parameterized by \\(\\phi\\)</p> <p>Step 4: Set up the minimax game</p> \\[\\min_\\theta \\max_\\phi F(\\theta,\\phi) = \\mathbb{E}_{x \\sim p_{data}}[T_\\phi(x)] - \\mathbb{E}_{x_{fake} \\sim p_{G_\\theta}}[f^*(T_\\phi(x_{fake}))]\\] <p>Understanding the Roles:</p> <ul> <li>Generator (\\(G_\\theta\\)): Tries to minimize the divergence estimate</li> <li>Discriminator (\\(T_\\phi\\)): Tries to tighten the lower bound by maximizing the dual objective</li> </ul> <p>Key Insight: The discriminator \\(T_\\phi\\) is not a binary classifier like in standard GANs, but rather a function.</p> <p>Important Distinction: Vanilla GAN vs f-GAN Generator</p> <p>There is a fundamental difference between how generators work in vanilla GANs versus f-GANs:</p> <p>Vanilla GAN Generator:</p> <ul> <li> <p>Explicit generator network: \\(G_\\theta(z)\\) where \\(z \\sim p(z)\\) (noise)</p> </li> <li> <p>Direct transformation: Noise \\(z\\) \u2192 Generated sample \\(G_\\theta(z)\\)</p> </li> <li> <p>Objective: \\(\\min_\\theta \\max_\\phi V(G_\\theta, D_\\phi) = \\mathbb{E}_{x \\sim p_{data}}[\\log D_\\phi(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1-D_\\phi(G_\\theta(z)))]\\)</p> </li> </ul> <p>f-GAN Generator:</p> <ul> <li> <p>No explicit generator network: We work with \\(p_{G_\\theta}\\) as a distribution directly</p> </li> <li> <p>Implicit generator: The \"generator\" is whatever mechanism produces samples from \\(p_{G_\\theta}\\)</p> </li> <li> <p>Objective: \\(\\min_\\theta \\max_\\phi F(\\theta,\\phi) = \\mathbb{E}_{x \\sim p_{data}}[T_\\phi(x)] - \\mathbb{E}_{x_{fake} \\sim p_{G_\\theta}}[f^*(T_\\phi(x_{fake}))]\\)</p> </li> </ul> <p>The Key Insight:</p> <p>In f-GAN, the \"generator\" is implicit - it's whatever mechanism produces samples from the distribution \\(p_{G_\\theta}\\). This could be:</p> <ol> <li>A neural network \\(G_\\theta\\) that transforms noise (like in vanilla GANs)</li> <li>A flow-based model that transforms a base distribution</li> <li>A VAE decoder that generates from a latent space</li> <li>Any other generative model that produces samples from \\(p_{G_\\theta}\\)</li> </ol> <p>Why This Matters:</p> <p>The f-GAN framework is more general than vanilla GANs because:</p> <ul> <li>Vanilla GANs: Require a specific generator architecture \\(G_\\theta(z)\\)</li> <li>f-GANs: Can work with any generative model that produces samples from \\(p_{G_\\theta}\\)</li> </ul> <p>Practical Implementation:</p> <p>In practice, when implementing f-GAN, you would typically:</p> <ol> <li>Choose a generative model (e.g., a neural network \\(G_\\theta\\))</li> <li>Use it to generate samples from \\(p_{G_\\theta}\\)</li> <li>Apply the f-GAN objective to train both the generator and discriminator</li> </ol> <p>So in the f-GAN formulation, there's no explicit \\(G_\\theta\\) network like in vanilla GANs. The \"generator\" is the abstract distribution \\(p_{G_\\theta}\\), and the actual implementation depends on what generative model you choose to use.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#advantages-of-f-gan","title":"Advantages of f-GAN","text":"<ol> <li>Unified Framework: One formulation covers many different GAN variants</li> <li>Theoretical Rigor: Based on well-established convex optimization theory</li> <li>Flexibility: Can adapt the divergence measure to the specific problem</li> <li>Stability: Some f-divergences may lead to more stable training</li> </ol>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>Choice of f: Different f-divergences have different properties</li> <li>Fenchel Conjugate: Must be computable for the chosen f-divergence</li> <li>Training: Similar alternating optimization as standard GANs</li> <li>Evaluation: Same challenges as other GAN variants</li> </ul>"},{"location":"ai/deep_generative_models/introduction/","title":"Introduction","text":"<p>Natural agents excel at discovering patterns, extracting knowledge, and performing complex reasoning based on the data they observe. How can we build artificial learning systems to do the same?</p> <p>Generative models view the world under the lens of probability. In such a worldview, we can think of any kind of observed data, say , as a finite set of samples from an underlying distribution, say  pdata. At its very core, the goal of any generative model is then to approximate this data distribution given access to the dataset . The hope is that if we are able to  learn  a good generative model, we can use the learned model for downstream  inference.</p>"},{"location":"ai/deep_generative_models/introduction/#learning","title":"Learning","text":"<p>We will be primarily interested in parametric approximations (parametric models assume a specific data distribution (like a normal distribution) and estimate parameters (like the mean and standard deviation) of that distribution, while non-parametric models make no assumptions about the underlying distribution) to the data distribution, which summarize all the information about the dataset  in a finite set of parameters. In contrast with non-parametric models, parametric models scale more efficiently with large datasets but are limited in the family of distributions they can represent.</p> <p>In the parametric setting, we can think of the task of learning a generative model as picking the parameters within a family of model distributions that minimizes some notion of distance between the model distribution and the data distribution.</p> <p> </p> <p>For instance, we might be given access to a dataset of dog images  and our goal is to learn the parameters of a generative model \u03b8 within a model family M such that the model distribution p\u03b8 is close to the data distribution over dogs  pdata. Mathematically, we can specify our goal as the following optimization problem:</p> <p>min d(pdata,p\u03b8) where \u03b8\u2208M</p> <p>where pdata is accessed via the dataset  and  d(\u22c5) is a notion of distance between probability distributions.</p> <p>It is interesting to take note of the difficulty of the problem at hand. A typical image from a modern phone camera has a resolution of approximately  700\u00d71400700\u00d71400 pixels. Each pixel has three channels: R(ed), G(reen) and B(lue) and each channel can take a value between 0 to 255. Hence, the number of possible images is given by 256700\u00d71400\u00d73\u224810800000256700\u00d71400\u00d73\u224810800000. In contrast, Imagenet, one of the largest publicly available datasets, consists of only about 15 million images. Hence, learning a generative model with such a limited dataset is a highly underdetermined problem.</p> <p>Fortunately, the real world is highly structured and automatically discovering the underlying structure is key to learning generative models. For example, we can hope to learn some basic artifacts about dogs even with just a few images: two eyes, two ears, fur etc. Instead of incorporating this prior knowledge explicitly, we will hope the model learns the underlying structure directly from data.  We will be primarily interested in the following questions:</p> <ul> <li>What is the representation for the model family M?</li> <li>What is the objective function  d(\u22c5)?</li> <li>What is the optimization procedure for minimizing  d(\u22c5)?</li> </ul>"},{"location":"ai/deep_generative_models/introduction/#generative-vs-discriminative-models","title":"Generative vs Discriminative models","text":""},{"location":"ai/deep_generative_models/introduction/#inference","title":"Inference","text":"<p>While the range of applications to which generative models have been used continue to grow, we can identify three fundamental inference queries for evaluating a generative model.:</p> <ol> <li> <p>Density estimation:  Given a datapoint  x, what is the probability assigned by the model, i.e.,  p\u03b8(x)?</p> </li> <li> <p>Sampling:  How can we  generate  novel data from the model distribution, i.e.,  xnew\u223cp\u03b8(x)?</p> </li> <li> <p>Unsupervised representation learning:  How can we learn meaningful feature representations for a datapoint  x?</p> </li> </ol> <p>Going back to our example of learning a generative model over dog images, we can intuitively expect a good generative model to work as follows. For density estimation, we expect  p\u03b8(x) to be high for dog images and low otherwise. Alluding to the name  generative model, sampling involves generating novel images of dogs beyond the ones we observe in our dataset. Finally, representation learning can help discover high-level structure in the data such as the breed of dogs.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/","title":"Normalizing Flow Models","text":"<p>So far we have learned two types of likelihood based generative models:</p> <p>Autoregressive Models: \\(p_\\theta(x) = \\prod_{i=1}^N p_\\theta(x_i|x_{&lt;i})\\)</p> <p>Variational autoencoders: \\(p_\\theta(x) = \\int p_\\theta(x,z)dz\\)</p> <p>The two methods have relative strengths and weaknesses. Autoregressive models provide tractable likelihoods but no direct mechanism for learning features, whereas variational autoencoders can learn feature representations but have intractable marginal likelihoods.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#change-of-variables-formula","title":"Change of Variables Formula","text":"<p>In normalizing flows, we wish to map simple distributions (easy to sample and evaluate densities) to complex ones (learned via data). The change of variables formula describes how to evaluate densities of a random variable that is a deterministic transformation from another variable.</p> <p>Let's start with the univariate case and then generalize to multivariate random variables.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#univariate-case","title":"Univariate Case","text":"<p>Consider two random variables \\(Z\\) and \\(X\\) related by a strictly monotonic function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) such that \\(X = f(Z)\\). We want to find the probability density function of \\(X\\) in terms of the density of \\(Z\\).</p> <p>The key insight comes from the fact that probabilities must be preserved under the transformation. For any interval \\([a, b]\\) in the \\(X\\) space:</p> \\[P(a \\leq X \\leq b) = P(f^{-1}(a) \\leq Z \\leq f^{-1}(b))\\] <p>This can be written as:</p> \\[\\int_a^b p_X(x) dx = \\int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz\\] <p>To perform the substitution \\(z = f^{-1}(x)\\), we need to express \\(dz\\) in terms of \\(dx\\). Since \\(z = f^{-1}(x)\\), we can use the chain rule to find:</p> \\[\\frac{dz}{dx} = \\frac{d}{dx}f^{-1}(x) = \\frac{1}{f'(f^{-1}(x))}\\] <p>This follows from the inverse function theorem: if \\(y = f(x)\\), then \\(\\frac{dx}{dy} = \\frac{1}{f'(x)}\\).</p> <p>Therefore, \\(dz = \\frac{1}{f'(f^{-1}(x))} dx\\). However, we need to take the absolute value because probability densities must be non-negative. If \\(f'(f^{-1}(x)) &lt; 0\\) (meaning \\(f\\) is decreasing), then \\(\\frac{1}{f'(f^{-1}(x))} &lt; 0\\), which would make the density negative. Therefore, we use:</p> \\[dz = \\frac{1}{|f'(f^{-1}(x))|} dx\\] <p>Substituting this into our integral:</p> \\[\\int_a^b p_X(x) dx = \\int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz = \\int_a^b p_Z(f^{-1}(x)) \\cdot \\frac{1}{|f'(f^{-1}(x))|} dx\\] <p>Since this equality must hold for all intervals \\([a, b]\\), the integrands must be equal:</p> \\[p_X(x) = p_Z(f^{-1}(x)) \\cdot \\frac{1}{|f'(f^{-1}(x))|}\\] <p>This is the univariate change of variables formula. The factor \\(\\frac{1}{|f'(f^{-1}(x))|}\\) accounts for how the transformation stretches or compresses the probability mass.</p> <p>Why should \\(f\\) be monotonic? The monotonicity requirement ensures that \\(f\\) is invertible (one-to-one), which is crucial for the change of variables formula to work correctly. If \\(f\\) were not monotonic, there could be multiple values of \\(z\\) that map to the same value of \\(x\\), making the inverse function \\(f^{-1}\\) ill-defined. This would violate the fundamental assumption that we can uniquely determine the original variable \\(z\\) from the transformed variable \\(x\\).</p> <p>For example, if \\(f(z) = z^2\\) (which is not monotonic on \\(\\mathbb{R}\\)), then both \\(z = 2\\) and \\(z = -2\\) map to \\(x = 4\\). This creates ambiguity in the inverse mapping and would require special handling to account for multiple pre-images.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#multivariate-case","title":"Multivariate Case","text":"<p>For the multivariate case, we have random variables \\(\\mathbf{Z}\\) and \\(\\mathbf{X}\\) related by a bijective function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(\\mathbf{X} = f(\\mathbf{Z})\\).</p> <p>The key insight is that the probability mass in any region must be preserved under the transformation. For any region \\(A\\) in the \\(\\mathbf{X}\\) space:</p> \\[P(\\mathbf{X} \\in A) = P(\\mathbf{Z} \\in f^{-1}(A))\\] <p>This can be written as:</p> \\[\\int_A p_X(\\mathbf{x}) d\\mathbf{x} = \\int_{f^{-1}(A)} p_Z(\\mathbf{z}) d\\mathbf{z}\\] <p>To perform the multivariate substitution \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\), we need to understand how the volume element \\(d\\mathbf{z}\\) transforms. The Jacobian matrix \\(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\) is an \\(n \\times n\\) matrix where:</p> \\[\\left[\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right]_{ij} = \\frac{\\partial f^{-1}_i(\\mathbf{x})}{\\partial x_j}\\] <p>This matrix describes how small changes in \\(\\mathbf{x}\\) correspond to changes in \\(\\mathbf{z}\\). In multivariate calculus, when we perform a change of variables \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\), the volume element transforms as:</p> \\[d\\mathbf{z} = \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right| d\\mathbf{x}\\] <p>This is the multivariate generalization of the univariate substitution \\(dz = \\frac{1}{|f'(f^{-1}(x))|} dx\\). The determinant of the Jacobian matrix measures how the transformation affects the volume of a small region: - If \\(|\\det(J)| &gt; 1\\), the transformation expands volume - If \\(|\\det(J)| &lt; 1\\), the transformation contracts volume - If \\(|\\det(J)| = 1\\), the transformation preserves volume</p> <p>Substituting this into our integral:</p> \\[\\int_A p_X(\\mathbf{x}) d\\mathbf{x} = \\int_{f^{-1}(A)} p_Z(\\mathbf{z}) d\\mathbf{z} = \\int_A p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right| d\\mathbf{x}\\] <p>Since this equality must hold for all regions \\(A\\), the integrands must be equal:</p> \\[p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>This is the multivariate change of variables formula. The determinant of the Jacobian matrix accounts for how the transformation affects the volume of probability mass.</p> <p>Alternative Form Using Forward Mapping: Using the property that \\(\\det(A^{-1}) = \\det(A)^{-1}\\) for any invertible matrix \\(A\\), we can rewrite this as:</p> \\[p_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right|^{-1}\\] <p>This form is often more convenient in practice because it uses the forward mapping \\(f\\) rather than the inverse mapping \\(f^{-1}\\).</p> <p>Final result: Let \\(Z\\) and \\(X\\) be random variables which are related by a mapping \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(X = f(Z)\\) and \\(Z = f^{-1}(X)\\). Then</p> \\[p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>There are several things to note here:</p> <ul> <li>\\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) need to be continuous and have the same dimension.</li> <li>\\(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\) is a matrix of dimension \\(n \\times n\\), where each entry at location \\((i,j)\\) is defined as \\(\\frac{\\partial f^{-1}(\\mathbf{x})_i}{\\partial x_j}\\). This matrix is also known as the Jacobian matrix.</li> <li>\\(\\det(A)\\) denotes the determinant of a square matrix \\(A\\).</li> </ul> <p>For any invertible matrix \\(A\\), \\(\\det(A^{-1}) = \\det(A)^{-1}\\), so for \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\) we have</p> \\[p_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right|^{-1}\\] <p>If \\(\\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right| = 1\\), then the mapping is volume preserving, which means that the transformed distribution \\(p_X\\) will have the same \"volume\" compared to the original one \\(p_Z\\).</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#normalizing-flow-models-deep-dive","title":"Normalizing Flow Models deep dive","text":"<p>Let us consider a directed, latent-variable model over observed variables \\(X\\) and latent variables \\(Z\\). In a normalizing flow model, the mapping between \\(Z\\) and \\(X\\), given by \\(f_\\theta: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\), is deterministic and invertible such that \\(X = f_\\theta(Z)\\) and \\(Z = f^{-1}_\\theta(X)\\).</p> <p>Using change of variables, the marginal likelihood \\(p(x)\\) is given by</p> \\[p_X(\\mathbf{x}; \\theta) = p_Z(f^{-1}_\\theta(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}_\\theta(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>The name \"normalizing flow\" can be interpreted as the following:</p> <ul> <li> <p>\"Normalizing\" means that the change of variables gives a normalized density after applying an invertible transformation. When we transform a random variable through an invertible function, the resulting density automatically integrates to 1 (is normalized) because the change of variables formula preserves the total probability mass. This is different from other methods where we might need to explicitly normalize or approximate the density.</p> </li> <li> <p>\"Flow\" means that the invertible transformations can be composed with each other to create more complex invertible transformations. If we have two invertible functions \\(f_1\\) and \\(f_2\\), then their composition \\(f_2 \\circ f_1\\) is also invertible. This allows us to build complex transformations by chaining simpler ones, creating a \"flow\" of transformations.</p> </li> </ul> <p>Different from autoregressive models and variational autoencoders, deep normalizing flow models require specific architectural structures:</p> <ol> <li> <p>The input and output dimensions must be the same - This is necessary for the transformation to be invertible. If the dimensions don't match, we can't uniquely map back and forth between the spaces.</p> </li> <li> <p>The transformation must be invertible - This is fundamental to the change of variables formula and allows us to compute both the forward transformation (for sampling) and the inverse transformation (for density evaluation).</p> </li> <li> <p>Computing the determinant of the Jacobian needs to be efficient (and differentiable) - The change of variables formula requires computing the determinant of the Jacobian matrix. For high-dimensional spaces, this can be computationally expensive, so we need architectures that make this computation tractable.</p> </li> </ol> <p>Next, we introduce several popular forms of flow models that satisfy these properties.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#planar-flow","title":"Planar Flow","text":"<p>The Planar Flow introduces the following invertible transformation:</p> \\[\\mathbf{x} = f_\\theta(\\mathbf{z}) = \\mathbf{z} + \\mathbf{u}h(\\mathbf{w}^\\top\\mathbf{z} + b)\\] <p>where \\(\\mathbf{u}, \\mathbf{w}, b\\) are parameters.</p> <p>The absolute value of the determinant of the Jacobian is given by:</p> \\[\\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right| = |1 + h'(\\mathbf{w}^\\top\\mathbf{z} + b)\\mathbf{u}^\\top\\mathbf{w}|\\] <p>However, \\(\\mathbf{u}, \\mathbf{w}, b, h(\\cdot)\\) need to be restricted in order to be invertible. For example, \\(h = \\tanh\\) and \\(h'(\\mathbf{w}^\\top\\mathbf{z} + b)\\mathbf{u}^\\top\\mathbf{w} \\geq -1\\). Note that while \\(f_\\theta(\\mathbf{z})\\) is invertible, computing \\(f^{-1}_\\theta(\\mathbf{z})\\) could be difficult analytically. The following models address this problem, where both \\(f_\\theta\\) and \\(f^{-1}_\\theta\\) have simple analytical forms.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#nice-and-realnvp","title":"NICE and RealNVP","text":"<p>The Nonlinear Independent Components Estimation (NICE) model and Real Non-Volume Preserving (RealNVP) model compose two kinds of invertible transformations: additive coupling layers and rescaling layers. The coupling layer in NICE partitions a variable \\(\\mathbf{z}\\) into two disjoint subsets, say \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\). Then it applies the following transformation:</p> <p>Forward mapping \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\):</p> <ul> <li>\\(\\mathbf{x}_1 = \\mathbf{z}_1\\), which is an identity mapping.</li> <li>\\(\\mathbf{x}_2 = \\mathbf{z}_2 + m_\\theta(\\mathbf{z}_1)\\), where \\(m_\\theta\\) is a neural network.</li> </ul> <p>Inverse mapping \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\):</p> <ul> <li>\\(\\mathbf{z}_1 = \\mathbf{x}_1\\), which is an identity mapping.</li> <li>\\(\\mathbf{z}_2 = \\mathbf{x}_2 - m_\\theta(\\mathbf{x}_1)\\), which is the inverse of the forward transformation.</li> </ul> <p>Therefore, the Jacobian of the forward mapping is lower triangular, whose determinant is simply the product of the elements on the diagonal, which is 1. Therefore, this defines a volume preserving transformation. RealNVP adds scaling factors to the transformation:</p> \\[\\mathbf{x}_2 = \\exp(s_\\theta(\\mathbf{z}_1)) \\odot \\mathbf{z}_2 + m_\\theta(\\mathbf{z}_1)\\] <p>where \\(\\odot\\) denotes elementwise product. This results in a non-volume preserving transformation.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#autoregressive-flow-models","title":"Autoregressive Flow Models","text":"<p>Some autoregressive models can also be interpreted as flow models. For a Gaussian autoregressive model, one receives some Gaussian noise for each dimension of \\(\\mathbf{x}\\), which can be treated as the latent variables \\(\\mathbf{z}\\). Such transformations are also invertible, meaning that given \\(\\mathbf{x}\\) and the model parameters, we can obtain \\(\\mathbf{z}\\) exactly.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#masked-autoregressive-flow-maf","title":"Masked Autoregressive Flow (MAF)","text":"<p>Masked Autoregressive Flow (MAF) uses this interpretation, where the forward mapping is an autoregressive model. However, sampling is sequential and slow, in \\(O(n)\\) time where \\(n\\) is the dimension of the samples.</p> <p>MAF Architecture and Mathematical Formulation:</p> <p>The MAF is comprised of Masked Autoencoder for Distribution Estimation (MADE) blocks, which has a special masking scheme at each layer such that the autoregressive property is preserved. In particular, we consider a Gaussian autoregressive model:</p> \\[p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i | \\mathbf{x}_{&lt;i})\\] <p>such that the conditional Gaussians \\(p(x_i | \\mathbf{x}_{&lt;i}) = \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\) are parameterized by neural networks \\(\\mu_i = f_{\\mu_i}(\\mathbf{x}_{&lt;i})\\) and \\(\\alpha_i = f_{\\alpha_i}(\\mathbf{x}_{&lt;i})\\). Note that \\(\\alpha_i\\) denotes the log standard deviation of the Gaussian \\(p(x_i | \\mathbf{x}_{&lt;i})\\).</p> <p>As seen in the change of variables formula, a normalizing flow uses a series of deterministic and invertible mappings \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(\\mathbf{x} = f(\\mathbf{z})\\) and \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\) to transform a simple prior distribution \\(p_z\\) (e.g. isotropic Gaussian) into a more expressive one. In particular, a normalizing flow which composes \\(k\\) invertible transformations \\(\\{f_j\\}_{j=1}^k\\) such that \\(\\mathbf{x} = f_k \\circ f_{k-1} \\circ \\cdots \\circ f_1(\\mathbf{z}_0)\\) takes advantage of the change-of-variables property:</p> \\[\\log p(\\mathbf{x}) = \\log p_z(f^{-1}(\\mathbf{x})) + \\sum_{j=1}^k \\log \\left|\\det\\left(\\frac{\\partial f_j^{-1}(\\mathbf{x}_j)}{\\partial \\mathbf{x}_j}\\right)\\right|\\] <p>In MAF, the forward mapping is: \\(x_i = \\mu_i + z_i \\cdot \\exp(\\alpha_i)\\), and the inverse mapping is: \\(z_i = (x_i - \\mu_i)/\\exp(\\alpha_i)\\). The log of the absolute value of the determinant of the Jacobian is:</p> \\[\\log \\left|\\det\\left(\\frac{\\partial f^{-1}}{\\partial \\mathbf{x}}\\right)\\right| = -\\sum_{i=1}^n \\alpha_i\\] <p>where \\(\\mu_i\\) and \\(\\alpha_i\\) are as defined above.</p> <p>Connection between \\(p(\\mathbf{x})\\) and \\(\\log p(\\mathbf{x})\\) formulations:</p> <p>The two formulations are equivalent but serve different purposes:</p> <ol> <li>\\(p(\\mathbf{x})\\) formulation (autoregressive view):</li> </ol> \\[p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i | \\mathbf{x}_{&lt;i}) = \\prod_{i=1}^n \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\] <ol> <li>\\(\\log p(\\mathbf{x})\\) formulation (flow view):</li> </ol> \\[\\log p(\\mathbf{x}) = \\log p_z(f^{-1}(\\mathbf{x})) + \\sum_{j=1}^k \\log \\left|\\det\\left(\\frac{\\partial f_j^{-1}(\\mathbf{x}_j)}{\\partial \\mathbf{x}_j}\\right)\\right|\\] <p>How they relate:</p> <p>Taking the logarithm of the autoregressive formulation:</p> \\[\\log p(\\mathbf{x}) = \\sum_{i=1}^n \\log p(x_i | \\mathbf{x}_{&lt;i}) = \\sum_{i=1}^n \\log \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\] <p>For a Gaussian distribution \\(\\mathcal{N}(x | \\mu, \\sigma^2)\\), we have:</p> \\[\\log \\mathcal{N}(x | \\mu, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\] <p>Substituting \\(\\sigma = \\exp(\\alpha_i)\\) and using the inverse mapping \\(z_i = (x_i - \\mu_i)/\\exp(\\alpha_i)\\):</p> \\[\\log p(\\mathbf{x}) = \\sum_{i=1}^n \\left[-\\frac{1}{2}\\log(2\\pi) - \\alpha_i - \\frac{z_i^2}{2}\\right] = \\sum_{i=1}^n \\log \\mathcal{N}(z_i | 0, 1) - \\sum_{i=1}^n \\alpha_i\\] <p>This shows that the autoregressive formulation (using conditional Gaussians) is equivalent to the flow formulation (using change of variables with a standard normal prior and the Jacobian determinant term \\(-\\sum_{i=1}^n \\alpha_i\\)).</p> <p>Key insight: The \\(\\alpha_i\\) terms serve dual purposes - they parameterize the conditional standard deviations in the autoregressive view, and they contribute to the Jacobian determinant in the flow view.</p> <p>What are \\(\\mu_1\\) and \\(\\alpha_1\\) in MAF?</p> <p>In MAF, for the first dimension (\\(i=1\\)):</p> <ul> <li> <p>\\(\\mu_1\\): This is the mean of the first conditional distribution \\(p(x_1)\\). Since \\(x_1\\) has no previous dimensions to condition on (\\(\\mathbf{x}_{&lt;1}\\) is empty), \\(\\mu_1\\) is typically a learned constant parameter or computed from a bias term in the neural network.</p> </li> <li> <p>\\(\\alpha_1\\): This is the log standard deviation of the first conditional distribution \\(p(x_1)\\). The actual standard deviation is \\(\\exp(\\alpha_1)\\), and \\(\\alpha_1\\) is also typically a learned constant parameter.</p> </li> </ul> <p>This makes sense because the first dimension has no autoregressive dependencies - it's the starting point of the autoregressive chain.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#made-blocks","title":"MADE Blocks","text":"<p>MADE (Masked Autoencoder for Distribution Estimation) is a key architectural component that enables efficient autoregressive modeling. MADE uses a special masking scheme to ensure that the autoregressive property is preserved while allowing for efficient parallel computation of all conditional parameters.</p> <p>How MADE Works:</p> <ol> <li> <p>Masking Scheme: Each layer in the neural network has a mask that ensures each output unit only depends on a subset of input units, maintaining the autoregressive ordering.</p> </li> <li> <p>Autoregressive Property: For dimension \\(i\\), the network can only access inputs \\(x_j\\) where \\(j &lt; i\\), ensuring that \\(p(x_i | \\mathbf{x}_{&lt;i})\\) only depends on previous dimensions.</p> </li> <li> <p>Parallel Parameter Computation: Despite the autoregressive constraints, MADE can compute all \\(\\mu_i\\) and \\(\\alpha_i\\) parameters in parallel during training, making it much more efficient than sequential autoregressive models.</p> </li> </ol> <p>Mathematical Implementation:</p> <p>The masking is implemented by multiplying the weight matrices with binary masks:</p> \\[W_{masked} = W \\odot M\\] <p>where \\(M\\) is a binary mask matrix that enforces the autoregressive dependencies. The mask ensures that: - Output \\(i\\) can only depend on inputs \\(j &lt; i\\) - This creates a lower triangular dependency structure</p> <p>Connection to MAF: MAF uses MADE blocks as its core building blocks, allowing it to efficiently compute all the conditional parameters \\(\\mu_i\\) and \\(\\alpha_i\\) while maintaining the autoregressive structure required for the flow transformation.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#detailed-maf-implementation-analysis","title":"Detailed MAF Implementation Analysis","text":"<p>Let's analyze a complete MAF implementation that demonstrates the concepts discussed above:</p> <p>Core Components:</p> <ol> <li>MaskedLinear: Implements the masking mechanism for autoregressive dependencies</li> <li>PermuteLayer: Reorders dimensions between flow layers</li> <li>MADE: Single MADE block with forward and inverse transformations</li> <li>MAF: Complete model with multiple MADE blocks</li> </ol> <p>1. MaskedLinear Layer:</p> <pre><code>class MaskedLinear(nn.Linear):\n    def __init__(self, input_size, output_size, mask):\n        super().__init__(input_size, output_size)\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, x):\n        return F.linear(x, self.mask * self.weight, self.bias)\n</code></pre> <p>Key Features: - Masking: The mask is a binary matrix that enforces autoregressive dependencies - Element-wise Multiplication: <code>self.mask * self.weight</code> zeros out forbidden connections - Autoregressive Property: Ensures output \\(i\\) only depends on inputs \\(j &lt; i\\)</p> <p>2. PermuteLayer:</p> <pre><code>class PermuteLayer(nn.Module):\n    def __init__(self, num_inputs):\n        super().__init__()\n        self.perm = np.array(np.arange(0, num_inputs)[::-1])\n\n    def forward(self, inputs):\n        return inputs[:, self.perm], torch.zeros(inputs.size(0), 1, device=inputs.device)\n\n    def inverse(self, inputs):\n        return inputs[:, self.perm], torch.zeros(inputs.size(0), 1, device=inputs.device)\n</code></pre> <p>Purpose: - Dimension Reordering: Reverses the order of dimensions between flow layers - Expressiveness: Allows different autoregressive orderings across layers - Jacobian: Since it's just a permutation, the Jacobian determinant is 1 (log_det = 0)</p> <p>3. MADE Block Implementation:</p> <p>Forward Method (z \u2192 x): <pre><code>def forward(self, z):\n    x = torch.zeros_like(z)\n    log_det = None\n    for i in range(self.input_size):\n        out = self.net(x)  # MADE network with masking\n        mean, alpha = out.chunk(2, dim=1)  # Split into mean and log_std\n        x[:, i] = mean[:, i] + z[:, i] * torch.exp(alpha[:, i])  # Transform\n        if log_det is None:\n            log_det = alpha[:, i].unsqueeze(1)\n        else:\n            log_det = torch.cat((log_det, alpha[:, i].unsqueeze(1)), dim=1)\n    log_det = -torch.sum(log_det, dim=1)  # Negative sum for change of variables\n    return x, log_det\n</code></pre></p> <p>Key Implementation Details: - Sequential Processing: Each dimension is processed one by one - Autoregressive Access: The MADE network can only access previously computed \\(x\\) values - Transformation: \\(x_i = \\mu_i + z_i \\cdot \\exp(\\alpha_i)\\) - Log Determinant: Accumulates \\(\\alpha_i\\) values and takes negative sum</p> <p>Inverse Method (x \u2192 z): <pre><code>def inverse(self, x):\n    out = self.net(x)  # MADE network with masking\n    mean, alpha = out.chunk(2, dim=1)  # Split into mean and log_std\n    z = (x - mean) * torch.exp(-alpha)  # Inverse transform\n    log_det = -torch.sum(alpha, dim=1)  # Negative sum for change of variables\n    return z, log_det\n</code></pre></p> <p>Key Implementation Details: - Parallel Processing: All dimensions can be processed simultaneously - Autoregressive Masking: The masking ensures proper dependencies - Inverse Transformation: \\(z_i = (x_i - \\mu_i) / \\exp(\\alpha_i)\\) - Log Determinant: Same formula as forward, but computed in parallel</p> <p>4. Complete MAF Model:</p> <p>Architecture: <pre><code>def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n    nf_blocks = []\n    for i in range(self.n_flows):\n        nf_blocks.append(MADE(self.input_size, hidden_size, n_hidden))\n        nf_blocks.append(PermuteLayer(self.input_size))\n    self.nf = nn.Sequential(*nf_blocks)\n</code></pre></p> <p>Structure: <pre><code>Input \u2192 MADE\u2081 \u2192 Permute\u2081 \u2192 MADE\u2082 \u2192 Permute\u2082 \u2192 ... \u2192 MADE\u2096 \u2192 Permute\u2096 \u2192 Output\n</code></pre></p> <p>Log Probability Computation: <pre><code>def log_probs(self, x):\n    log_det_list = []\n    for flow in self.nf:\n        x, log_det = flow.inverse(x)  # Transform x \u2192 z\n        log_det_list.append(log_det)\n\n    sum_log_det = torch.stack(log_det_list, dim=1).sum(dim=1)\n    z = x  # Final z after all transformations\n    p_z = self.base_dist.log_prob(z).sum(-1)  # Prior log probability\n    log_prob = (p_z + sum_log_det).mean()  # Change of variables formula\n    return log_prob\n</code></pre></p> <p>Sampling Process: <pre><code>def sample(self, device, n):\n    x_sample = torch.randn(n, self.input_size).to(device)  # Sample from prior\n    for flow in self.nf[::-1]:  # Reverse order for sampling\n        x_sample, log_det = flow.forward(x_sample)  # Transform z \u2192 x\n    return x_sample.cpu().data.numpy()\n</code></pre></p> <p>Understanding the Flow Methods:</p> <ol> <li> <p>During Training (likelihood computation): <pre><code># We have x, want to compute log p(x)\nfor flow in self.nf:  # Forward order\n    x, log_det = flow.inverse(x)  # x \u2192 z (inverse of this flow)\n</code></pre></p> </li> <li> <p>During Sampling: <pre><code># We have z, want to get x\nfor flow in self.nf[::-1]:  # Reverse order\n    x_sample, log_det = flow.forward(x_sample)  # z \u2192 x (forward of this flow)\n</code></pre></p> </li> </ol> <p>In other words:</p> <ul> <li>Each flow's <code>forward()</code> method: Transforms \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\) for that specific flow</li> <li>Each flow's <code>inverse()</code> method: Transforms \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\) for that specific flow</li> <li>During training: We use <code>inverse()</code> to go from data space to latent space</li> <li>During sampling: We use <code>forward()</code> to go from latent space to data space</li> </ul> <p>Mathematical Perspective: Let \\(f_i\\) denote the forward transformation of the \\(i\\)-th flow (from \\(\\mathbf{z}\\) to \\(\\mathbf{x}\\)), and \\(f_i^{-1}\\) denote its inverse transformation (from \\(\\mathbf{x}\\) to \\(\\mathbf{z}\\)).</p> <ul> <li>Training: \\(f_k^{-1} \\circ f_{k-1}^{-1} \\circ \\cdots \\circ f_1^{-1}(\\mathbf{x}) = \\mathbf{z}\\) (using <code>inverse()</code> methods)</li> <li>Sampling: \\(f_1 \\circ f_2 \\circ \\cdots \\circ f_k(\\mathbf{z}) = \\mathbf{x}\\) (using <code>forward()</code> methods)</li> </ul> <p>What is \\(k\\)?</p> <p>The parameter \\(k\\) represents the total number of flow layers in the MAF model. In the implementation, this corresponds to <code>n_flows</code> in the MAF constructor.</p> <p>In the MAF Architecture: <pre><code>def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n    # n_flows = k (total number of flow layers)\n    for i in range(self.n_flows):  # i goes from 0 to k-1\n        nf_blocks.append(MADE(self.input_size, hidden_size, n_hidden))\n        nf_blocks.append(PermuteLayer(self.input_size))\n</code></pre></p> <p>Flow Composition Structure: <pre><code>Input \u2192 MADE\u2081 \u2192 Permute\u2081 \u2192 MADE\u2082 \u2192 Permute\u2082 \u2192 ... \u2192 MADE\u2096 \u2192 Permute\u2096 \u2192 Output\n</code></pre></p> <p>Where: - \\(f_1\\): First MADE block (MADE\u2081) - \\(f_2\\): Second MADE block (MADE\u2082) - ... - \\(f_k\\): Last MADE block (MADE\u2096)</p> <p>Example with \\(k = 3\\): - Training: \\(f_3^{-1} \\circ f_2^{-1} \\circ f_1^{-1}(\\mathbf{x}) = \\mathbf{z}\\) - Sampling: \\(f_1 \\circ f_2 \\circ f_3(\\mathbf{z}) = \\mathbf{x}\\)</p> <p>Key Insight: The <code>forward()</code> method of each flow is designed to be the inverse transformation for the overall model's training direction. This is why we use <code>forward()</code> during sampling in reverse order.</p> <p>This implementation demonstrates how the theoretical concepts of MAF translate into practical code, showing the interplay between autoregressive structure, masking, and flow transformations.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#inverse-autoregressive-flow-iaf","title":"Inverse Autoregressive Flow (IAF)","text":"<p>To address the sampling problem (sequential) in MAF, the Inverse Autoregressive Flow (IAF) simply inverts the generating process. In this case, the sampling (generation), is still parallelized. However, computing the likelihood of new data points is slow.</p> <p>Forward mapping from \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\) (parallel):</p> <ol> <li> <p>Sample \\(z_i \\sim \\mathcal{N}(0,1)\\) for \\(i = 1, \\ldots, n\\)</p> </li> <li> <p>Compute all \\(\\mu_i, \\alpha_i\\) (can be done in parallel)</p> </li> <li> <p>Let \\(x_1 = \\exp(\\alpha_1)z_1 + \\mu_1\\)</p> </li> <li> <p>Let \\(x_2 = \\exp(\\alpha_2)z_2 + \\mu_2\\)</p> </li> <li> <p>\\(\\ldots\\)</p> </li> </ol> <p>Inverse mapping from \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\) (sequential):</p> <ol> <li> <p>Let \\(z_1 = (x_1 - \\mu_1)/\\exp(\\alpha_1)\\)</p> </li> <li> <p>Compute \\(\\mu_2(z_1), \\alpha_2(z_1)\\)</p> </li> <li> <p>Let \\(z_2 = (x_2 - \\mu_2)/\\exp(\\alpha_2)\\)</p> </li> <li> <p>Compute \\(\\mu_3(z_1,z_2), \\alpha_3(z_1,z_2)\\)</p> </li> <li> <p>\\(\\ldots\\)</p> </li> </ol> <p>Key insight: Fast to sample from, slow to evaluate likelihoods of data points (train).</p> <p>Efficient Likelihood for Generated Points: However, for generated points the likelihood can be computed efficiently (since the noise are already obtained). When we generate samples using IAF, we start with known noise values \\(\\mathbf{z}\\) and transform them to get \\(\\mathbf{x}\\). Since we already have the noise values, we don't need to perform the expensive sequential inverse mapping to recover them. We can directly compute the likelihood using the change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) - \\sum_{i=1}^n \\alpha_i\\] <p>where we already know all the \\(\\alpha_i\\) values from the forward pass. This is much faster than the \\(O(n)\\) sequential computation required for arbitrary data points.</p> <p>Derivation of the Change of Variables Formula for IAF:</p> <p>Let's derive how we get this formula. Starting with the general change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) + \\log \\left|\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>For IAF, the forward transformation is:</p> \\[x_i = \\exp(\\alpha_i)z_i + \\mu_i\\] <p>The inverse transformation is:</p> \\[z_i = \\frac{x_i - \\mu_i}{\\exp(\\alpha_i)}\\] <p>The Jacobian matrix \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\) is diagonal because each \\(z_i\\) only depends on \\(x_i\\):</p> \\[\\frac{\\partial z_i}{\\partial x_j} = \\begin{cases}  \\frac{1}{\\exp(\\alpha_i)} &amp; \\text{if } i = j \\\\ 0 &amp; \\text{if } i \\neq j \\end{cases}\\] <p>Therefore, the determinant is the product of the diagonal elements:</p> \\[\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right) = \\prod_{i=1}^n \\frac{1}{\\exp(\\alpha_i)} = \\exp\\left(-\\sum_{i=1}^n \\alpha_i\\right)\\] <p>Taking the absolute value and logarithm:</p> \\[\\log \\left|\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)\\right| = \\log \\exp\\left(-\\sum_{i=1}^n \\alpha_i\\right) = -\\sum_{i=1}^n \\alpha_i\\] <p>Substituting back into the change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) - \\sum_{i=1}^n \\alpha_i\\] <p>This derivation shows why the likelihood computation is efficient for generated samples - we already have all the \\(\\alpha_i\\) values from the forward pass, so we just need to sum them up.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/","title":"Recap at this point","text":""},{"location":"ai/deep_generative_models/recap_at_this_point/#the-generative-modeling-framework","title":"The Generative Modeling Framework","text":"<p>We have some data from an unknown probability distribution, that we denote \\(p_{data}\\). We have a model family \\(\\mathcal{M}\\) which is a set of probability distributions. We denote some kind of notion of similarity between \\(p_{data}\\) and the distributions in \\(\\mathcal{M}\\). We try to find the probability distribution \\(p_\\theta\\) that is the closest to \\(p_{data}\\) in this notion of similarity.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#model-families-weve-explored","title":"Model Families We've Explored","text":"<p>We have seen different ways of constructing probability distributions in \\(\\mathcal{M}\\):</p> <ol> <li>Autoregressive Models: \\(p_\\theta(x) = \\prod_{i=1}^N p_\\theta(x_i|x_{&lt;i})\\)</li> <li>Variational Autoencoders (VAEs): \\(p_\\theta(x) = \\int p_\\theta(x,z)dz\\)</li> <li>Normalizing Flow Models: \\(p_\\theta(x) = p_z(f^{-1}_\\theta(x)) \\left|\\det\\left(\\frac{\\partial f^{-1}_\\theta(x)}{\\partial x}\\right)\\right|\\)</li> </ol>"},{"location":"ai/deep_generative_models/recap_at_this_point/#the-likelihood-based-training-paradigm","title":"The Likelihood-Based Training Paradigm","text":"<p>The key thing is we always try to assign some probability assigned by the model to a data point. All the above families are trained by minimizing KL divergence \\(D_{KL}(p_{data} || p_\\theta)\\), or equivalently maximizing likelihoods (or approximations). In these techniques, the machinery involves setting up models such that we can evaluate likelihoods (or approximations) pretty efficiently.</p> <p>Mathematical Foundation:</p> \\[\\arg\\min_\\theta D_{KL}(p_{data} || p_\\theta) = \\arg\\max_\\theta \\mathbb{E}_{x \\sim p_{data}}[\\log p_\\theta(x)]\\] <p>This equivalence shows that minimizing KL divergence is equivalent to maximizing the expected log-likelihood of the data.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#alternative-similarity-measures","title":"Alternative Similarity Measures","text":"<p>However, the training objective of maximizing likelihoods is not the only way to measure similarity between \\(p_{data}\\) and \\(p_\\theta\\). There are other ways to measure the notion of similarity:</p> <ol> <li>Wasserstein Distance: Measures the minimum \"cost\" of transporting mass from one distribution to another</li> <li>Maximum Mean Discrepancy (MMD): Compares distributions using kernel methods</li> <li>Adversarial Training: Uses a discriminator to distinguish between real and generated samples</li> <li>Energy-Based Models: Learn an energy function that assigns low energy to real data and high energy to fake data</li> </ol>"},{"location":"ai/deep_generative_models/recap_at_this_point/#the-likelihood-vs-sample-quality-dilemma","title":"The Likelihood vs. Sample Quality Dilemma","text":"<p>The Problem: It is possible that models with high likelihood could be bad at sample generation and vice versa. This creates a fundamental tension in generative modeling.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#why-this-disconnect-occurs","title":"Why This Disconnect Occurs","text":"<p>1. Likelihood Measures Average Performance:</p> \\[\\mathbb{E}_{x \\sim p_{data}}[\\log p_\\theta(x)] = \\int p_{data}(x) \\log p_\\theta(x) dx\\] <p>This measures how well the model assigns probability to the average data point, not necessarily how well it captures the fine-grained structure needed for high-quality generation.</p> <p>2. Sample Quality Requires Fine-Grained Structure: High-quality generation requires the model to capture: - Sharp boundaries between different modes - Fine details and textures - Proper spatial relationships - Realistic variations within modes</p> <p>3. Different Optimization Objectives: - Likelihood: Optimizes for probability assignment over the entire distribution - Sample Quality: Requires optimization of perceptual and structural properties</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#implications-for-model-design","title":"Implications for Model Design","text":"<p>The Training Objective Trade-off: Although training objective of maximizing likelihoods could be a good one, it might not be the best one if the objective is to generate the best samples.</p> <p>This suggests that it might be useful to disentangle likelihoods and sample quality.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#alternative-training-paradigms","title":"Alternative Training Paradigms","text":"<p>1. Adversarial Training (GANs): - Objective: Direct optimization of sample quality through adversarial training - Advantage: Can generate very high-quality samples - Disadvantage: No explicit likelihood computation, training instability</p> <p>2. Hybrid Approaches: - VAE-GAN: Combines likelihood-based training with adversarial training - Flow-GAN: Combines normalizing flows with adversarial training - Objective: Balance between likelihood and sample quality</p> <p>3. Perceptual Losses: - Objective: Use pre-trained networks to measure perceptual similarity - Advantage: Better alignment with human perception - Example: LPIPS (Learned Perceptual Image Patch Similarity)</p> <p>4. Multi-Objective Training: - Objective: Combine multiple loss functions - Example: \\(\\mathcal{L} = \\mathcal{L}_{likelihood} + \\lambda \\mathcal{L}_{perceptual} + \\mu \\mathcal{L}_{adversarial}\\)</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#conclusion","title":"Conclusion","text":"<p>The tension between likelihood and sample quality is a fundamental challenge in generative modeling. While likelihood-based training provides a principled framework, it may not always lead to the best sample quality. This motivates the exploration of alternative training paradigms and evaluation metrics that better align with the ultimate goal of generating high-quality, diverse samples.</p> <p>The key insight is that generative modeling is not just about fitting a distribution to data, but about creating models that can generate samples that are both high-quality and diverse. This requires careful consideration of both the training objective and the evaluation metrics used to assess model performance.</p>"},{"location":"ai/deep_generative_models/score_based_diffusion_models/","title":"Score Based Diffusion Models","text":""},{"location":"ai/deep_generative_models/score_based_diffusion_models/#quick-recap-score-based-models","title":"Quick Recap: Score Based Models","text":"<p>From our exploration of score-based generative modeling, we learned several key concepts:</p> <p>Score Function: The gradient of the log probability density, \\(\\nabla_x \\log p(x)\\), which points \"uphill\" in the probability landscape toward high-density regions.</p> <p>Score Matching: A training objective that learns the score function by minimizing the Fisher divergence between the learned and true score functions.</p> <p>Score Matching Objective: The original score matching objective is:</p> \\[\\mathcal{L}(\\theta) = \\mathbb{E}_{x \\sim p_{data}(x)} \\left[ \\frac{1}{2} \\| s_\\theta(x) \\|_2^2 + \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] <p>where \\(\\text{tr}(\\nabla_x s_\\theta(x))\\) is the trace of the Jacobian of the score function, which is computationally expensive to evaluate.</p> <p>Denoising Score Matching (DSM): A practical variant that trains the score function to predict the direction from noisy to clean data, avoiding the need to compute the true score function.</p> <p>DSM Objective: The denoising score matching objective is:</p> \\[\\mathcal{L}(\\theta) = \\mathbb{E}_{y \\sim p_{data}(y)} \\mathbb{E}_{x \\sim \\mathcal{N}(x; y, \\sigma^2 I)} \\left[ \\frac{1}{2} \\left\\| s_\\theta(x) - \\frac{y - x}{\\sigma^2} \\right\\|_2^2 \\right]\\] <p>where \\(s_\\theta(x)\\) learns to predict the score function of the noise-perturbed distribution, and \\(\\frac{y - x}{\\sigma^2}\\) is the target score function that points from noisy sample \\(x\\) toward clean data \\(y\\).</p> <p>Langevin Dynamics: A continuous-time stochastic process that uses the score function to guide sampling:</p> \\[dx_t = \\nabla_x \\log p(x_t) dt + \\sqrt{2} dW_t\\] <p>Discretized Form: For practical implementation:</p> \\[x_{t+1} = x_t + \\frac{\\epsilon}{2} \\cdot s_\\theta(x_t) + \\sqrt{2\\epsilon} \\cdot \\eta_t\\] <p>Mode Collapse: Standard Langevin dynamics struggles with multi-modal distributions and low-density regions.</p> <p>Annealed Langevin Dynamics: Addresses this by using multiple noise scales \\(\\sigma_1 &lt; \\sigma_2 &lt; \\ldots &lt; \\sigma_L\\), creating a sequence of increasingly noisy distributions that are easier to sample from.</p> <p>Stochastic Differential Equations (SDEs): General framework for continuous-time stochastic processes:</p> \\[dx = f(x, t)dt + g(t)dw\\] <p>Reverse SDE: Any SDE has a corresponding reverse process for sampling:</p> \\[dx = [f(x, t) - g^2(t)\\nabla_x \\log p_t(x)]dt + g(t)d\\bar{w}\\] <p>Time-Dependent Score Models: Neural networks that learn \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\) for continuous-time processes.</p> <p>Key insights:</p> <ol> <li>Score functions act as denoisers: They point from noisy to clean data</li> <li>Multiple noise scales help: Annealing from high to low noise improves sampling</li> <li>Continuous-time generalizes discrete: SDEs provide a unified framework</li> <li>Reverse processes enable generation: The reverse SDE naturally incorporates the score function for sampling</li> </ol>"},{"location":"ai/deep_generative_models/score_based_diffusion_models/#diffusion-models-as-score-based-models-hierarchical-vaes","title":"Diffusion Models as Score Based Models &amp; Hierarchical VAEs","text":"<p>Iterative Denoising perspective: In annealed Langevin dynamics with multiple noise scales, the sampling process can be viewed as iterative denoising. Starting from high noise levels and gradually reducing noise, each step uses the score function to denoise the sample, progressively refining it from a noisy state toward the clean data distribution.</p> <p>Training perspective: The inverse process involves iteratively adding Gaussian noise to clean data during training. By corrupting data with increasing levels of noise, the model learns to predict the score function at each noise level, enabling it to reverse the corruption process during sampling.</p> <p></p> <p>VAE Perspective: This entire framework can be viewed as a VAE where:</p> <ul> <li> <p>Encoder process: The forward process that converts clean data to noise through iterative corruption</p> </li> <li> <p>Decoder process: The reverse process that generates samples by iteratively denoising from noise</p> </li> </ul> <p>Noise Perturbation process: Each \\(x_t\\) represents a noise-perturbed density that is obtained by adding Gaussian noise to \\(x_{t-1}\\). This creates a Markov chain where each step adds a small amount of noise to the previous state.</p> <p>We can write the forward process as a conditional distribution:</p> \\[q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)\\] <p>where \\(\\beta_t\\) is the noise schedule that determines how much noise is added at each step.</p> <p>The joint distribution of the entire forward process is:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\] <p>This factorization follows from the chain rule of probability and the Markov property of the forward process:</p> <p>Chain Rule: For any joint distribution, we can write:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = q(x_1 | x_0) \\cdot q(x_2 | x_0, x_1) \\cdot q(x_3 | x_0, x_1, x_2) \\cdots q(x_T | x_0, x_1, \\ldots, x_{T-1})\\] <p>Markov Property: In the forward process, each \\(x_t\\) depends only on \\(x_{t-1}\\), not on earlier states:</p> \\[q(x_t | x_0, x_1, \\ldots, x_{t-1}) = q(x_t | x_{t-1})\\] <p>Substituting the Markov property into the chain rule:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = q(x_1 | x_0) \\cdot q(x_2 | x_1) \\cdot q(x_3 | x_2) \\cdots q(x_T | x_{T-1})\\] <p>This can be written compactly as:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\] <p>This represents the probability of the entire noise corruption sequence, where each step depends only on the previous step (Markov property).</p> <p>Comparison with VAEs: In a typical VAE, you would take \\(x_0\\) and map it via a neural network to obtain some mean and standard deviation to parameterize the distribution of the latent variable. Here, we obtain the distribution of the latent variables through the predefined noise corruption procedure we defined above, rather than learning it with a neural network.</p> <p>Multistep transitions: A key advantage of this process is that we can compute transitions between any two time steps efficiently. For example, we can directly compute \\(q(x_t | x_0)\\) without going through all intermediate steps.</p> <p>Starting from \\(x_0\\), we can write:</p> \\[x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}\\] <p>where \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\epsilon_{t-1} \\sim \\mathcal{N}(0, I)\\).</p> <p>Recursively substituting:</p> \\[x_t = \\sqrt{\\alpha_t} (\\sqrt{\\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_{t-1}} \\epsilon_{t-2}) + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}\\] <p>Continuing this recursion, we get:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>where \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\) and \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>Result: The multistep transition is:</p> \\[q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\] <p>This allows us to sample \\(x_t\\) directly from \\(x_0\\) in a single step, making training much more efficient.</p> <p>Diffusion analogy: We can think of this as a diffusion process. This is like a diffuser where given an initial state, we keep adding noise at every step. This is analogous to heat diffusion in a space- just as heat spreads out and becomes more uniform over time, our data distribution becomes increasingly noisy and uniform Gaussian as we add more noise at each step.</p> <p>The process gradually \"diffuses\" the structured information in the data into random noise, creating a smooth transition from the complex data distribution to a simple Gaussian noise distribution.</p> <p></p> <p>The ideal sampling process would be:</p> <ol> <li>Sample \\(x_T\\) from \\(\\pi(x_T)\\). Start with pure noise from the prior distribution</li> <li>Iteratively sample from the true denoising distribution \\(q(x_{t-1} | x_t)\\).</li> </ol> <p>This would generate samples by following the exact reverse of the forward diffusion process, gradually denoising from pure noise back to clean data.</p> <p>The challenge however, is that we don't know the true denoising distributions \\(q(x_{t-1} | x_t)\\). While the forward process \\(q(x_t | x_{t-1})\\) is predefined and tractable, the reverse process is not.</p> <p>However, we can learn an approximation \\(p_\\theta(x_{t-1} | x_t)\\) which is a Gaussian distribution with learned parameters:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>where \\(\\mu_\\theta(x_t, t)\\) is a neural network that learns the mean of the denoising distribution, and \\(\\sigma_t^2 I\\) is the fixed variance schedule.</p> <p>This is similar to a VAE decoder:</p> <p>VAE Decoder:</p> \\[p_\\theta(x | z) = \\mathcal{N}(x; \\mu_\\theta(z), \\sigma_\\theta^2(z) I)\\] <p>Diffusion reverse process:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>The diffusion decoder \\(p_\\theta(x_{t-1} | x_t)\\) is trying to learn to approximate the true denoising distributions \\(q(x_{t-1} | x_t)\\).</p> <p>The joint distribution of the learned reverse process is:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>Let's derive the joint distribution of the learned reverse process step by step.</p> <p>In the general case of \\(n\\) random variables \\(X_1, X_2, \\ldots, X_n\\), the values of an arbitrary subset of variables can be known and one can ask for the joint probability of all other variables. For example, if the values of \\(X_{k+1}, X_{k+2}, \\ldots, X_n\\) are known, the probability for \\(X_1, X_2, \\ldots, X_k\\) given these known values is:</p> \\[P(X_1, X_2, \\ldots, X_k|X_{k+1}, X_{k+2}, \\ldots, X_n) = \\frac{P(X_1, X_2, \\ldots, X_n)}{P(X_{k+1}, X_{k+2}, \\ldots, X_n)}\\] <p>This is the fundamental definition of conditional probability for multiple random variables.</p> <p>For any three events \\(A\\), \\(B\\), and \\(C\\), the joint conditional probability is defined as:</p> \\[P(A, B|C) = \\frac{P(A, B, C)}{P(C)}\\] <p>We can write the joint probability \\(P(A, B, C)\\) using the chain rule:</p> \\[P(A, B, C) = P(A|B, C) \\cdot P(B, C)\\] <p>Substituting this into our definition:</p> \\[P(A, B|C) = \\frac{P(A|B, C) \\cdot P(B, C)}{P(C)}\\] <p>We can write \\(P(B, C)\\) as:</p> \\[P(B, C) = P(B|C) \\cdot P(C)\\] \\[P(A, B|C) = \\frac{P(A|B, C) \\cdot P(B|C) \\cdot P(C)}{P(C)}\\] <p>The \\(P(C)\\) terms cancel out:</p> \\[P(A, B|C) = P(A|B, C) \\cdot P(B|C)\\] <p>The learned reverse process consists of a sequence of conditional distributions:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>where \\(\\mu_\\theta(x_t, t)\\) is a neural network that learns the mean of the denoising distribution.</p> <p>For the joint distribution \\(p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T)\\), we can apply the chain rule:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = p_\\theta(x_0 | x_1, \\ldots, x_T) \\cdot p_\\theta(x_1 | x_2, \\ldots, x_T) \\cdots p_\\theta(x_{T-1} | x_T)\\] <p>In the reverse process, we assume that each \\(x_{t-1}\\) depends only on \\(x_t\\), not on future states. This is the reverse Markov property:</p> \\[p_\\theta(x_{t-1} | x_t, x_{t+1}, \\ldots, x_T) = p_\\theta(x_{t-1} | x_t)\\] <p>Substituting the reverse Markov property into the chain rule:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = p_\\theta(x_0 | x_1) \\cdot p_\\theta(x_1 | x_2) \\cdots p_\\theta(x_{T-1} | x_T)\\] <p>This can be written compactly as:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>To get the complete joint distribution, we need to include the prior distribution over \\(x_T\\):</p> \\[p_\\theta(x_0, x_1, \\ldots, x_T) = p(x_T) \\cdot p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T)\\] \\[p_\\theta(x_0, x_1, \\ldots, x_T) = p(x_T) \\cdot \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>A crucial aspect of the diffusion process is choosing the values of \\(\\bar{\\alpha}_t\\) such that after many steps, we are left with pure noise. This ensures that the forward process converges to a simple, known distribution.</p> <p>Common choices for the noise schedule include:</p> <ol> <li>Linear Schedule: \\(\\beta_t = \\frac{t}{T} \\cdot \\beta_{\\text{max}}\\)</li> <li>Cosine Schedule: \\(\\beta_t = \\cos\\left(\\frac{t}{T} \\cdot \\frac{\\pi}{2}\\right)\\)</li> <li>Quadratic Schedule: \\(\\beta_t = \\left(\\frac{t}{T}\\right)^2 \\cdot \\beta_{\\text{max}}\\)</li> </ol> <p>Example: For a linear schedule with \\(\\beta_{\\text{max}} = 0.02\\) and \\(T = 1000\\), we get \\(\\beta_1 = 0.00002\\), \\(\\beta_{500} = 0.01\\) and \\(\\beta_{1000} = 0.02\\).</p> <p>Once we have trained the diffusion model and learned the reverse process \\(p_\\theta(x_{t-1} | x_t)\\), we can generate new samples by running the reverse process. Here's how sampling works. </p> <p>Sample \\(x_T\\) from the prior distribution \\(x_T \\sim \\mathcal{N}(x_T; 0, I)\\).</p> <p>For \\(t = T, T-1, \\ldots, 1\\), sample from the learned reverse process \\(x_{t-1} \\sim p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\).</p> <p>After \\(T\\) steps, we obtain \\(x_0\\), which is our generated sample.</p> <p>This entire diffusion framework can be viewed as a Variational Autoencoder (VAE) with a crucial difference: the encoder is fixed and predefined, while only the decoder is learned.</p> <p>Standard VAE Structure:</p> <ul> <li> <p>Encoder: \\(q_\\phi(z | x) = \\mathcal{N}(z; \\mu_\\phi(x), \\sigma_\\phi^2(x) I)\\)</p> </li> <li> <p>Decoder: \\(p_\\theta(x | z) = \\mathcal{N}(x; \\mu_\\theta(z), \\sigma_\\theta^2(z) I)\\)</p> </li> <li> <p>Prior: \\(p(z) = \\mathcal{N}(z; 0, I)\\)</p> </li> </ul> <p>Vanilla VAE ELBO (Non-KL form):</p> \\[ELBO_{\\text{VAE}} = \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right]\\] <p>Hierarchical VAE Structure (z\u2082 \u2192 z\u2081 \u2192 x):</p> <ul> <li> <p>Encoder: \\(q_\\phi(z_1, z_2 | x) = q_\\phi(z_1 | x) \\cdot q_\\phi(z_2 | z_1)\\)</p> </li> <li> <p>\\(q_\\phi(z_1 | x) = \\mathcal{N}(z_1; \\mu_\\phi(x), \\sigma_\\phi^2(x) I)\\)</p> </li> <li> <p>\\(q_\\phi(z_2 | z_1) = \\mathcal{N}(z_2; \\mu_\\phi(z_1), \\sigma_\\phi^2(z_1) I)\\)</p> </li> <li> <p>Decoder: \\(p_\\theta(x, z_1 | z_2) = p_\\theta(x | z_1) \\cdot p_\\theta(z_1 | z_2)\\)</p> </li> <li> <p>\\(p_\\theta(x | z_1) = \\mathcal{N}(x; \\mu_\\theta(z_1), \\sigma_\\theta^2(z_1) I)\\)</p> </li> <li> <p>\\(p_\\theta(z_1 | z_2) = \\mathcal{N}(z_1; \\mu_\\theta(z_2), \\sigma_\\theta^2(z_2) I)\\)</p> </li> <li> <p>Prior: \\(p(z_2) = \\mathcal{N}(z_2; 0, I)\\)</p> </li> </ul> <p>Hierarchical VAE ELBO (Non-KL form):</p> \\[ELBO_{\\text{HVAE}} = \\mathbb{E}_{q_\\phi(z_1,z_2|x)} \\left[ \\log \\frac{p_\\theta(x, z_1, z_2)}{q_\\phi(z_1, z_2|x)} \\right]\\] <p>Following the hierarchical VAE formulation, we can write the ELBO for diffusion models. In diffusion models, we have a sequence of latent variables \\(x_1, x_2, \\ldots, x_T\\) where \\(x_T\\) is the most abstract (pure noise) and \\(x_0\\) is the data.</p> <p>Diffusion Model Structure (x_T \u2192 x_{T-1} \u2192 ... \u2192 x_1 \u2192 x_0):</p> <ul> <li> <p>Encoder: \\(q(x_1, x_2, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\) - Fixed noise corruption process</p> </li> <li> <p>Decoder: \\(p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\) - Learned denoising process</p> </li> <li> <p>Prior: \\(p(x_T) = \\mathcal{N}(x_T; 0, I)\\)</p> </li> </ul> <p>Diffusion Model ELBO (Non-KL form):</p> \\[ELBO_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ \\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>The Negative Evidence Lower BOund (NELBO) is the negative of the ELBO, which is what we actually minimize during training:</p> \\[\\mathcal{L}_{\\text{Diff}} = -\\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ \\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>This can be rewritten as:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>The decoder learns to predict the mean function \\(\\mu_\\theta(x_t, t)\\) for the reverse process. Let's derive how this function is parameterized.</p> <p>The true reverse process \\(q(x_{t-1} | x_t, x_0)\\) can be derived using Bayes' theorem. For Gaussian distributions, this gives us:</p> \\[q(x_{t-1} | x_t, x_0) = \\mathcal{N}(x_{t-1}; \\mu_t(x_t, x_0), \\sigma_t^2 I)\\] <p>where it can be shown that:</p> \\[\\mu_t(x_t, x_0) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\right)\\] <p>and:</p> \\[\\sigma_t^2 = \\frac{\\beta_t(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\\] <p>The learned reverse process is:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>Since we want the learned process to approximate the true reverse process, we parameterize \\(\\mu_\\theta(x_t, t)\\) to match the form of \\(\\mu_t(x_t, x_0)\\):</p> \\[\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\] <p>where \\(\\epsilon_\\theta(x_t, t)\\) is a neural network that predicts the noise \\(\\epsilon\\) given \\(x_t\\) and \\(t\\).</p>"},{"location":"ai/deep_generative_models/score_based_diffusion_models/#rewriting-the-elbo-for-diffusion-models","title":"Rewriting the ELBO for Diffusion Models","text":"<p>Let's rewrite the diffusion model ELBO and transform it to resemble denoising score matching.</p> <p>Starting with the diffusion model ELBO:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>We can expand this as:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log p_\\theta(x_0, x_1, \\ldots, x_T) + \\log q(x_1, \\ldots, x_T|x_0) \\right]\\] <p>The learned joint distribution is:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_T) = p(x_T) \\cdot \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>The true joint distribution is:</p> \\[q(x_1, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\] <p>Substituting these:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log p(x_T) - \\sum_{t=1}^T \\log p_\\theta(x_{t-1} | x_t) + \\sum_{t=1}^T \\log q(x_t | x_{t-1}) \\right]\\] <p>For Gaussian distributions, the log-likelihood is:</p> \\[\\log \\mathcal{N}(x; \\mu, \\sigma^2 I) = -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 + C\\] <p>where \\(C\\) is a constant that doesn't depend on the parameters.</p> <p>For the learned reverse process:</p> \\[\\log p_\\theta(x_{t-1} | x_t) = -\\frac{1}{2\\sigma_t^2} \\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 + C\\] <p>For the true forward process:</p> \\[\\log q(x_t | x_{t-1}) = -\\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2 + C\\] <p>Using the definition of \\(\\mu_\\theta(x_t, t)\\):</p> \\[\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\] <p>The squared error term becomes:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\left\\|x_{t-1} - \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\right\\|^2\\] <p>From the forward process, we know:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>And from the multistep transition:</p> \\[x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\epsilon_{t-1}\\] <p>Substituting these into the squared error:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\left\\|\\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\epsilon_{t-1} - \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\right\\|^2\\] <p>Using the relationship \\(\\bar{\\alpha}_t = \\bar{\\alpha}_{t-1} \\cdot \\alpha_t\\), we can simplify:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\left\\|\\frac{\\beta_t}{\\sqrt{\\alpha_t(1 - \\bar{\\alpha}_t)}} (\\epsilon - \\epsilon_\\theta(x_t, t))\\right\\|^2\\] <p>This simplifies to:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\frac{\\beta_t^2}{\\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\] <p>Substituting back into the ELBO:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log p(x_T) + \\sum_{t=1}^T \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2 + \\sum_{t=1}^T \\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2 \\right]\\] <p>The key term in the ELBO is:</p> \\[\\sum_{t=1}^T \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\] <p>Let's define:</p> \\[\\lambda_t = \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)}\\] <p>From the forward process, we know:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>The expectation \\(\\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)}\\) can be rewritten as:</p> \\[\\mathbb{E}_{x_0 \\sim p_{data}(x_0)} \\mathbb{E}_{\\epsilon_1, \\ldots, \\epsilon_T \\sim \\mathcal{N}(0, I)}\\] <p>Since each \\(x_t\\) is generated as:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t\\] <p>The ELBO can be simplified to:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\lambda_t \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|^2 \\right] + \\text{constant terms}\\] <p>where:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t\\] <p>This can be written more compactly as:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\lambda_t \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|^2 \\right] + \\text{constant terms}\\] <p>where \\(x_0 \\sim p_{data}(x_0)\\) (clean data), \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) (noise), \\(t \\sim \\text{Uniform}(1, T)\\) (timestep)</p> <p>The diffusion model ELBO is equivalent to:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\lambda_t \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|^2 \\right]\\] <p>where \\(\\lambda_t = \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)}\\) is the weighting factor for each timestep.</p> <p>Note In the original ELBO, we had two summation-terms:</p> <ol> <li> <p>\\(\\sum_{t=1}^T \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\) (noise prediction term)</p> </li> <li> <p>\\(\\sum_{t=1}^T \\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2\\) (forward process term)</p> </li> </ol> <p>The second summation-term \\(\\sum_{t=1}^T \\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2\\) represents the log-likelihood of the forward process \\(q(x_t | x_{t-1})\\). This summation-term does not depend on the model parameters \\(\\theta\\) because the forward process is fixed and predefined. It only depends on the noise schedule \\(\\beta_t\\) and the data. When we take the gradient with respect to \\(\\theta\\) to optimize the model, this summation-term vanishes.</p>"},{"location":"ai/deep_generative_models/score_based_diffusion_models/#sampling","title":"Sampling","text":"<p>While the ELBO loss \\(\\mathcal{L}_{\\text{Diff}}\\) and the score-based objective are roughly equivalent in terms of what they learn, the sampling procedures differ between these two approaches.</p> <p>In a Score-Based Model (SBM), sampling is performed using Langevin dynamics. In a Diffusion Model (VAE form), sampling follows the learned reverse process.</p> <p>The connection between the two approaches comes from the relationship between the score function and the noise predictor:</p> <p>Score function: \\(s_\\theta(x_t, t) = \\nabla_x \\log p_t(x_t)\\)</p> <p>Noise predictor: \\(\\epsilon_\\theta(x_t, t)\\) predicts the noise added during the forward process</p> <p>Relationship: For Gaussian noise, the score function is proportional to the negative noise.</p> <p>From the forward process, we have:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>The distribution of \\(x_t\\) given \\(x_0\\) is:</p> \\[q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\] <p>The score function is the gradient of the log probability density:</p> \\[s(x_t, t) = \\nabla_{x_t} \\log q(x_t | x_0)\\] <p>For the Gaussian distribution \\(q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\):</p> \\[\\log q(x_t | x_0) = -\\frac{1}{2(1 - \\bar{\\alpha}_t)} \\|x_t - \\sqrt{\\bar{\\alpha}_t} x_0\\|^2 + C\\] <p>where \\(C\\) is a constant that doesn't depend on \\(x_t\\).</p> <p>Taking the gradient with respect to \\(x_t\\):</p> \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} (x_t - \\sqrt{\\bar{\\alpha}_t} x_0)\\] <p>From the forward process, we can express \\(x_0\\) in terms of \\(x_t\\) and \\(\\epsilon\\):</p> \\[x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon}{\\sqrt{\\bar{\\alpha}_t}}\\] \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} \\left(x_t - \\sqrt{\\bar{\\alpha}_t} \\cdot \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon}{\\sqrt{\\bar{\\alpha}_t}}\\right)\\] <p>Simplifying the expression:</p> \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} \\left(x_t - (x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon)\\right)\\] \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} \\cdot \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{\\epsilon}{\\sqrt{1 - \\bar{\\alpha}_t}}\\] <p>Therefore, the score function is:</p> \\[s(x_t, t) = \\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{\\epsilon}{\\sqrt{1 - \\bar{\\alpha}_t}}\\] <p>In practice, we learn:</p> <ul> <li> <p>Score function: \\(s_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log q(x_t | x_0)\\)</p> </li> <li> <p>Noise predictor: \\(\\epsilon_\\theta(x_t, t) \\approx \\epsilon\\)</p> </li> </ul> <p>Therefore, \\(s_\\theta(x_t, t) \\approx -\\frac{\\epsilon_\\theta(x_t, t)}{\\sqrt{1 - \\bar{\\alpha}_t}}\\).</p> <p>Both sampling methods work because they're learning the same underlying structure:</p> <ol> <li>Score-based: Learns the gradient of the log-density at each noise level</li> <li>Diffusion: Learns the noise that was added during the forward process</li> </ol> <p>Since the score function and noise predictor are mathematically related, both approaches can generate high-quality samples, but they use different sampling algorithms.</p>"},{"location":"ai/deep_generative_models/score_based_generative_modeling/","title":"Score Based Generative Modeling","text":""},{"location":"ai/deep_generative_models/score_based_generative_modeling/#langevin-dynamics-sampling","title":"Langevin Dynamics Sampling","text":"<p>Langevin dynamics is a powerful MCMC method that uses gradient information to efficiently sample from complex probability distributions. For score-based models, it provides a natural way to generate samples by following the learned score field.</p> <p>Mathematical Foundation</p> <p>Langevin dynamics is based on the Langevin equation, a stochastic differential equation that describes the motion of particles in a potential field:</p> \\[dx_t = \\nabla_x \\log p(x_t) dt + \\sqrt{2} dW_t\\] <p>where:</p> <ul> <li> <p>\\(x_t\\) is the particle position at time \\(t\\)</p> </li> <li> <p>\\(\\nabla_x \\log p(x_t)\\) is the score function (gradient of log probability)</p> </li> <li> <p>\\(W_t\\) is a Wiener process (Brownian motion)</p> </li> <li> <p>The first term is the drift term (gradient guidance)</p> </li> <li> <p>The second term is the diffusion term (random exploration)</p> </li> </ul> <p>Discretized Langevin Dynamics</p> <p>For practical implementation, we discretize the continuous-time equation:</p> \\[x_{t+1} = x_t + \\epsilon \\cdot \\nabla_x \\log p(x_t) + \\sqrt{2\\epsilon} \\cdot \\eta_t\\] <p>where:</p> <ul> <li> <p>\\(\\epsilon\\) is the step size (time discretization)</p> </li> <li> <p>\\(\\eta_t \\sim \\mathcal{N}(0, I)\\) is Gaussian noise</p> </li> <li> <p>\\(t\\) indexes the discrete time steps</p> </li> </ul> <p>Score-Based Langevin Sampling</p> <p>For our trained score function \\(s_\\theta(x) \\approx \\nabla_x \\log p_{data}(x)\\), the sampling algorithm becomes:</p> <p>Algorithm: Score-Based Langevin Sampling</p> <ol> <li> <p>Initialize: \\(x_0 \\sim \\mathcal{N}(0, I)\\) (random noise)</p> </li> <li> <p>Iterate: For \\(t = 0, 1, 2, \\ldots, T-1\\):</p> </li> <li> <p>Compute score: \\(s_t = s_\\theta(x_t)\\)</p> </li> <li> <p>Add gradient step: \\(x_{t+1} = x_t + \\frac{\\epsilon}{2} \\cdot s_t + \\sqrt{2\\epsilon} \\cdot \\eta_t\\)</p> </li> <li> <p>Where \\(\\eta_t \\sim \\mathcal{N}(0, I)\\)</p> </li> <li> <p>Return: \\(x_T\\) as the generated sample</p> </li> </ol> <p>Intuition Behind Langevin Dynamics</p> <p>The Drift Term (\\(\\frac{\\epsilon}{2} \\cdot s_\\theta(x_t)\\)):</p> <ul> <li> <p>Pushes the sample toward high-probability regions</p> </li> <li> <p>The score function points \"uphill\" in the probability landscape</p> </li> <li> <p>Larger step sizes \\(\\epsilon\\) lead to more aggressive movement</p> </li> <li> <p>The factor of \\(\\frac{1}{2}\\) comes from proper discretization of the continuous Langevin equation</p> </li> </ul> <p>The Diffusion Term (\\(\\sqrt{2\\epsilon} \\cdot \\eta_t\\)):</p> <ul> <li> <p>Adds random exploration to avoid getting stuck in local modes</p> </li> <li> <p>Balances the deterministic gradient guidance</p> </li> <li> <p>Ensures the chain can escape local optima and explore the full distribution</p> </li> </ul> <p>Balance Between Drift and Diffusion:</p> <ul> <li> <p>Small \\(\\epsilon\\): More exploration, slower convergence, better mixing</p> </li> <li> <p>Large \\(\\epsilon\\): Faster convergence, but may miss modes or become unstable</p> </li> <li> <p>Optimal \\(\\epsilon\\): Depends on the data distribution and model architecture</p> </li> </ul> <p>Convergence Guarantees</p> <p>Under mild conditions on the target distribution and score function, Langevin dynamics provides strong theoretical guarantees:</p> <p>Asymptotic Convergence:</p> <p>If \\(\\epsilon \\to 0\\) and \\(T \\to \\infty\\), we are guaranteed that \\(x_T \\sim p_{data}(x)\\).</p> <p>Mathematical Interpretation:</p> <ul> <li>\\(\\epsilon \\to 0\\): The discretization becomes arbitrarily fine, approaching the continuous Langevin equation</li> <li>\\(T \\to \\infty\\): The Markov chain runs for an infinite number of steps, allowing it to reach the stationary distribution</li> <li>\\(x_T \\sim p_{data}(x)\\): The final sample is distributed according to the target data distribution</li> </ul> <p>Challenge in Low Density Regions:</p> <p>One significant limitation of Langevin dynamics is its poor performance in low density regions of the target distribution:</p> <ul> <li>Weak Score Signals: In regions where \\(p_{data}(x) \\approx 0\\), the score function \\(\\nabla_x \\log p_{data}(x)\\) becomes very small or noisy</li> <li>Mode Collapse Risk: The algorithm may fail to explore all modes (mode is a region where the probability density is high, i.e., data points are concentrated) of a multi-modal distribution</li> <li>Slow convergence: Langevin Dynamics converges very slowly. Might not even converge if we have zero probability somewhere.</li> </ul> <p>This challenge motivates the development of annealed Langevin dynamics and other advanced sampling techniques that can better handle complex, multi-modal distributions.</p>"},{"location":"ai/deep_generative_models/score_based_generative_modeling/#annealed-langevin-dynamics","title":"Annealed Langevin Dynamics","text":"<p>Mathematical Formulation</p> <p>We define a sequence of annealed distributions indexed by noise level \\(\\sigma_t\\):</p> \\[p_t(x) = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_t^2 I) dy\\] <p>where each \\(p_t(x)\\) is a smoothed version of the original data distribution.</p> <p>This equation is derived from the convolution of the data distribution with the noise distribution. Here's the step-by-step reasoning:</p> <p>If \\(Y \\sim p_{data}(y)\\) and \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_i^2 I)\\), then the noisy sample is \\(X = Y + \\epsilon\\).</p> <p>The joint distribution of \\((Y, X)\\) is:</p> \\[p(y, x) = p_{data}(y) \\cdot \\mathcal{N}(x; y, \\sigma_i^2 I)\\] <p>The joint distribution is derived using the chain rule of probability:</p> \\[p(y, x) = p(y) \\cdot p(x | y)\\] <p>where:</p> <ul> <li> <p>\\(p(y) = p_{data}(y)\\) is the marginal distribution of the clean data</p> </li> <li> <p>\\(p(x | y)\\) is the conditional distribution of the noisy sample given the clean data</p> </li> </ul> <p>Since \\(X = Y + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_i^2 I)\\), the conditional distribution is:</p> \\[p(x | y) = \\mathcal{N}(x; y, \\sigma_i^2 I)\\] <p>This is because adding a constant (\\(y\\)) to a Gaussian random variable shifts the mean but preserves the variance. Therefore:</p> \\[p(y, x) = p_{data}(y) \\cdot \\mathcal{N}(x; y, \\sigma_i^2 I)\\] <p>To get the distribution of \\(X\\) alone, we marginalize over \\(Y\\):</p> \\[p_{\\sigma_i}(x) = \\int p(y, x) dy = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_i^2 I) dy\\] <p>We're using the law of total probability (also called marginalization). When we have a joint distribution \\(p(y, x)\\), to find the marginal distribution of \\(x\\) alone, we integrate out the other variable:</p> \\[p_{\\sigma_i}(x) = \\int p(y, x) dy\\] <p>This is because:</p> <ul> <li> <p>The joint distribution \\(p(y, x)\\) gives us the probability of both \\(y\\) AND \\(x\\) occurring together</p> </li> <li> <p>To find the probability of just \\(x\\) (regardless of what \\(y\\) is), we sum over all possible values of \\(y\\)</p> </li> <li> <p>In continuous probability, \"summing\" becomes integration</p> </li> </ul> <p>Intuition: We're asking \"What's the probability of observing a noisy sample \\(x\\)?\" The answer is the sum of probabilities over all possible clean samples \\(y\\) that could have generated this noisy sample.</p> <p>Final Form: The noise-perturbed distribution is:</p> \\[p_{\\sigma_i}(x) = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_i^2 I) dy\\] <p>We use multiple scales of noise perturbations simultaneously. Suppose we always perturb the data with isotropic Gaussian noise, and let there be a total of \\(L\\) increasing standard deviations \\(\\sigma_1 &lt; \\sigma_2 &lt; \\ldots &lt; \\sigma_L\\). We first perturb the data distribution \\(p_{data}(y)\\) with each of the Gaussian noise \\(\\mathcal{N}(0, \\sigma_i^2 I)\\) to obtain a noise-perturbed distribution (the final form we derived above):</p> \\[p_{\\sigma_i}(x) = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_i^2 I) dy\\] <p>Note that we can easily draw samples from \\(p_{\\sigma_i}(x)\\) by sampling \\(y \\sim p_{data}(y)\\) and computing \\(x = y + \\sigma_i \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>We estimate the score function of each noise-perturbed distribution, \\(\\nabla_x \\log p_{\\sigma_i}(x)\\), by training a Denoising Score Matching Model (when parameterized with a neural network) with score matching, such that \\(s_\\theta(x, \\sigma_i) \\approx \\nabla_x \\log p_{\\sigma_i}(x)\\) for all \\(i\\). The training objective for \\(s_\\theta\\) is a weighted sum of Fisher divergences for all noise scales. In particular, we use the objective below:</p> \\[\\mathcal{L}(\\theta) = \\frac{1}{L} \\sum_{i=1}^L \\lambda(\\sigma_i) \\mathbb{E}_{p_{\\sigma_i}(x)} \\left[ \\| s_\\theta(x, \\sigma_i) - \\nabla_x \\log p_{\\sigma_i}(x) \\|_2^2 \\right]\\] <p>where \\(\\lambda(\\sigma_i)\\) is a positive weighting function, often chosen to be \\(\\lambda(\\sigma_i) = \\sigma_i^2\\). The objective \\(\\mathcal{L}(\\theta)\\) can be optimized with score matching, exactly as in optimizing the naive score-based model.</p> <p>Denoising Score Matching Format:</p> <p>We can rewrite the objective in Denoising Score Matching model format:</p> \\[\\mathcal{L}(\\theta) = \\frac{1}{L} \\sum_{i=1}^L \\lambda(\\sigma_i) \\mathbb{E}_{y \\sim p_{data}(y), x \\sim \\mathcal{N}(x; y, \\sigma_i^2 I)} \\left[ \\left\\| s_\\theta(x, \\sigma_i) - \\frac{y - x}{\\sigma_i^2} \\right\\|_2^2 \\right]\\] <p>Note: The noise scales \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_L\\) are typically chosen to be in a geometric progression, meaning \\(\\sigma_{i+1} = \\alpha \\cdot \\sigma_i\\) for some constant \\(\\alpha &lt; 1\\). This ensures that the noise levels decrease exponentially, providing a smooth annealing schedule from high noise to low noise.</p> <p>Perturbing an image with multiple scales of Gaussian noise: </p> <p>After training our score-based model \\(s_\\theta(x, \\sigma_i)\\), we can produce samples from it by running Langevin dynamics for \\(\\sigma_L, \\sigma_{L-1}, \\ldots, \\sigma_1\\) in sequence. This method is called Annealed Langevin dynamics, since the noise scale decreases (anneals) gradually over time.</p> <p>We can start from unstructured noise, modify images according to the scores, and generate nice samples: </p>"},{"location":"ai/deep_generative_models/score_based_generative_modeling/#generative-modeling-via-stochastic-differential-equations-sdes","title":"Generative Modeling via Stochastic Differential Equations (SDEs)","text":"<p>When the number of noise scales approaches infinity, we essentially perturb the data distribution with continuously growing levels of noise. In this case, the noise perturbation procedure is a continuous-time stochastic process, as demonstrated below.</p> <p></p> <p>How can we represent a stochastic process in a concise way? Many stochastic processes are solutions of stochastic differential equations (SDEs). In general, an SDE possesses the following form:</p> \\[dx = f(x, t)dt + g(t)dw\\] <p>where \\(f(x, t)\\) is a vector-valued function called the drift coefficient, \\(g(t)\\) is a real-valued function called the diffusion coefficient, \\(w\\) denotes a standard Brownian motion, and \\(dw\\) can be viewed as infinitesimal white noise. The solution of a stochastic differential equation is a continuous collection of random variables \\(\\{x(t)\\}_{t \\in [0, T]}\\).</p> <p>These random variables trace stochastic trajectories as the time index \\(t\\) grows from the start time \\(0\\) to the end time \\(T\\). Let \\(p_t(x)\\) denote the (marginal) probability density function of \\(x(t)\\). Here \\(p_t(x)\\) is analogous to \\(p_{\\sigma_i}(x)\\) when we had a finite number of noise scales, and \\(t\\) is analogous to \\(\\sigma_i\\). Clearly, \\(p_0(x)\\) is the data distribution since no perturbation is applied to data at \\(t = 0\\). After perturbing \\(p_0(x)\\) with the stochastic process for a sufficiently long time \\(T\\), \\(p_T(x)\\) becomes close to a tractable noise distribution \\(p_T(x) \\approx \\pi(x)\\), called a prior distribution. We note that \\(\\pi(x)\\) is analogous to \\(p_{\\sigma_L}(x)\\) in the case of finite noise scales, which corresponds to applying the largest noise perturbation \\(\\sigma_L\\) to the data.</p> <p>There are numerous ways to add noise perturbations, and the choice of SDEs is not unique. For example, the following SDE</p> \\[dx = e^t dw\\] <p>perturbs data with a Gaussian noise of mean zero and exponentially growing variance. Therefore, the SDE should be viewed as part of the model, much like \\(\\sigma_i\\).</p> <p>Recall that with a finite number of noise scales, we can generate samples by reversing the perturbation process with annealed Langevin dynamics, i.e., sequentially sampling from each noise-perturbed distribution using Langevin dynamics. For infinite noise scales, we can analogously reverse the perturbation process for sample generation by using the reverse SDE.</p> <p>Importantly, any SDE has a corresponding reverse SDE, whose closed form is given by</p> \\[dx = [f(x, t) - g^2(t)\\nabla_x \\log p_t(x)]dt + g(t)d\\bar{w}\\] <p>Here \\(dt\\) represents a negative infinitesimal time step, since the SDE needs to be solved backwards in time (from \\(T\\) to \\(0\\)). In order to compute the reverse SDE, we need to estimate \\(\\nabla_x \\log p_t(x)\\), which is exactly the score function of \\(p_t(x)\\).</p> <p></p> <p>Note: Langevin dynamics is a specific instance of the reverse SDE where \\(f(x, t) = 0\\) (no forward drift) and \\(g(t) = \\sqrt{2}\\) (constant diffusion). This shows how Langevin dynamics naturally emerges as a special case of the reverse SDE when we want to sample from a target distribution.</p>"},{"location":"ai/deep_generative_models/score_based_generative_modeling/#estimating-the-reverse-sde-with-score-based-models-and-score-matching","title":"Estimating the reverse SDE with score-based models and score matching","text":"<p>Solving the reverse SDE requires us to know the terminal distribution \\(p_T(x)\\), and the score function \\(\\nabla_x \\log p_t(x)\\). By design, the former is close to the prior distribution \\(\\pi(x)\\) which is fully tractable. In order to estimate \\(\\nabla_x \\log p_t(x)\\), we train a Time-Dependent Score-Based Model \\(s_\\theta(x, t)\\), such that \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\). This is analogous to the denoising score matching model \\(s_\\theta(x, \\sigma_i)\\) used for finite noise scales, trained such that \\(s_\\theta(x, \\sigma_i) \\approx \\nabla_x \\log p_{\\sigma_i}(x)\\).</p> <p>Our training objective for \\(s_\\theta(x, t)\\) is a continuous weighted combination of Fisher divergences, given by</p> \\[\\mathcal{L}(\\theta) = \\mathbb{E}_{t \\sim \\mathcal{U}[0, T]} \\mathbb{E}_{x \\sim p_t(x)} \\left[ \\lambda(t) \\| s_\\theta(x, t) - \\nabla_x \\log p_t(x) \\|_2^2 \\right]\\] <p>where \\(\\mathcal{U}[0, T]\\) denotes a uniform distribution over the time interval \\([0, T]\\), and \\(\\lambda(t)\\) is a positive weighting function.</p> <p>As before, our weighted combination of Fisher divergences can be efficiently optimized with score matching methods, such as denoising score matching and sliced score matching. Once our score-based model \\(s_\\theta(x, t)\\) is trained to optimality, we can plug it into the expression of the reverse SDE to obtain an estimated reverse SDE.</p> <p>We can start with \\(x(T) \\sim p_T(x)\\), and solve the above reverse SDE to obtain a sample \\(x(0)\\). Let us denote the distribution of \\(x(0)\\) obtained in such way as \\(p_\\theta(x)\\). When the score-based model \\(s_\\theta(x, t)\\) is well-trained, we have \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\), in which case \\(x(0)\\) is an approximate sample from the data distribution \\(p_0(x)\\).</p> <p>By solving the estimated reverse SDE with numerical SDE solvers, we can simulate the reverse stochastic process for sample generation. Perhaps the simplest numerical SDE solver is the Euler-Maruyama method. When applied to our estimated reverse SDE, it discretizes the SDE using finite time steps and small Gaussian noise. Specifically, it chooses a small negative time step \\(\\Delta t\\), initializes \\(x(T) \\sim p_T(x)\\), and iterates the following procedure until \\(t = 0\\):</p> \\[\\Delta x = [f(x, t) - g^2(t)s_\\theta(x, t)]\\Delta t + g(t)\\sqrt{|\\Delta t|}\\eta_t\\] <p>where \\(\\eta_t \\sim \\mathcal{N}(0, I)\\).</p> <p>Then update: \\(x \\leftarrow x + \\Delta x\\) and \\(t \\leftarrow t + \\Delta t\\).</p> <p>Note: The function \\(f(x, t)\\) in the Euler-Maruyama equation is the drift coefficient from the original forward SDE. Common examples include:</p> <ul> <li> <p>\\(f(x, t) = 0\\) (pure diffusion): Used in simple noise perturbation</p> </li> <li> <p>\\(f(x, t) = -\\frac{1}{2}\\beta(t)x\\) (linear drift): Used in variance-preserving diffusion</p> </li> <li> <p>\\(f(x, t) = -x^2\\) (quadratic drift): Creates potential wells</p> </li> <li> <p>\\(f(x, t) = x - x^3\\) (polynomial drift): Creates multiple stable equilibria</p> </li> </ul> <p>Most Common in Practice: For score-based generative modeling, the most commonly used forms are \\(f(x, t) = 0\\) (pure diffusion) and \\(f(x, t) = -\\frac{1}{2}\\beta(t)x\\) (VP diffusion). The choice of \\(f(x, t)\\) determines how the data is perturbed during the forward process.</p> <p>The Euler-Maruyama method is qualitatively similar to Langevin dynamics\u2014 both update \\(x\\) by following score functions perturbed with Gaussian noise.</p>"},{"location":"ai/deep_generative_models/score_based_models/","title":"Score Based Models","text":""},{"location":"ai/deep_generative_models/score_based_models/#score-matching","title":"Score Matching","text":"<p>Energy-Based Model Probability Distribution</p> <p>In Energy-Based Models, the probability distribution is defined as:</p> \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} e^{f_\\theta(x)}\\] <p>where:</p> <ul> <li> <p>\\(f_\\theta(x)\\) is the energy function (neural network)</p> </li> <li> <p>\\(Z(\\theta) = \\int e^{f_\\theta(x)} dx\\) is the partition function (intractable)</p> </li> </ul> <p>Taking the logarithm of the probability distribution:</p> \\[\\log p_\\theta(x) = f_\\theta(x) - \\log Z(\\theta)\\] <p>Notice that the partition function \\(Z(\\theta)\\) appears as a constant term that doesn't depend on \\(x\\).</p> <p>Stein Score Function</p> <p>The Stein score function \\(s_\\theta(x)\\) is defined as the gradient of the log probability with respect to \\(x\\):</p> \\[s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\] <p>For Energy-Based Models, the score function equals the gradient of the energy function:</p> \\[s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = \\nabla_x (f_\\theta(x) - \\log Z(\\theta)) = \\nabla_x f_\\theta(x)\\] <p>The partition function term \\(\\log Z(\\theta)\\) disappears because it doesn't depend on \\(x\\).</p> <p>Score as a Vector Field</p> <p>The score function \\(s_\\theta(x)\\) is a vector field that assigns a vector to each point \\(x\\) in the data space. This vector has both:</p> <ol> <li> <p>Magnitude: How quickly the log probability changes</p> </li> <li> <p>Direction: The direction of steepest increase in log probability</p> </li> </ol> <p>Intuition: The score vector points \"uphill\" in the log probability landscape, indicating the direction where the model assigns higher probability.</p> <p>Example: Gaussian Distribution</p> <p>Consider a Gaussian distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\):</p> \\[p(x) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\right)\\] <p>Log Probability:</p> \\[\\log p(x) = -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu) - \\frac{1}{2}\\log((2\\pi)^d |\\Sigma|)\\] <p>Score Function:</p> \\[s(x) = \\nabla_x \\log p(x) = -\\Sigma^{-1}(x - \\mu)\\] <p>Interpretation:</p> <ul> <li> <p>The score points toward the mean \\(\\mu\\) (direction of higher probability)</p> </li> <li> <p>The magnitude is proportional to the distance from the mean</p> </li> <li> <p>For isotropic Gaussian (\\(\\Sigma = \\sigma^2 I\\)): \\(s(x) = -\\frac{1}{\\sigma^2}(x - \\mu)\\)</p> </li> </ul> <p>This example shows how the score function naturally guides samples toward high-probability regions of the distribution.</p>"},{"location":"ai/deep_generative_models/score_based_models/#score-matching-comparing-distributions-via-vector-fields","title":"Score Matching: Comparing Distributions via Vector Fields","text":"<p>The core idea of score matching is that we want to compare two probability distributions by comparing their respective vector fields of gradients (score functions).</p> <p>The Key Insight:</p> <p>Instead of directly comparing probability densities \\(p_{data}(x)\\) and \\(p_\\theta(x)\\) (which requires computing the intractable partition function), we compare their score functions:</p> <ul> <li>Data Score: \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\)</li> <li>Model Score: \\(s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = \\nabla_x f_\\theta(x)\\)</li> </ul> <p>This measures how different the \"pointing directions\" are at each location \\(x\\).</p> <p>L2 Distance Between Score Functions</p> <p>One way to compare the score functions is to calculate the average L2 distance between the score of \\(p_{data}\\) and \\(p_\\theta\\):</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x) - s_{data}(x)\\|^2 \\right]\\] <p>Note: This loss function is also called the Fisher divergence between \\(p_{data}(x)\\) and \\(p_\\theta(x)\\). The Fisher divergence measures the difference between two probability distributions by comparing their score functions (gradients of log densities) rather than the densities themselves.</p> <p>Understanding the L2 Distance:</p> <p>The L2 norm \\(\\|s_\\theta(x) - s_{data}(x)\\|^2\\) measures the squared Euclidean distance between two vectors:</p> \\[\\|s_\\theta(x) - s_{data}(x)\\|^2 = \\sum_{i=1}^d (s_\\theta(x)_i - s_{data}(x)_i)^2\\] <p>where \\(d\\) is the dimension of the data space.</p> <p>Score matching is a method for training Energy-Based Models by minimizing the Fisher divergence between the data distribution \\(p_{data}(x)\\) and the model distribution \\(p_\\theta(x)\\):</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x) - s_{data}(x)\\|^2 \\right]\\] <p>where \\(s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\) and \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\) are the score functions of the model and data distributions respectively.</p> <p>But how do we figure out \\(\\nabla_x \\log p_{data}(x)\\) given only samples?</p> <p>Score Matching Reformulation (Univariate Case)</p> <p>For the univariate case where \\(x \\in \\mathbb{R}\\), we can rewrite the score matching objective to avoid needing the data score. Let's expand the squared difference:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_\\theta(x) - \\frac{d}{dx} \\log p_{data}(x)\\right)^2 \\right]\\] <p>Expanding the square:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_\\theta(x)\\right)^2 - \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) + \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_{data}(x)\\right)^2 \\right]\\] <p>The key insight is to use integration by parts on the cross term. For any function \\(f(x)\\) and \\(g(x)\\):</p> \\[\\int f(x) \\frac{d}{dx} g(x) dx = f(x)g(x) - \\int \\frac{d}{dx} f(x) \\cdot g(x) dx\\] <p>Setting \\(f(x) = \\frac{d}{dx} \\log p_\\theta(x)\\) and \\(g(x) = p_{data}(x)\\), we get:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\right] = \\int \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\cdot p_{data}(x) dx\\] <p>Using the chain rule: \\(\\frac{d}{dx} \\log p_{data}(x) \\cdot p_{data}(x) = \\frac{d}{dx} p_{data}(x)\\), we get:</p> \\[= \\int \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} p_{data}(x) dx\\] <p>Using integration by parts:</p> \\[= \\left. \\frac{d}{dx} \\log p_\\theta(x) \\cdot p_{data}(x) \\right|_{-\\infty}^{\\infty} - \\int \\frac{d^2}{dx^2} \\log p_\\theta(x) \\cdot p_{data}(x) dx\\] <p>Why does the boundary term vanish?</p> <p>The boundary term \\(\\left. \\frac{d}{dx} \\log p_\\theta(x) \\cdot p_{data}(x) \\right|_{-\\infty}^{\\infty}\\) vanishes under reasonable assumptions:</p> <ol> <li>Data distribution decay: \\(p_{data}(x) \\to 0\\) as \\(|x| \\to \\infty\\) (most real-world distributions have finite support or decay to zero)</li> <li>Model score boundedness: \\(\\frac{d}{dx} \\log p_\\theta(x)\\) grows at most polynomially as \\(|x| \\to \\infty\\)</li> <li>Product decay: The product \\(\\frac{d}{dx} \\log p_\\theta(x) \\cdot p_{data}(x) \\to 0\\) as \\(|x| \\to \\infty\\)</li> </ol> <p>This is a standard assumption in score matching literature and holds for most practical distributions.</p> <p>Assuming the boundary term vanishes (which is reasonable for well-behaved distributions), we get:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\right] = -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d^2}{dx^2} \\log p_\\theta(x) \\right]\\] <p>Substituting back into the original objective:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_\\theta(x)\\right)^2 + \\frac{d^2}{dx^2} \\log p_\\theta(x) \\right] + \\text{constant}\\] <p>where the constant term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}} \\left[ \\left(\\frac{d}{dx} \\log p_{data}(x)\\right)^2 \\right]\\) doesn't depend on \\(\\theta\\) and can be ignored during optimization.</p> <p>Key Insight: This reformulation allows us to train the model using only samples from \\(p_{data}(x)\\) and the derivatives of our model's log probability, without needing access to the data score function.</p> <p>Score Matching Reformulation (Multivariate Case)</p> <p>For the multivariate case where \\(x \\in \\mathbb{R}^d\\), we can extend the univariate derivation. The score matching objective becomes:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x) - \\nabla_x \\log p_{data}(x)\\|^2 \\right]\\] <p>Expanding the squared norm:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x)\\|^2 - \\nabla_x \\log p_\\theta(x)^T \\nabla_x \\log p_{data}(x) + \\frac{1}{2} \\|\\nabla_x \\log p_{data}(x)\\|^2 \\right]\\] <p>The key insight is to use integration by parts on the cross term. For the multivariate case, we need to handle each component separately. Let \\(s_\\theta(x)_i\\) and \\(s_{data}(x)_i\\) denote the \\(i\\)-th component of the respective score functions.</p> <p>For each component \\(i\\), we have:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)_i \\cdot s_{data}(x)_i \\right] = \\int s_\\theta(x)_i \\cdot s_{data}(x)_i \\cdot p_{data}(x) dx\\] <p>Using the chain rule: \\(s_{data}(x)_i \\cdot p_{data}(x) = \\frac{\\partial}{\\partial x_i} p_{data}(x)\\), we get:</p> \\[= \\int s_\\theta(x)_i \\cdot \\frac{\\partial}{\\partial x_i} p_{data}(x) dx\\] <p>Using integration by parts (assuming boundary terms vanish):</p> \\[= -\\int \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\cdot p_{data}(x) dx = -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\right]\\] <p>Why do the boundary terms vanish in the multivariate case?</p> <p>For each component \\(i\\), the boundary term is:</p> \\[\\left. s_\\theta(x)_i \\cdot p_{data}(x) \\right|_{x_i = -\\infty}^{x_i = \\infty}\\] <p>This vanishes under similar assumptions as the univariate case:</p> <ol> <li>Data distribution decay: \\(p_{data}(x) \\to 0\\) as \\(\\|x\\| \\to \\infty\\) in any direction</li> <li>Model score boundedness: Each component \\(s_\\theta(x)_i\\) grows at most polynomially as \\(\\|x\\| \\to \\infty\\)</li> <li>Product decay: The product \\(s_\\theta(x)_i \\cdot p_{data}(x) \\to 0\\) as \\(\\|x\\| \\to \\infty\\) for each component</li> </ol> <p>These assumptions ensure that the boundary terms vanish for all components, allowing us to apply integration by parts component-wise.</p> <p>Summing over all components:</p> \\[\\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)_i \\cdot s_{data}(x)_i \\right] = -\\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\right] = -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] <p>where \\(\\text{tr}(\\nabla_x s_\\theta(x)) = \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i\\) is the trace of the Jacobian matrix of the score function.</p> <p>Substituting back into the original objective:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x)\\|^2 + \\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x)) \\right] + \\text{constant}\\] <p>where the constant term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}} \\left[ \\|\\nabla_x \\log p_{data}(x)\\|^2 \\right]\\) doesn't depend on \\(\\theta\\) and can be ignored during optimization.</p> <p>Key Insight: The multivariate case introduces the trace of the Hessian matrix \\(\\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x))\\).</p>"},{"location":"ai/deep_generative_models/score_based_models/#score-matching-algorithm","title":"Score Matching Algorithm","text":"<p>The score matching algorithm follows these steps:</p> <p>Sample a mini-batch of datapoints: \\(\\{x_1, x_2, \\ldots, x_n\\} \\sim p_{data}(x)\\)</p> <p>Estimate the score matching loss with the empirical mean: </p> \\[\\mathcal{L}_{SM}(\\theta) \\approx \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x_i)\\|^2 + \\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x_i)) \\right]\\] <p>Stochastic gradient descent: Update parameters using gradients of the estimated loss</p> <p>Advantages: * No need to sample from EBM: Unlike other training methods for energy-based models, score matching doesn't require generating samples from the model during training. This avoids the computational expense and potential instability of MCMC sampling. * Direct optimization: The objective directly measures how well the model's score function matches the data distribution's score function. * Theoretically sound: Score matching provides a consistent estimator under mild conditions.</p> <p>Disadvantages: * Computing the Hessian is expensive: The term \\(\\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x))\\) requires computing second derivatives, which scales quadratically with the input dimension and can be computationally prohibitive for large models. * Memory requirements: Storing and computing Hessians for large neural networks requires significant memory. * Numerical instability: Second derivatives can be numerically unstable, especially for deep networks.</p> <p>Computational Complexity: For a model with \\(d\\) input dimensions and \\(m\\) parameters, computing the Hessian trace requires \\(O(d^2 \\cdot m)\\) operations, making it impractical for high-dimensional data like images.</p>"},{"location":"ai/deep_generative_models/score_based_models/#recap-distances-for-training-ebms","title":"Recap: Distances for Training EBMs","text":"<p>When training Energy-Based Models, we need to measure how close our model distribution \\(p_\\theta(x)\\) is to the data distribution \\(p_{data}(x)\\). Here are the main approaches:</p>"},{"location":"ai/deep_generative_models/score_based_models/#contrastive-divergence","title":"Contrastive Divergence","text":"<p>Contrastive divergence measures the difference between the data distribution and the model distribution using KL divergence:</p> \\[\\mathcal{L}_{CD}(\\theta) = D_{KL}(p_{data}(x) \\| p_\\theta(x)) - D_{KL}(p_\\theta(x) \\| p_{data}(x))\\] <p>Key insight: This objective encourages the model to match the data distribution while preventing mode collapse.</p> <p>Challenge: Computing the KL divergence requires sampling from the model distribution \\(p_\\theta(x)\\), which is typically done using MCMC methods like Langevin dynamics or Hamiltonian Monte Carlo.</p>"},{"location":"ai/deep_generative_models/score_based_models/#fisher-divergence-score-matching","title":"Fisher Divergence (Score Matching)","text":"<p>Fisher divergence measures the difference between the score functions (gradients of log densities) of the two distributions:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x) - \\nabla_x \\log p_{data}(x)\\|^2 \\right]\\] <p>Key insight: Instead of comparing probability densities directly, we compare their gradients, which avoids the need to compute the intractable partition function.</p> <p>Advantage: No need to sample from the model during training, making it computationally more efficient than contrastive divergence.</p> <p>Challenge: Requires computing second derivatives (Hessian) of the log probability, which can be expensive for high-dimensional data.</p>"},{"location":"ai/deep_generative_models/score_based_models/#noise-contrastive-estimation","title":"Noise Contrastive Estimation","text":"<p>Learning an EBM by contrasting it with a noise distribution.</p> <p>We have the data distribution \\(p_{data}(x)\\). We have the noise distribution \\(p_n(x)\\) which should be analytically tractable and easy to sample from. We can train a discriminator \\(D(x) \\in [0, 1]\\) to distinguish between data samples and noise samples.</p>"},{"location":"ai/deep_generative_models/score_based_models/#optimal-discriminator","title":"Optimal Discriminator","text":"<p>The optimal discriminator \\(D^*(x)\\) that maximizes this objective is given by:</p> \\[D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\]"},{"location":"ai/deep_generative_models/score_based_models/#parameterizing-the-discriminator-as-an-ebm","title":"Parameterizing the Discriminator as an EBM","text":"<p>Key Insight: Instead of training a separate discriminator, we can parameterize it directly in terms of an Energy-Based Model.</p> <p>Let's define a parameterized version of the discriminator as:</p> \\[D_\\theta(x) = \\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)}\\] <p>where \\(p_\\theta(x) = \\frac{1}{Z(\\theta)} e^{f_\\theta(x)}\\) is our Energy-Based Model.</p> <p>Implicit Learning of the Data Distribution</p> <p>By training the discriminator \\(D_\\theta(x)\\) to distinguish between data samples and noise samples, we are implicitly learning the Energy-Based Model \\(p_\\theta(x)\\) to approximate the true data distribution \\(p_{data}(x)\\).</p> <p>Why This Works:</p> <p>Recall that the optimal discriminator (when trained to perfection) satisfies:</p> \\[D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\] <p>But we've parameterized our discriminator as:</p> \\[D_\\theta(x) = \\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)}\\] <p>The Key Insight: When we train \\(D_\\theta(x)\\) to match the optimal discriminator \\(D^*(x)\\), we're essentially forcing:</p> \\[\\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)} \\approx \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\] <p>This equality holds if and only if \\(p_\\theta(x) \\approx p_{data}(x)\\) (assuming \\(p_n(x) &gt; 0\\) everywhere).</p> <p>Mathematical Justification:</p> <p>If \\(\\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)} = \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\), then:</p> \\[p_\\theta(x) \\cdot (p_{data}(x) + p_n(x)) = p_{data}(x) \\cdot (p_\\theta(x) + p_n(x))\\] \\[p_\\theta(x) \\cdot p_{data}(x) + p_\\theta(x) \\cdot p_n(x) = p_{data}(x) \\cdot p_\\theta(x) + p_{data}(x) \\cdot p_n(x)\\] \\[p_\\theta(x) \\cdot p_n(x) = p_{data}(x) \\cdot p_n(x)\\] <p>Since \\(p_n(x) &gt; 0\\), we can divide both sides to get:</p> \\[p_\\theta(x) = p_{data}(x)\\] <p>Modeling the Partition Function as a Trainable Parameter</p> <p>The EBM Equation:</p> <p>Our Energy-Based Model is defined as:</p> \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} e^{f_\\theta(x)}\\] <p>where \\(f_\\theta(x)\\) is the energy function (neural network) and \\(Z(\\theta) = \\int e^{f_\\theta(x)} dx\\) is the partition function.</p> <p>The Partition Function Constraint Problem:</p> <p>The constraint \\(Z(\\theta) = \\int e^{f_\\theta(x)} dx\\) is computationally intractable to satisfy exactly because:</p> <ol> <li>High-dimensional integration: Computing \\(\\int e^{f_\\theta(x)} dx\\) over high-dimensional spaces is extremely expensive</li> <li>No closed form: For complex energy functions, there's no analytical solution</li> <li>Dynamic updates: The integral changes every time we update the energy function parameters</li> </ol> <p>Solution: Treat Z as a Trainable Parameter</p> <p>Instead of enforcing the constraint, we model \\(Z(\\theta)\\) as an additional trainable parameter \\(Z\\) that is not explicitly constrained to satisfy \\(Z = \\int e^{f_\\theta(x)} dx\\).</p> <p>This gives us the modified EBM:</p> \\[p_{\\theta, Z}(x) = \\frac{e^{f_\\theta(x)}}{Z}\\] <p>Why Z Converges to the Correct Partition Function:</p> <p>As we train \\(p_{\\theta, Z}(x)\\) to approximate \\(p_{data}(x)\\), the parameter \\(Z\\) automatically converges to the correct partition function value.</p> <p>Mathematical Justification:</p> <p>When training converges, we have \\(p_{\\theta, Z}(x) \\approx p_{data}(x)\\). This means:</p> \\[\\frac{e^{f_\\theta(x)}}{Z} \\approx p_{data}(x)\\] <p>A direct argument comes from the fact that \\(p_{\\theta, Z}(x)\\) must approximate \\(p_{data}(x)\\), which must integrate to 1:</p> \\[\\int p_{\\theta, Z}(x) dx = \\int \\frac{e^{f_\\theta(x)}}{Z} dx \\approx 1\\] <p>This immediately gives us:</p> \\[Z \\approx \\int e^{f_\\theta(x)} dx\\] <p>Deriving the Discriminator for the Modified EBM</p> <p>Now let's derive the discriminator \\(D_{\\theta, Z}(x)\\) for our modified EBM \\(p_{\\theta, Z}(x) = \\frac{e^{f_\\theta(x)}}{Z}\\).</p> <p>Starting with the discriminator definition:</p> \\[D_{\\theta, Z}(x) = \\frac{p_{\\theta, Z}(x)}{p_{\\theta, Z}(x) + p_n(x)}\\] <p>Substituting our modified EBM:</p> \\[D_{\\theta, Z}(x) = \\frac{\\frac{e^{f_\\theta(x)}}{Z}}{\\frac{e^{f_\\theta(x)}}{Z} + p_n(x)}\\] \\[D_{\\theta, Z}(x) = \\frac{e^{f_\\theta(x)}}{e^{f_\\theta(x)} + Z \\cdot p_n(x)}\\] <p>Noise Contrastive Estimation Training Objective</p> <p>The NCE objective maximizes the log-likelihood of correctly classifying data vs noise samples:</p> \\[\\mathcal{L}_{NCE}(\\theta, Z) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log D_{\\theta, Z}(x) \\right] + \\mathbb{E}_{x \\sim p_n} \\left[ \\log(1 - D_{\\theta, Z}(x)) \\right]\\] <p>In theory, we could have any noise distribution to make this work. But in pratice, a noise distribution that similar (if we can manage) to the data distribution works very well. At the end of the day you learn an EBM and you learn a partition function. In the limit of infinite data and perfect optimization, the EBM matches the data distribution and Z matches the true partition function of the EBM.</p> <p>There is no evolving Generator like we had in GAN. The generator here is fixed, which is the noise distribution. We are training a special Discriminator.</p> <p>Note: The NCE objective function does not guide us to sample any data. We still need to use something like MCMC (e.g., Langevin dynamics, Hamiltonian Monte Carlo) to generate samples from the trained EBM. NCE only provides a way to train the energy function and partition function without computing the intractable partition function integral.</p> <p>Substituting our discriminator:</p> \\[\\mathcal{L}_{NCE}(\\theta, Z) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log \\frac{e^{f_\\theta(x)}}{e^{f_\\theta(x)} + Z \\cdot p_n(x)} \\right] + \\mathbb{E}_{x \\sim p_n} \\left[ \\log \\frac{Z \\cdot p_n(x)}{e^{f_\\theta(x)} + Z \\cdot p_n(x)} \\right]\\] <p>Using the sigmoid formulation with \\(h_{\\theta, Z}(x) = f_\\theta(x) - \\log p_n(x) - \\log Z\\):</p> \\[\\mathcal{L}_{NCE}(\\theta, Z) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log \\sigma(h_{\\theta, Z}(x)) \\right] + \\mathbb{E}_{x \\sim p_n} \\left[ \\log(1 - \\sigma(h_{\\theta, Z}(x))) \\right]\\] <p>This is exactly the binary cross-entropy loss for a binary classifier that distinguishes between data and noise samples.</p> <p>Training Algorithm:</p> <ol> <li> <p>Sample data batch: \\(\\{x_1, x_2, \\ldots, x_n\\} \\sim p_{data}(x)\\)</p> </li> <li> <p>Sample noise batch: \\(\\{\\tilde{x}_1, \\tilde{x}_2, \\ldots, \\tilde{x}_n\\} \\sim p_n(x)\\)</p> </li> <li> <p>Compute logits: \\(h_{\\theta, Z}(x_i)\\) and \\(h_{\\theta, Z}(\\tilde{x}_i)\\)</p> </li> <li> <p>Compute binary cross-entropy loss</p> </li> <li> <p>Update both \\(\\theta\\) (energy function parameters) and \\(Z\\) (partition function parameter) via gradient descent</p> </li> </ol> <p>Key Advantage: Unlike other EBM training methods (like contrastive divergence), NCE does not require sampling from the EBM during the training process. This eliminates the computational expense and potential instability of MCMC sampling during training, making NCE much more efficient and stable.</p>"},{"location":"ai/deep_generative_models/score_based_models/#comparing-nce-and-gan","title":"Comparing NCE and GAN","text":"<p>Both NCE and GANs use binary classification objectives to train generative models, but they differ significantly in their approach and properties.</p> <p>Similarities</p> <ol> <li>Binary Classification Objective: Both use binary cross-entropy loss to distinguish between real and fake samples</li> <li>No Likelihood Computation: Neither requires computing or maximizing explicit likelihood</li> <li>Stable Training: Both avoid the computational challenges of direct likelihood-based training</li> </ol> <p>Key Differences</p> Aspect NCE GAN Generator Fixed noise distribution \\(p_n(x)\\) Learnable generator network \\(G_\\phi(z)\\) Discriminator Parameterized as EBM: \\(D_{\\theta,Z}(x) = \\frac{e^{f_\\theta(x)}}{e^{f_\\theta(x)} + Z \\cdot p_n(x)}\\) Separate neural network \\(D_\\theta(x)\\) Training Single objective: \\(\\mathcal{L}_{NCE}(\\theta, Z)\\) Min-max game: \\(\\min_G \\max_D \\mathcal{L}_{GAN}(G, D)\\) Sampling Requires MCMC after training Direct sampling via generator Mode Coverage Depends on noise distribution choice Can adapt to cover all data modes Convergence Single optimization problem Requires careful balance between generator and discriminator <p>When to Use Each</p> <p>Use NCE when: - You need interpretable energy functions - Training stability is crucial - You want theoretical guarantees - You can afford MCMC sampling at inference time</p> <p>Use GAN when: - Fast sampling is required - You need high-quality, diverse samples - You have computational resources for adversarial training - You want to avoid MCMC entirely</p>"},{"location":"ai/deep_generative_models/score_based_models/#training-score-based-models","title":"Training Score Based Models","text":"<p>Is Score Matching Limited to EBMs?</p> <p>No, score matching is not limited to Energy-Based Models. We can use score matching for other generative model types as well:</p> <ul> <li>Autoregressive Models: Can be trained using score matching</li> <li>Normalizing Flow Models: Can also be trained using score matching</li> <li>Variational Autoencoders: Score matching can be applied to VAEs</li> </ul> <p>But what's the point since likelihoods are tractable?</p> <p>For models like autoregressive models and normalizing flows, the likelihood is indeed tractable, so we could use maximum likelihood estimation (MLE) instead of score matching. However, in principle, we could still train these models using score matching.</p> <p>Practical Considerations: MLE is often preferred when likelihood is tractable because it's more direct and efficient. Score matching might be useful when the likelihood computation is numerically unstable</p> <p>The core ddea of Score-Based Models</p> <p>The fundamental insight behind score-based models is that instead of modeling the energy function or probability density directly, we model the score function \\(s_\\theta(x)\\).</p> <p>What is the Score Function?</p> <p>The score function is the gradient of the log probability density:</p> \\[s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\] <p>Direct Modeling Approach:</p> <p>Instead of learning an energy function \\(f_\\theta(x)\\) and computing \\(s_\\theta(x) = \\nabla_x f_\\theta(x)\\), we directly model:</p> \\[s_\\theta(x): \\mathbb{R}^d \\rightarrow \\mathbb{R}^d\\] <p>This is a vector-valued function that maps from the data space to the same space, representing the gradient field.</p> <p>Key Properties:</p> <ol> <li>Vector Field: \\(s_\\theta(x)\\) is a vector field that assigns a gradient vector to each point \\(x\\) in the data space</li> <li>No Partition Function: We don't need to compute or approximate the partition function \\(Z(\\theta)\\)</li> <li>Direct Approximation: \\(s_\\theta(x) \\approx \\nabla_x \\log p_{data}(x)\\)</li> </ol> <p></p> <p>Deriving the Score Matching Objective</p> <p>Fisher Divergence objective:</p> <p>We want to minimize the Fisher divergence between the data distribution and the distribution induced by our score function:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x) - s_{data}(x)\\|^2 \\right]\\] <p>This measures how well our learned score function \\(s_\\theta(x)\\) approximates the true score function \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\).</p> <p>The Challenge:</p> <p>We don't have access to \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\) since we only have samples from \\(p_{data}(x)\\), not its analytical form.</p> <p>We can rewrite the Fisher divergence to avoid needing the true score function. Let's expand the squared norm:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x)\\|^2 - s_\\theta(x)^T s_{data}(x) + \\frac{1}{2} \\|s_{data}(x)\\|^2 \\right]\\] <p>The key insight is to handle the cross term \\(s_\\theta(x)^T s_{data}(x)\\) using integration by parts.</p> <p>For the univariate case (\\(x \\in \\mathbb{R}\\)), we have:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x) \\cdot s_{data}(x) \\right] = \\int s_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\cdot p_{data}(x) dx\\] \\[= \\int s_\\theta(x) \\cdot \\frac{d}{dx} p_{data}(x) dx\\] <p>Using integration by parts: \\(\\int u \\cdot \\frac{d}{dx} v \\, dx = u \\cdot v - \\int \\frac{d}{dx} u \\cdot v \\, dx\\)</p> <p>Setting \\(u = s_\\theta(x)\\) and \\(v = p_{data}(x)\\):</p> \\[= \\left. s_\\theta(x) \\cdot p_{data}(x) \\right|_{-\\infty}^{\\infty} - \\int \\frac{d}{dx} s_\\theta(x) \\cdot p_{data}(x) dx\\] <p>Assuming the boundary term vanishes (reasonable for well-behaved distributions):</p> \\[= -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d}{dx} s_\\theta(x) \\right]\\] <p>Multivariate Case:</p> <p>For \\(x \\in \\mathbb{R}^d\\), we apply integration by parts component-wise:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)^T s_{data}(x) \\right] = \\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)_i \\cdot s_{data}(x)_i \\right]\\] \\[= -\\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\right]\\] \\[= -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] <p>where \\(\\text{tr}(\\nabla_x s_\\theta(x)) = \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i\\) is the trace of the Jacobian matrix.</p> <p>Final Score Matching Objective:</p> <p>Substituting back into the Fisher divergence:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x)\\|^2 + \\text{tr}(\\nabla_x s_\\theta(x)) \\right] + \\text{constant}\\] <p>where the constant term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}} \\left[ \\|s_{data}(x)\\|^2 \\right]\\) doesn't depend on \\(\\theta\\) and can be ignored during optimization.</p> <p>Key Insight:</p> <p>This reformulation allows us to train the score function using only samples from \\(p_{data}(x)\\) and the derivatives of our score model, without needing access to the true score function \\(s_{data}(x)\\).</p> <p>The computational cost of the second term makes score matching challenging for high-dimensional data, which motivates alternative approaches like denoising score matching and sliced score matching.</p>"},{"location":"ai/deep_generative_models/score_based_models/#denoising-score-matching","title":"Denoising Score Matching","text":"<p>Denoising score matching addresses the computational challenges of standard score matching by adding noise to the data.</p> <p>The Key Idea:</p> <p>Instead of trying to learn the score function of the original data distribution \\(p_{data}(x)\\), we learn the score function of a noisy version of the data.</p> <p>Noise Distribution:</p> <p>We define a noise distribution \\(q_\\sigma(\\tilde{x} | x)\\) that adds noise to clean data points. A common choice is Gaussian noise:</p> \\[q_\\sigma(\\tilde{x} | x) = \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)\\] <p>This means: \\(\\tilde{x} = x + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\)</p> <p>Noisy Data Distribution:</p> <p>The noisy data distribution is the convolution of the original data distribution with the noise:</p> \\[q_\\sigma(\\tilde{x}) = \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx\\] <p>The denoising score matching objective minimizes the Fisher divergence between the noise perturbed data distribution \\(q_\\sigma(\\tilde{x})\\) and our score model \\(s_\\theta(\\tilde{x})\\):</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\frac{1}{2} \\|s_\\theta(\\tilde{x}) - \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right]\\] \\[\\mathcal{L}_{DSM}(\\theta) = \\int q_\\sigma(\\tilde{x}) \\left[ \\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2 - s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) + \\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right] d\\tilde{x}\\] <p>Focusing on the Cross Term:</p> <p>The cross term \\(-s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\) is the challenging part. First, let's write the cross term as an integral:</p> \\[\\int q_\\sigma(\\tilde{x}) \\left[ -s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) \\right] d\\tilde{x}\\] <p>Using the chain rule: \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) \\cdot q_\\sigma(\\tilde{x}) = \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x})\\), we get:</p> \\[= -\\int s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x}) d\\tilde{x}\\] <p>Substituting the noisy data distribution:</p> <p>Recall that \\(q_\\sigma(\\tilde{x}) = \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx\\). Substituting this into the integral:</p> \\[= -\\int s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\left[ \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx \\right] d\\tilde{x}\\] <p>Since the gradient operator \\(\\nabla_{\\tilde{x}}\\) acts only on \\(\\tilde{x}\\) and not on \\(x\\), we can interchange the gradient and the integral over \\(x\\):</p> \\[= -\\int s_\\theta(\\tilde{x})^T \\left[ \\int \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x} | x) p_{data}(x) dx \\right] d\\tilde{x}\\] <p>We can rearrange this as a double integral:</p> \\[= -\\iint s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x} | x) \\cdot p_{data}(x) \\, dx \\, d\\tilde{x}\\] <p>Now we can use the chain rule in reverse: \\(\\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x} | x) = \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) \\cdot q_\\sigma(\\tilde{x} | x)\\)</p> \\[= -\\iint s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) \\cdot q_\\sigma(\\tilde{x} | x) \\cdot p_{data}(x) \\, dx \\, d\\tilde{x}\\] <p>Final Expression:</p> <p>This can be written as an expectation:</p> \\[= -\\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) \\right]\\] <p>Or equivalently:</p> \\[= -\\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right]\\] <p>Completing the Denoising Score Matching Objective:</p> <p>Now let's bring this back to the complete objective function. Recall that we started with:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\int q_\\sigma(\\tilde{x}) \\left[ \\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2 - s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) + \\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right] d\\tilde{x}\\] <p>We've derived the cross term. Now let's handle all three terms:</p> <p>Term 1: \\(\\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2\\)</p> \\[\\int q_\\sigma(\\tilde{x}) \\cdot \\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2 d\\tilde{x} = \\frac{1}{2} \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right]\\] <p>Substituting \\(q_\\sigma(\\tilde{x}) = \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx\\):</p> \\[= \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right]\\] <p>Term 2: Cross term (already derived)</p> \\[- \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right]\\] <p>Term 3: \\(\\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2\\)</p> \\[\\int q_\\sigma(\\tilde{x}) \\cdot \\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 d\\tilde{x} = \\frac{1}{2} \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right]\\] <p>This term is a constant with respect to \\(\\theta\\) and can be ignored during optimization.</p> <p>Combining all terms:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right] - \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right] + \\text{const}\\] <p>Simplifying to the final form:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right] - \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right] + \\text{const}\\] <p>To see how this becomes the final form, let's expand the squared difference:</p> \\[\\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2 = \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\|^2 - 2 \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) + \\|s_\\theta(\\tilde{x})\\|^2\\] <p>Taking the expectation and multiplying by \\(\\frac{1}{2}\\):</p> \\[\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2 \\right] = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right] - \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right] + \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\|^2 \\right]\\] <p>The last term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\|^2 \\right]\\) is a constant with respect to \\(\\theta\\) and can be absorbed into the constant term.</p> <p>This can be written as:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2 \\right] + \\text{const}\\] <p>Key Advantage: easy computation of the Target Score Function</p> <p>The major advantage of denoising score matching is that \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\) is easy to compute analytically, unlike the true data score function \\(\\nabla_x \\log p_{data}(x)\\).</p> <p>For Gaussian Noise:</p> <p>The most common choice is Gaussian noise: \\(q_\\sigma(\\tilde{x} | x) = \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)\\)</p> <p>The log probability is:</p> \\[\\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{2\\sigma^2} \\|\\tilde{x} - x\\|^2 - \\frac{d}{2} \\log(2\\pi\\sigma^2)\\] <p>Taking the gradient with respect to \\(\\tilde{x}\\):</p> \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\] <p>This gradient is analytically tractable and computationally cheap.</p> <p>Comparison with Standard Score Matching:</p> <p>In standard score matching, we need to compute:</p> <ul> <li> <p>\\(\\nabla_x \\log p_\\theta(x)\\) (our model's score function)</p> </li> <li> <p>\\(\\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x))\\) (Hessian trace - expensive!)</p> </li> </ul> <p>In denoising score matching, we only need:</p> <ul> <li> <p>\\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\) (known analytically)</p> </li> <li> <p>\\(s_\\theta(\\tilde{x})\\) (our model's score function)</p> </li> </ul> <p>Training Algorithm:</p> <ol> <li>Sample clean data: \\(x \\sim p_{data}(x)\\)</li> <li>Add noise: \\(\\tilde{x} = x + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\)</li> <li>Compute target: \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x) = -\\frac{1}{\\sigma^2} \\epsilon\\)</li> <li>Compute prediction: \\(s_\\theta(\\tilde{x})\\)</li> <li>Minimize: \\(\\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2\\)</li> </ol> <p>Monte Carlo Estimation:</p> <p>For a batch of \\(N\\) samples \\(\\{x_1, x_2, \\ldots, x_N\\}\\), the Monte Carlo estimate of the loss is:</p> \\[\\mathcal{L}_{DSM}(\\theta) \\approx \\frac{1}{2N} \\sum_{i=1}^N \\|\\nabla_{\\tilde{x}_i} \\log q_\\sigma(\\tilde{x}_i | x_i) - s_\\theta(\\tilde{x}_i)\\|^2\\] <p>where \\(\\tilde{x}_i = x_i + \\epsilon_i\\) with \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I)\\).</p> <p>Practical Implementation:</p> <pre><code># For a batch of data\ndef denoising_score_matching_loss(model, data_batch, sigma):\n    # Add noise to data\n    noise = torch.randn_like(data_batch) * sigma\n    noisy_data = data_batch + noise\n\n    # Compute target score (gradient of log noise distribution)\n    target_score = -noise / (sigma ** 2)\n\n    # Compute model prediction\n    predicted_score = model(noisy_data)\n\n    # Compute loss\n    loss = 0.5 * torch.mean((target_score - predicted_score) ** 2)\n\n    return loss\n</code></pre> <p>Intuition behind the loss function</p> <p>The denoising score matching objective has a beautiful intuition: we're teaching our model to estimate the score function of noisy data, and when the noise is very small, this approximates the score function of the clean data distribution.</p> <p>The Core Idea:</p> <ol> <li> <p>Learning Noisy Data Structure: Instead of trying to learn the score function of the complex, unknown data distribution \\(p_{data}(x)\\), we learn the score function of a simpler, known noisy distribution \\(q_\\sigma(\\tilde{x})\\).</p> </li> <li> <p>Denoising as Learning: By learning to predict \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\), our model learns to \"point\" from noisy points \\(\\tilde{x}\\) back toward their clean counterparts \\(x\\).</p> </li> </ol> <p>What does \"point\" mean?</p> <p>The score function \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\) is a vector field that assigns a direction vector to each point \\(\\tilde{x}\\) in the data space. This vector \"points\" in the direction of steepest increase in the log probability.</p> <p>For Gaussian noise, this vector is:</p> \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\] <p>Interpretation:</p> <ul> <li> <p>Direction: The vector points from the noisy point \\(\\tilde{x}\\) toward the clean point \\(x\\)</p> </li> <li> <p>Magnitude: The length of the vector is proportional to the distance between \\(\\tilde{x}\\) and \\(x\\), scaled by \\(\\frac{1}{\\sigma^2}\\)</p> </li> <li> <p>Purpose: This vector tells us \"if you want to increase the probability of this noisy point, move in this direction\"</p> </li> </ul> <p>Visual Example: Imagine a 2D space where:</p> <ul> <li> <p>Clean point \\(x = (0, 0)\\)</p> </li> <li> <p>Noisy point \\(\\tilde{x} = (1, 1)\\) </p> </li> <li> <p>Noise level \\(\\sigma = 1\\)</p> </li> </ul> <p>The score vector at \\(\\tilde{x}\\) is:</p> \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -(1, 1)\\] <p>This vector points from \\((1, 1)\\) toward \\((0, 0)\\), indicating the direction to move to increase the probability of the noisy point under the noise distribution.</p> <p>Why this matters: When our model learns to predict this vector, it's learning to identify the direction that leads back to the clean data. This implicitly teaches it about the local structure of the data manifold - where the \"good\" data points are located relative to any given noisy point.</p> <p>Why this is a denoiser:</p> <p>The score function \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\) acts as a denoiser because it provides the exact direction and magnitude needed to remove the noise from a noisy data point.</p> <p>When our model learns to predict this score function, it's learning to:</p> <ul> <li> <p>Identify noise: Recognize what part of the data is noise</p> </li> <li> <p>Compute denoising direction: Determine which direction to move to remove the noise</p> </li> <li> <p>Estimate noise magnitude: Understand how much to move in that direction</p> </li> </ul> <p>This is why denoising score matching is so powerful - by learning to denoise, the model implicitly learns the structure of the clean data distribution, even though it never directly sees the clean data score function.</p> <p>Con: Estimates the score of the noise-perturbed data, not the score of the actual data. We have shifted the goal post basically.</p> <p>Generating Samples with MCMC:</p> <p>Once we've trained our score function \\(s_\\theta(x)\\), we can generate samples using Markov Chain Monte Carlo (MCMC) methods, typically Langevin dynamics.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/","title":"Variational autoencoders","text":""},{"location":"ai/deep_generative_models/variational_autoencoders/#representation","title":"Representation","text":"<p>Consider a directed, latent variable model as shown below.</p> <p></p> <p>In the model above, \\(z\\) and \\(x\\) denote the latent and observed variables respectively. The joint distribution expressed by this model is given as</p> \\[p(x,z) = p(x|z)p(z).\\] <p>From a generative modeling perspective, this model describes a generative process for the observed data \\(x\\) using the following procedure:</p> \\[z \\sim p(z)\\] \\[x \\sim p(x|z)\\] <p>If one adopts the belief that the latent variables \\(z\\) somehow encode semantically meaningful information about \\(x\\), it is natural to view this generative process as first generating the \"high-level\" semantic information about \\(x\\) first before fully generating \\(x\\).</p> <p>We now consider a family of distributions \\(\\mathcal{Z}\\) where \\(p(z) \\in \\mathcal{Z}\\) describes a probability distribution over \\(z\\). Next, consider a family of conditional distributions \\(\\mathcal{X|Z}\\) where \\(p(x|z) \\in \\mathcal{X|Z}\\) describes a conditional probability distribution over \\(x\\) given \\(z\\). Then our hypothesis class of generative models is the set of all possible combinations</p> \\[\\mathcal{X,Z} = \\{p(x,z) \\mid p(z) \\in \\mathcal{Z}, p(x|z) \\in \\mathcal{X|Z}\\}.\\] <p>Given a dataset \\(\\mathcal{D} = \\{x^{(1)}, \\ldots, x^{(n)}\\}\\), we are interested in the following learning and inference tasks:</p> <ol> <li>Selecting \\(p \\in \\mathcal{X,Z}\\) that \"best\" fits \\(\\mathcal{D}\\).</li> <li>Given a sample \\(x\\) and a model \\(p \\in \\mathcal{X,Z}\\), what is the posterior distribution over the latent variables \\(z\\)?</li> </ol> <p>The posterior distribution \\(p(z|x)\\) represents our updated beliefs about the latent variables \\(z\\) after observing the data \\(x\\). In other words, it tells us what values of \\(z\\) are most likely to have generated the observed \\(x\\). This is particularly useful for tasks like feature extraction, where we want to understand what latent factors might have generated our observed data.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#learning-directed-latent-variable-models","title":"Learning Directed Latent Variable Models","text":"<p>One way to measure how closely \\(p(x,z)\\) fits the observed dataset \\(\\mathcal{D}\\) is to measure the Kullback-Leibler (KL) divergence between the data distribution (which we denote as \\(p_{data}(x)\\)) and the model's marginal distribution \\(p(x) = \\int p(x,z)dz\\). The distribution that \"best\" fits the data is thus obtained by minimizing the KL divergence.</p> \\[\\min_{p \\in \\mathcal{X,Z}} D_{KL}(p_{data}(x) \\| p(x)).\\] <p>As we have seen previously, optimizing an empirical estimate of the KL divergence is equivalent to maximizing the marginal log-likelihood \\(\\log p(x)\\) over \\(\\mathcal{D}\\):</p> \\[\\max_{p \\in \\mathcal{X,Z}} \\sum_{x \\in \\mathcal{D}} \\log p(x) = \\max_{p \\in \\mathcal{X,Z}} \\sum_{x \\in \\mathcal{D}} \\log \\int p(x,z)dz.\\] <p>However, it turns out this problem is generally intractable for high-dimensional \\(z\\) as it involves an integration (or sums in the case \\(z\\) is discrete) over all the possible latent sources of variation \\(z\\). This intractability arises from several challenges:</p> <ol> <li> <p>Computational Complexity: The integral \\(\\int p(x,z)dz\\) requires evaluating the joint distribution \\(p(x,z)\\) for all possible values of \\(z\\). In high-dimensional spaces, this becomes computationally prohibitive as the number of points to evaluate grows exponentially with the dimension of \\(z\\).</p> </li> <li> <p>Numerical Integration: Even if we could evaluate the integrand at all points, computing the integral numerically becomes increasingly difficult as the dimension of \\(z\\) grows. Traditional numerical integration methods like quadrature become impractical in high dimensions.</p> </li> <li> <p>Posterior Inference: The intractability of the marginal likelihood also makes it difficult to compute the posterior distribution \\(p(z|x)\\), which is crucial for tasks like feature extraction and data generation.</p> </li> </ol> <p>This intractability motivates the need for approximate inference methods, such as variational inference. One option is to estimate the objective via Monte Carlo. For any given datapoint \\(x\\), we can obtain the following estimate for its marginal log-likelihood:</p> \\[\\log p(x) \\approx \\log \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>This Monte Carlo estimate is derived as follows:</p> <p>First, recall that the marginal likelihood \\(p(x)\\) can be written as an expectation:</p> \\[p(x) = \\int p(x|z)p(z)dz = \\mathbb{E}_{z \\sim p(z)}[p(x|z)]\\] <p>The Monte Carlo method approximates this expectation by drawing \\(k\\) samples from \\(p(z)\\) and computing their average:</p> \\[\\mathbb{E}_{z \\sim p(z)}[p(x|z)] \\approx \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>Taking the logarithm of both sides gives us our final estimate:</p> \\[\\log p(x) \\approx \\log \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>This approximation becomes more accurate as \\(k\\) increases, but at the cost of more computational resources. The key insight is that we're using random sampling to approximate the intractable integral, trading exact computation for statistical estimation.</p> <p>Rather than maximizing the log-likelihood directly, an alternate is to instead construct a lower bound that is more amenable to optimization. To do so, we note that evaluating the marginal likelihood \\(p(x)\\) is at least as difficult as as evaluating the posterior \\(p(z|x)\\) for any latent vector \\(z\\) since by definition \\(p(z|x) = p(x,z)/p(x)\\).</p> <p>Next, we introduce a variational family \\(\\mathcal{Q}\\) of distributions that approximate the true, but intractable posterior \\(p(z|x)\\). Further henceforth, we will assume a parameteric setting where any distribution in the model family \\(\\mathcal{X,Z}\\) is specified via a set of parameters \\(\\theta \\in \\Theta\\) and distributions in the variational family \\(\\mathcal{Q}\\) are specified via a set of parameters \\(\\lambda \\in \\Lambda\\).</p> <p>Given \\(\\mathcal{X,Z}\\) and \\(\\mathcal{Q}\\), we note that the following relationships hold true for any \\(x\\) and all variational distributions \\(q_\\lambda(z) \\in \\mathcal{Q}\\):</p> \\[\\log p_\\theta(x) = \\log \\int p_\\theta(x,z)dz = \\log \\int \\frac{q_\\lambda(z)}{q_\\lambda(z)}p_\\theta(x,z)dz \\geq \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] := \\text{ELBO}(x;\\theta,\\lambda)\\] <p>where we have used Jensen's inequality in the final step. The key insight here is that since the logarithm function is concave, Jensen's inequality tells us that for any random variable \\(X\\) and concave function \\(f\\), we have \\(\\mathbb{E}[f(X)] \\leq f(\\mathbb{E}[X])\\). In our case:</p> <p>We first multiply and divide by \\(q_\\lambda(z)\\) inside the integral to get:</p> \\[\\log \\int \\frac{q_\\lambda(z)}{q_\\lambda(z)}p_\\theta(x,z)dz = \\log \\int q_\\lambda(z)\\frac{p_\\theta(x,z)}{q_\\lambda(z)}dz\\] <p>The integral \\(\\int q_\\lambda(z)\\frac{p_\\theta(x,z)}{q_\\lambda(z)}dz\\) can be seen as an expectation \\(\\mathbb{E}_{q_\\lambda(z)}\\left[\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\)</p> <p>Since \\(\\log\\) is a concave function, Jensen's inequality gives us:</p> \\[\\log \\mathbb{E}_{q_\\lambda(z)}\\left[\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] \\geq \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\] <p>This inequality is what allows us to obtain a lower bound on the log-likelihood, which we call the Evidence Lower BOund (ELBO). The ELBO admits a tractable unbiased Monte Carlo estimator</p> \\[\\frac{1}{k}\\sum_{i=1}^k \\log\\frac{p_\\theta(x,z^{(i)})}{q_\\lambda(z^{(i)})}, \\text{ where } z^{(i)} \\sim q_\\lambda(z),\\] <p>so long as it is easy to sample from and evaluate densities for \\(q_\\lambda(z)\\).</p> <p>In summary, we can learn a latent variable model by maximizing the ELBO with respect to both the model parameters \\(\\theta\\) and the variational parameters \\(\\lambda\\) for any given datapoint \\(x\\):</p> \\[\\max_\\theta \\sum_{x \\in \\mathcal{D}} \\max_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right].\\] <p>This optimization objective can be broken down into two parts:</p> <ol> <li>Inner Optimization: For each datapoint \\(x\\), we find the best variational parameters \\(\\lambda\\) that make \\(q_\\lambda(z)\\) as close as possible to the true posterior \\(p(z|x)\\). This is done by maximizing the ELBO with respect to \\(\\lambda\\). </li> </ol> <p>Why do we need \\(q_\\lambda(z)\\) to approximate \\(p(z|x)\\)? Since \\(p(x) = p(x,z)/p(z|x)\\), as \\(q_\\lambda(z)\\) tends to \\(p(z|x)\\), the ratio \\(p(x,z)/q_\\lambda(z)\\) tends to \\(p(x)\\). This means that by making our variational approximation closer to the true posterior, we get a better estimate of the marginal likelihood \\(p(x)\\).</p> <ol> <li>Outer Optimization: Across all datapoints in the dataset \\(\\mathcal{D}\\), we find the best model parameters \\(\\theta\\) that maximize the average ELBO. This improves the generative model's ability to explain the data.</li> </ol> <p>The outer sum \\(\\sum_{x \\in \\mathcal{D}}\\) is necessary because we want to learn a model that works well for all datapoints in our dataset, not just a single example. This is equivalent to maximizing the average ELBO across all datapoints.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#black-box-variational-inference","title":"Black-Box Variational Inference","text":"<p>We shall focus on first-order stochastic gradient methods for optimizing the ELBO. This inspires Black-Box Variational Inference (BBVI), a general-purpose Expectation-Maximization-like algorithm for variational learning of latent variable models, where, for each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), the following two steps are performed.</p> <p>Step 1</p> <p>We first do per-sample optimization of \\(q\\) by iteratively applying the update</p> \\[\\lambda^{(i)} \\leftarrow \\lambda^{(i)} + \\tilde{\\nabla}_\\lambda \\text{ELBO}(x^{(i)}; \\theta, \\lambda^{(i)}),\\] <p>where \\(\\text{ELBO}(x; \\theta, \\lambda) = \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\), and \\(\\tilde{\\nabla}_\\lambda\\) denotes an unbiased estimate of the ELBO gradient. This step seeks to approximate the log-likelihood \\(\\log p_\\theta(x^{(i)})\\).</p> <p>Step 2</p> <p>We then perform a single update step based on the mini-batch</p> \\[\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_i \\text{ELBO}(x^{(i)}; \\theta, \\lambda^{(i)}),\\] <p>which corresponds to the step that hopefully moves \\(p_\\theta\\) closer to \\(p_{data}\\).</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#gradient-estimation","title":"Gradient Estimation","text":"<p>The gradients \\(\\nabla_\\lambda \\text{ELBO}\\) and \\(\\nabla_\\theta \\text{ELBO}\\) can be estimated via Monte Carlo sampling. While it is straightforward to construct an unbiased estimate of \\(\\nabla_\\theta \\text{ELBO}\\) by simply pushing \\(\\nabla_\\theta\\) through the expectation operator, the same cannot be said for \\(\\nabla_\\lambda\\). Instead, we see that</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\mathbb{E}_{q_\\lambda(z)}\\left[\\left(\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z)\\right].\\] <p>This equality follows from the log-derivative trick (also commonly referred to as the REINFORCE trick). To derive this, we start with the gradient of the expectation:</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\nabla_\\lambda \\int q_\\lambda(z) \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>Using the product rule and chain rule:</p> \\[= \\int \\nabla_\\lambda q_\\lambda(z) \\cdot \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} + q_\\lambda(z) \\cdot \\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>The second term vanishes because: \\(\\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} = \\nabla_\\lambda [\\log p_\\theta(x,z) - \\log q_\\lambda(z)]\\). Since \\(p_\\theta(x,z)\\) doesn't depend on \\(\\lambda\\), \\(\\nabla_\\lambda \\log p_\\theta(x,z) = 0\\). Therefore, \\(\\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} = -\\nabla_\\lambda \\log q_\\lambda(z)\\).  When we multiply by \\(q_\\lambda(z)\\) and integrate, we get:</p> \\[\\int q_\\lambda(z) \\cdot (-\\nabla_\\lambda \\log q_\\lambda(z)) dz = -\\int \\nabla_\\lambda q_\\lambda(z) dz = -\\nabla_\\lambda \\int q_\\lambda(z) dz = -\\nabla_\\lambda 1 = 0\\] <p>where we used the fact that \\(\\int q_\\lambda(z) dz = 1\\) for any valid probability distribution.</p> <p>For the first term, we use the identity \\(\\nabla_\\lambda q_\\lambda(z) = q_\\lambda(z) \\nabla_\\lambda \\log q_\\lambda(z)\\):</p> \\[= \\int q_\\lambda(z) \\nabla_\\lambda \\log q_\\lambda(z) \\cdot \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>This can be rewritten as an expectation:</p> \\[= \\mathbb{E}_{q_\\lambda(z)}\\left[\\left(\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z)\\right]\\] <p>The gradient estimator \\(\\tilde{\\nabla}_\\lambda \\text{ELBO}\\) is thus</p> \\[\\frac{1}{k}\\sum_{i=1}^k \\left[\\left(\\log\\frac{p_\\theta(x,z^{(i)})}{q_\\lambda(z^{(i)})}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z^{(i)})\\right], \\text{ where } z^{(i)} \\sim q_\\lambda(z).\\] <p>However, it is often noted that this estimator suffers from high variance. One of the key contributions of the variational autoencoder paper is the reparameterization trick, which introduces a fixed, auxiliary distribution \\(p(\\epsilon)\\) and a differentiable function \\(T(\\epsilon; \\lambda)\\) such that the procedure</p> \\[\\epsilon \\sim p(\\epsilon)\\] \\[z \\leftarrow T(\\epsilon; \\lambda),\\] <p>is equivalent to sampling from \\(q_\\lambda(z)\\). This two-step procedure works as follows:</p> <ol> <li>First, we sample \\(\\epsilon\\) from a fixed distribution \\(p(\\epsilon)\\) that doesn't depend on \\(\\lambda\\) (e.g., standard normal)</li> <li>Then, we transform this sample using a deterministic function \\(T(\\epsilon; \\lambda)\\) that depends on \\(\\lambda\\)</li> </ol> <p>The key insight is that if we choose \\(T\\) appropriately, the distribution of \\(z = T(\\epsilon; \\lambda)\\) will be exactly \\(q_\\lambda(z)\\). For example, if \\(q_\\lambda(z)\\) is a normal distribution with mean \\(\\mu_\\lambda\\) and standard deviation \\(\\sigma_\\lambda\\), we can use:</p> <p>\\(p(\\epsilon) = \\mathcal{N}(0, 1)\\)</p> <p>\\(T(\\epsilon; \\lambda) = \\mu_\\lambda + \\sigma_\\lambda \\cdot \\epsilon\\)</p> <p>By the Law of the Unconscious Statistician, we can see that</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\mathbb{E}_{p(\\epsilon)}\\left[\\nabla_\\lambda \\log\\frac{p_\\theta(x,T(\\epsilon; \\lambda))}{q_\\lambda(T(\\epsilon; \\lambda))}\\right].\\] <p>In contrast to the REINFORCE trick, the reparameterization trick is often noted empirically to have lower variance and thus results in more stable training.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#parameterizing-distributions-via-deep-neural-networks","title":"Parameterizing Distributions via Deep Neural Networks","text":"<p>So far, we have described \\(p_\\theta(x,z)\\) and \\(q_\\lambda(z)\\) in the abstract. To instantiate these objects, we consider choices of parametric distributions for \\(p_\\theta(z)\\), \\(p_\\theta(x|z)\\), and \\(q_\\lambda(z)\\). A popular choice for \\(p_\\theta(z)\\) is the unit Gaussian</p> \\[p_\\theta(z) = \\mathcal{N}(z|0,I),\\] <p>in which case \\(\\theta\\) is simply the empty set since the prior is a fixed distribution.</p> <p>In the case where \\(p_\\theta(x|z)\\) is a Gaussian distribution, we can thus represent it as</p> \\[p_\\theta(x|z) = \\mathcal{N}(x|\\mu_\\theta(z), \\Sigma_\\theta(z)),\\] <p>where \\(\\mu_\\theta(z)\\) and \\(\\Sigma_\\theta(z)\\) are neural networks that specify the mean and covariance matrix for the Gaussian distribution over \\(x\\) when conditioned on \\(z\\).</p> <p>Finally, the variational family for the proposal distribution \\(q_\\lambda(z)\\) needs to be chosen judiciously so that the reparameterization trick is possible. Many continuous distributions in the location-scale family can be reparameterized. In practice, a popular choice is again the Gaussian distribution, where</p> \\[\\begin{align*} \\lambda &amp;= (\\mu, \\Sigma) \\\\ q_\\lambda(z) &amp;= \\mathcal{N}(z|\\mu, \\Sigma) \\\\ p(\\varepsilon) &amp;= \\mathcal{N}(z|0,I) \\\\ T(\\varepsilon; \\lambda) &amp;= \\mu + \\Sigma^{1/2}\\varepsilon, \\end{align*}\\] <p>where \\(\\Sigma^{1/2}\\) is the Cholesky decomposition of \\(\\Sigma\\). For simplicity, practitioners often restrict \\(\\Sigma\\) to be a diagonal matrix (which restricts the distribution family to that of factorized Gaussians).</p> <p>The reparameterization trick consists of four key steps:</p> <ol> <li> <p>Parameter Definition: We define the variational parameters \\(\\lambda\\) as a tuple containing the mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\) of our Gaussian distribution. These parameters will be learned during training.</p> </li> <li> <p>Variational Distribution: We specify that our variational distribution \\(q_\\lambda(z)\\) is a Gaussian distribution parameterized by \\(\\mu\\) and \\(\\Sigma\\). This is the distribution we ideally want to sample from.</p> </li> <li> <p>Auxiliary Distribution: Instead of sampling directly from \\(q_\\lambda(z)\\), we introduce a fixed auxiliary distribution \\(p(\\varepsilon)\\) which is a standard normal distribution (mean 0, identity covariance). This distribution doesn't depend on our parameters \\(\\lambda\\).</p> </li> <li> <p>Transformation Function: We define a deterministic function \\(T(\\varepsilon; \\lambda)\\) that transforms samples from the auxiliary distribution into samples from our variational distribution. The transformation is given by \\(\\mu + \\Sigma^{1/2}\\varepsilon\\), where \\(\\Sigma^{1/2}\\) is the Cholesky decomposition of \\(\\Sigma\\).</p> </li> </ol> <p>The key insight is that instead of sampling directly from \\(q_\\lambda(z)\\), we can: 1. Sample \\(\\varepsilon\\) from the standard normal distribution \\(p(\\varepsilon)\\) 2. Transform it using \\(T(\\varepsilon; \\lambda)\\) to make it seem like we're getting a sample from \\(q_\\lambda(z)\\)</p> <p>This trick is crucial because it allows us to compute gradients with respect to \\(\\lambda\\) through the sampling process. Since the transformation \\(T\\) is differentiable, we can backpropagate through it to update the parameters \\(\\lambda\\) during training. This is why the reparameterization trick often leads to lower variance in gradient estimates compared to the REINFORCE trick.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#amortized-variational-inference","title":"Amortized Variational Inference","text":"<p>A noticeable limitation of black-box variational inference is that Step 1 executes an optimization subroutine that is computationally expensive. Recall that the goal of Step 1 is to find</p> \\[\\lambda^* = \\arg\\max_{\\lambda \\in \\Lambda} \\text{ELBO}(x; \\theta, \\lambda).\\] <p>For a given choice of \\(\\theta\\), there is a well-defined mapping from \\(x \\mapsto \\lambda^*\\). A key realization is that this mapping can be learned. In particular, one can train an encoding function (parameterized by \\(\\phi\\)) \\(f_\\phi: \\mathcal{X} \\to \\Lambda\\) (where \\(\\Lambda\\) is the space of \\(\\lambda\\) parameters) on the following objective</p> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, f_\\phi(x)).\\] <p>It is worth noting at this point that \\(f_\\phi(x)\\) can be interpreted as defining the conditional distribution \\(q_\\phi(z|x)\\). With a slight abuse of notation, we define</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right],\\] <p>and rewrite the optimization problem as</p> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, \\phi).\\] <p>It is also worth noting that optimizing \\(\\phi\\) over the entire dataset as a subroutine every time we sample a new mini-batch is clearly not reasonable. However, if we believe that \\(f_\\phi\\) is capable of quickly adapting to a close-enough approximation of \\(\\lambda^*\\) given the current choice of \\(\\theta\\), then we can interleave the optimization of \\(\\phi\\) and \\(\\theta\\). This yields the following procedure, where for each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), we perform the following two updates jointly:</p> \\[\\begin{align*} \\phi &amp;\\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\\\ \\theta &amp;\\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi), \\end{align*}\\] <p>rather than running BBVI's Step 1 as a subroutine. By leveraging the learnability of \\(x \\mapsto \\lambda^*\\), this optimization procedure amortizes the cost of variational inference. If one further chooses to define \\(f_\\phi\\) as a neural network, the result is the variational autoencoder.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#steps-of-amortized-variational-inference","title":"Steps of Amortized Variational Inference","text":"<p>Let's break down the amortized variational inference procedure in detail:</p> <ol> <li>Initial Setup:</li> <li>We have a dataset \\(\\mathcal{D} = \\{x^{(1)}, \\ldots, x^{(n)}\\}\\)</li> <li>We have a generative model \\(p_\\theta(x,z)\\) with parameters \\(\\theta\\)</li> <li> <p>We want to learn both the model parameters \\(\\theta\\) and the variational parameters \\(\\lambda\\) for each datapoint</p> </li> <li> <p>Traditional BBVI Approach:</p> </li> <li>For each datapoint \\(x\\), we would need to run an optimization to find:</li> </ol> \\[\\lambda^* = \\arg\\max_{\\lambda \\in \\Lambda} \\text{ELBO}(x; \\theta, \\lambda)\\] <ul> <li> <p>This is computationally expensive as it requires running an optimization subroutine for each datapoint</p> </li> <li> <p>Key Insight - Learnable Mapping:</p> </li> <li>Instead of optimizing \\(\\lambda\\) separately for each \\(x\\), we realize that there's a mapping from \\(x\\) to \\(\\lambda^*\\)</li> <li>This mapping can be learned using a function \\(f_\\phi: \\mathcal{X} \\to \\Lambda\\) parameterized by \\(\\phi\\)</li> <li> <p>The function \\(f_\\phi\\) takes a datapoint \\(x\\) and outputs the variational parameters \\(\\lambda\\)</p> </li> <li> <p>Training the Encoder:</p> </li> <li>We train \\(f_\\phi\\) to maximize the ELBO across all datapoints:</li> </ul> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, f_\\phi(x))\\] <ul> <li> <p>This is equivalent to learning a conditional distribution \\(q_\\phi(z|x)\\)</p> </li> <li> <p>Joint Optimization:</p> </li> <li>Instead of running BBVI's Step 1 as a subroutine, we interleave the optimization of \\(\\phi\\) and \\(\\theta\\)</li> <li>For each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), we perform two updates:</li> </ul> \\[\\begin{align*} \\phi &amp;\\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\\\ \\theta &amp;\\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\end{align*}\\] <ol> <li>Practical Implementation:</li> <li>When \\(f_\\phi\\) is implemented as a neural network, we get a variational autoencoder</li> <li>The encoder network \\(f_\\phi\\) maps inputs \\(x\\) to variational parameters</li> <li>The decoder network maps latent variables \\(z\\) to reconstructed inputs</li> <li>Both networks are trained end-to-end using the ELBO objective</li> </ol> <p>In practice, the encoder neural network \\(f_\\phi\\) outputs the parameters of a diagonal Gaussian distribution:</p> \\[q_\\phi(z|x) = \\mathcal{N}(z|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))\\] <p>where \\(\\mu_\\phi(x)\\) and \\(\\sigma^2_\\phi(x)\\) are the mean and variance vectors output by the encoder network. To sample from this distribution during training, we use the reparameterization trick:</p> \\[z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,I)\\] <p>where \\(\\odot\\) denotes element-wise multiplication. This allows us to backpropagate through the sampling process and train the encoder network end-to-end.</p> <p>The key advantage of this approach is that it amortizes the cost of variational inference by learning a single function \\(f_\\phi\\) that can quickly approximate the optimal variational parameters for any input \\(x\\), rather than running a separate optimization for each datapoint.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#decomposition-of-the-negative-elbo","title":"Decomposition of the Negative ELBO","text":"<p>Starting with the definition of the ELBO:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right]\\] <p>We can expand the joint distribution \\(p_\\theta(x,z)\\) using the chain rule of probability:</p> \\[p_\\theta(x,z) = p_\\theta(x|z)p_\\theta(z)\\] <p>Substituting this into the ELBO:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x|z)p_\\theta(z)}{q_\\phi(z|x)}\\right]\\] <p>Using the properties of logarithms, we can split this into three terms:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)]\\] <p>The second and third terms can be combined to form the KL divergence between \\(q_\\phi(z|x)\\) and \\(p_\\theta(z)\\):</p> \\[\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)] = -\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{q_\\phi(z|x)}{p_\\theta(z)}\\right] = -D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\] <p>Therefore, the ELBO can be written as:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\] <p>It is insightful to note that the negative ELBO can be decomposed into two terms:</p> \\[-\\text{ELBO}(x; \\theta, \\phi) = \\underbrace{-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{Reconstruction Loss}} + \\underbrace{D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))}_{\\text{KL Divergence}}\\] <p>This decomposition reveals two key components of the training objective:</p> <ol> <li>Reconstruction Loss: \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\)</li> <li>This term measures how well the model can reconstruct the input \\(x\\) from its latent representation \\(z\\)</li> <li>It encourages the encoder to produce latent codes that preserve the essential information about the input</li> <li> <p>In practice, this is often implemented as the mean squared error or binary cross-entropy between the input and its reconstruction</p> </li> <li> <p>KL Divergence: \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\)</p> </li> <li>This term measures how far the approximate posterior \\(q_\\phi(z|x)\\) is from the prior \\(p_\\theta(z)\\)</li> <li>It encourages the latent space to follow the prior distribution (typically a standard normal distribution)</li> </ol>"},{"location":"ai/deep_generative_models/variational_autoencoders/#practical-implementation-of-elbo-computation","title":"Practical Implementation of ELBO Computation","text":"<p>Let's look at how the ELBO is actually computed in practice. Here's a detailed implementation with explanations:</p> <p>We implement the (rec+kl) decomposed form for practicality and clarity because:</p> <ul> <li>KL has a closed form (for two Gaussians \\(q_\\phi(z|x) \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and \\(p(z) \\sim \\mathcal{N}(0,I)\\), the KL term can be computed analytically). A closed form means we can compute the exact value using a finite number of standard operations (addition, multiplication, logarithms, etc.) without needing numerical integration or approximation. This closed form is derived as follows:</li> </ul> <p>For two multivariate Gaussians \\(q_\\phi(z|x) = \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(z) = \\mathcal{N}(0,I)\\), the KL divergence is:</p> \\[D_{KL}(q_\\phi(z|x) \\| p(z)) = \\frac{1}{2}\\left[\\text{tr}(\\Sigma) + \\mu^T\\mu - d - \\log|\\Sigma|\\right]\\] <p>where \\(\\text{tr}(\\Sigma)\\) is the trace of the covariance matrix \\(\\Sigma\\) (the sum of its diagonal elements), \\(\\mu^T\\mu\\) is the squared L2 norm of the mean vector, \\(d\\) is the dimension of the latent space, and \\(|\\Sigma|\\) is the determinant of \\(\\Sigma\\). For diagonal covariance matrices \\(\\Sigma = \\text{diag}(\\sigma^2)\\), this simplifies to:</p> \\[D_{KL}(q_\\phi(z|x) \\| p(z)) = \\frac{1}{2}\\sum_{i=1}^d (\\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1)\\] <p>This analytical solution is not only computationally efficient but also provides exact gradients, unlike Monte Carlo estimates which would require sampling.</p> <ul> <li> <p>The analytical KL avoids noisy gradients that arise from computing KL via sampling so the decomposition makes training more stable. When using Monte Carlo estimation, the gradients can have high variance due to the randomness in sampling. The analytical form provides deterministic gradients, which leads to more stable optimization. This is particularly important because the KL term acts as a regularizer, and having stable gradients for this term helps prevent the model from either collapsing to a degenerate solution (where the KL term becomes too small) or failing to learn meaningful representations (where the KL term dominates).</p> </li> <li> <p>The decomposed form allows you to monitor reconstruction loss and KL separately which is very helpful in debugging and understanding model behavior</p> </li> </ul> <pre><code>def negative_elbo_bound(self, x):\n    \"\"\"\n    Computes the Evidence Lower Bound, KL and, Reconstruction costs\n\n    Args:\n        x: tensor: (batch, dim): Observations\n\n    Returns:\n        nelbo: tensor: (): Negative evidence lower bound\n        kl: tensor: (): ELBO KL divergence to prior\n        rec: tensor: (): ELBO Reconstruction term\n    \"\"\"\n    # Step 1: Get the parameters of the approximate posterior q_phi(z|x)\n    q_phi_z_given_x_m, q_phi_z_given_x_v = self.enc(x)\n\n    # Step 2: Compute the KL divergence term\n    # This computes D_KL(q_phi(z|x) || p_theta(z))\n    kl = ut.kl_normal(q_phi_z_given_x_m, q_phi_z_given_x_v,\n                      self.z_prior_m, self.z_prior_v)\n\n    # Step 3: Take m samples from the approximate posterior using reparameterization\n    # This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)\n    z_samples = ut.sample_gaussian(\n        q_phi_z_given_x_m.expand(x.shape[0], self.z_dim),\n        q_phi_z_given_x_v.expand(x.shape[0], self.z_dim))\n\n    # Step 4: Get the decoder outputs (logits)\n    # These parameterize the Bernoulli distributions for reconstruction\n    f_theta_of_z = self.dec(z_samples)\n\n    # Step 5: Compute the reconstruction term\n    # This computes -E_q[log p_theta(x|z)] using binary cross-entropy\n    rec = -ut.log_bernoulli_with_logits(x, f_theta_of_z)\n\n    # Step 6: Combine terms to get the negative ELBO\n    nelbo = kl + rec\n\n    # Step 7: Average over the batch\n    nelbo_avg = torch.mean(nelbo)\n    kl_avg = torch.mean(kl)\n    rec_avg = torch.mean(rec)\n\n    return nelbo_avg, kl_avg, rec_avg\n</code></pre> <p>Let's break down each step:</p> <ol> <li>Encoder Output: </li> <li>The encoder network takes input \\(x\\) and outputs the parameters of the approximate posterior \\(q_\\phi(z|x)\\)</li> <li> <p>These parameters are the mean (\\(\\mu_\\phi(x)\\)) and variance (\\(\\sigma^2_\\phi(x)\\)) of a diagonal Gaussian</p> </li> <li> <p>KL Divergence:</p> </li> <li>Computes \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\)</li> <li>For diagonal Gaussians, this has a closed-form solution</li> <li> <p>The prior \\(p_\\theta(z)\\) is typically a standard normal distribution</p> </li> <li> <p>Sampling:</p> </li> <li>Uses the reparameterization trick to sample from \\(q_\\phi(z|x)\\)</li> <li>Implements \\(z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon\\) where \\(\\varepsilon \\sim \\mathcal{N}(0,I)\\)</li> <li> <p>The samples are used to estimate the reconstruction term</p> </li> <li> <p>Decoder Output:</p> </li> <li>The decoder network takes the sampled \\(z\\) and outputs logits</li> <li> <p>These logits parameterize Bernoulli distributions for each element of \\(x\\)</p> </li> <li> <p>Reconstruction Term:</p> </li> <li>Computes \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\)</li> <li>Uses binary cross-entropy loss which takes logits directly</li> <li> <p>The sigmoid function is incorporated into the loss computation</p> </li> <li> <p>Final ELBO:</p> </li> <li>Combines the KL divergence and reconstruction terms</li> <li> <p>The negative ELBO is what we minimize during training</p> </li> <li> <p>Batch Averaging:</p> </li> <li>Averages the losses over the batch</li> <li>This gives us the final training objective</li> </ol> <p>This implementation shows how the theoretical ELBO decomposition we discussed earlier is actually computed in practice, with all the necessary components for training a VAE on binary data.</p> <p>Note on Sampling from \\(q_\\phi(z|x)\\): The sampling step in the implementation is crucial for two reasons:</p> <p>Monte Carlo Estimation: The reconstruction term \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\) involves an expectation over \\(q_\\phi(z|x)\\). We estimate this expectation using Monte Carlo sampling:</p> \\[-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] \\approx -\\frac{1}{K}\\sum_{k=1}^K \\log p_\\theta(x|z^{(k)})\\] <p>where \\(z^{(k)} \\sim q_\\phi(z|x)\\). In practice, we often use \\(K=1\\) (a single sample) as it works well and is computationally efficient.</p> <p>Gradient Estimation: We need to compute gradients of this expectation with respect to both \\(\\phi\\) (encoder parameters) and \\(\\theta\\) (decoder parameters). The reparameterization trick allows us to: - Sample from a fixed distribution \\(p(\\varepsilon)\\) that doesn't depend on \\(\\phi\\) - Transform these samples using a deterministic function that depends on \\(\\phi\\) - Backpropagate through this transformation to compute gradients - This results in lower variance gradient estimates compared to the REINFORCE trick</p> <p>The sampling step is therefore essential for both estimating the ELBO and computing its gradients during training.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#-vae","title":"\u03b2-VAE","text":"<p>A popular variation of the normal VAE is called the \u03b2-VAE. The \u03b2-VAE optimizes the following objective:</p> \\[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\beta D_{KL}(q_\\phi(z|x) || p(z)) \\] <p>Here, \u03b2 is a positive real number. From a training objective, we want to decrease the negative of ELBO, also called NELBO:</p> \\[ \\text{NELBO} = \\text{Reconstruction Loss} + \\beta D_{KL}(q_\\phi(z|x) \\| p(z)) \\] <p>We see that the second term acts as a regularization term. \u03b2 can be thought of as a hyperparameter that adjusts how much we want to regularize. Greater the \u03b2, more is the training optimized to reduce KL divergence, and a higher possibility of overfitting (and also more the \\(q_\\phi(z|x)\\) closely approximates \\(p(z)\\)). Lesser the \u03b2, optimization is geared towards increasing the KL divergence, leading to a more general model. When \u03b2 is set to 1 however, we get the standard VAE.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#importance-weighted-autoencoder-iwae","title":"Importance Weighted Autoencoder (IWAE)","text":"<p>While the ELBO serves as a lower bound to the true marginal log-likelihood, it may be loose if the variational posterior \\(q_\\phi(z|x)\\) is a poor approximation to the true posterior \\(p_\\theta(z|x)\\). The key idea behind IWAE is to use \\(m &gt; 1\\) samples from the approximate posterior \\(q_\\phi(z|x)\\) to obtain the following IWAE bound:</p> \\[ \\mathcal{L}_m(x; \\theta,\\phi) = \\mathbb{E}_{z^{(1)},...,z^{(m)} \\text{ i.i.d.} \\sim q_\\phi(z|x)} \\log \\frac{1}{m}\\sum_{i=1}^m \\frac{p_\\theta(x,z^{(i)})}{q_\\phi(z^{(i)}|x)} \\] <p>Notice that for the special case of \\(m=1\\), the IWAE objective \\(\\mathcal{L}_m\\) reduces to the standard ELBO \\(\\mathcal{L}_1 = \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\).</p> <p>As a pseudocode, the main modification to the standard VAE would be:</p> <pre><code># Step 3: Take m samples from the approximate posterior using reparameterization\n# This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)\nz_samples = ut.sample_gaussian(\n    q_phi_z_given_x_m.expand(x.shape[0], self.z_dim),\n    q_phi_z_given_x_v.expand(x.shape[0], self.z_dim))\n</code></pre>"},{"location":"ai/deep_generative_models/variational_autoencoders/#gaussian-mixture-vae-gmvae","title":"Gaussian Mixture VAE (GMVAE)","text":"<p>The VAE's prior distribution was a parameter-free isotropic Gaussian \\(p_\\theta(z) = \\mathcal{N}(z|0,I)\\). While this original setup works well, there are settings in which we desire more expressivity to better model our data. Let's look at GMVAE, which has a mixture of Gaussians as the prior distribution.</p> \\[p_\\theta(z) = \\sum_{i=1}^k \\frac{1}{k}\\mathcal{N}(z|\\mu_i, \\text{diag}(\\sigma^2_i))\\] <p>where \\(i \\in \\{1, ..., k\\}\\) denotes the \\(i\\)th cluster index. For notational simplicity, we shall subsume our mixture of Gaussian parameters \\(\\{\\mu_i, \\sigma_i\\}_{i=1}^k\\) into our generative model parameters \\(\\theta\\). For simplicity, we have also assumed fixed uniform weights \\(1/k\\) over the possible different clusters.</p> <p>Apart from the prior, the GMVAE shares an identical setup as the VAE:</p> \\[q_\\phi(z|x) = \\mathcal{N}(z|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))\\] \\[p_\\theta(x|z) = \\text{Bern}(x|f_\\theta(z))\\] <p>Although the ELBO for the GMVAE: \\(\\mathbb{E}_{q_\\phi(z)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\) is identical to that of the VAE, we note that the KL term \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\) cannot be computed analytically between a Gaussian distribution \\(q_\\phi(z|x)\\) and a mixture of Gaussians \\(p_\\theta(z)\\). However, we can obtain its unbiased estimator via Monte Carlo sampling:</p> \\[D_{KL}(q_\\phi(z|x) \\| p_\\theta(z)) \\approx \\log q_\\phi(z^{(1)}|x) - \\log p_\\theta(z^{(1)})\\] \\[= \\underbrace{\\log\\mathcal{N}(z^{(1)}|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))}_{\\text{log normal}} - \\underbrace{\\log\\sum_{i=1}^k \\frac{1}{k}\\mathcal{N}(z^{(1)}|\\mu_i, \\text{diag}(\\sigma^2_i))}_{\\text{log normal mixture}}\\] <p>where \\(z^{(1)} \\sim q_\\phi(z|x)\\) denotes a single sample.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#the-semi-supervised-vae-ssvae","title":"The Semi-Supervised VAE (SSVAE)","text":"<p>The Semi-Supervised VAE (SSVAE) extends the standard VAE to handle both labeled and unlabeled data. In a semi-supervised setting, we have a dataset \\(\\mathcal{D}\\) that consists of: - Labeled data: \\(\\mathcal{D}_l = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N_l}\\) - Unlabeled data: \\(\\mathcal{D}_u = \\{x^{(i)}\\}_{i=1}^{N_u}\\)</p> <p>where \\(y^{(i)}\\) represents the class label for the \\(i\\)-th labeled example. The SSVAE introduces an additional latent variable \\(y\\) to model the class labels, and the joint distribution is factorized as:</p> \\[ p_\\theta(x, y, z) = p_\\theta(x|y,z)p_\\theta(y|z)p_\\theta(z) \\] <p>This factorization is derived from the chain rule of probability. We first factorize \\(p_\\theta(x, y, z)\\) as \\(p_\\theta(x|y,z)p_\\theta(y,z)\\), and then further factorize \\(p_\\theta(y,z)\\) as \\(p_\\theta(y|z)p_\\theta(z)\\). This reflects the generative process where: 1. First, we sample \\(z\\) from the prior \\(p_\\theta(z)\\) 2. Then, we sample \\(y\\) conditioned on \\(z\\) from \\(p_\\theta(y|z)\\) 3. Finally, we generate \\(x\\) conditioned on both \\(y\\) and \\(z\\) from \\(p_\\theta(x|y,z)\\)</p> <p>The approximate posterior for labeled data is:</p> \\[ q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y|x) \\] <p>This factorization is derived from the chain rule of probability for the approximate posterior. The chain rule states that for any random variables \\(A\\), \\(B\\), and \\(C\\), we can write:</p> \\[p(A,B|C) = p(A|B,C)p(B|C)\\] <p>This equation is derived from the definition of conditional probability. Let's break it down step by step:</p> <ol> <li>First, recall that conditional probability is defined as:</li> </ol> \\[p(A|B) = \\frac{p(A,B)}{p(B)}\\] <ol> <li>For our case with three variables, we can write:</li> </ol> \\[p(A,B|C) = \\frac{p(A,B,C)}{p(C)}\\] <ol> <li>We can also write:</li> </ol> \\[p(A|B,C) = \\frac{p(A,B,C)}{p(B,C)}\\] <p>and</p> \\[p(B|C) = \\frac{p(B,C)}{p(C)}\\] <ol> <li>Multiplying these last two equations:</li> </ol> \\[p(A|B,C)p(B|C) = \\frac{p(A,B,C)}{p(B,C)} \\cdot \\frac{p(B,C)}{p(C)} = \\frac{p(A,B,C)}{p(C)} = p(A,B|C)\\] <p>Therefore, we have proven that:</p> \\[p(A,B|C) = p(A|B,C)p(B|C)\\] <p>In our case, we can identify: - \\(A\\) as \\(z\\) (the latent code) - \\(B\\) as \\(y\\) (the label) - \\(C\\) as \\(x\\) (the observed data)</p> <p>Therefore, applying the chain rule:</p> \\[q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y|x)\\] <p>This means: 1. First, we predict the label \\(y\\) from \\(x\\) using \\(q_\\phi(y|x)\\) 2. Then, we infer the latent code \\(z\\) using both \\(x\\) and the predicted \\(y\\) through \\(q_\\phi(z|x,y)\\)</p> <p>and for unlabeled data:</p> \\[ q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y) \\] <p>For unlabeled data, since we don't know the true label \\(y\\), we use a prior distribution \\(q_\\phi(y)\\) (typically a uniform distribution over classes) instead of \\(q_\\phi(y|x)\\). The factorization reflects that: 1. We sample a label \\(y\\) from the prior \\(q_\\phi(y)\\) 2. Then, we infer the latent code \\(z\\) using both \\(x\\) and the sampled \\(y\\) through \\(q_\\phi(z|x,y)\\)</p> <p>The training objective for SSVAE combines: 1. The ELBO for labeled data 2. The ELBO for unlabeled data 3. A classification loss for labeled data</p> <p>This allows the model to learn both the data distribution and the class labels in a semi-supervised manner.</p>"},{"location":"ai/deep_learning_for_computer_vision/a_review_of_generative_models/","title":"A review of Generative Models","text":"<ul> <li>Introduction - Introduction to Generative Models</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/a_review_of_rnns_and_transformers/","title":"A review of RNNs and Transformers","text":""},{"location":"ai/deep_learning_for_computer_vision/a_review_of_rnns_and_transformers/#recurrent-neural-networks-rnns","title":"Recurrent Neural Networks (RNNs)","text":"<ul> <li>Recurrent Neural Networks - Introduction to RNNs and their architecture</li> <li>Gated Recurrent Units (GRUs) - GRU architecture as an alternative to LSTMs</li> <li>Long-Short Term Memory (LSTM) Networks - LSTM networks for handling long-term dependencies</li> <li>Seq2Seq Models - Sequence-to-sequence models using RNNs</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/a_review_of_rnns_and_transformers/#transformers","title":"Transformers","text":"<ul> <li>Attention Mechanism - General attention mechanism concepts</li> <li>How large language models work, a visual intro to transformers - Visual introduction to how LLMs work</li> <li>Attention in transformers, visually explained - Visual explanation of attention mechanisms in transformers</li> <li>The basic Transformer - Overview of the basic transformer architecture</li> <li>Decoder-only Transformers - Decoder-only transformer architecture (e.g., GPT models)</li> <li>Differences between the basic Transformer and the Decoder-Only Transformer - Comparison of transformer architectures</li> <li>Self-Attention and Transformers, a mathematical approach - Comprehensive mathematical treatment of self-attention and transformers</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/","title":"Backpropagation","text":""},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#introduction","title":"Introduction","text":"<p>We are given some function \\(f(x)\\) where \\(x\\) is a vector of inputs and we are interested in computing the gradient of \\(f\\) at \\(x\\) (i.e. \\(\\nabla f(x)\\)).</p> <p>The primary reason we are interested in this problem is that in the specific case of neural networks, \\(f\\) will correspond to the loss function (\\(L\\)) and the inputs \\(x\\) will consist of the training data and the neural network weights. For example, the loss could be the SVM loss function and the inputs are both the training data \\((x_i, y_i), i=1\\ldots N\\) and the weights and biases \\(W, b\\). Note that (as is usually the case in Machine Learning) we think of the training data as given and fixed, and of the weights as variables we have control over.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#simple-expressions-and-interpretation-of-the-gradient","title":"Simple expressions and interpretation of the gradient","text":"<p>Consider a simple multiplication function of two numbers \\(f(x,y) = xy\\). It is a matter of simple calculus to derive the partial derivative for either input:</p> \\[f(x,y) = xy \\rightarrow \\frac{\\partial f}{\\partial x} = y \\quad \\frac{\\partial f}{\\partial y} = x\\] <p>A derivative at a point represents the instantaneous rate of change of a function at the given point, equivalent to the slope of the tangent line to the function's graph at that point. :</p> \\[\\frac{df(x)}{dx} = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}\\] <p>A technical note is that the division sign on the left-hand side is, unlike the division sign on the right-hand side, not a division. Instead, this notation indicates that the operator \\(\\frac{d}{dx}\\) is being applied to the function \\(f\\), and returns a different function (the derivative). A nice way to think about the expression above is that when \\(h\\) is very small, then the function is well-approximated by a straight line, and the derivative is its slope. In other words, the derivative on each variable tells you the sensitivity of the whole expression on its value. </p> <p>For example, if \\(x = 4, y = -3\\) then \\(f(x,y) = -12\\) and the derivative on \\(x\\) \\(\\frac{\\partial f}{\\partial x} = -3\\). This tells us that if we were to increase the value of this variable by a tiny amount (\\(h\\)), the effect on the whole expression would be to decrease it (due to the negative sign), and by three times that amount (\\(3h\\)). This can be seen by rearranging the above equation (\\(f(x+h) = f(x) + h\\frac{df(x)}{dx}\\)). Analogously, since \\(\\frac{\\partial f}{\\partial y} = 4\\), we expect that increasing the value of \\(y\\) by some very small amount \\(h\\) would also increase the output of the function (due to the positive sign), and by \\(4h\\).</p> <p>As mentioned, the gradient \\(\\nabla f\\) is the vector of partial derivatives, so we have that \\(\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}] = [y, x]\\). Even though the gradient is technically a vector, we will often use terms such as \"the gradient on \\(x\\)\" instead of the technically correct phrase \"the partial derivative on \\(x\\)\" for simplicity.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#compound-expressions-with-chain-rule","title":"Compound expressions with chain rule","text":"<p>Let's now start to consider more complicated expressions that involve multiple composed functions, such as \\(f(x,y,z) = (x+y)z\\). This expression is still simple enough to differentiate directly, but we'll take a particular approach to it that will be helpful with understanding the intuition behind backpropagation. In particular, note that this expression can be broken down into two expressions: \\(q = x + y\\) and \\(f = qz\\). Moreover, we know how to compute the derivatives of both expressions separately. \\(f\\) is just multiplication of \\(q\\) and \\(z\\), so \\(\\frac{\\partial f}{\\partial q} = z, \\frac{\\partial f}{\\partial z} = q\\), and \\(q\\) is addition of \\(x\\) and \\(y\\) so \\(\\frac{\\partial q}{\\partial x} = 1, \\frac{\\partial q}{\\partial y} = 1\\). However, we don't necessarily care about the gradient on the intermediate value \\(q\\) - the value of \\(\\frac{\\partial f}{\\partial q}\\) is not useful. Instead, we are ultimately interested in the gradient of \\(f\\) with respect to its inputs \\(x, y, z\\). The chain rule tells us that the correct way to \"chain\" these gradient expressions together is through multiplication. For example, \\(\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}\\). In practice this is simply a multiplication of the two numbers that hold the two gradients.</p> <p>Let's see this in action with a concrete example:</p> <pre><code># set some inputs\nx = -2; y = 5; z = -4\n\n# perform the forward pass\nq = x + y # q becomes 3\nf = q * z # f becomes -12\n\n# perform the backward pass (backpropagation) in reverse order:\n# first backprop through f = q * z\ndfdz = q # df/dz = q, so gradient on z becomes 3\ndfdq = z # df/dq = z, so gradient on q becomes -4\ndqdx = 1.0\ndqdy = 1.0\n# now backprop through q = x + y\ndfdx = dfdq * dqdx  # The multiplication here is the chain rule!\ndfdy = dfdq * dqdy  \n</code></pre> <p>Forward pass: We compute the function values step by step, storing intermediate results.</p> <p>Backward pass: We compute gradients in reverse order, using the chain rule to propagate gradients backward through the computation graph.</p> <p>We are left with the gradient in the variables <code>[dfdx, dfdy, dfdz]</code>, which tell us the sensitivity of the variables \\(x, y, z\\) on \\(f\\)! This is the simplest example of backpropagation. Going forward, we will use a more concise notation that omits the <code>df</code> prefix. For example, we will simply write <code>dq</code> instead of <code>dfdq</code>, and always assume that the gradient is computed on the final output.</p> <p>This computation can also be nicely visualized with a circuit diagram.</p> <p></p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#intuitive-understanding-of-backpropagation","title":"Intuitive understanding of backpropagation","text":"<p>Notice that backpropagation is a beautifully local process. Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the local gradient of its output with respect to its inputs. Notice that the gates can do this completely independently without being aware of any of the details of the full circuit that they are embedded in.</p> <p>Let's get an intuition for how this works by referring again to the example. The add gate received inputs \\([-2, 5]\\) and computed output \\(3\\). Since the gate is computing the addition operation, its local gradient for both of its inputs is \\(+1\\). The rest of the circuit computed the final value, which is \\(-12\\). During the backward pass in which the chain rule is applied recursively backwards through the circuit, the add gate (which is an input to the multiply gate) learns that the gradient for its output was \\(-4\\). If we anthropomorphize the circuit as wanting to output a higher value (which can help with intuition), then we can think of the circuit as \"wanting\" the output of the add gate to be lower (due to negative sign), and with a force of \\(4\\). To continue the recurrence and to chain the gradient, the add gate takes that gradient and multiplies it to all of the local gradients for its inputs (making the gradient on both \\(x\\) and \\(y\\) \\(1 \\times -4 = -4\\)). Notice that this has the desired effect: If \\(x, y\\) were to decrease (responding to their negative gradient) then the add gate's output would decrease, which in turn makes the multiply gate's output increase.</p> <p>Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#modularity-sigmoid-example","title":"Modularity: Sigmoid example","text":"<p>The gates we introduced above are relatively arbitrary. Any kind of differentiable function can act as a gate, and we can group multiple gates into a single gate, or decompose a function into multiple gates whenever it is convenient. Let's look at another expression that illustrates this point:</p> \\[f(w,x) = \\frac{1}{1 + e^{-(w_0x_0 + w_1x_1 + w_2)}}\\] <p>This expression describes a 2-dimensional neuron (with inputs \\(x\\) and weights \\(w\\)) that uses the sigmoid activation function. But for now let's think of this very simply as just a function from inputs \\(w, x\\) to a single number. The function is made up of multiple gates. In addition to the ones described already above (add, mul, max), there are four more:</p> \\[f(x) = \\frac{1}{x} \\rightarrow \\frac{df}{dx} = -\\frac{1}{x^2}\\] \\[f_c(x) = c + x \\rightarrow \\frac{df}{dx} = 1\\] \\[f(x) = e^x \\rightarrow \\frac{df}{dx} = e^x\\] \\[f_a(x) = ax \\rightarrow \\frac{df}{dx} = a\\] <p>where the functions \\(f_c, f_a\\) translate the input by a constant of \\(c\\) and scale the input by a constant of \\(a\\), respectively. These are technically special cases of addition and multiplication, but we introduce them as (new) unary gates here since we do not need the gradients for the constants \\(c, a\\). The full circuit then looks as follows.</p> <p></p> <p>In the example above, we see a long chain of function applications that operates on the result of the dot product between \\(w, x\\). The function that these operations implement is called the sigmoid function \\(\\sigma(x)\\). It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator):</p> \\[\\sigma(x) = \\frac{1}{1 + e^{-x}} \\rightarrow \\frac{d\\sigma(x)}{dx} = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\left(\\frac{1 + e^{-x} - 1}{1 + e^{-x}}\\right)\\left(\\frac{1}{1 + e^{-x}}\\right) = (1 - \\sigma(x))\\sigma(x)\\] <p>As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be \\((1 - 0.73) \\times 0.73 \\approx 0.2\\), as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Let's see the backprop for this neuron in code.</p> <pre><code>w = [2,-3,-3] # assume some random weights and data\nx = [-1, -2]\n\n# forward pass\ndot = w[0]*x[0] + w[1]*x[1] + w[2]\nf = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n\n# backward pass through the neuron (backpropagation)\nddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\ndx = [w[0] * ddot, w[1] * ddot] # backprop into x\ndw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w\n# we're done! we have the gradients on the inputs to the circuit\n</code></pre> <p>Forward pass: We compute the dot product and apply the sigmoid function.</p> <p>Backward pass: We use the simplified sigmoid gradient formula and propagate gradients back to the inputs using the chain rule.</p> <p>As shown in the code above, in practice it is always helpful to break down the forward pass into stages that are easily backpropped through. For example here we created an intermediate variable <code>dot</code> which holds the output of the dot product between <code>w</code> and <code>x</code>. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. <code>ddot</code>, and ultimately <code>dw</code>, <code>dx</code>) that hold the gradients of those variables.</p> <p>The point of this section is that the details of how the backpropagation is performed, and which parts of the forward function we think of as gates, is a matter of convenience. It helps to be aware of which parts of the expression have easy local gradients, so that they can be chained together with the least amount of code and effort.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#backprop-in-practice-staged-computation","title":"Backprop in practice: Staged computation","text":"<p>Let's see this with another example. Suppose that we have a function of the form:</p> \\[f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}\\] <p>Here is how we would structure the forward pass of such expression.</p> <pre><code>x = 3 # example values\ny = -4\n\n# forward pass\nsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)\nnum = x + sigy # numerator                               #(2)\nsigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)\nxpy = x + y                                              #(4)\nxpysqr = xpy**2                                          #(5)\nden = sigx + xpysqr # denominator                        #(6)\ninvden = 1.0 / den                                       #(7)\nf = num * invden # done!                                 #(8)\n</code></pre> <p>Notice that we have structured the code in such way that it contains multiple intermediate variables, each of which are only simple expressions for which we already know the local gradients. Therefore, computing the backprop pass is easy: We'll go backwards and for every variable along the way in the forward pass (<code>sigy</code>, <code>num</code>, <code>sigx</code>, <code>xpy</code>, <code>xpysqr</code>, <code>den</code>, <code>invden</code>) we will have the same variable, but one that begins with a <code>d</code>, which will hold the gradient of the output of the circuit with respect to that variable. Additionally, note that every single piece in our backprop will involve computing the local gradient of that expression, and chaining it with the gradient on that expression with a multiplication. For each row, we also highlight which part of the forward pass it refers to the following.</p> <pre><code># backprop f = num * invden\ndnum = invden # gradient on numerator                             #(8)\ndinvden = num                                                     #(8)\n# backprop invden = 1.0 / den \ndden = (-1.0 / (den**2)) * dinvden                                #(7)\n# backprop den = sigx + xpysqr\ndsigx = (1) * dden                                                #(6)\ndxpysqr = (1) * dden                                              #(6)\n# backprop xpysqr = xpy**2\ndxpy = (2 * xpy) * dxpysqr                                        #(5)\n# backprop xpy = x + y\ndx = (1) * dxpy                                                   #(4)\ndy = (1) * dxpy                                                   #(4)\n# backprop sigx = 1.0 / (1 + math.exp(-x))\ndx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)\n# backprop num = x + sigy\ndx += (1) * dnum                                                  #(2)\ndsigy = (1) * dnum                                                #(2)\n# backprop sigy = 1.0 / (1 + math.exp(-y))\ndy += ((1 - sigy) * sigy) * dsigy                                 #(1)\n# done! phew\n</code></pre> <p>Note: Gradients add up at forks. The forward expression involves the variables \\(x,y\\) multiple times, so when we perform backpropagation we must be careful to use <code>+=</code> instead of <code>=</code> to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add.</p> <p>Simple example: Consider \\(f(x) = x^2 + x^3\\). Here, \\(x\\) branches out to two different operations. The gradient is:</p> \\[\\frac{df}{dx} = \\frac{d}{dx}(x^2) + \\frac{d}{dx}(x^3) = 2x + 3x^2\\] <p>In backpropagation terms: if we have intermediate variables \\(a = x^2\\) and \\(b = x^3\\), then:</p> <ul> <li> <p>\\(da = 2x\\) (gradient from \\(x^2\\) path)</p> </li> <li> <p>\\(db = 3x^2\\) (gradient from \\(x^3\\) path)  </p> </li> <li> <p>\\(dx = da + db = 2x + 3x^2\\) (gradients add up!)</p> </li> </ul> <p>This is why we use <code>+=</code> instead of <code>=</code> when a variable appears in multiple places.</p> <p>Another example: Consider \\(f(x) = x^2/x\\) (which simplifies to \\(f(x) = x\\)). Here, \\(x\\) appears in both the numerator and denominator. The gradient is:</p> \\[\\frac{df}{dx} = \\frac{d}{dx}\\left(\\frac{x^2}{x}\\right) = \\frac{d}{dx}(x) = 1\\] <p>But if we think of it as \\(f(x) = x^2 \\cdot x^{-1}\\), then using the product rule:</p> \\[\\frac{df}{dx} = \\frac{d}{dx}(x^2) \\cdot x^{-1} + x^2 \\cdot \\frac{d}{dx}(x^{-1}) = 2x \\cdot \\frac{1}{x} + x^2 \\cdot \\left(-\\frac{1}{x^2}\\right) = 2 - 1 = 1\\] <p>In backpropagation terms: if we have intermediate variables \\(a = x^2\\) and \\(b = x^{-1}\\), then:</p> <ul> <li> <p>\\(da = 2x\\) (gradient from numerator path)</p> </li> <li> <p>\\(db = -x^{-2}\\) (gradient from denominator path)</p> </li> <li> <p>\\(dx = da \\cdot b + a \\cdot db = 2x \\cdot \\frac{1}{x} + x^2 \\cdot \\left(-\\frac{1}{x^2}\\right) = 1\\)</p> </li> </ul> <p>This shows how gradients from different paths combine through the chain rule.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#patterns-in-backward-flow","title":"Patterns in backward flow","text":"<p>It is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (add, mul, max), all have very simple interpretations in terms of how they act during backpropagation. Consider this example circuit.</p> <p></p> <p>Looking at the diagram above as an example, we can see that:</p> <p>The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0. In the example circuit above, note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged.</p> <p>The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values. In the example circuit above, the max operation routed the gradient of 2.00 to the z variable, which had a higher value than w, and the gradient on w remains zero.</p> <p>The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. In the example above, the gradient on x is -8.00, which is -4.00 x 2.00.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#unintuitive-effects-and-their-consequences","title":"Unintuitive effects and their consequences","text":"<p>Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted \\(w^T x_i\\) (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples \\(x_i\\) by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you'd have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#gradients-for-vectorized-operations","title":"Gradients for vectorized operations","text":"<p>One must pay closer attention to dimensions and transpose operations. Possibly the most tricky operation is the matrix-matrix multiplication (which generalizes all matrix-vector and vector-vector) multiply operations.</p> <pre><code># forward pass\nW = np.random.randn(5, 10)\nX = np.random.randn(10, 3)\nD = W.dot(X)\n\n# now suppose we had the gradient on D already calculated\ndD = np.random.randn(*D.shape) # same shape as D\ndW = dD.dot(X.T) #.T gives the transpose of the matrix\ndX = W.T.dot(dD)\n</code></pre> <p>Tip: use dimension analysis! Note that you do not need to remember the expressions for <code>dW</code> and <code>dX</code> because they are easy to re-derive based on dimensions. For instance, we know that the gradient on the weights <code>dW</code> must be of the same size as <code>W</code> after it is computed, and that it must depend on matrix multiplication of <code>X</code> and <code>dD</code> (as is the case when both <code>X,W</code> are single numbers and not matrices). There is always exactly one way of achieving this so that the dimensions work out. For example, <code>X</code> is of size [10 x 3] and <code>dD</code> of size [5 x 3], so if we want <code>dW</code> and <code>W</code> has shape [5 x 10], then the only way of achieving this is with <code>dD.dot(X.T)</code>, as shown above.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#additional-references","title":"Additional References","text":"<ul> <li>Automatic differentiation in machine learning: a survey</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation_for_a_linear_layer/","title":"Backpropagation for a Linear Layer","text":"<p>We will explicitly derive the equations to use when backpropagating through a linear layer, using minibatches.</p> <p>Prerequisites: This document assumes familiarity with derivatives. For a comprehensive review of derivatives, see The TL;DR version of Derivatives.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation_for_a_linear_layer/#forward-pass","title":"Forward Pass","text":"<p>During the forward pass, the linear layer takes an input \\(X\\) of shape \\(N \\times D\\) and a weight matrix \\(W\\) of shape \\(D \\times M\\), and computes an output \\(Y = XW\\) of shape \\(N \\times M\\) by computing the matrix product of the two inputs. To make things even more concrete, we will consider the case \\(N = 2\\), \\(D = 2\\), \\(M = 3\\).</p> <p>We can then write out the forward pass in terms of the elements of the inputs:</p> \\[X = \\begin{pmatrix} x_{1,1} &amp; x_{1,2} \\\\ x_{2,1} &amp; x_{2,2} \\end{pmatrix}, \\quad W = \\begin{pmatrix} w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\\\ w_{2,1} &amp; w_{2,2} &amp; w_{2,3} \\end{pmatrix}\\] \\[Y = XW = \\begin{pmatrix} x_{1,1}w_{1,1} + x_{1,2}w_{2,1} &amp; x_{1,1}w_{1,2} + x_{1,2}w_{2,2} &amp; x_{1,1}w_{1,3} + x_{1,2}w_{2,3} \\\\ x_{2,1}w_{1,1} + x_{2,2}w_{2,1} &amp; x_{2,1}w_{1,2} + x_{2,2}w_{2,2} &amp; x_{2,1}w_{1,3} + x_{2,2}w_{2,3} \\end{pmatrix}\\] <p>After the forward pass, we assume that the output will be used in other parts of the network, and will eventually be used to compute a scalar loss \\(L\\).</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation_for_a_linear_layer/#backward-pass","title":"Backward Pass","text":"<p>During the backward pass through the linear layer, we assume that the derivative \\(\\frac{\\partial L}{\\partial Y}\\) has already been computed. For example if the linear layer is part of a linear classifier, then the matrix \\(Y\\) gives class scores; these scores are fed to a loss function (such as the softmax or multiclass SVM loss) which computes the scalar loss \\(L\\) and derivative \\(\\frac{\\partial L}{\\partial Y}\\) of the loss with respect to the scores.</p> <p>Since \\(L\\) is a scalar and \\(Y\\) is a matrix of shape \\(N \\times M\\), the gradient \\(\\frac{\\partial L}{\\partial Y}\\) will be a matrix with the same shape as \\(Y\\), where each element of \\(\\frac{\\partial L}{\\partial Y}\\) gives the derivative of the loss \\(L\\) with respect to one element of \\(Y\\):</p> \\[\\frac{\\partial L}{\\partial Y} = \\begin{pmatrix} \\frac{\\partial L}{\\partial y_{1,1}} &amp; \\frac{\\partial L}{\\partial y_{1,2}} &amp; \\frac{\\partial L}{\\partial y_{1,3}} \\\\ \\frac{\\partial L}{\\partial y_{2,1}} &amp; \\frac{\\partial L}{\\partial y_{2,2}} &amp; \\frac{\\partial L}{\\partial y_{2,3}} \\end{pmatrix}\\] <p>During the backward pass our goal is to use \\(\\frac{\\partial L}{\\partial Y}\\) in order to compute \\(\\frac{\\partial L}{\\partial X}\\) and \\(\\frac{\\partial L}{\\partial W}\\). Again, since \\(L\\) is a scalar we know that \\(\\frac{\\partial L}{\\partial X}\\) must have the same shape as \\(X\\) (\\(N \\times D\\)) and \\(\\frac{\\partial L}{\\partial W}\\) must have the same shape as \\(W\\) (\\(D \\times M\\)).</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation_for_a_linear_layer/#computing-gradients-using-the-chain-rule","title":"Computing Gradients Using the Chain Rule","text":"<p>By the chain rule, we know that:</p> \\[\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\frac{\\partial Y}{\\partial X}\\] \\[\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Y} \\frac{\\partial Y}{\\partial W}\\] <p>The terms \\(\\frac{\\partial Y}{\\partial X}\\) and \\(\\frac{\\partial Y}{\\partial W}\\) are Jacobian matrices containing the partial derivative of each element of \\(Y\\) with respect to each element of the inputs \\(X\\) and \\(W\\).</p> <p>However we do not want to form the Jacobian matrices \\(\\frac{\\partial Y}{\\partial X}\\) and \\(\\frac{\\partial Y}{\\partial W}\\) explicitly, because they will be very large. In a typical neural network we might have \\(N = 64\\) and \\(M = D = 4096\\); then \\(\\frac{\\partial Y}{\\partial X}\\) consists of \\(64 \\times 4096 \\times 64 \\times 4096\\) scalar values; this is more than 68 billion numbers; using 32-bit floating point, this Jacobian matrix will take 256 GB of memory to store. Therefore it is completely hopeless to try and explicitly store and manipulate the Jacobian matrix.</p> <p>However it turns out that for most common neural network layers, we can derive expressions that compute the product \\(\\frac{\\partial Y}{\\partial X} \\frac{\\partial L}{\\partial Y}\\) without explicitly forming the Jacobian \\(\\frac{\\partial Y}{\\partial X}\\). Even better, we can typically derive this expression without even computing an explicit expression for the Jacobian \\(\\frac{\\partial Y}{\\partial X}\\); in many cases we can work out a small case on paper and then infer the general formula.</p> <p>Let's see how this works out for our specific case of \\(N = 2\\), \\(D = 2\\), \\(M = 3\\). We first tackle \\(\\frac{\\partial L}{\\partial X}\\). Again, we know that \\(\\frac{\\partial L}{\\partial X}\\) must have the same shape as \\(X\\):</p> \\[X = \\begin{pmatrix} x_{1,1} &amp; x_{1,2} \\\\ x_{2,1} &amp; x_{2,2} \\end{pmatrix} \\implies \\frac{\\partial L}{\\partial X} = \\begin{pmatrix} \\frac{\\partial L}{\\partial x_{1,1}} &amp; \\frac{\\partial L}{\\partial x_{1,2}} \\\\ \\frac{\\partial L}{\\partial x_{2,1}} &amp; \\frac{\\partial L}{\\partial x_{2,2}} \\end{pmatrix}\\] <p>We can proceed one element at a time. First we will compute \\(\\frac{\\partial L}{\\partial x_{1,1}}\\). By the chain rule, we know that</p> \\[\\frac{\\partial L}{\\partial x_{1,1}} = \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial y_{i,j}} \\frac{\\partial y_{i,j}}{\\partial x_{1,1}} = \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial x_{1,1}}\\] <p>In the above equation \\(L\\) and \\(x_{1,1}\\) are scalars so \\(\\frac{\\partial L}{\\partial x_{1,1}}\\) is also a scalar. If we view \\(Y\\) not as a matrix but as a collection of intermediate scalar variables, then we can use the chain rule to write \\(\\frac{\\partial L}{\\partial x_{1,1}}\\) solely in terms of scalar derivatives.</p> <p>To avoid working with sums, it is convenient to collect all terms \\(\\frac{\\partial L}{\\partial y_{i,j}}\\) into a single matrix \\(\\frac{\\partial L}{\\partial Y}\\); here \\(L\\) is a scalar and \\(Y\\) is a matrix, so \\(\\frac{\\partial L}{\\partial Y}\\) has the same shape as \\(Y\\) (\\(N \\times M\\)), where each element of \\(\\frac{\\partial L}{\\partial Y}\\) gives the derivative of \\(L\\) with respect to one element of \\(Y\\). We similarly collect all terms \\(\\frac{\\partial y_{i,j}}{\\partial x_{1,1}}\\) into a single matrix \\(\\frac{\\partial Y}{\\partial x_{1,1}}\\); since \\(Y\\) is a matrix and \\(x_{1,1}\\) is a scalar, \\(\\frac{\\partial Y}{\\partial x_{1,1}}\\) is a matrix with the same shape as \\(Y\\) (\\(N \\times M\\)).</p> <p>Since \\(\\frac{\\partial L}{\\partial x_{1,1}}\\) is a scalar, we know that the product of \\(\\frac{\\partial L}{\\partial Y}\\) and \\(\\frac{\\partial Y}{\\partial x_{1,1}}\\) must be a scalar; by inspecting the expression using only scalar derivatives, it is clear that in this context the product of \\(\\frac{\\partial L}{\\partial Y}\\) and \\(\\frac{\\partial Y}{\\partial x_{1,1}}\\) must be a dot product.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation_for_a_linear_layer/#computing-fracpartial-lpartial-x-element-by-element","title":"Computing \\(\\frac{\\partial L}{\\partial X}\\) element by element","text":"<p>In the backward pass we are already given \\(\\frac{\\partial L}{\\partial Y}\\), so we only need to compute \\(\\frac{\\partial Y}{\\partial x_{1,1}}\\); we can easily compute this from the forward pass equation:</p> \\[\\frac{\\partial Y}{\\partial x_{1,1}} = \\begin{pmatrix} w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}\\] <p>Now combining the chain rule gives:</p> \\[\\frac{\\partial L}{\\partial x_{1,1}} = \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial x_{1,1}}\\] \\[= \\begin{pmatrix} \\frac{\\partial L}{\\partial y_{1,1}} &amp; \\frac{\\partial L}{\\partial y_{1,2}} &amp; \\frac{\\partial L}{\\partial y_{1,3}} \\\\ \\frac{\\partial L}{\\partial y_{2,1}} &amp; \\frac{\\partial L}{\\partial y_{2,2}} &amp; \\frac{\\partial L}{\\partial y_{2,3}} \\end{pmatrix} \\cdot \\begin{pmatrix} w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}\\] \\[= \\frac{\\partial L}{\\partial y_{1,1}} w_{1,1} + \\frac{\\partial L}{\\partial y_{1,2}} w_{1,2} + \\frac{\\partial L}{\\partial y_{1,3}} w_{1,3}\\] <p>We can now repeat the process to compute the other entries of \\(\\frac{\\partial L}{\\partial X}\\), one element at a time:</p> \\[\\frac{\\partial L}{\\partial x_{1,2}} = \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial x_{1,2}}\\] \\[= \\begin{pmatrix} \\frac{\\partial L}{\\partial y_{1,1}} &amp; \\frac{\\partial L}{\\partial y_{1,2}} &amp; \\frac{\\partial L}{\\partial y_{1,3}} \\\\ \\frac{\\partial L}{\\partial y_{2,1}} &amp; \\frac{\\partial L}{\\partial y_{2,2}} &amp; \\frac{\\partial L}{\\partial y_{2,3}} \\end{pmatrix} \\cdot \\begin{pmatrix} w_{2,1} &amp; w_{2,2} &amp; w_{2,3} \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}\\] \\[= \\frac{\\partial L}{\\partial y_{1,1}} w_{2,1} + \\frac{\\partial L}{\\partial y_{1,2}} w_{2,2} + \\frac{\\partial L}{\\partial y_{1,3}} w_{2,3}\\] \\[\\frac{\\partial L}{\\partial x_{2,1}} = \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial x_{2,1}}\\] \\[= \\begin{pmatrix} \\frac{\\partial L}{\\partial y_{1,1}} &amp; \\frac{\\partial L}{\\partial y_{1,2}} &amp; \\frac{\\partial L}{\\partial y_{1,3}} \\\\ \\frac{\\partial L}{\\partial y_{2,1}} &amp; \\frac{\\partial L}{\\partial y_{2,2}} &amp; \\frac{\\partial L}{\\partial y_{2,3}} \\end{pmatrix} \\cdot \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\end{pmatrix}\\] \\[= \\frac{\\partial L}{\\partial y_{2,1}} w_{1,1} + \\frac{\\partial L}{\\partial y_{2,2}} w_{1,2} + \\frac{\\partial L}{\\partial y_{2,3}} w_{1,3}\\] \\[\\frac{\\partial L}{\\partial x_{2,2}} = \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial x_{2,2}}\\] \\[= \\begin{pmatrix} \\frac{\\partial L}{\\partial y_{1,1}} &amp; \\frac{\\partial L}{\\partial y_{1,2}} &amp; \\frac{\\partial L}{\\partial y_{1,3}} \\\\ \\frac{\\partial L}{\\partial y_{2,1}} &amp; \\frac{\\partial L}{\\partial y_{2,2}} &amp; \\frac{\\partial L}{\\partial y_{2,3}} \\end{pmatrix} \\cdot \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ w_{2,1} &amp; w_{2,2} &amp; w_{2,3} \\end{pmatrix}\\] \\[= \\frac{\\partial L}{\\partial y_{2,1}} w_{2,1} + \\frac{\\partial L}{\\partial y_{2,2}} w_{2,2} + \\frac{\\partial L}{\\partial y_{2,3}} w_{2,3}\\] <p>Finally we can combine all these results to give a single expression for \\(\\frac{\\partial L}{\\partial X}\\) in terms of \\(W\\) and \\(\\frac{\\partial L}{\\partial Y}\\):</p> \\[\\frac{\\partial L}{\\partial X} = \\begin{pmatrix} \\frac{\\partial L}{\\partial y_{1,1}} w_{1,1} + \\frac{\\partial L}{\\partial y_{1,2}} w_{1,2} + \\frac{\\partial L}{\\partial y_{1,3}} w_{1,3} &amp; \\frac{\\partial L}{\\partial y_{1,1}} w_{2,1} + \\frac{\\partial L}{\\partial y_{1,2}} w_{2,2} + \\frac{\\partial L}{\\partial y_{1,3}} w_{2,3} \\\\ \\frac{\\partial L}{\\partial y_{2,1}} w_{1,1} + \\frac{\\partial L}{\\partial y_{2,2}} w_{1,2} + \\frac{\\partial L}{\\partial y_{2,3}} w_{1,3} &amp; \\frac{\\partial L}{\\partial y_{2,1}} w_{2,1} + \\frac{\\partial L}{\\partial y_{2,2}} w_{2,2} + \\frac{\\partial L}{\\partial y_{2,3}} w_{2,3} \\end{pmatrix}\\] \\[= \\begin{pmatrix} \\frac{\\partial L}{\\partial y_{1,1}} &amp; \\frac{\\partial L}{\\partial y_{1,2}} &amp; \\frac{\\partial L}{\\partial y_{1,3}} \\\\ \\frac{\\partial L}{\\partial y_{2,1}} &amp; \\frac{\\partial L}{\\partial y_{2,2}} &amp; \\frac{\\partial L}{\\partial y_{2,3}} \\end{pmatrix} \\begin{pmatrix} w_{1,1} &amp; w_{2,1} \\\\ w_{1,2} &amp; w_{2,2} \\\\ w_{1,3} &amp; w_{2,3} \\end{pmatrix}\\] \\[= \\frac{\\partial L}{\\partial Y} W^T\\] <p>Recall that \\(\\frac{\\partial L}{\\partial Y}\\) is a matrix of shape \\(N \\times M\\) and \\(W\\) is a matrix of shape \\(D \\times M\\); thus \\(\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W^T\\) has shape \\(N \\times D\\), which is the same shape as \\(X\\).</p> <p>Using the same strategy of thinking about components one at a time, you can derive a similarly simple equation to compute \\(\\frac{\\partial L}{\\partial W}\\) without explicitly forming the Jacobian \\(\\frac{\\partial Y}{\\partial W}\\):</p> \\[\\frac{\\partial L}{\\partial W} = X^T \\frac{\\partial L}{\\partial Y}\\] <p>In this equation \\(\\frac{\\partial L}{\\partial W}\\) must have the same shape as \\(W\\) (\\(D \\times M\\)); on the right hand side \\(X\\) is a matrix of shape \\(N \\times D\\) and \\(\\frac{\\partial L}{\\partial Y}\\) is a matrix of shape \\(N \\times M\\), so the matrix-matrix product on the right will produce a matrix of shape \\(D \\times M\\).</p> <p>This strategy of thinking one element at a time can help you to derive equations for backpropagation for a layer even when the inputs and outputs to the layer are tensors of arbitrary shape; this can be particularly valuable for example when deriving backpropagation for a convolutional layer.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/","title":"Convolutional Neural Networks: Architectures, Convolution / Pooling Layers","text":""},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#architecture-overview","title":"Architecture Overview","text":"<p>Regular Neural Nets don't scale well to full images. In CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32323 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 120,000 weights. Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.</p> <p>Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. Note that the word depth here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network. For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions 32x32x3 (width, height, depth respectively). As we will soon see, the neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. Moreover, the final output layer would for CIFAR-10 have dimensions 1x1x10, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension.</p> <p>A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#layers-used-to-build-convnets","title":"Layers used to build ConvNets","text":"<p>We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture.</p> <p>We will go into more details below, but a simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC]. In more detail:</p> <ul> <li>INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.</li> <li>CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.</li> <li>RELU layer will apply an elementwise activation function, such as the max(0,x) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).</li> <li>POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].</li> <li>FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.</li> </ul> <p></p> <p>The above image shows the activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and the last volume stores the class scores (right). Each volume of activations along the processing path is shown as a column. The last layer volume holds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one. The full web-based demo is shown in the header of this website. The architecture shown here is a tiny VGG Net.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#convolutional-layer","title":"Convolutional Layer","text":""},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#local-connectivity","title":"Local Connectivity","text":"<p>When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron (equivalently this is the filter size). The extent of the connectivity along the depth axis is always equal to the depth of the input volume. It is important to emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: the connections are local in 2D space (along width and height), but always full along the entire depth of the input volume.</p> <p>Example: For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 75 weights (and +1 bias parameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is the depth of the input volume.</p> <p>Example: Suppose an input volume had size [16x16x20]. Then using an example receptive field size of 3x3, every neuron in the Conv Layer would now have a total of 180 connections to the input volume. Notice that, again, the connectivity is local in 2D space (e.g. 3x3), but full along the input depth (20).</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#spatial-arrangement","title":"Spatial arrangement","text":"<p>We have explained the connectivity of each neuron in the Conv Layer to the input volume, but we haven't yet discussed how many neurons there are in the output volume or how they are arranged. Three hyperparameters control the size of the output volume: the depth, stride and zero-padding.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#depth","title":"Depth","text":"<p>The depth of the output volume is a hyperparameter: it corresponds to the number of filters we would like to use, each learning to look for something different in the input.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#stride","title":"Stride","text":"<p>We must specify the stride with which we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#zero-padding","title":"Zero-padding","text":"<p>Sometimes it will be convenient to pad the input volume with zeros around the border. The size of this zero-padding is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes (most commonly as we'll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).</p> <p>We can compute the spatial size of the output volume as a function of the input volume size (\\(W\\)), the receptive field size of the Conv Layer neurons (\\(F\\)), the stride with which they are applied (\\(S\\)), and the amount of zero padding used (\\(P\\)) on the border. You can convince yourself that the correct formula for calculating how many neurons \"fit\" is given by \\((W-F+2P)/S+1\\). For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output.</p> <p>In general, setting zero padding to be \\(P=(F-1)/2\\) when the stride is \\(S=1\\) ensures that the input volume and output volume will have the same size spatially. It is very common to use zero-padding in this way.</p> <p>Note again that the spatial arrangement hyperparameters have mutual constraints. For example, when the input has size \\(W=10\\), no zero-padding is used \\(P=0\\), and the filter size is \\(F=3\\), then it would be impossible to use stride \\(S=2\\), since \\((W-F+2P)/S+1=(10-3+0)/2+1=4.5\\), i.e. not an integer, indicating that the neurons don't \"fit\" neatly and symmetrically across the input. Therefore, this setting of the hyperparameters is considered to be invalid, and a ConvNet library could throw an exception or zero pad the rest to make it fit, or crop the input to make it fit, or something.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#case-study-detailed-example","title":"Case study (detailed example)","text":"<p>Suppose that the input volume X has shape <code>X.shape: (11,11,4)</code>. Suppose further that we use no zero padding (\\(P=0\\)), that the filter size is \\(F=5\\), and that the stride is \\(S=2\\). The output volume would therefore have spatial size \\((11-5)/2+1 = 4\\), giving a volume with width and height of 4. The activation map in the output volume (call it V), would then look as follows (only some of the elements are computed in this example):</p> <pre><code>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0\nV[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0\nV[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0\nV[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0\n</code></pre> <p>Remember that in numpy, the operation <code>*</code> above denotes elementwise multiplication between the arrays. Notice also that the weight vector W0 is the weight vector of that neuron and b0 is the bias. Here, W0 is assumed to be of shape <code>W0.shape: (5,5,4)</code>, since the filter size is 5 and the depth of the input volume is 4. Notice that at each point, we are computing the dot product as seen before in ordinary neural networks. Also, we see that we are using the same weight and bias (due to parameter sharing), and the dimensions along the width are increasing in steps of 2 (i.e. the stride). To construct a second activation map in the output volume, we would have:</p> <pre><code>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1\nV[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1\nV[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1\nV[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1\nV[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1\nV[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1\n</code></pre> <p>where we see that we are indexing into the second depth dimension in V (at index 1) because we are computing the second activation map, and that a different set of parameters (W1) is now used. In the example above, we are for brevity leaving out some of the other operations the Conv Layer would perform to fill the other parts of the output array V. Additionally, recall that these activation maps are often followed elementwise through an activation function such as ReLU, but this is not shown here.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#convolution-demo","title":"Convolution Demo","text":"<p>Below is a running demo of a CONV layer. Since 3D volumes are hard to visualize, all the volumes (the input volume (in blue), the weight volumes (in red), the output volume (in green)) are visualized with each depth slice stacked in rows. The input volume is of size \\(W_1=5, H_1=5, D_1=3\\), and the CONV layer parameters are \\(K=2, F=3, S=2, P=1\\). That is, we have two filters of size \\(3 \\times 3\\), and they are applied with a stride of 2. Therefore, the output volume size has spatial size \\((5 - 3 + 2)/2 + 1 = 3\\). Moreover, notice that a padding of \\(P=1\\) is applied to the input volume, making the outer border of the input volume zero. The visualization below iterates over the output activations (green), and shows that each element is computed by elementwise multiplying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#pooling-layer","title":"Pooling Layer","text":"<p>It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer:</p> <ul> <li>Accepts a volume of size \\(W_1 \\times H_1 \\times D_1\\)</li> <li>Requires two hyperparameters:</li> <li>their spatial extent \\(F\\),</li> <li>the stride \\(S\\),</li> <li>Produces a volume of size \\(W_2 \\times H_2 \\times D_2\\) where:</li> <li>\\(W_2 = (W_1 - F)/S + 1\\)</li> <li>\\(H_2 = (H_1 - F)/S + 1\\)</li> <li>\\(D_2 = D_1\\)</li> <li>Introduces zero parameters since it computes a fixed function of the input</li> <li>For Pooling layers, it is not common to pad the input using zero-padding.</li> </ul> <p>It is worth noting that there are only two commonly seen variations of the max pooling layer found in practice: A pooling layer with \\(F=3, S=2\\) (also called overlapping pooling), and more commonly \\(F=2, S=2\\). Pooling sizes with larger receptive fields are too destructive.</p> <p> </p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#general-pooling","title":"General pooling","text":"<p>In addition to max pooling, the pooling units can also perform other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#fully-connected-layer","title":"Fully-connected layer","text":"<p>Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#batch-normalization","title":"Batch Normalization","text":"<p>To understand the goal of batch normalization, it is important to first recognize that machine learning methods tend to perform better with input data consisting of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features. This will ensure that the first layer of the network sees data that follows a nice distribution. However, even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance, since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.</p> <p>The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, they propose to insert into the network some layers that normalize batches. At training time, such a layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.</p> <p>It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.</p> <p>Forward pass implementation:</p> <pre><code>def batchnorm_forward(x, gamma, beta, bn_param):\n    mode = bn_param[\"mode\"]\n    eps = bn_param.get(\"eps\", 1e-5)\n    momentum = bn_param.get(\"momentum\", 0.9)\n\n    N, D = x.shape\n    running_mean = bn_param.get(\"running_mean\", np.zeros(D, dtype=x.dtype))\n    running_var = bn_param.get(\"running_var\", np.zeros(D, dtype=x.dtype))\n\n    out = None\n    if mode == \"train\":\n        \"\"\"\n        In NumPy, axis=0 refers to the first dimension of an array. In a two-dimensional\n        array, this corresponds to the rows. When you specify axis=0 for a calculation,\n        you are telling NumPy to perform the operation across the rows, collapsing them\n        and producing a result for each column. Here, for each feature, we're computing\n        the mean and variance across the mini-batch.\n        Thus, sample_mean = [mean_feature_0, mean_feature_1, ...]\n        \"\"\"\n        sample_mean = np.mean(x, axis=0)\n        sample_var = np.var(x, axis=0)\n        x_normalized = (x - sample_mean) / np.sqrt(sample_var + eps)\n        out = gamma * x_normalized + beta\n        running_mean = (1 - momentum) * running_mean + momentum * sample_mean\n        running_var = (1 - momentum) * running_var + momentum * sample_var\n    elif mode == \"test\":\n        x_normalized = (x - running_mean) / np.sqrt(running_var + eps)\n        out = gamma * x_normalized + beta\n    else:\n        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n\n    bn_param[\"running_mean\"] = running_mean\n    bn_param[\"running_var\"] = running_var\n\n    return out\n</code></pre> <p>[1] Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", ICML 2015.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#convnet-architectures","title":"ConvNet Architectures","text":""},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#layer-patterns","title":"Layer Patterns","text":"<p>The most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image has been merged spatially to a small size. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architecture follows the pattern:</p> <p>INPUT -&gt; [[CONV -&gt; RELU] * N -&gt; POOL?] * M -&gt; [FC -&gt; RELU] * K -&gt; FC</p> <p>where the * indicates repetition, and the POOL? indicates an optional pooling layer. Moreover, N &gt;= 0 (and usually N &lt;= 3), M &gt;= 0, K &gt;= 0 (and usually K &lt; 3). For example, here are some common ConvNet architectures you may see that follow this pattern:</p> <ul> <li>INPUT -&gt; FC, implements a linear classifier. Here N = M = K = 0.</li> <li>INPUT -&gt; CONV -&gt; RELU -&gt; FC</li> <li>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL] * 2 -&gt; FC -&gt; RELU -&gt; FC. Here we see that there is a single CONV layer between every POOL layer.</li> <li>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL] * 3 -&gt; [FC -&gt; RELU] * 2 -&gt; FC Here we see two CONV layers stacked before every POOL layer. This is generally a good idea for larger and deeper networks, because multiple stacked CONV layers can develop more complex features of the input volume before the destructive pooling operation.</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#prefer-a-stack-of-small-filter-conv-to-one-large-receptive-field-conv-layer","title":"Prefer a stack of small filter CONV to one large receptive field CONV layer","text":"<p>Suppose that you stack three 3x3 CONV layers on top of each other (with non-linearities in between, of course). In this arrangement, each neuron on the first CONV layer has a 3x3 view of the input volume. A neuron on the second CONV layer has a 3x3 view of the first CONV layer, and hence by extension a 5x5 view of the input volume. Similarly, a neuron on the third CONV layer has a 3x3 view of the 2nd CONV layer, and hence a 7x7 view of the input volume. Suppose that instead of these three layers of 3x3 CONV, we only wanted to use a single CONV layer with 7x7 receptive fields. These neurons would have a receptive field size of the input volume that is identical in spatial extent (7x7), but with several disadvantages. First, the neurons would be computing a linear function over the input, while the three stacks of CONV layers contain non-linearities that make their features more expressive. Second, if we suppose that all the volumes have \\(C\\) channels (in practice, you might see architectures that increase channels with depth (e.g., 64 \u2192 128 \u2192 256), but for this theoretical comparison, keeping \\(C\\) constant makes the parameter count analysis clear and fair), then it can be seen that the single 7x7 CONV layer would contain \\(C \\times (7 \\times 7 \\times C) = 49C^2\\) parameters, while the three 3x3 CONV layers would only contain \\(3 \\times (C \\times (3 \\times 3 \\times C)) = 27C^2\\) parameters.</p> <p>For the single 7\u00d77 CONV layer:</p> <ul> <li> <p>Each filter has size \\(7 \\times 7 \\times C\\) (7\u00d77 spatial dimensions \u00d7 C input channels)</p> </li> <li> <p>We have \\(C\\) output channels (filters)</p> </li> <li> <p>Total parameters = \\(C \\times (7 \\times 7 \\times C) = C \\times 49C = 49C^2\\)</p> </li> </ul> <p>For the three stacked 3\u00d73 CONV layers:</p> <ul> <li> <p>Each 3\u00d73 filter has size \\(3 \\times 3 \\times C\\) </p> </li> <li> <p>We have \\(C\\) output channels (filters) per layer</p> </li> <li> <p>Total parameters per layer = \\(C \\times (3 \\times 3 \\times C) = C \\times 9C = 9C^2\\)</p> </li> <li> <p>Total for 3 layers = \\(3 \\times 9C^2 = 27C^2\\)</p> </li> </ul> <p>Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerful features of the input, and with fewer parameters. As a practical disadvantage, we might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#layer-sizing-patterns","title":"Layer Sizing Patterns","text":"<p>Until now we've omitted mentions of common hyperparameters used in each of the layers in a ConvNet. We will first state the common rules of thumb for sizing the architectures.</p> <p>The input layer (that contains the image) should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.</p> <p>The conv layers should be using small filters (e.g. 3x3 or at most 5x5), using a stride of \\(S=1\\), and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. That is, when \\(F=3\\), then using \\(P=1\\) will retain the original size of the input. When \\(F=5\\), \\(P=2\\). For a general \\(F\\), it can be seen that \\(P=(F-1)/2\\) preserves the input size. If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that is looking at the input image.</p> <p>The pool layers are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. \\(F=2\\)), and with a stride of 2 (i.e. \\(S=2\\)). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this makes \"fitting\" more complicated (e.g., a 32x32x3 layer would require zero padding to be used with a max-pooling layer with 3x3 receptive field and stride 2). It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#why-use-stride-of-1-in-conv","title":"Why use stride of 1 in CONV?","text":"<p>Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#why-use-padding","title":"Why use padding?","text":"<p>In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be \"washed away\" too quickly.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#case-studies","title":"Case studies","text":"<p>There are several architectures in the field of Convolutional Networks that have a name. The most common are:</p> <p>LeNet: The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990's. Of these, the best known is the LeNet architecture that was used to read zip codes, digits, etc.</p> <p>AlexNet: The first work that popularized Convolutional Networks in Computer Vision was the AlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).</p> <p>ZF Net: The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the ZFNet (short for Zeiler &amp; Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.</p> <p>GoogLeNet: The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al. from Google. Its main contribution was the development of an Inception Module that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several followup versions to the GoogLeNet, most recently Inception-v4.</p> <p>VGGNet: The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Their pretrained model is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.</p> <p>ResNet: Residual Network developed by Kaiming He et al. was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization. The architecture is also missing fully connected layers at the end of the network. The reader is also referred to Kaiming's presentation (video, slides), and some recent experiments that reproduce these networks in Torch. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016). In particular, also see more recent developments that tweak the original architecture from Kaiming He et al. Identity Mappings in Deep Residual Networks (published March 2016).</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#vggnet-in-detail","title":"VGGNet in detail","text":"<p>Let's break down the VGGNet in more detail as a case study. The whole VGGNet is composed of CONV layers that perform 3x3 convolutions with stride 1 and pad 1, and of POOL layers that perform 2x2 max pooling with stride 2 (and no padding). We can write out the size of the representation at each step of the processing and keep track of both the representation size and the total number of weights:</p> <pre><code>INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n</code></pre> <p>TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)</p> <p>TOTAL params: 138M parameters</p> <p>As is common with Convolutional Networks, notice that most of the memory (and also compute time) is used in the early CONV layers, and that most of the parameters are in the last FC layers. In this particular case, the first FC layer contains 100M weights, out of a total of 140M.</p>"},{"location":"ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/#additional-resources","title":"Additional Resources","text":"<p>Additional resources related to implementation:</p> <ol> <li> <p>Soumith benchmarks for CONV performance</p> </li> <li> <p>ConvNetJS CIFAR-10 demo allows you to play with ConvNet architectures and see the results and computations in real time, in the browser.</p> </li> <li> <p>Caffe, one of the popular ConvNet libraries.</p> </li> <li> <p>State of the art ResNets in Torch7</p> </li> </ol>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/","title":"Image Classification with Linear Classifiers","text":""},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#the-image-classification-task","title":"The Image Classification Task","text":"<p>Image classification is a fundamental computer vision task where, given an image and a set of labels, the goal is to assign the image to one of the labels. This is essentially a supervised learning problem where we want to learn a mapping from input images to discrete class labels.</p> <p>Formally given a dataset \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n\\) where:</p> <ul> <li> <p>\\(x_i\\) is an input image (typically represented as a high-dimensional vector of pixel values)</p> </li> <li> <p>\\(y_i\\) is the corresponding class label from a predefined set of categories \\(\\{1, 2, \\ldots, C\\}\\)</p> </li> <li> <p>The task is to learn a function \\(f: \\mathbb{R}^d \\rightarrow \\{1, 2, \\ldots, C\\}\\) that can accurately predict the class label for new, unseen images</p> </li> </ul> <p>Key challenges:</p> <ul> <li> <p>High dimensionality: Images are typically represented as very high-dimensional vectors (e.g., a 224\u00d7224 RGB image has 150,528 dimensions)</p> </li> <li> <p>Variability: The same object can appear in different poses, lighting conditions, scales, backgrounds, camera movements, with background clutter, at different scales (zoom) within the image, with partial occlusion, deformation, and varying contextual information</p> </li> <li> <p>Intra-class variation: Objects within the same class can look very different</p> </li> <li> <p>Inter-class similarity: Objects from different classes can sometimes look very similar</p> </li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#machine-learning-data-driven-approach","title":"Machine Learning: Data-Driven Approach","text":"<p>The machine learning approach to image classification follows a systematic, data-driven methodology that can be broken down into three main steps:</p> <p>1. Collect a Dataset of Images and Labels: The first step involves gathering a comprehensive dataset where each image is paired with its corresponding class label.</p> <p>2. Use ML Algorithms to Train a Classifier: Once we have the dataset, we employ machine learning algorithms like Linear Regression, Support Vector Machines (SVM), or Logistic Regression to learn a mapping from images to class labels. The choice of algorithm depends on the complexity of the problem, dataset size, and computational constraints. </p> <p>3. Evaluate the Classifier on New Images: The final step is to assess how well the trained classifier performs on previously unseen images. This evaluation process includes measuring accuracy, precision, recall, and F1-score on the held-out test set. Also includes validation techniques like k-fold cross-validation to get robust performance estimates.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#k-nearest-neighbors-k-nn-algorithm","title":"k-Nearest Neighbors (k-NN) Algorithm","text":"<p>The k-Nearest Neighbors algorithm is one of the simplest and most intuitive machine learning algorithms for classification. It's a non-parametric, instance-based learning method that makes predictions based on the similarity of new examples to previously seen training examples.</p> <p>Training Phase: k-NN is a \"lazy learner\" - it doesn't actually learn a model during training. Instead, it simply stores all the training examples and their labels.</p> <p>Prediction Phase: For a new test image, k-NN:</p> <ol> <li> <p>Computes the distance between the test image and all training images</p> </li> <li> <p>Identifies the k nearest training examples (neighbors)</p> </li> <li> <p>Assigns the class label that appears most frequently among these k neighbors</p> </li> </ol> <p>The choice of distance metric is crucial for k-NN performance:</p> <ul> <li> <p>Euclidean Distance: \\(d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\\)</p> </li> <li> <p>Manhattan Distance: \\(d(x, y) = \\sum_{i=1}^{n}|x_i - y_i|\\)</p> </li> <li> <p>Cosine Similarity: \\(d(x, y) = 1 - \\frac{x \\cdot y}{||x|| \\cdot ||y||}\\)</p> </li> </ul> <p>Understanding the computational efficiency of k-NN requires analyzing its time complexity using Big O notation.</p> <p>Big O Notation: \\(O(f(n))\\) describes how the runtime of an algorithm grows as the input size \\(n\\) increases. It provides an upper bound on the worst-case performance, focusing on the dominant term and ignoring constants and lower-order terms.</p> <p>k-NN Time Complexity:</p> <ul> <li> <p>Training Time: \\(O(1)\\) - k-NN doesn't perform any computation during training; it simply stores the training data</p> </li> <li> <p>Prediction Time: \\(O(N)\\) - For each prediction, k-NN must compute distances to all \\(N\\) training examples</p> </li> </ul> <p>In real-world applications, we typically want classifiers that are fast at prediction and can tolerate slow training because:</p> <ol> <li>Training happens once: We train the model offline, often overnight or over days, so training time is less critical</li> <li>Prediction happens repeatedly: Once deployed, the model makes thousands or millions of predictions per day</li> <li>Real-time requirements: Many applications (autonomous vehicles, medical diagnosis, security systems) need immediate predictions</li> <li>Scalability: As the dataset grows, k-NN prediction time grows linearly, making it impractical for large-scale systems</li> </ol> <p>This is why we often prefer parametric models (like linear classifiers) that invest computational effort upfront during training to enable fast predictions later.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#linear-classifiers","title":"Linear Classifiers","text":"<p>kNN has a number of disadvantages:</p> <ul> <li>The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.</li> <li>Classifying a test image is expensive since it requires a comparison to all training images.</li> </ul> <p>We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to Neural Networks. The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#parameterized-mapping-from-images-to-label-scores","title":"Parameterized mapping from images to label scores","text":"<p>The first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class. We will develop the approach with a concrete example. Let's assume a training dataset of images \\(x_i \\in \\mathbb{R}^D\\), each associated with a label \\(y_i\\). Here \\(i = 1 \\ldots N\\) and \\(y_i \\in 1 \\ldots K\\). That is, we have \\(N\\) examples (each with a dimensionality \\(D\\)) and \\(K\\) distinct categories. For example, in CIFAR-10 we have a training set of \\(N = 50,000\\) images, each with \\(D = 32 \\times 32 \\times 3 = 3072\\) pixels, and \\(K = 10\\), since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function \\(f: \\mathbb{R}^D \\mapsto \\mathbb{R}^K\\) that maps the raw image pixels to class scores.</p> <p>We will start out with arguably the simplest possible function, a linear mapping:</p> \\[f(x_i, W, b) = Wx_i + b\\] <p>In the above equation, we are assuming that the image \\(x_i\\) has all of its pixels flattened out to a single column vector of shape \\([D \\times 1]\\). The matrix \\(W\\) (of size \\([K \\times D]\\)), and the vector \\(b\\) (of size \\([K \\times 1]\\)) are the parameters of the function. In CIFAR-10, \\(x_i\\) contains all pixels in the \\(i\\)-th image flattened into a single \\([3072 \\times 1]\\) column, \\(W\\) is \\([10 \\times 3072]\\) and \\(b\\) is \\([10 \\times 1]\\), so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in \\(W\\) are often called the weights, and \\(b\\) is called the bias vector because it influences the output scores, but without interacting with the actual data \\(x_i\\).</p> <p>There are a few things to note:</p> <ol> <li> <p>First, note that the single matrix multiplication \\(Wx_i\\) is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of \\(W\\).</p> </li> <li> <p>Notice also that we think of the input data \\((x_i, y_i)\\) as given and fixed, but we have control over the setting of the parameters \\(W, b\\). Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set. We will go into much more detail about how this is done, but intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes.</p> </li> <li> <p>An advantage of this approach is that the training data is used to learn the parameters \\(W, b\\), but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores.</p> </li> <li> <p>Lastly, note that classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images.</p> </li> </ol> <p>Geometric Interpretation</p> <p>To understand linear classifiers geometrically, let's first consider the general form of a plane in \u211d\u00b3:</p> \\[ax + by + cz = d\\] <p>where \\((a, b, c)\\) is the normal vector to the plane and \\(d\\) determines the plane's position in space.</p> <ul> <li> <p>The normal vector \\((a, b, c)\\) is perpendicular to the plane</p> </li> <li> <p>The plane divides 3D space into two half-spaces</p> </li> <li> <p>Points on one side of the plane satisfy \\(ax + by + cz &gt; d\\)</p> </li> <li> <p>Points on the other side satisfy \\(ax + by + cz &lt; d\\)</p> </li> <li> <p>Points on the plane satisfy \\(ax + by + cz = d\\)</p> </li> </ul> <p>Example: The plane \\(2x - 3y + 4z = 12\\) has normal vector \\((2, -3, 4)\\).</p> <p>Now, let's see how this relates to linear classifiers. In a binary classification problem, our linear classifier computes:</p> \\[f(x) = w^T x + b\\] <p>where \\(w\\) is the weight vector and \\(b\\) is the bias.</p> <p>The Decision Boundary: The equation \\(w^T x + b = 0\\) defines a hyperplane in the input space that separates the two classes. This is exactly analogous to the plane equation \\(ax + by + cz = d\\). This is how:</p> <ul> <li> <p>\\(w\\) is the normal vector to the hyperplane (just like \\((a, b, c)\\) in the plane equation)</p> </li> <li> <p>\\(b\\) determines the position of the hyperplane (just like \\(d\\) in the plane equation)</p> </li> <li> <p>The hyperplane divides the input space into two half-spaces</p> </li> <li> <p>Points in one half-space are classified as class 1 (\\(w^T x + b &gt; 0\\))</p> </li> <li> <p>Points in the other half-space are classified as class 2 (\\(w^T x + b &lt; 0\\))</p> </li> </ul> <p>Multi-class extension: For \\(K\\) classes, we have \\(K\\) hyperplanes, each defined by a row of the weight matrix \\(W\\). </p> <p>In the multi-class case, we have \\(K\\) score functions:</p> \\[f_1(x) = w_1^T x + b_1\\] \\[f_2(x) = w_2^T x + b_2\\] \\[\\vdots\\] \\[f_K(x) = w_K^T x + b_K\\] <p>The classifier predicts class \\(i\\) if \\(f_i(x) &gt; f_j(x)\\) for all \\(j \\neq i\\).</p> <p>The decision boundary between classes \\(i\\) and \\(j\\) is the set of points where the classifier is indifferent between the two classes, i.e., where \\(f_i(x) = f_j(x)\\).</p> <p>Starting with:</p> \\[f_i(x) = f_j(x)\\] <p>Substituting the score functions:</p> \\[w_i^T x + b_i = w_j^T x + b_j\\] <p>Rearranging terms:</p> \\[w_i^T x - w_j^T x = b_j - b_i\\] <p>Factoring out \\(x\\):</p> \\[(w_i - w_j)^T x = b_j - b_i\\] <p>Moving all terms to one side:</p> \\[(w_i - w_j)^T x + (b_i - b_j) = 0\\] <p>The decision boundary between classes \\(i\\) and \\(j\\) is the hyperplane:</p> \\[(w_i - w_j)^T x + (b_i - b_j) = 0\\] <p>Note:</p> <ul> <li> <p>The normal vector to this decision boundary is \\((w_i - w_j)\\)</p> </li> <li> <p>The bias term is \\((b_i - b_j)\\)</p> </li> <li> <p>Points on one side of this hyperplane are classified as class \\(i\\)</p> </li> <li> <p>Points on the other side are classified as class \\(j\\)</p> </li> <li> <p>Points on the hyperplane are exactly at the decision boundary</p> </li> </ul> <p>Why this matters: This geometric interpretation helps us understand that:</p> <ol> <li> <p>Linear classifiers create linear decision boundaries</p> </li> <li> <p>The weight vector \\(w\\) determines the orientation of the decision boundary</p> </li> <li> <p>The bias \\(b\\) determines the position of the decision boundary</p> </li> <li> <p>The classifier's performance depends on how well the data can be separated by these linear boundaries</p> </li> </ol> <p>This connection between planes in \u211d\u00b3 and hyperplanes in linear classifiers provides an intuitive way to visualize and understand how linear classifiers work geometrically.</p> <p>Coming back to CIFAR-10, we cannot visualize 3072-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing.</p> <p></p> <p>Above figure shows a cartoon representation of the image space, where each image is a single point, and three classifiers are visualized. For example, take the car classifier (in red), where the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores.</p> <p>As we saw above, every row of \\(W\\) is a classifier for one of the classes. The geometric interpretation of these numbers is that as we change one of the rows of \\(W\\), the corresponding line in the pixel space will rotate in different directions. The biases \\(b\\), on the other hand, allow our classifiers to translate the lines.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#template-matching-interpretation","title":"Template Matching interpretation","text":"<p>Another interpretation for the weights \\(W\\) is that each row of \\(W\\) corresponds to a template (or sometimes also called a prototype) for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using an inner product (or dot product) one by one to find the one that \"fits\" best. With this terminology, the linear classifier is doing template matching, where the templates are learned. Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#bias-trick","title":"Bias Trick","text":"<p>Recall that we defined the score function as:</p> \\[f(x_i, W, b) = Wx_i + b\\] <p>It is a little cumbersome to keep track of two sets of parameters (the biases \\(b\\) and weights \\(W\\)) separately. A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector \\(x_i\\) with one additional dimension that always holds the constant 1- a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply:</p> \\[f(x_i, W) = Wx_i\\] <p>With our CIFAR-10 example, \\(x_i\\) is now \\([3073 \\times 1]\\) instead of \\([3072 \\times 1]\\) - (with the extra dimension holding the constant 1), and \\(W\\) is now \\([10 \\times 3073]\\) instead of \\([10 \\times 3072]\\). The extra column that \\(W\\) now corresponds to the bias \\(b\\). An illustration might help clarify this transformation.</p> <p></p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#loss-function","title":"Loss Function","text":"<p>We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if we\u2019re doing a poor job of classifying the training data, and it will be low if we\u2019re doing well.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#multiclass-support-vector-machine-loss","title":"Multiclass Support Vector Machine loss","text":"<p>The SVM loss is set up so that the SVM \"wants\" the correct class for each image to have a score higher than the incorrect classes by some fixed margin \\(\\Delta\\).</p> <p>Recall that for the \\(i\\)-th example we are given the pixels of image \\(x_i\\) and the label \\(y_i\\) that specifies the index of the correct class. The score function takes the pixels and computes the vector \\(f(x_i,W)\\) of class scores, which we will abbreviate to \\(s\\) (short for scores). For example, the score for the \\(j\\)-th class is the \\(j\\)-th element: \\(s_j = f(x_i,W)_j\\). The Multiclass SVM loss for the \\(i\\)-th example is then formalized as follows:</p> \\[L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)\\] <p>Example. Let's unpack this with an example to see how it works. Suppose that we have three classes that receive the scores \\(s = [13, -7, 11]\\), and that the first class is the true class (i.e. \\(y_i = 0\\)). Also assume that \\(\\Delta\\) (a hyperparameter we will go into more detail about soon) is 10. The expression above sums over all incorrect classes (\\(j \\neq y_i\\)), so we get two terms:</p> \\[L_i = \\max(0, -7 - 13 + 10) + \\max(0, 11 - 13 + 10)\\] <p>You can see that the first term gives zero since \\([-7 - 13 + 10]\\) gives a negative number, which is then thresholded to zero with the \\(\\max(0, -)\\) function. We get zero loss for this pair because the correct class score (13) was greater than the incorrect class score (-7) by at least the margin 10. In fact the difference was 20, which is much greater than 10 but the SVM only cares that the difference is at least 10; any additional difference above the margin is clamped at zero with the max operation. The second term computes \\([11 - 13 + 10]\\) which gives 8. That is, even though the correct class had a higher score than the incorrect class (13 &gt; 11), it was not greater by the desired margin of 10. The difference was only 2, which is why the loss comes out to 8 (i.e. how much higher the difference would have to be to meet the margin). In summary, the SVM loss function wants the score of the correct class \\(y_i\\) to be larger than the incorrect class scores by at least by \\(\\Delta\\) (delta). If this is not the case, we will accumulate loss.</p> <p></p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#softmax-classifier","title":"Softmax Classifier","text":"<p>Unlike the SVM which treats the outputs \\(f(x_i,W)\\) as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping \\(f(x_i;W) = Wx_i\\) stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss. The function \\(f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\\) is called the softmax function: it takes a vector of arbitrary real-valued scores (in \\(z\\)) and squashes it to a vector of values between zero and one that sum to one.</p> <p>The cross-entropy between a \"true\" distribution \\(p\\) and an estimated distribution \\(q\\) is defined as:</p> \\[H(p, q) = -\\sum_x p(x) \\log q(x)\\] <p>Note: This derivation is for the \\(i\\)-th sample, where \\(y_i\\) is the correct class label for that sample.</p> <p>For the Softmax classifier, we have:</p> <ul> <li> <p>True distribution: \\(p_i = [0, \\ldots, 1, \\ldots, 0]\\) (one-hot vector with 1 at position \\(y_i\\))</p> </li> <li> <p>Estimated distribution: \\(q_{i,j} = \\frac{e^{f_j}}{\\sum_k e^{f_k}}\\) (softmax probabilities)</p> </li> </ul> <p>Substituting into the cross-entropy formula:</p> \\[H(p_i,q_i) = -\\sum_{j=1}^{K} p_{i,j} \\log q_{i,j}\\] <p>Since \\(p_i\\) is a one-hot vector, only \\(p_{i,y_i} = 1\\) and all other \\(p_{i,j} = 0\\):</p> \\[H(p_i,q_i) = -p_{i,y_i} \\log q_{i,y_i} - \\sum_{j \\neq y_i} p_{i,j} \\log q_{i,j} = -1 \\cdot \\log q_{i,y_i} - \\sum_{j \\neq y_i} 0 \\cdot \\log q_{i,j}\\] \\[H(p_i,q_i) = -\\log q_{i,y_i} = -\\log\\left(\\frac{e^{f_{y_i}}}{\\sum_k e^{f_k}}\\right) = L_i\\] \\[L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\\right) \\quad \\text{or equivalently} \\quad L_i = -f_{y_i} + \\log\\sum_j e^{f_j}\\] <p>where we are using the notation \\(f_j\\) to mean the \\(j\\)-th element of the vector of class scores \\(f\\). As before, the full loss for the dataset is the mean of \\(L_i\\) over all training examples together with a regularization term \\(R(W)\\). The function \\(f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\\) is called the softmax function: it takes a vector of arbitrary real-valued scores (in \\(z\\)) and squashes it to a vector of values between zero and one that sum to one.</p> <p>Therefore, the cross-entropy between the true one-hot distribution and the estimated softmax distribution is exactly the Softmax loss \\(L_i\\).</p> <p>The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities (\\(q_{i,j} = e^{f_{y_i}}/\\sum_j e^{f_j}\\) as seen above) and the \"true\" distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. \\(p_i = [0, \\ldots, 1, \\ldots, 0]\\) contains a single 1 at the \\(y_i\\)-th position.).</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#the-kl-divergence-connection","title":"The KL Divergence connection","text":"<p>Before diving into KL divergence, let's understand entropy. The entropy \\(H(p)\\) of a distribution \\(p\\) measures the average amount of information (or uncertainty) in the distribution:</p> \\[H(p) = -\\sum_x p_{x} \\log p_{x}\\] <ul> <li> <p>High entropy: Distribution is spread out, high uncertainty (e.g., uniform distribution)</p> </li> <li> <p>Low entropy: Distribution is concentrated, low uncertainty (e.g., one-hot distribution)</p> </li> </ul> <p>Note: This derivation is for the \\(i\\)-th sample, where \\(y_i\\) is the correct class label for that sample.</p> <p>For our one-hot true distribution \\(p_i\\) where \\(p_{i,y_i} = 1\\) and \\(p_{i,j} = 0\\) for \\(j \\neq y_i\\):</p> \\[H(p_i) = -p_{i,y_i} \\log p_{i,y_i} - \\sum_{j \\neq y_i} p_{i,j} \\log p_{i,j} = -1 \\cdot \\log 1 - \\sum_{j \\neq y_i} 0 \\cdot \\log 0 = 0\\] <p>The entropy is zero because there's no uncertainty - we know exactly which class is correct.</p> <p>The KL divergence \\(D_{KL}(p||q)\\) measures how much information is lost when we use distribution \\(q\\) to approximate distribution \\(p\\). It's defined as:</p> \\[D_{KL}(p||q) = \\sum_x p_{x} \\log\\left(\\frac{p_{x}}{q_{x}}\\right)\\] <p>Note: This derivation is for the \\(i\\)-th sample, where \\(y_i\\) is the correct class label for that sample.</p> <p>For our one-hot true distribution \\(p_i\\) where \\(p_{i,y_i} = 1\\) and \\(p_{i,j} = 0\\) for \\(j \\neq y_i\\):</p> \\[D_{KL}(p_i||q_i) = p_{i,y_i} \\log\\left(\\frac{p_{i,y_i}}{q_{i,y_i}}\\right) + \\sum_{j \\neq y_i} p_{i,j} \\log\\left(\\frac{p_{i,j}}{q_{i,j}}\\right)\\] \\[D_{KL}(p_i||q_i) = 1 \\cdot \\log\\left(\\frac{1}{q_{i,y_i}}\\right) + \\sum_{j \\neq y_i} 0 \\cdot \\log\\left(\\frac{0}{q_{i,j}}\\right) = -\\log q_{i,y_i}\\] <p>Since \\(H(p_i) = 0\\) (the entropy of a deterministic distribution is zero), we have:</p> \\[H(p_i,q_i) = H(p_i) + D_{KL}(p_i||q_i) = 0 + (-\\log q_{i,y_i}) = -\\log q_{i,y_i} = L_i\\] <p>This shows that minimizing the cross-entropy loss is exactly equivalent to minimizing the KL divergence between the true one-hot distribution and the predicted softmax distribution. The KL divergence is zero only when \\(q_{i,y_i} = 1\\) (perfect confidence in the correct class), and it increases as the predicted probability of the correct class decreases.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#practical-issues-numeric-stability","title":"Practical issues: numeric stability","text":"<p>When you're writing code for computing the Softmax function in practice, the intermediate terms \\(e^{f_{y_i}}\\) and \\(\\sum_j e^{f_j}\\) may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant \\(C\\) and push it into the sum, we get the following (mathematically equivalent) expression:</p> \\[\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} = \\frac{C e^{f_{y_i}}}{C \\sum_j e^{f_j}} = \\frac{e^{f_{y_i} + \\log C}}{\\sum_j e^{f_j + \\log C}}\\] <p>We are free to choose the value of \\(C\\). This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for \\(C\\) is to set \\(\\log C = -\\max_j f_j\\). This simply states that we should shift the values inside the vector \\(f\\) so that the highest value is zero.</p> <p>Example code:</p> <pre><code>f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\np = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n\n# instead: first shift the values of f so that the highest number is 0:\nf -= np.max(f) # f becomes [-666, -333, 0]\np = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#svm-vs-softmax","title":"SVM vs. Softmax","text":"<p>A picture might help clarify the distinction between the Softmax and SVM classifiers using an example.</p> <p></p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#final-notes-on-terminology","title":"Final notes on terminology","text":"<p>1. Softmax</p> <p>The softmax function is not a classifier itself, but it is an essential component of a classifier. The term \"softmax classifier\" is often used informally to describe a multi-class classification model that uses the softmax function in its final layer. However, the actual classification work\u2014the process of learning to predict the correct classes\u2014is done by the entire model, not just the softmax layer.</p> <p>2. SVM</p> <p>A \"linear classifier + SVM loss function\" is a Linear SVM. This is not a new or different kind of model; it is simply a Support Vector Machine that uses a linear function to find the optimal decision boundary.</p> <p>The key components of a Linear SVM are:</p> <ul> <li> <p>Linear decision function: \\(f(x) = Wx + b\\)</p> </li> <li> <p>Hinge loss: \\(L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)\\)</p> </li> <li> <p>Margin maximization: The SVM tries to maximize the margin between classes</p> </li> </ul> <p>The term \"Linear SVM\" emphasizes that the decision boundary is linear (a hyperplane), as opposed to kernel SVMs that can have non-linear decision boundaries through the kernel trick.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#additional-references","title":"Additional References","text":"<p>Here are some (optional) links you may find interesting for further reading:</p> <p>A Few Useful Things to Know about Machine Learning, where especially section 6 is related but the whole paper is a warmly recommended reading.</p> <p>Recognizing and Learning Object Categories, a short course of object categorization at ICCV 2005.</p> <p>Deep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.</p>"},{"location":"ai/deep_learning_for_computer_vision/introduction/","title":"Introduction","text":"<p>Machine Learning is a subset of artificial intelligence that focuses on algorithms and statistical models that enable computer systems to improve their performance on a specific task through experience, without being explicitly programmed for every scenario. At its core, ML is about finding patterns in data and using those patterns to make predictions or decisions. The field encompasses supervised learning (learning from labeled examples), unsupervised learning (finding hidden patterns in unlabeled data), and reinforcement learning (learning through interaction with an environment).</p> <p>Deep Learning is a specialized branch of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to model and understand complex patterns in data. These networks are inspired by the structure and function of the human brain, with interconnected nodes (neurons) that process information through weighted connections. Deep learning excels at tasks involving high-dimensional data like images, audio, and text, where traditional machine learning methods often struggle. Key architectures include convolutional neural networks (CNNs) for computer vision, recurrent neural networks (RNNs) and transformers for natural language processing, and generative adversarial networks (GANs) for creating synthetic data. The success of deep learning is largely due to the availability of large datasets, powerful computing resources (especially GPUs), and sophisticated optimization techniques like backpropagation. Deep learning has revolutionized fields like computer vision, natural language processing, speech recognition, and has enabled breakthroughs in areas ranging from protein folding prediction to creative AI applications.</p>"},{"location":"ai/deep_learning_for_computer_vision/introduction/#hubel-and-wiesels-experiments","title":"Hubel and Wiesel's Experiments","text":"<p>The foundation of modern computer vision and convolutional neural networks can be traced back to the groundbreaking work of David Hubel and Torsten Wiesel in the 1950s and 1960s. Their experiments on the visual cortex of cats and monkeys revealed fundamental principles about how the brain processes visual information.</p> <p>Hubel and Wiesel inserted microelectrodes into the primary visual cortex (V1) of cats and monkeys to record the electrical activity of individual neurons while presenting various visual stimuli. They discovered that neurons in the visual cortex respond selectively to specific features rather than responding to all visual input uniformly.</p> <p>Simple Cells: These neurons respond to oriented edges and lines at specific locations in the visual field. Each simple cell has a preferred orientation (e.g., horizontal, vertical, or diagonal) and responds most strongly when a line or edge of that orientation is presented at a particular location.</p> <p>Complex Cells: These neurons also respond to oriented edges but are less sensitive to the exact position of the stimulus. They maintain their response even when the stimulus is shifted slightly within their receptive field, making them more invariant to position.</p> <p>Hierarchical Organization: Hubel and Wiesel discovered that the visual cortex is organized hierarchically, with simple cells feeding into complex cells, which in turn feed into even more complex feature detectors. This hierarchical processing allows the brain to build increasingly complex representations from simple visual features.</p> <p>The experiments revealed several fundamental principles that directly inspired the design of convolutional neural networks:</p> <ol> <li> <p>Local Receptive Fields: Neurons respond to stimuli in small, localized regions of the visual field, not the entire image.</p> </li> <li> <p>Feature Detection: The visual system detects specific features (edges, orientations) rather than processing raw pixel values.</p> </li> <li> <p>Hierarchical Processing: Complex visual patterns are built up from simpler features through multiple layers of processing.</p> </li> <li> <p>Translation Invariance: Higher-level neurons become increasingly invariant to the exact position of features.</p> </li> <li> <p>Shared Weights: Similar feature detectors are replicated across different spatial locations.</p> </li> </ol> <p>These insights provided the biological inspiration for convolutional neural networks, where convolutional layers detect local features (like edges), pooling layers provide translation invariance, and multiple layers build up hierarchical representations of increasing complexity. Hubel and Wiesel's work earned them the Nobel Prize in Physiology or Medicine in 1981 and remains foundational to our understanding of both biological and artificial vision systems.</p>"},{"location":"ai/deep_learning_for_computer_vision/introduction/#what-solving-computer-vision-means","title":"What solving Computer Vision means","text":"<p>Computer vision aims to enable machines to interpret and understand visual information from the world, essentially replicating the human ability to \"see\" and make sense of visual data.</p> <p>Core Tasks: Object detection and recognition, scene understanding, motion analysis, 3D reconstruction, and visual reasoning. The ultimate goal is to extract meaningful information from images and videos that can be used for decision-making, navigation, interaction, and understanding.</p>"},{"location":"ai/deep_learning_for_computer_vision/introduction/#how-nature-has-solved-vision","title":"How Nature has solved Vision","text":"<p>Compound Eyes (Insects): Insects like bees and dragonflies have compound eyes composed of thousands of individual photoreceptor units (ommatidia). Each unit captures light from a small portion of the visual field, creating a mosaic image. This design provides excellent motion detection and wide field of view, crucial for navigation and predator avoidance. The honeybee's visual system can detect polarized light, enabling navigation using the sun's position even on cloudy days.</p> <p>Camera Eyes (Vertebrates): Most vertebrates, including humans, have camera-like eyes with a single lens focusing light onto a retina. The retina contains specialized photoreceptor cells (rods for low-light vision, cones for color vision) and complex neural processing layers. This design provides high resolution and excellent color discrimination, enabling detailed object recognition and complex visual tasks.</p> <p>Specialized Vision Systems: Different species have evolved unique adaptations. Mantis shrimp have 12-16 different photoreceptor types (compared to humans' 3), enabling them to see a vast spectrum of colors including ultraviolet and polarized light. Birds of prey have exceptional visual acuity - eagles can spot prey from kilometers away. Nocturnal animals like cats have enhanced low-light vision with reflective tapetum layers.</p> <p>Neural Processing: All these systems share common principles: hierarchical feature detection (from simple edges to complex objects), parallel processing of different visual attributes (motion, color, depth), and extensive neural plasticity allowing adaptation to environmental changes. The visual cortex processes information in specialized regions - V1 for basic features, V2-V4 for intermediate processing, and higher areas for object recognition and scene understanding.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/","title":"Neural Networks: Learning and Evaluation","text":""},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#learning","title":"Learning","text":""},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#gradient-checks","title":"Gradient Checks","text":"<p>In theory, performing a gradient check is as simple as comparing the analytic gradient to the numerical gradient. In practice, the process is much more involved and error prone. Here are some tips, tricks, and issues to watch out for:</p> <p>Use the centered formula: The formula you may have seen for the finite difference approximation when evaluating the numerical gradient looks as follows:</p> \\[\\frac{df(x)}{dx} = \\frac{f(x+h) - f(x)}{h}\\] <p>where \\(h\\) is a small number, which in practice is approximately 1e-5 or so. In practice, it is often much better to use the centered formula:</p> \\[\\frac{df(x)}{dx} = \\frac{f(x+h) - f(x-h)}{2h}\\] <p>The forward difference formula can reuse the baseline evaluation \\(f(x)\\) across all dimensions, requiring only \\(1 + n\\) total evaluations (where \\(n\\) is the number of parameters). However, the centered formula needs \\(2n\\) evaluations because each dimension requires two separate function calls: \\(f(x+h)\\) and \\(f(x-h)\\) for that specific parameter. This is nearly twice as expensive, but the gradient approximation turns out to be much more precise.</p> <p>Example with \\(x = [x_1, x_2, x_3, x_4]\\):</p> <p>Forward difference formula:</p> <ul> <li> <p>\\(f(x)\\) (baseline, reused for all dimensions)</p> </li> <li> <p>\\(f([x_1+h, x_2, x_3, x_4])\\) (for \\(\\partial f/\\partial x_1\\))</p> </li> <li> <p>\\(f([x_1, x_2+h, x_3, x_4])\\) (for \\(\\partial f/\\partial x_2\\))  </p> </li> <li> <p>\\(f([x_1, x_2, x_3+h, x_4])\\) (for \\(\\partial f/\\partial x_3\\))</p> </li> <li> <p>\\(f([x_1, x_2, x_3, x_4+h])\\) (for \\(\\partial f/\\partial x_4\\))</p> </li> <li> <p>Total: 5 evaluations</p> </li> </ul> <p>Centered difference formula:</p> <ul> <li> <p>\\(f([x_1+h, x_2, x_3, x_4])\\) and \\(f([x_1-h, x_2, x_3, x_4])\\) (for \\(\\partial f/\\partial x_1\\))</p> </li> <li> <p>\\(f([x_1, x_2+h, x_3, x_4])\\) and \\(f([x_1, x_2-h, x_3, x_4])\\) (for \\(\\partial f/\\partial x_2\\))</p> </li> <li> <p>\\(f([x_1, x_2, x_3+h, x_4])\\) and \\(f([x_1, x_2, x_3-h, x_4])\\) (for \\(\\partial f/\\partial x_3\\))</p> </li> <li> <p>\\(f([x_1, x_2, x_3, x_4+h])\\) and \\(f([x_1, x_2, x_3, x_4-h])\\) (for \\(\\partial f/\\partial x_4\\))</p> </li> <li> <p>Total: 8 evaluations</p> </li> </ul> <p>Use relative error for the comparison: What are the details of comparing the numerical gradient \\(f'_n\\) to the analytic gradient \\(f'_a\\)? You might be tempted to keep track of whether their difference is greater than some threshold (e.g. 1e-4). However, this is problematic. For example, consider the case where their difference is 1e-4, and if the analytic gradient is 1e-2 then we'd consider the quantities to be very close, and hence the gradient would be okay. But if we consider the case where the analytic gradient is 1e-5, then we'd consider 1e-4 to be a huge difference, and hence the gradient would be bad. It is more appropriate to consider the relative error:</p> \\[\\frac{|f'_a - f'_n|}{\\max(|f'_a|, |f'_n|)}\\] <p>which considers their ratio of the differences to the ratio of the absolute values of both gradients. In practice:</p> <ul> <li>relative error &gt; 1e-2 usually means the gradient is probably wrong</li> <li>1e-2 &gt; relative error &gt; 1e-4 should make you feel uncomfortable</li> <li>1e-4 &gt; relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities), then 1e-4 is too high.</li> <li>1e-7 and less you should be happy.</li> </ul> <p>Also keep in mind that the deeper the network, the higher the relative errors will be. So if you are gradient checking the input data for a 10-layer network, a relative error of 1e-2 might be okay because the errors build up on the way. Conversely, an error of 1e-2 for a single differentiable function likely indicates incorrect gradient.</p> <p>Use double precision: A common pitfall is using single precision floating point to compute gradient check. It is often that case that you might get high relative errors (as high as 1e-2) even with a correct gradient implementation. Sometimes relative errors plummet from 1e-2 to 1e-8 by switching to double precision.</p> <p>Stick around active range of floating point: It's a good idea to read through \"What Every Computer Scientist Should Know About Floating-Point Arithmetic\", as it may demystify your errors and enable you to write more careful code. For example, in neural nets it can be common to normalize the loss function over the batch. However, if your gradients per datapoint are very small, then additionally dividing them by the number of data points is starting to give very small numbers, which in turn will lead to more numerical issues. You may want to temporarily scale your loss function up by a constant to bring them to a \"nicer\" range where floats are more dense- ideally on the order of 1e-3 to 1e-1.</p> <p>Be careful with the step size h: It is not necessarily the case that smaller \\(h\\) is better, because when \\(h\\) is much smaller, you might start running into numerical precision issues. Sometimes when the gradient doesn't check, it is possible that you change \\(h\\) to be \\(1e-4\\) or \\(1e-6\\) and suddenly the gradient will pass. This has to do with numerical precision issues with the finite difference approximation.</p> <p>Gradcheck during a \"characteristic\" mode of operation: It is important to realize that a gradient check is performed at a particular (and usually random), single point in the space of parameters. Even if the gradient check succeeds at that point, it is not immediately certain that the gradient is correctly implemented globally. Additionally, a random initialization might not be the most \"characteristic\" point in the space of parameters and may in fact introduce pathological situations where the gradient seems to be correctly implemented but isn't. To be safe it is best to use a short burn-in time during which the network is allowed to learn and perform the gradient check after the loss starts to go down. The danger of performing it at the first iteration is that this could introduce pathological edge cases and mask an incorrect implementation of the gradient.</p> <p>Don't let the regularization overwhelm the data: It is often the case that a loss function is a sum of the data loss and the regularization loss (e.g. L2 penalty on weights). One danger to be aware of is that the regularization loss may overwhelm the data loss, in which case the gradients will be primarily coming from the regularization term (which usually has a much simpler gradient expression). This can mask an incorrect implementation of the data loss gradient. Therefore, it is recommended to turn off regularization and check the data loss alone first, and then the regularization term second and independently.</p> <p>Remember to turn off dropout: Turn off any non-deterministic effects in your network, such as dropout. Otherwise the gradient check will fail. Dropout introduces randomness by randomly setting some neurons to zero during training. This creates several problems for gradient checking. Each time you evaluate the loss function \\(f(x)\\), dropout randomly masks different neurons, so \\(f(x)\\) and \\(f(x+h)\\) are computed with different random patterns. This means the numerical gradient \\(\\frac{f(x+h) - f(x)}{h}\\) includes noise from the random dropout patterns, not just the true gradient. The noise introduced by dropout can be much larger than the actual gradient differences you're trying to measure. The analytic gradient is computed assuming all neurons are active (no dropout), but the numerical gradient is computed with random dropout applied. This creates a fundamental mismatch between what you're comparing. Always set your network to evaluation mode (which disables dropout) before performing gradient checks. In PyTorch, this means calling <code>model.eval()</code>, and in TensorFlow, ensure dropout layers are disabled during the gradient check.</p> <p>Check only a few dimensions. Gradient checks can be expensive to run. If you have many parameters, it can be good practice to check only some of the dimensions of the gradient and assume that the others are correct.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#before-learning-sanity-checks-tipstricks","title":"Before learning: sanity checks Tips/Tricks","text":"<p>Here are a few sanity checks you might consider running before you plunge into expensive optimization.</p> <ul> <li>Make sure you're getting the loss you expect when you initialize with small parameters. It's best to first check the data loss alone (so set regularization strength to zero). For example, for CIFAR-10 with a Softmax classifier we would expect the initial loss to be 2.302, because we expect a diffuse probability of 0.1 for each class (since there are 10 classes), and Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302. If you're not seeing these losses there might be issue with initialization.</li> <li>As a second sanity check, increasing the regularization strength should increase the loss.</li> <li>Lastly and most importantly, before training on the full dataset try to train on a tiny portion (e.g. 20 examples) of your data and make sure you can achieve zero cost. For this experiment it's also best to set regularization to zero, otherwise this can prevent you from getting zero cost. Unless you pass this sanity check with a small dataset it is not worth proceeding to the full dataset.</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#babysitting-the-learning-process","title":"Babysitting the learning process","text":"<p>There are multiple useful quantities you should monitor during training of a neural network. These plots are the window into the training process and should be utilized to get intuitions about different hyperparameter settings and how they should be changed for more efficient learning.</p> <p>The x-axis of the plots below are always in units of epochs, which measure how many times every example has been seen during training in expectation (e.g. one epoch means that every example has been seen once). It is preferable to track epochs rather than iterations since the number of iterations depends on the arbitrary setting of batch size.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#loss-function","title":"Loss function","text":"<p>The first quantity that is useful to track during training is the loss, as it is evaluated on the individual batches during the forward pass. Below is a cartoon diagram showing the loss over time, and especially what the shape might tell you about the learning rate.</p> <p> </p> <p>Top: A cartoon depicting the effects of different learning rates. With low learning rates the improvements will be linear. With high learning rates they will start to look more exponential. Higher learning rates will decay the loss faster, but they get stuck at worse values of loss (green line). This is because there is too much \"energy\" in the optimization and the parameters are bouncing around chaotically, unable to settle in a nice spot in the optimization landscape. Down: An example of a typical loss function over time, while training a small network on CIFAR-10 dataset. This loss function looks reasonable (it might indicate a slightly too small learning rate based on its speed of decay, but it's hard to say), and also indicates that the batch size might be a little too low (since the cost is a little too noisy).</p> <p>The amount of \"wiggle\" in the loss is related to the batch size. When the batch size is 1, the wiggle will be relatively high. When the batch size is the full dataset, the wiggle will be minimal because every gradient update should be improving the loss function monotonically (unless the learning rate is set too high).</p> <p>Sometimes loss functions can look funny lossfunctions.tumblr.com.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#trainval-accuracy","title":"Train/Val accuracy","text":"<p>The second important quantity to track while training a classifier is the validation/training accuracy. This plot can give you valuable insights into the amount of overfitting in your model:</p> <p></p> <p>The gap between the training and validation accuracy indicates the amount of overfitting. Two possible cases are shown in the diagram on the left. The blue validation error curve shows very small validation accuracy compared to the training accuracy, indicating strong overfitting (note, it's possible for the validation accuracy to even start to go down after some point). When you see this in practice you probably want to increase regularization (stronger L2 weight penalty, more dropout, etc.) or collect more data. The other possible case is when the validation accuracy tracks the training accuracy fairly well.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#ratio-of-weightsupdates","title":"Ratio of weights:updates","text":"<p>The last quantity you might want to track is the ratio of the update magnitudes to the value magnitudes. Note: updates, not the raw gradients (e.g. in vanilla sgd this would be the gradient multiplied by the learning rate). You might want to evaluate and log the norm of the weights (or some subset of the weights), and the norm of the updates (or again, some subset). Looking at the ratio of these two quantities can be helpful. For a single weight this ratio should be somewhere around \\(1e-3\\). If it is much smaller than this then the learning rate might be too low. If it is much higher then the learning rate might be too high.</p> <pre><code># assume parameter vector W and its gradient vector dW\nparam_scale = np.linalg.norm(W.ravel())\nupdate = -learning_rate*dW # simple SGD update\nupdate_scale = np.linalg.norm(update.ravel())\nW += update # the actual update\nprint update_scale / param_scale # want ~1e-3\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#first-layer-visualizations","title":"First-layer Visualizations","text":"<p>Lastly, when one is working with image pixels it can be helpful and satisfying to plot the first-layer features visually.</p> <p> </p> <p>Examples of visualized weights for the first layer of a neural network. Top: Noisy features indicate could be a symptom: Unconverged network, improperly set learning rate, very low weight regularization penalty. Bottom: Nice, smooth, clean and diverse features are a good indication that the training is proceeding well.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#parameter-updates","title":"Parameter updates","text":"<p>Once the analytic gradient is computed with backpropagation, the gradients are used to perform a parameter update. There are several approaches for performing the update, which we discuss next.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#sgd-and-bells-and-whistles","title":"SGD and bells and whistles","text":"<p>Vanilla update: The simplest form of update is to change the parameters along the negative gradient direction (since the gradient indicates the direction of increase, but we usually wish to minimize a loss function). Assuming a vector of parameters \\(x\\) and the gradient \\(dx\\), the simplest update has the form:</p> <pre><code># Vanilla update\nx += - learning_rate * dx\n</code></pre> <p>where \\(learning\\_rate\\) is a hyperparameter- a fixed constant. When evaluated on the full dataset, and when the learning rate is low enough, this is guaranteed to make non-negative progress on the loss function.</p> <p>Momentum update: Another approach that almost always enjoys better converge rates on deep networks. Here is the momentum update:</p> <pre><code># Momentum update\nv = mu * v - learning_rate * dx\nx += v\n</code></pre> <p>Here we see an introduction of a \\(v\\) variable that is initialized at zero, and an additional hyperparameter (\\(mu\\)). As an unfortunate misnomer, this variable is in optimization referred to as momentum (its typical value is about 0.9), but its physical meaning is more consistent with the coefficient of friction. Effectively, this variable damps the effect of the learning rate. When cross-validated, this parameter is usually set to values such as [0.5, 0.9, 0.95, 0.99]. Similar to annealing schedules for learning rates (discussed later, below), optimization can sometimes benefit a little from momentum schedules, where the momentum is increased in later stages of learning.</p> <p>Nesterov Momentum is a slightly different version of the momentum update. It enjoys stronger theoretical converge guarantees for convex functions and in practice it also consistenly works slightly better than standard momentum.</p> <p>The core idea behind Nesterov momentum is that when the current parameter vector is at some position \\(x\\), then looking at the momentum update above, we know that the momentum term alone (i.e. ignoring the second term with the gradient) is about to nudge the parameter vector by \\(mu \\cdot v\\). Therefore, if we are about to compute the gradient, we can treat the future approximate position \\(x + mu \\cdot v\\) as a \"lookahead\" - this is a point in the vicinity of where we are soon going to end up. Hence, it makes sense to compute the gradient at \\(x + mu \\cdot v\\) instead of at the \"old/stale\" position \\(x\\).</p> <p></p> <p>Nesterov momentum. Instead of evaluating gradient at the current position (red circle), we know that our momentum is about to carry us to the tip of the green arrow. With Nesterov momentum we therefore instead evaluate the gradient at this \"looked-ahead\" position.</p> <p>That is, in a slightly awkward notation, we would like to do the following:</p> <pre><code>x_ahead = x + mu * v\n# evaluate dx_ahead (the gradient at x_ahead instead of at x)\nv = mu * v - learning_rate * dx_ahead\nx += v\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#annealing-the-learning-rate","title":"Annealing the learning rate","text":"<p>In training deep networks, it is usually helpful to anneal the learning rate over time. Good intuition to have in mind is that with a high learning rate, the system contains too much kinetic energy and the parameter vector bounces around chaotically, unable to settle down into deep, narrow valleys of the loss function (where the optimum might be). Knowing when to decay the learning rate can be tricky: Decay it slowly and you'll be wasting computation bouncing around chaotically with little improvement for a long time. But decay it too aggressively and the system will cool too quickly, unable to reach the best position it can.</p> <p>There are three common types of implementing the learning rate decay:</p> <p>Step decay: Reduce the learning rate by some factor every few epochs. Typical values might be reducing the learning rate by a half every 5 epochs, or by a factor of 0.1 every 20 epochs. These numbers depend heavily on the type of problem and the model.</p> <p>Exponential decay: Has the mathematical form \\(\\alpha = \\alpha_0 e^{-kt}\\), where \\(\\alpha_0, k\\) are hyperparameters and \\(t\\) is the iteration number (but you can also use units of epochs).</p> <p>1/t decay: Has the mathematical form \\(\\alpha = \\alpha_0/(1+kt)\\) where \\(\\alpha_0, k\\) are hyperparameters and \\(t\\) is the iteration number.</p> <p>In practice, we find that the step decay is slightly preferable because the hyperparameters it involves are more interpretable than the hyperparameters involved in the other two methods.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#second-order-methods","title":"Second order methods","text":"<p>A second, popular group of methods for optimization in context of deep learning is based on Newton's method, which iterates the following update:</p> \\[x \\leftarrow x - [H f(x)]^{-1} \\nabla f(x)\\] <p>Here, \\(H f(x)\\) is the Hessian matrix, which is a square matrix of second-order partial derivatives of the function. The term \\(\\nabla f(x)\\) is the gradient vector, as seen in Gradient Descent. Intuitively, the Hessian describes the local curvature of the loss function, which allows us to perform a more efficient update. In particular, multiplying by the inverse Hessian leads the optimization to take more aggressive steps in directions of shallow curvature and shorter steps in directions of steep curvature. Note, crucially, the absence of any learning rate hyperparameters in the update formula, which the proponents of these methods cite this as a large advantage over first-order methods.</p> <p>However, the update above is impractical for most deep learning applications because computing (and inverting) the Hessian in its explicit form is a very costly process in both space and time. For instance, a Neural Network with one million parameters would have a Hessian matrix of size \\([1,000,000 \\times 1,000,000]\\), occupying approximately 3725 gigabytes of RAM. In practice, it is currently not common to second-order methods applied to large-scale Deep Learning and Convolutional Neural Networks. Instead, SGD variants based on (Nesterov's) momentum are more standard because they are simpler and scale more easily.</p> <p>Additional references:</p> <ul> <li>Large Scale Distributed Deep Networks is a paper from the Google Brain team, comparing L-BFGS and SGD variants in large-scale distributed optimization.</li> <li>SFO algorithm strives to combine the advantages of SGD with advantages of L-BFGS.</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#per-parameter-adaptive-learning-rates","title":"Per-parameter adaptive learning rates","text":"<p>All previous approaches we've discussed so far manipulated the learning rate in a dimension-independent way. The following methods adapt the learning rate on a per-parameter basis, and can give more fine-grained control.</p> <p>Adagrad: Adaptive learning rate method originally proposed by Duchi et al.</p> <pre><code># Assume the gradient dx and parameter vector x\ncache += dx**2\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\n</code></pre> <p>Notice that the variable \\(cache\\) has size equal to the gradient matrix, and keeps track of per-parameter sum of squared gradients. This is then used to normalize the parameter update step, element-wise. The net effect is that parameters that receive big gradients will have their effective learning rate reduced, while parameters that receive small or infrequent updates will have their effective learning rate increased. Amusingly, the square root operation turns out to be very important and without it the algorithm performs much worse. The smoothing term \\(eps\\) (usually set somewhere in range from 1e-4 to 1e-8) avoids division by zero. </p> <p>A downside of Adagrad is that in case of Deep Learning, a monotonically decreasing learning rate usually proves too aggressive and stops learning too early. This happens because the cumulative sum of squared gradients grows indefinitely, making the effective learning rate <code>learning_rate / \u221a(cache)</code> smaller and smaller over time.</p> <p>RMSprop: A very effective, but currently unpublished adaptive learning rate method. Amusingly, everyone who uses this method in their work currently cites slide 29 of Lecture 6 of Geoff Hinton's Coursera class. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. In particular, it uses a moving average of squared gradients instead.</p> <pre><code>cache = decay_rate * cache + (1 - decay_rate) * dx**2\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\n</code></pre> <p>Here, \\(decay\\_rate\\) is a hyperparameter and typical values are [0.9, 0.99, 0.999]. Notice that the \\(x+=\\) update is identical to Adagrad. Hence, RMSProp still modulates the learning rate of each weight based on the magnitudes of its gradients, which has a beneficial equalizing effect, but unlike Adagrad the updates do not get monotonically smaller.</p> <p>Adam: Works well in practice and compares favorably to RMSProp. For more details see the paper.</p> <pre><code># Adam update\nm = beta1*m + (1-beta1)*dx\nv = beta2*v + (1-beta2)*(dx**2)\nx += - learning_rate * m / (np.sqrt(v) + eps)\n</code></pre> <p>Notice that the update looks exactly as RMSProp update. Recommended values in the paper are \\(eps = 1e-8\\), \\(beta1 = 0.9\\), \\(beta2 = 0.999\\). In practice Adam is currently recommended as the default algorithm to use, and often works slightly better than RMSProp. However, it is often also worth trying SGD+Nesterov Momentum as an alternative. The full Adam update also includes a bias correction mechanism, which compensates for the fact that in the first few time steps the vectors \\(m,v\\) are both initialized and therefore biased at zero, before they fully \"warm up\". With the bias correction mechanism, the update looks as follows:</p> <pre><code># t is your iteration counter going from 1 to infinity\nm = beta1*m + (1-beta1)*dx\nmt = m / (1-beta1**t)\nv = beta2*v + (1-beta2)*(dx**2)\nvt = v / (1-beta2**t)\nx += - learning_rate * mt / (np.sqrt(vt) + eps)\n</code></pre> <p>Note that the update is now a function of the iteration as well as the other parameters.</p> <p>Unit Tests for Stochastic Optimization proposes a series of tests as a standardized benchmark for stochastic optimization.</p> <p> </p> <p>Animations that may help your intuitions about the learning process dynamics. Top: Contours of a loss surface and time evolution of different optimization algorithms. Notice the \"overshooting\" behavior of momentum-based methods, which make the optimization look like a ball rolling down the hill. Bottom: A visualization of a saddle point in the optimization landscape, where the curvature along different dimension has different signs (one dimension curves up and another down). Notice that SGD has a very hard time breaking symmetry and gets stuck on the top. Conversely, algorithms such as RMSprop will see very low gradients in the saddle direction. Due to the denominator term in the RMSprop update, this will increase the effective learning rate along this direction, helping RMSProp proceed.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#hyperparameter-optimization","title":"Hyperparameter optimization","text":"<p>As we've seen, training Neural Networks can involve many hyperparameter settings. The most common hyperparameters in context of Neural Networks include the initial learning rate, the learning rate decay schedule (such as the decay constant), and the regularization strength (L2 penalty, dropout strength). But as we saw, there are many more relatively less sensitive hyperparameters, for example in per-parameter adaptive learning methods, the setting of momentum and its schedule, etc.</p> <p>Search for hyperparameters on log scale. For example, a typical sampling of the learning rate would look as follows: \\(learning\\_rate = 10^{uniform(-6, 1)}\\). That is, we are generating a random number from a uniform distribution, but then raising it to the power of 10. Some parameters (e.g. dropout) are instead usually searched in the original scale (e.g. \\(dropout = uniform(0,1)\\)).</p> <p>As argued by Bergstra and Bengio in Random Search for Hyper-Parameter Optimization, \"randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid\". As it turns out, this is also usually easier to implement.</p> <p></p> <p>Core illustration from Random Search for Hyper-Parameter Optimization by Bergstra and Bengio. It is very often the case that some of the hyperparameters matter much more than others. Performing random search rather than grid search allows you to much more precisely discover good values for the important ones.</p> <p>Sometimes it can happen that you're searching for a hyperparameter (e.g. learning rate) in a bad range. For example, suppose we use \\(learning\\_rate = 10^{uniform(-6, 1)}\\). Once we receive the results, it is important to double check that the final learning rate is not at the edge of this interval, or otherwise you may be missing more optimal hyperparameter setting beyond the interval.</p> <p>In practice, it can be helpful to first search in coarse ranges (e.g. \\(10^{[-6, 1]}\\)), and then depending on where the best results are turning up, narrow the range. Also, it can be helpful to perform the initial coarse search while only training for 1 epoch or even less, because many hyperparameter settings can lead the model to not learn at all, or immediately explode with infinite cost. The second stage could then perform a narrower search with 5 epochs, and the last stage could perform a detailed search in the final range for many more epochs (for example).</p> <p>Bayesian Hyperparameter Optimization is a whole area of research devoted to coming up with algorithms that try to more efficiently navigate the space of hyperparameters. The core idea is to appropriately balance the exploration - exploitation trade-off when querying the performance at different hyperparameters. Multiple libraries have been developed based on these models as well, among some of the better known ones are Spearmint, SMAC, and Hyperopt. However, in practical settings with ConvNets it is still relatively difficult to beat random search in a carefully-chosen intervals.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#evaluation","title":"Evaluation","text":""},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#model-ensembles","title":"Model Ensembles","text":"<p>In practice, one reliable approach to improving the performance of Neural Networks by a few percent is to train multiple independent models, and at test time average their predictions. As the number of models in the ensemble increases, the performance typically monotonically improves (though with diminishing returns). Moreover, the improvements are more dramatic with higher model variety in the ensemble. There are a few approaches to forming an ensemble.</p> <ul> <li> <p>Same model, different initializations: Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization.</p> </li> <li> <p>Top models discovered during cross-validation: Use cross-validation to determine the best hyperparameters, then pick the top few (e.g. 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it doesn't require additional retraining of models after cross-validation.</p> </li> <li> <p>Different checkpoints of a single model: If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that is very cheap.</p> </li> </ul> <p>One disadvantage of model ensembles is that they take longer to evaluate on test example. An interested reader may find the recent work from Geoff Hinton on \"Dark Knowledge\" inspiring, where the idea is to \"distill\" a good ensemble back to a single model by incorporating the ensemble log likelihoods into a modified objective.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/#additional-references","title":"Additional References","text":"<ul> <li>SGD tips and tricks from Leon Bottou</li> <li>Efficient BackProp (pdf) from Yann LeCun</li> <li>Practical Recommendations for Gradient-Based Training of Deep Architectures from Yoshua Bengio</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/","title":"Neural Networks: Setting up the Architecture","text":""},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#modeling-one-neuron","title":"Modeling one neuron","text":"<p>The area of Neural Networks has originally been primarily inspired by the goal of modeling biological neural systems, but has since diverged and become a matter of engineering and achieving good results in Machine Learning tasks. Nonetheless, we begin our discussion with a very brief and high-level description of the biological system that a large portion of this area has been inspired by.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#biological-motivation-and-connections","title":"Biological motivation and connections","text":"<p>The basic computational unit of the brain is a neuron. Approximately 86 billion neurons can be found in the human nervous system and they are connected with approximately 10^14 - 10^15 synapses. Each neuron receives input signals from its dendrites and produces output signals along its (single) axon. The axon eventually branches out and connects via synapses to dendrites of other neurons. In the computational model of a neuron, the signals that travel along the axons (e.g. \\(x_0\\)) interact multiplicatively (e.g. \\(w_0 x_0\\)) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. \\(w_0\\)). The idea is that the synaptic strengths (the weights \\(w\\)) are learnable and control the strength of influence (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another. In the basic model, the dendrites carry the signal to the cell body where they all get summed. If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon. In the computational model, we assume that the precise timings of the spikes do not matter, and that only the frequency of the firing communicates information. Based on this rate code interpretation, we model the firing rate of the neuron with an activation function \\(f\\), which represents the frequency of the spikes along the axon. Historically, a common choice of activation function is the sigmoid function \\(\\sigma\\), since it takes a real-valued input (the signal strength after the sum) and squashes it to range between 0 and 1.</p> <p> </p> <p>An example code for forward-propagating a single neuron might look as follows:</p> <pre><code>class Neuron(object):\n  # ... \n  def forward(self, inputs):\n    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n    cell_body_sum = np.sum(inputs * self.weights) + self.bias\n    firing_rate = 1.0 / (1 + math.exp(-cell_body_sum)) # sigmoid activation function\n    return firing_rate\n</code></pre> <p>In other words, each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity (or activation function), in this case the sigmoid \\(\\sigma(x) = 1/(1+e^{-x})\\).</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#single-neuron-as-a-linear-classifier","title":"Single neuron as a linear classifier","text":"<p>The mathematical form of the model Neuron's forward computation might look familiar to you. As we saw with linear classifiers, a neuron has the capacity to \"like\" (activation near one) or \"dislike\" (activation near zero) certain linear regions of its input space. Hence, with an appropriate loss function on the neuron's output, we can turn a single neuron into a linear classifier:</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#binary-softmax-classifier","title":"Binary Softmax classifier","text":"<p>For example, we can interpret \\(\\sigma(\\sum_iw_ix_i + b)\\) to be the probability of one of the classes \\(P(y_i = 1 \\mid x_i; w)\\). The probability of the other class would be \\(P(y_i = 0 \\mid x_i; w) = 1 - P(y_i = 1 \\mid x_i; w)\\), since they must sum to one. With this interpretation, we can formulate the cross-entropy loss, and optimizing it would lead to a binary Softmax classifier (also known as logistic regression). Since the sigmoid function is restricted to be between 0-1, the predictions of this classifier are based on whether the output of the neuron is greater than 0.5.</p> <p>A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers)</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#commonly-used-activation-functions","title":"Commonly used activation functions","text":"<p>Every activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it.</p> <p> </p> <p>Top: Sigmoid non-linearity squashes real numbers to range between [0,1] Bottom: The tanh non-linearity squashes real numbers to range between [-1,1].</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#sigmoid","title":"Sigmoid","text":"<p>The sigmoid non-linearity has the mathematical form \\(\\sigma(x) = 1 / (1 + e^{-x})\\) and takes a real-valued number and \"squashes\" it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1. The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). In practice, the sigmoid non-linearity has fallen out of favor and it is rarely ever used.</p> <p>The Saturation Problem: The most critical issue with sigmoid neurons is that they suffer from gradient saturation. When the neuron's activation saturates at either tail of 0 or 1, the gradient at these regions becomes almost zero. The derivative of the sigmoid function is:</p> \\[\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x))\\] <p>Notice that when \\(\\sigma(x) \\approx 0\\) (saturated at low values) or \\(\\sigma(x) \\approx 1\\) (saturated at high values), the derivative approaches zero. The consequence of this can be shown with an example below.</p> <p>Example: Consider a simple neural network with one sigmoid neuron:</p> <p>Forward pass: Input \\(x = 5\\), weight \\(w = 2\\), bias \\(b = 0\\)</p> <ul> <li> <p>Pre-activation: \\(z = wx + b = 2 \\times 5 + 0 = 10\\)</p> </li> <li> <p>Sigmoid output: \\(\\sigma(10) = \\frac{1}{1 + e^{-10}} \\approx 0.99995\\) (saturated!)</p> </li> </ul> <p>Backward pass: Suppose the gradient from the output layer is \\(d_{out} = 0.1\\)</p> <ul> <li> <p>Local gradient: \\(\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1 - \\sigma(z)) = 0.99995 \\times (1 - 0.99995) \\approx 0.00005\\)</p> </li> <li> <p>Gradient through linear layer: \\(\\frac{dz}{dw} = \\frac{d(wx + b)}{dw} = x = 5\\)</p> </li> <li> <p>Gradient to weight: \\(\\frac{dL}{dw} = d_{out} \\times \\frac{d\\sigma(z)}{dz} \\times \\frac{dz}{dw} = 0.1 \\times 0.00005 \\times 5 = 0.000025\\)</p> </li> </ul> <p>The gradient is now 4000 times smaller than the original error signal! This means the weight will barely update, making learning extremely slow or impossible.</p> <p>In deep networks, this gradient vanishing effect compounds across layers. If multiple sigmoid neurons are saturated, the gradients can become so small that they're lost to numerical precision, effectively stopping learning entirely. This is why sigmoid activations are rarely used in modern deep learning architectures.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#tanh","title":"Tanh","text":"<p>The tanh non-linearity squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: \\(\\tanh(x) = 2 \\sigma(2x) -1\\).</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#relu","title":"ReLU","text":"<p>The Rectified Linear Unit is very popular. It computes the function \\(f(x) = \\max(0, x)\\). In other words, the activation is simply thresholded at zero.</p> <p> </p> <p>Top: Rectified Linear Unit (ReLU) activation function, which is zero when x &lt; 0 and then linear with slope 1 when x &gt; 0. Down: A plot from Krizhevsky et al. (pdf) paper indicating the 6x improvement in convergence with the ReLU unit compared to the tanh unit.</p> <p>There are several pros and cons to using the ReLUs:</p> <ul> <li>(+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions.</li> <li>(+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.</li> <li>(-) Unfortunately, ReLU units can be fragile during training and can \"die\". For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. A high learning rate can do this. Once the ReLU neuron cannot activate, it is considered \"dead\". For example, you may find that as much as 40% of your network can be \"dead\" (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#leaky-relu","title":"Leaky ReLU","text":"<p>Leaky ReLUs are one attempt to fix the \"dying ReLU\" problem. Instead of the function being zero when x &lt; 0, a leaky ReLU will instead have a small positive slope (of 0.01, or so). That is, the function computes \\(f(x) = \\mathbb{1}(x &lt; 0) (\\alpha x) + \\mathbb{1}(x&gt;=0) (x)\\) where \\(\\alpha\\) is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear.</p> <p>As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.</p> <p>TLDR: \"What neuron type should I use?\" Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of \"dead\" units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#neural-network-architectures","title":"Neural Network architectures","text":""},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#layer-wise-organization","title":"Layer-wise organization","text":"<p>Neural Networks as neurons in graphs: Neural Networks are modeled as collections of neurons that are connected in an acyclic graph. In other words, the outputs of some neurons can become inputs to other neurons. Cycles are not allowed since that would imply an infinite loop in the forward pass of a network. For regular neural networks, the most common layer type is the fully-connected layer in which neurons between two adjacent layers are fully pairwise connected, but neurons within a single layer share no connections.</p> <p> </p> <p>Top: A 2-layer Neural Network (one hidden layer of 4 neurons (or units) and one output layer with 2 neurons), and three inputs. Bottom: A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Notice that in both cases there are connections (synapses) between neurons across layers, but not within a layer.</p> <p>The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with example networks:</p> <ul> <li>A 2-layer network might have 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters.</li> <li>A 3-layer network might have 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters.</li> </ul> <p>To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#representational-power","title":"Representational power","text":"<p>One way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. A natural question that arises is: What is the representational power of this family of functions? In particular, are there functions that cannot be modeled with a Neural Network?</p> <p>It turns out that Neural Networks with at least one hidden layer are universal approximators. That is, it can be shown (e.g. see Approximation by Superpositions of Sigmoidal Function from 1989 (pdf), or this intuitive explanation from Michael Nielsen) that given any continuous function \\(f(x)\\) and some \\(\\epsilon &gt; 0\\), there exists a Neural Network \\(g(x)\\) with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that \\(\\forall x, \\mid f(x) - g(x) \\mid &lt; \\epsilon\\). In other words, the neural network can approximate any continuous function.</p> <p>If one hidden layer suffices to approximate any function, why use more layers and go deeper? The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice. In one dimension, the \"sum of indicator bumps\" function \\(g(x) = \\sum_i c_i \\mathbb{1}(a_i &lt; x &lt; b_i)\\) where \\(a,b,c\\) are parameter vectors is also a universal approximator, but noone would suggest that we use this functional form in Machine Learning.</p> <p>Consider Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.</p> <p>If you are interested in these topics we recommend for further reading:</p> <ul> <li>Deep Learning book in press by Bengio, Goodfellow, Courville, in particular Chapter 6.4.</li> <li>Do Deep Nets Really Need to be Deep?</li> <li>FitNets: Hints for Thin Deep Nets</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#setting-number-of-layers-and-their-sizes","title":"Setting number of layers and their sizes","text":"<p>How do we decide on what architecture to use when faced with a practical problem? Should we use no hidden layers? One hidden layer? Two hidden layers? How large should each layer be? First, note that as we increase the size and number of layers in a Neural Network, the capacity of the network increases. That is, the space of representable functions grows since the neurons can collaborate to express many different functions. For example, suppose we had a binary classification problem in two dimensions. We could train three separate neural networks, each with one hidden layer of some size and obtain the following classifiers as shown below.</p> <p></p> <p>Larger Neural Networks can represent more complicated functions. The data are shown as circles colored by their class, and the decision regions by a trained neural network are shown underneath.</p> <p>Neural Networks with more neurons can express more complicated functions. However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. For example, a model with 20 hidden neurons might fit all the training data but at the cost of segmenting the space into many disjoint decision regions. A model with 3 hidden neurons only has the representational power to classify the data in broad strokes. It models the data as two blobs and interprets the few outliers as noise. In practice, this could lead to better generalization on the test set.</p> <p>Based on our discussion above, it seems that smaller neural networks can be preferred if the data is not complex enough to prevent overfitting. However, this is incorrect- there are many other preferred ways to prevent overfitting in Neural Networks such as L2 regularization, dropout, etc. In practice, it is always better to use these methods to control overfitting instead of the number of neurons.</p> <p>The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: It's clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. On the other hand, if you train a large network you'll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.</p> <p>To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network.</p> <p></p> <p>The effects of regularization strength: Each neural network above has 20 hidden neurons, but changing the regularization strength makes its final decision regions smoother with a higher regularization.</p> <p>The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#additional-references","title":"Additional References","text":"<ul> <li>ConvNetJS demos for intuitions</li> <li>Michael Nielsen's tutorials</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/","title":"Neural Networks: Setting up the Data","text":"<p>A Neural Network performs a sequence of linear mappings with interwoven non-linearities.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#data-preprocessing","title":"Data Preprocessing","text":"<p>There are three common forms of data preprocessing a data matrix <code>X</code>, where we will assume that <code>X</code> is of size <code>[N x D]</code> (<code>N</code> is the number of data, <code>D</code> is their dimensionality).</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#mean-subtraction","title":"Mean subtraction","text":"<p>This is the most common form of preprocessing. It involves subtracting the mean across every individual feature in the data, and has the geometric interpretation of centering the cloud of data around the origin along every dimension. In numpy, this operation would be implemented as: <code>X -= np.mean(X, axis = 0)</code>. With images specifically, for convenience it can be common to subtract a single value from all pixels (e.g. <code>X -= np.mean(X)</code>), or to do so separately across the three color channels.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#normalization","title":"Normalization","text":"<p>This refers to normalizing the data dimensions so that they are of approximately the same scale. There are two common ways of achieving this normalization. One is to divide each dimension by its standard deviation, once it has been zero-centered: (<code>X /= np.std(X, axis = 0)</code>). Another form of this preprocessing normalizes each dimension so that the min and max along the dimension is -1 and 1 respectively. It only makes sense to apply this preprocessing if you have a reason to believe that different input features have different scales (or units), but they should be of approximately equal importance to the learning algorithm. In case of images, the relative scales of pixels are already approximately equal (and in range from 0 to 255), so it is not strictly necessary to perform this additional preprocessing step.</p> <p></p> <p>Common data preprocessing pipeline. Left: Original toy, 2-dimensional input data. Middle: The data is zero-centered by subtracting the mean in each dimension. The data cloud is now centered around the origin. Right: Each dimension is additionally scaled by its standard deviation. The red lines indicate the extent of the data - they are of unequal length in the middle, but of equal length on the right.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#pca-and-whitening","title":"PCA and Whitening","text":"<p>This is another form of preprocessing. Out of scope for this document.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#weight-initialization","title":"Weight Initialization","text":"<p>Before we can begin to train the network we have to initialize its parameters.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#pitfall-all-zero-initialization","title":"Pitfall: all zero initialization","text":"<p>Let's start with what we should not do. Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative (with proper normalization, the input features have been centered so that each feature has mean \u2248 0). A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the \"best guess\" in expectation. This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#small-random-numbers","title":"Small random numbers","text":"<p>Therefore, we still want the weights to be very close to zero, but as we have argued above, not identically zero. As a solution, it is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. The implementation for one weight matrix might look like <code>W = 0.01* np.random.randn(D,H)</code>, where <code>randn</code> samples from a zero mean, unit standard deviation gaussian. With this formulation, every neuron's weight vector is initialized as a random vector sampled from a multi-dimensional gaussian, so the neurons point in random direction in the input space. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.</p> <p>Warning: It's not necessarily the case that smaller numbers will work strictly better. For example, a Neural Network layer that has very small weights will during backpropagation compute very small gradients on its data (since this gradient, \\(\\frac{\\partial L}{\\partial x}\\) is proportional to the value of the weights). This could greatly diminish the \"gradient signal\" flowing backward through a network, and could become a concern for deep networks. The gradient being discussed here is \\(\\frac{\\partial L}{\\partial x}\\), where \\(x\\) represents the input to the current layer (which is the output of the previous layer). This gradient is needed to train the entire network - it flows backward from layer to layer, enabling all layers to learn. While the original training data is constant, the intermediate activations (outputs of each layer) change during training, and we need gradients with respect to these to update the weights of previous layers. Small weights make these gradients small, which can slow down or prevent learning in earlier layers of deep networks.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#calibrating-the-variances-with-1sqrtn","title":"Calibrating the variances with 1/sqrt(n)","text":""},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#the-problem-with-randomly-initialized-weights","title":"The problem with randomly initialized weights","text":"<p>One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. Based on the principle that the sum of more independent random variables increases variance, a neuron with more inputs will produce a larger variance in its initial output. This has several consequences for training a neural network.</p> <p>A neuron's output is calculated by a weighted sum of its inputs, followed by an activation function. Before training, the weights and inputs can be considered independent random variables.</p> \\[\\text{pre-activation} = z = \\sum_{i=1}^{n} w_i x_i\\] <p>If the number of inputs (\\(n\\)) increases, the variance of this sum increases.</p> \\[\\text{Var}(z) = \\text{Var}\\left(\\sum_{i=1}^{n} w_i x_i\\right)\\] <p>Assuming the weights (\\(w_i\\)) and inputs (\\(x_i\\)) are independent and identically distributed, the variance of the pre-activation will be proportional to the number of inputs, \\(n\\).</p> \\[\\text{Var}(z) \\propto n\\] <p>The vanishing/exploding gradient problem: This uncontrolled increase in variance is the primary problem:</p> <ul> <li> <p>Deep Networks: In deep networks, the variance of the pre-activation will grow exponentially with each successive layer.</p> </li> <li> <p>Exploding Gradients: During backpropagation, the gradients are multiplied by the weights of each layer. If the weights are too large (due to high variance from initialization), the gradients will also grow exponentially, leading to instability during training. This is known as the exploding gradient problem.</p> </li> <li> <p>Vanishing Gradients: Conversely, if the initial weights are too small, the gradients can shrink exponentially with each layer, becoming too small to update the weights effectively. This is the vanishing gradient problem.</p> </li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#the-solution","title":"The solution","text":"<p>It turns out that we can normalize the variance of each neuron's output to 1 by scaling its weight vector by the square root of its fan-in (i.e. its number of inputs). That is, the recommended heuristic is to initialize each neuron's weight vector as: <code>w = np.random.randn(n) / sqrt(n)</code>, where <code>n</code> is the number of its inputs. This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence.</p> <p>The sketch of the derivation is as follows: Consider the inner product \\(s = \\sum_i^n w_i x_i\\) between the weights \\(w\\) and input \\(x\\), which gives the raw activation of a neuron before the non-linearity. We can examine the variance of \\(s\\):</p> \\[\\begin{align} \\text{Var}(s) &amp;= \\text{Var}(\\sum_i^n w_ix_i) \\\\\\\\ &amp;= \\sum_i^n \\text{Var}(w_ix_i) \\\\\\\\ &amp;= \\sum_i^n [E(w_i)]^2\\text{Var}(x_i) + [E(x_i)]^2\\text{Var}(w_i) + \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\ &amp;= \\sum_i^n \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\ &amp;= \\left( n \\text{Var}(w) \\right) \\text{Var}(x) \\end{align}\\] <p>where in the first 2 steps we have used properties of variance. In third step we assumed zero mean inputs and weights, so \\(E[x_i] = E[w_i] = 0\\). Note that this is not generally the case: for example ReLU units will have a positive mean. In the last step we assumed that all \\(w_i, x_i\\) are identically distributed. From this derivation we can see that if we want \\(s\\) to have the same variance as all of its inputs \\(x\\), then during initialization we should make sure that the variance of every weight \\(w\\) is \\(1/n\\). And since \\(\\text{Var}(aX) = a^2\\text{Var}(X)\\) for a random variable \\(X\\) and a scalar \\(a\\), this implies that we should draw from unit gaussian and then scale it by \\(a = \\sqrt{1/n}\\), to make its variance \\(1/n\\). This gives the initialization <code>w = np.random.randn(n) / sqrt(n)</code>.</p> <p>A more recent paper on this topic, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification by He et al., derives an initialization specifically for ReLU neurons, reaching the conclusion that the variance of neurons in the network should be \\(2.0/n\\). This gives the initialization <code>w = np.random.randn(n) * sqrt(2.0/n)</code>, and is the current recommendation for use in practice in the specific case of neural networks with ReLU neurons.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#initializing-the-biases","title":"Initializing the biases","text":"<p>It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#batch-normalization","title":"Batch Normalization","text":"<p>A recently developed technique by Ioffe and Szegedy called Batch Normalization alleviates a lot of headaches with properly initializing neural networks by explicitly forcing the activations throughout a network to take on a unit gaussian distribution at the beginning of the training. The core observation is that this is possible because normalization is a simple differentiable operation. In the implementation, applying this technique usually amounts to insert the BatchNorm layer immediately after fully connected layers (or convolutional layers, as we'll soon see), and before non-linearities. We do not expand on this technique here because it is well described in the linked paper, but note that it has become a very common practice to use Batch Normalization in neural networks. In practice networks that use Batch Normalization are significantly more robust to bad initialization. Additionally, batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network architecture in a differentiable manner.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#regularization","title":"Regularization","text":"<p>There are several ways of controlling the capacity of Neural Networks to prevent overfitting.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#l2-regularization","title":"L2 regularization","text":"<p>This is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. That is, for every weight \\(w\\) in the network, we add the term \\(\\frac{1}{2} \\lambda w^2\\) to the objective, where \\(\\lambda\\) is the regularization strength. It is common to see the factor of \\(\\frac{1}{2}\\) in front because then the gradient of this term with respect to the parameter \\(w\\) is simply \\(\\lambda w\\) instead of \\(2 \\lambda w\\). The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the appealing property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. Lastly, notice that during gradient descent parameter update, using the L2 regularization ultimately means that every weight is decayed linearly: \\(W += -\\lambda \\cdot W\\) towards zero.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#l1-regularization","title":"L1 regularization","text":"<p>This is another relatively common form of regularization, where for each weight \\(w\\) we add the term \\(\\lambda |w|\\) to the objective. It is possible to combine the L1 regularization with the L2 regularization: \\(\\lambda_1 |w| + \\lambda_2 w^2\\) (this is called Elastic net regularization). The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the \"noisy\" inputs. In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give better performance than L1.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#max-norm-constraints","title":"Max norm constraints","text":"<p>Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector \\(\\vec{w}\\) of every neuron to satisfy \\(||\\vec{w}||_2 &lt; c\\). Typical values of \\(c\\) are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that even if the learning rate is set too high, the network cannot \"explode\" because the updates are always bounded.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#dropout","title":"Dropout","text":"<p>This is an extremely effective, simple and recently introduced regularization technique by Srivastava et al. in Dropout: A Simple Way to Prevent Neural Networks from Overfitting (pdf) that complements the other methods (L1, L2, maxnorm). While training, dropout is implemented by only keeping a neuron active with some probability \\(p\\) (a hyperparameter), or setting it to zero otherwise.</p> <p></p> <p>Left: A standard 2-layer Neural Network. Right: An example of a 2-layer Neural Network with dropout applied. Crossed units have been randomly \"dropped out\" of the network. During testing there is no dropout applied</p> <p>Vanilla dropout in an example 3-layer Neural Network would be implemented as follows:</p> <pre><code>\"\"\" Vanilla Dropout: Not recommended implementation (see notes below) \"\"\"\n\np = 0.5 # probability of keeping a unit active. higher = less dropout\n\ndef train_step(X):\n  \"\"\" X contains the data \"\"\"\n\n  # forward pass for example 3-layer neural network\n  H1 = np.maximum(0, np.dot(W1, X) + b1)\n  U1 = np.random.rand(*H1.shape) &lt; p # first dropout mask\n  H1 *= U1 # drop!\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  U2 = np.random.rand(*H2.shape) &lt; p # second dropout mask\n  H2 *= U2 # drop!\n  out = np.dot(W3, H2) + b3\n\n  # backward pass: compute gradients... (not shown)\n  # perform parameter update... (not shown)\n\ndef predict(X):\n  # ensembling forward pass\n  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # scale the activations\n  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # scale the activations\n  out = np.dot(W3, H2) + b3\n</code></pre> <p>Crucially, note that in the predict function we are not dropping anymore, but we are performing a scaling of both hidden layer outputs by \\(p\\). This is important because at test time all neurons see all their inputs, so we want the outputs of neurons at test time to be identical to their expected outputs at training time. For example, in case of \\(p=0.5\\), the neurons must halve their outputs at test time to have the same output as they had during training time (in expectation). To see this, consider an output of a neuron \\(x\\) (before dropout). With dropout, the expected output from this neuron will become \\(px+(1-p)0\\), because the neuron's output will be set to zero with probability \\(1-p\\). At test time, when we keep the neuron always active, we must adjust \\(x \\rightarrow px\\) to keep the same expected output.</p> <p>The undesirable property of the scheme presented above is that we must scale the activations by \\(p\\) at test time. Since test-time performance is so critical, it is always preferable to use inverted dropout, which performs the scaling at train time, leaving the forward pass at test time untouched. Additionally, this has the appealing property that the prediction code can remain untouched when you decide to tweak where you apply dropout, or if at all. Inverted dropout looks as follows.</p> <pre><code>\"\"\" \nInverted Dropout: Recommended implementation example.\nWe drop and scale at train time and don't do anything at test time.\n\"\"\"\n\np = 0.5 # probability of keeping a unit active. higher = less dropout\n\ndef train_step(X):\n  # forward pass for example 3-layer neural network\n  H1 = np.maximum(0, np.dot(W1, X) + b1)\n  U1 = (np.random.rand(*H1.shape) &lt; p) / p # first dropout mask. Notice /p!\n  H1 *= U1 # drop!\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  U2 = (np.random.rand(*H2.shape) &lt; p) / p # second dropout mask. Notice /p!\n  H2 *= U2 # drop!\n  out = np.dot(W3, H2) + b3\n\n  # backward pass: compute gradients... (not shown)\n  # perform parameter update... (not shown)\n\ndef predict(X):\n  # ensembled forward pass\n  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  out = np.dot(W3, H2) + b3\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/optimization/","title":"Optimization","text":""},{"location":"ai/deep_learning_for_computer_vision/optimization/#visualizing-the-loss-function","title":"Visualizing the loss function","text":"<p>We can generate a random weight matrix \\(W\\), then march along a ray and record the loss function value along the way. That is, we can generate a random direction \\(W_1\\) and compute the loss along this direction by evaluating \\(L(W + aW_1)\\) for different values of \\(a\\). This process generates a simple plot with the value of \\(a\\) as the x-axis and the value of the loss function as the y-axis. We can also carry out the same procedure with two dimensions by evaluating the loss \\(L(W + aW_1 + bW_2)\\) as we vary \\(a, b\\). In a plot, \\(a, b\\) could then correspond to the x-axis and the y-axis, and the value of the loss function can be visualized with a color.</p> <p></p> <p>To reiterate, the loss function lets us quantify the quality of any particular set of weights W. The goal of optimization is to find W that minimizes the loss function.</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#strategies","title":"Strategies","text":""},{"location":"ai/deep_learning_for_computer_vision/optimization/#strategy-1-a-first-very-bad-idea-solution-random-search","title":"Strategy #1: A first very bad idea solution: Random search","text":"<p>Since it is so simple to check how good a given set of parameters \\(W\\) is, the first (very bad) idea that may come to mind is to simply try out many different random weights and keep track of what works best. This procedure might look as follows.</p> <pre><code># assume X_train is the data where each column is an example (e.g. 3073 x 50,000)\n# assume Y_train are the labels (e.g. 1D array of 50,000)\n# assume the function L evaluates the loss function\n\nbestloss = float(\"inf\") # Python assigns the highest possible float value\nfor num in range(1000):\n  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n  if loss &lt; bestloss: # keep track of the best solution\n    bestloss = loss\n    bestW = W\n  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)\n\n# prints:\n# in attempt 0 the loss was 9.401632, best 9.401632\n# in attempt 1 the loss was 8.959668, best 8.959668\n# in attempt 2 the loss was 9.044034, best 8.959668\n# in attempt 3 the loss was 9.278948, best 8.959668\n# in attempt 4 the loss was 8.857370, best 8.857370\n# in attempt 5 the loss was 8.943151, best 8.857370\n# in attempt 6 the loss was 8.605604, best 8.605604\n# ... (trunctated: continues for 1000 lines)\n\n# Assume X_test is [3073 x 10000], Y_test [10000 x 1]\nscores = bestW.dot(Xte_cols) # 10 x 10000, the class scores for all test examples\n# find the index with max score in each column (the predicted class)\nYte_predict = np.argmax(scores, axis = 0)\n# and calculate accuracy (fraction of predictions that are correct)\nnp.mean(Yte_predict == Yte)\n# returns 0.1555\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#strategy-2-random-local-search","title":"Strategy #2: Random Local Search","text":"<p>The first strategy you may think of is to try to extend one foot in a random direction and then take a step only if it leads downhill. Concretely, we will start out with a random \\(W\\), generate random perturbations \\(\\delta W\\) to it and if the loss at the perturbed \\(W + \\delta W\\) is lower, we will perform an update. The code for this procedure is as follows.</p> <pre><code>W = np.random.randn(10, 3073) * 0.001 # generate random starting W\nbestloss = float(\"inf\")\nfor i in range(1000):\n  step_size = 0.0001\n  Wtry = W + np.random.randn(10, 3073) * step_size\n  loss = L(Xtr_cols, Ytr, Wtry)\n  if loss &lt; bestloss:\n    W = Wtry\n    bestloss = loss\n  print 'iter %d loss is %f' % (i, bestloss)\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#strategy-3-following-the-gradient","title":"Strategy #3: Following the Gradient","text":"<p>In the previous strategy we tried to find a direction in the weight-space that would improve our weight vector (and give us a lower loss). It turns out that there is no need to randomly search for a good direction: we can compute the best direction along which we should change our weight vector that is mathematically guaranteed to be the direction of the steepest descent of the loss function. This direction will be related to the gradient of the loss function. In a hiking analogy, this approach roughly corresponds to feeling the slope of the hill below our feet and stepping down the direction that feels steepest.</p> <p>In one-dimensional functions, the slope is the instantaneous rate of change of the function at any point you might be interested in. The gradient is just a vector of slopes (more commonly referred to as derivatives) for each dimension in the input space. The mathematical expression for the derivative of a 1-D function with respect its input is:</p> \\[\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\\] <p>When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension.</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#computing-the-gradient","title":"Computing the gradient","text":"<p>There are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone (implementation wise) way that requires calculus (analytic gradient).</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#computing-the-gradient-numerically-with-finite-differences","title":"Computing the gradient numerically with finite differences","text":"<pre><code>def eval_numerical_gradient(f, x):\n  \"\"\"\n  a naive implementation of numerical gradient of f at x\n  - f should be a function that takes a single argument\n  - x is the point (numpy array) to evaluate the gradient at\n  \"\"\"\n\n  fx = f(x) # evaluate function value at original point\n  grad = np.zeros(x.shape)\n  h = 0.00001\n\n  # iterate over all indexes in x\n  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n  while not it.finished:\n\n    # evaluate function at x+h\n    ix = it.multi_index\n    old_value = x[ix]\n    x[ix] = old_value + h # increment by h\n    fxh = f(x) # evalute f(x + h)\n    x[ix] = old_value # restore to previous value (very important!)\n\n    # compute the partial derivative\n    grad[ix] = (fxh - fx) / h # the slope\n    it.iternext() # step to next dimension\n\n  return grad\n</code></pre> <p>You may have noticed that evaluating the numerical gradient has complexity linear in the number of parameters. If our example had 30730 parameters in total, we therefore would have had to perform 30,731 evaluations of the loss function to evaluate the gradient and to perform only a single parameter update. This problem only gets worse, since modern Neural Networks can easily have tens of millions of parameters. Clearly, this strategy is not scalable and we need something better.</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#computing-the-gradient-analytically-with-calculus","title":"Computing the gradient analytically with Calculus","text":"<p>The numerical gradient is very simple to compute using the finite difference approximation, but the downside is that it is approximate (since we have to pick a small value of h, while the true gradient is defined as the limit as h goes to zero), and that it is very computationally expensive to compute. The second way to compute the gradient is analytically using Calculus, which allows us to derive a direct formula for the gradient (no approximations) that is also very fast to compute. However, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a gradient check.</p> <p>Let's use the example of the SVM loss function for a single datapoint:</p> \\[L_i = \\sum_{j \\neq y_i} \\left[\\max(0, w_j^T x_i - w_{y_i}^T x_i + \\Delta)\\right]\\] <p>We can differentiate the function with respect to the weights. For example, taking the gradient with respect to \\(w_{y_i}\\) we obtain:</p> \\[\\nabla_{w_{y_i}} L_i = -\\left(\\sum_{j \\neq y_i} \\mathbb{1}(w_j^T x_i - w_{y_i}^T x_i + \\Delta &gt; 0)\\right) x_i\\] <p>where \\(\\mathbb{1}\\) is the indicator function that is one if the condition inside is true or zero otherwise. While the expression may look scary when it is written out, when you're implementing this in code you'd simply count the number of classes that didn't meet the desired margin (and hence contributed to the loss function) and then the data vector \\(x_i\\) scaled by this number is the gradient. Notice that this is the gradient only with respect to the row of \\(W\\) that corresponds to the correct class. For the other rows where \\(j \\neq y_i\\) the gradient is:</p> \\[\\nabla_{w_j} L_i = \\mathbb{1}(w_j^T x_i - w_{y_i}^T x_i + \\Delta &gt; 0) x_i\\] <p>Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update.</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#gradient-descent","title":"Gradient Descent","text":"<p>Now that we can compute the gradient of the loss function, the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent. Its vanilla version looks as follows.</p> <pre><code># Vanilla Gradient Descent\n\nwhile True:\n  weights_grad = evaluate_gradient(loss_fun, data, weights)\n  weights += - step_size * weights_grad # perform parameter update\n</code></pre> <p>In large-scale applications, the training data can have on order of millions of examples. Hence, it seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update. A very common approach to addressing this challenge is to compute the gradient over batches of the training data. For example, in current state of the art ConvNets, a typical batch contains 256 examples from the entire training set of 1.2 million. This batch is then used to perform a parameter update.</p> <pre><code># Vanilla Minibatch Gradient Descent\n\nwhile True:\n  data_batch = sample_training_data(data, 256) # sample 256 examples\n  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n  weights += - step_size * weights_grad # perform parameter update\n</code></pre> <p>The extreme case of this is a setting where the mini-batch contains only a single example. This process is called Stochastic Gradient Descent (SGD) (or also sometimes on-line gradient descent). Even though SGD technically refers to using a single example at a time to evaluate the gradient, you will hear people use the term SGD even when referring to mini-batch gradient descent. The size of the mini-batch is a hyperparameter but it is not very common to cross-validate it. It is usually based on memory constraints (if any), or set to some value, e.g. 32, 64 or 128. We use powers of 2 in practice because many vectorized operation implementations work faster when their inputs are sized in powers of 2.</p>"},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/","title":"Putting it together: Minimal Neural Network Case Study","text":""},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#introduction","title":"Introduction","text":"<p>We\u2019ll walk through a complete implementation of a toy Neural Network in 2 dimensions. We\u2019ll first implement a simple linear classifier and then extend the code to a 2-layer Neural Network. As we\u2019ll see, this extension is surprisingly simple.</p>"},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#generating-some-data","title":"Generating some data","text":"<p>Let's generate a classification dataset that is not easily linearly separable. Our favorite example is the spiral dataset, which can be generated as follows.</p> <p><pre><code>N = 100 # number of points per class\nD = 2 # dimensionality\nK = 3 # number of classes\nX = np.zeros((N*K,D)) # data matrix (each row = single example)\ny = np.zeros(N*K, dtype='uint8') # class labels\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # radius\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix] = j\n# let's visualize the data:\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.show()\n</code></pre> </p> <p>Normally we would want to preprocess the dataset so that each feature has zero mean and unit standard deviation, but in this case the features are already in a nice range from -1 to 1.</p>"},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#training-a-softmax-linear-classifier","title":"Training a Softmax Linear Classifier","text":""},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#initialize-the-parameters","title":"Initialize the parameters","text":"<p>Let's first train a Softmax classifier on this classification dataset. The parameters of the linear classifier consist of a weight matrix \\(W\\) and a bias vector \\(b\\) for each class. Let's first initialize these parameters to be random numbers:</p> <pre><code># initialize parameters randomly\nW = 0.01 * np.random.randn(D,K)\nb = np.zeros((1,K))\n</code></pre> <p>Recall that we \\(D = 2\\) is the dimensionality and \\(K = 3\\) is the number of classes.</p>"},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#compute-the-class-scores","title":"Compute the class scores","text":"<p>Since this is a linear classifier, we can compute all class scores very simply in parallel with a single matrix multiplication:</p> <pre><code># compute class scores for a linear classifier\nscores = np.dot(X, W) + b\n</code></pre> <p>In this example we have 300 2-D points, so after this multiplication the array scores will have size \\([300 \\times 3]\\), where each row gives the class scores corresponding to the 3 classes (blue, red, yellow).</p>"},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#compute-the-loss","title":"Compute the loss","text":"<p>The second key ingredient we need is a loss function, which is a differentiable objective that quantifies our unhappiness with the computed class scores. Intuitively, we want the correct class to have a higher score than the other classes. When this is the case, the loss should be low and otherwise the loss should be high. Recall that if \\(f\\) is the array of class scores for a single example (e.g. array of 3 numbers here), then the Softmax classifier computes the loss for that example as:</p> \\[L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\\right)\\] <p>Here, \\(y_i\\) is the true class label for example \\(i\\). We can see that the Softmax classifier interprets every element of \\(f\\) as holding the (unnormalized) log probabilities of the three classes. We exponentiate these to get (unnormalized) probabilities, and then normalize them to get probabilites. Therefore, the expression inside the log is the normalized probability of the correct class. Note how this expression works: this quantity is always between 0 and 1. When the probability of the correct class is very small (near 0), the loss will go towards (positive) infinity. Conversely, when the correct class probability goes towards 1, the loss will go towards zero because \\(\\log(1) = 0\\). Hence, the expression for \\(L_i\\) is low when the correct class probability is high, and it's very high when it is low.</p> <p>Recall also that the full Softmax classifier loss is then defined as the average cross-entropy loss over the training examples and the regularization:</p> \\[L = \\frac{1}{N}\\sum_i L_i + \\frac{1}{2}\\lambda\\sum_k\\sum_l W_{k,l}^2\\] <p>Given the array of scores we've computed above, we can compute the loss.</p> <pre><code>num_examples = X.shape[0]\n# get unnormalized probabilities\nexp_scores = np.exp(scores)\n# normalize them for each example\nprobs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n</code></pre> <p>We now have an array probs of size \\([300 \\times 3]\\), where each row now contains the class probabilities. In particular, since we've normalized them every row now sums to one. We can now query for the log probabilities assigned to the correct classes in each example:</p> <pre><code>correct_logprobs = -np.log(probs[range(num_examples),y])\n</code></pre> <p>The array correct_logprobs is a 1D array of just the probabilities assigned to the correct classes for each example. The full loss is then the average of these log probabilities and the regularization loss:</p> <pre><code># compute the loss: average cross-entropy loss and regularization\ndata_loss = np.sum(correct_logprobs)/num_examples\nreg_loss = 0.5*reg*np.sum(W*W)\nloss = data_loss + reg_loss\n</code></pre> <p>In this code, the regularization strength \\(\\lambda\\) is stored inside the reg. The convenience factor of 0.5 multiplying the regularization will become clear in a second. Evaluating this in the beginning (with random parameters) might give us loss = 1.1, which is -np.log(1.0/3), since with small initial random weights all probabilities assigned to all classes are about one third. We now want to make the loss as low as possible, with loss = 0 as the absolute lower bound.</p>"},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#computing-the-analytic-gradient-with-backpropagation","title":"Computing the Analytic Gradient with Backpropagation","text":"<p>We have a way of evaluating the loss, and now we have to minimize it. We'll do so with gradient descent. That is, we start with random parameters (as shown above), and evaluate the gradient of the loss function with respect to the parameters, so that we know how we should change the parameters to decrease the loss. Let's introduce the intermediate variable \\(p\\), which is a vector of the (normalized) probabilities. The loss for one example is:</p> \\[p_k = \\frac{e^{f_k}}{\\sum_j e^{f_j}}\\] \\[L_i = -\\log(p_{y_i})\\] <p>We now wish to understand how the computed scores inside \\(f\\) should change to decrease the loss \\(L_i\\) that this example contributes to the full objective. In other words, we want to derive the gradient \\(\\partial L_i/\\partial f_k\\). </p> <p>Here, \\(y_i\\) is the true class label for example \\(i\\), and \\(k\\) is an index over all possible classes (also \\(0, 1, \\text{or } 2\\)). The gradient \\(\\partial L_i/\\partial f_k\\) tells us how the loss for example \\(i\\) changes when we modify the score \\(f_k\\) for class \\(k\\).</p> \\[\\frac{\\partial L_i}{\\partial f_k} = p_k - \\mathbb{1}(y_i = k)\\] <p>Notice how elegant and simple this expression is. Suppose the probabilities we computed were p = [0.2, 0.3, 0.5], and that the correct class was the middle one (with probability 0.3). According to this derivation the gradient on the scores would be df = [0.2, -0.7, 0.5]. Recalling what the interpretation of the gradient, we see that this result is highly intuitive: increasing the first or last element of the score vector f (the scores of the incorrect classes) leads to an increased loss (due to the positive signs +0.2 and +0.5) - and increasing the loss is bad, as expected. However, increasing the score of the correct class has negative influence on the loss. The gradient of -0.7 is telling us that increasing the correct class score would lead to a decrease of the loss \\(L_i\\), which makes sense.</p> <p>All of this boils down to the following code. Recall that probs stores the probabilities of all classes (as rows) for each example. To get the gradient on the scores, which we call dscores, we proceed as follows:</p> <pre><code>dscores = probs\ndscores[range(num_examples),y] -= 1\ndscores /= num_examples\n</code></pre> <p>Lastly, we had that scores = np.dot(X, W) + b, so armed with the gradient on scores (stored in dscores), we can now backpropagate into W and b:</p> <pre><code>dW = np.dot(X.T, dscores)\ndb = np.sum(dscores, axis=0, keepdims=True)\ndW += reg*W # don't forget the regularization gradient\n</code></pre> <p>Where we see that we have backpropped through the matrix multiply operation, and also added the contribution from the regularization. Note that the regularization gradient has the very simple form reg*W since we used the constant 0.5 for its loss contribution (i.e. \\(\\frac{d}{dw}(\\frac{1}{2}\\lambda w^2) = \\lambda w\\)). This is a common convenience trick that simplifies the gradient expression.</p>"},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#performing-a-parameter-update","title":"Performing a parameter update","text":"<p>Now that we've evaluated the gradient we know how every parameter influences the loss function. We will now perform a parameter update in the negative gradient direction to decrease the loss:</p> <pre><code># perform a parameter update\nW += -step_size * dW\nb += -step_size * db\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#putting-it-all-together-training-a-softmax-classifier","title":"Putting it all together: Training a Softmax Classifier","text":"<p>Putting all of this together, here is the full code for training a Softmax classifier with Gradient descent:</p> <pre><code>#Train a Linear Classifier\n\n# initialize parameters randomly\nW = 0.01 * np.random.randn(D,K)\nb = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in range(200):\n\n  # evaluate class scores, [N x K]\n  scores = np.dot(X, W) + b\n\n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n\n  # compute the loss: average cross-entropy loss and regularization\n  correct_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(correct_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W)\n  loss = data_loss + reg_loss\n  if i % 10 == 0:\n    print \"iteration %d: loss %f\" % (i, loss)\n\n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n\n  # backpropate the gradient to the parameters (W,b)\n  dW = np.dot(X.T, dscores)\n  db = np.sum(dscores, axis=0, keepdims=True)\n\n  dW += reg*W # regularization gradient\n\n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db\n</code></pre> <p>Running this prints the output:</p> <pre><code>iteration 0: loss 1.096956\niteration 10: loss 0.917265\niteration 20: loss 0.851503\niteration 30: loss 0.822336\niteration 40: loss 0.807586\niteration 50: loss 0.799448\niteration 60: loss 0.794681\niteration 70: loss 0.791764\niteration 80: loss 0.789920\niteration 90: loss 0.788726\niteration 100: loss 0.787938\niteration 110: loss 0.787409\niteration 120: loss 0.787049\niteration 130: loss 0.786803\niteration 140: loss 0.786633\niteration 150: loss 0.786514\niteration 160: loss 0.786431\niteration 170: loss 0.786373\niteration 180: loss 0.786331\niteration 190: loss 0.786302\n</code></pre> <p>We see that we've converged to something after about 190 iterations. We can evaluate the training set accuracy:</p> <pre><code># evaluate training set accuracy\nscores = np.dot(X, W) + b\npredicted_class = np.argmax(scores, axis=1)\nprint 'training accuracy: %.2f' % (np.mean(predicted_class == y))\n</code></pre> <p>This prints 49%. Not very good at all, but also not surprising given that the dataset is constructed so it is not linearly separable. We can also plot the learned decision boundaries:</p> <p></p> <p>Linear classifier fails to learn the toy spiral dataset.</p>"},{"location":"ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/#training-a-neural-network","title":"Training a Neural Network","text":"<p>Clearly, a linear classifier is inadequate for this dataset and we would like to use a Neural Network. One additional hidden layer will suffice for this toy data. We will now need two sets of weights and biases (for the first and second layers):</p> <pre><code># initialize parameters randomly\nh = 100 # size of hidden layer\nW = 0.01 * np.random.randn(D,h)\nb = np.zeros((1,h))\nW2 = 0.01 * np.random.randn(h,K)\nb2 = np.zeros((1,K))\n</code></pre> <p>The forward pass to compute scores now changes form:</p> <pre><code># evaluate class scores with a 2-layer Neural Network\nhidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\nscores = np.dot(hidden_layer, W2) + b2\n</code></pre> <p>Notice that the only change from before is one extra line of code, where we first compute the hidden layer representation and then the scores based on this hidden layer. Crucially, we've also added a non-linearity, which in this case is simple ReLU that thresholds the activations on the hidden layer at zero.</p> <p>Everything else remains the same. We compute the loss based on the scores exactly as before, and get the gradient for the scores dscores exactly as before. However, the way we backpropagate that gradient into the model parameters now changes form, of course.</p> <pre><code># backpropate the gradient to the parameters\n# first backprop into parameters W2 and b2\ndW2 = np.dot(hidden_layer.T, dscores)\ndb2 = np.sum(dscores, axis=0, keepdims=True)\n</code></pre> <p>However, unlike before we are not yet done, because hidden_layer is itself a function of other parameters and the data! We need to continue backpropagation through this variable. Its gradient can be computed as:</p> <pre><code>dhidden = np.dot(dscores, W2.T)\n</code></pre> <p>Now we have the gradient on the outputs of the hidden layer. Next, we have to backpropagate the ReLU non-linearity. This turns out to be easy because ReLU during the backward pass is effectively a switch. Since \\(r = \\max(0,x)\\), we have that \\(\\frac{dr}{dx} = \\mathbb{1}(x &gt; 0)\\). Combined with the chain rule, we see that the ReLU unit lets the gradient pass through unchanged if its input was greater than 0, but kills it if its input was less than zero during the forward pass. Hence, we can backpropagate the ReLU in place simply with:</p> <pre><code># backprop the ReLU non-linearity\ndhidden[hidden_layer &lt;= 0] = 0\n</code></pre> <p>And now we finally continue to the first layer weights and biases:</p> <pre><code># finally into W,b\ndW = np.dot(X.T, dhidden)\ndb = np.sum(dhidden, axis=0, keepdims=True)\n</code></pre> <p>We're done! We have the gradients dW,db,dW2,db2 and can perform the parameter update. Everything else remains unchanged. The full code looks very similar:</p> <pre><code># initialize parameters randomly\nh = 100 # size of hidden layer\nW = 0.01 * np.random.randn(D,h)\nb = np.zeros((1,h))\nW2 = 0.01 * np.random.randn(h,K)\nb2 = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in range(10000):\n\n  # evaluate class scores, [N x K]\n  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n  scores = np.dot(hidden_layer, W2) + b2\n\n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n\n  # compute the loss: average cross-entropy loss and regularization\n  correct_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(correct_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n  loss = data_loss + reg_loss\n  if i % 1000 == 0:\n    print \"iteration %d: loss %f\" % (i, loss)\n\n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n\n  # backpropate the gradient to the parameters\n  # first backprop into parameters W2 and b2\n  dW2 = np.dot(hidden_layer.T, dscores)\n  db2 = np.sum(dscores, axis=0, keepdims=True)\n  # next backprop into hidden layer\n  dhidden = np.dot(dscores, W2.T)\n  # backprop the ReLU non-linearity\n  dhidden[hidden_layer &lt;= 0] = 0\n  # finally into W,b\n  dW = np.dot(X.T, dhidden)\n  db = np.sum(dhidden, axis=0, keepdims=True)\n\n  # add regularization gradient contribution\n  dW2 += reg * W2\n  dW += reg * W\n\n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db\n  W2 += -step_size * dW2\n  b2 += -step_size * db2\n</code></pre> <p>This prints:</p> <pre><code>iteration 0: loss 1.098744\niteration 1000: loss 0.294946\niteration 2000: loss 0.259301\niteration 3000: loss 0.248310\niteration 4000: loss 0.246170\niteration 5000: loss 0.245649\niteration 6000: loss 0.245491\niteration 7000: loss 0.245400\niteration 8000: loss 0.245335\niteration 9000: loss 0.245292\n</code></pre> <p>The training accuracy is now:</p> <pre><code># evaluate training set accuracy\nhidden_layer = np.maximum(0, np.dot(X, W) + b)\nscores = np.dot(hidden_layer, W2) + b2\npredicted_class = np.argmax(scores, axis=1)\nprint 'training accuracy: %.2f' % (np.mean(predicted_class == y))\n</code></pre> <p>Which prints 98%!. We can also visualize the decision boundaries:</p> <p></p> <p>Neural Network classifier crushes the spiral dataset.</p>"},{"location":"ai/deep_learning_for_computer_vision/regularization/","title":"Regularization","text":"<p>Increasing the amount of training data is one way of reducing overfitting. Fortunately, there are other techniques which can reduce overfitting, even when we have a fixed network and fixed training data. These are known as regularization techniques.</p>"},{"location":"ai/deep_learning_for_computer_vision/regularization/#cross-entropy-loss-with-l2-regularization","title":"Cross-Entropy Loss with L2 Regularization","text":"<p>The complete loss function for a softmax classifier with L2 regularization combines the cross-entropy loss with a regularization term:</p> \\[L = \\frac{1}{N} \\sum_{i=1}^{N} L_i + \\lambda R(W)\\] <p>where:</p> <ul> <li> <p>\\(L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\\right)\\) is the cross-entropy loss for the \\(i\\)-th sample</p> </li> <li> <p>\\(R(W) = \\sum_k \\sum_l W_{k,l}^2\\) is the L2 regularization term (sum of squared weights)</p> </li> <li> <p>\\(\\lambda\\) is the regularization strength hyperparameter</p> </li> <li> <p>\\(N\\) is the number of training samples</p> </li> </ul> <p>Intuitively, the effect of regularization is to make it so the network prefers to learn small weights, all other things being equal. Put another way, regularization can be viewed as a way of compromising between finding small weights and minimizing the original loss function. The relative importance of the two elements of the compromise depends on the value of \\(\\lambda\\): when \\(\\lambda\\) is small we prefer to minimize the original loss function, but when is large we prefer small weights.</p> <p>The compromise exists because the two objectives in our loss function can conflict with each other:</p> <ol> <li> <p>Minimizing the original loss function: This encourages the model to fit the training data as well as possible, which often requires large weights to capture complex patterns and noise in the data. To understand why large weights are often needed, consider what happens when we try to fit training data perfectly. Real training data often contains noise, outliers, or mislabeled examples. To achieve near zero training error, the model must learn to classify these noisy examples correctly. The model must thus be sensitive to noisy samples. Having large weights makes the model sensitive to variation in input data. The model might learn to recognize very specific features of individual training examples (like particular pixel patterns, lighting conditions, or background elements) rather than generalizable features. This often requires large weights to amplify these specific signals. In essence, the original loss function asks: \"How can I classify every training example correctly?\". The answer often involves learning very specific, complex patterns with noise that require large weights to implement.</p> </li> <li> <p>Minimizing the regularization term: This encourages small weights, which constrains the model's capacity to fit complex patterns.</p> </li> </ol> <p>Example: Which weights does the regularizer prefer?</p> <p>Consider the following example with input \\(x = [1, 1, 1, 1]\\) and two different weight vectors:</p> <ul> <li>\\(w_1 = [1, 0, 0, 0]\\) </li> <li>\\(w_2 = [0.25, 0.25, 0.25, 0.25]\\)</li> </ul> <p>Both weight vectors produce the same output: \\(w_1 \\cdot x = 1\\) and \\(w_2 \\cdot x = 1\\). However, the L2 regularizer strongly prefers \\(w_2\\) over \\(w_1\\).</p> <p>L2 regularization calculation:</p> <ul> <li> <p>\\(R(w_1) = \\sum_{i} w_{1,i}^2 = 1^2 + 0^2 + 0^2 + 0^2 = 1\\)</p> </li> <li> <p>\\(R(w_2) = \\sum_{i} w_{2,i}^2 = 0.25^2 + 0.25^2 + 0.25^2 + 0.25^2 = 4 \\times 0.0625 = 0.25\\)</p> </li> </ul> <p>Even though both weight vectors produce identical predictions, the regularizer penalizes \\(w_1\\) four times more heavily than \\(w_2\\) because \\(w_1\\) concentrates all its \"weight\" in a single dimension, while \\(w_2\\) distributes the same total \"influence\" more evenly across all dimensions.</p> <p>This example illustrates why regularization encourages weight spreading rather than weight concentration- it prefers solutions that use many small weights rather than a few large ones, even when both approaches achieve the same functional result.</p>"},{"location":"ai/natural_language_processing/attention_in_transformers_visually_explained/","title":"Attention in transformers, visually explained","text":"<p>Consider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole. We know that the word mole has different meanings in each one of these, based on the context.</p> <p></p> <p>But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.</p> <p></p> <p>It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.</p> <p></p> <p>The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context.</p> <p>To take another example, consider the embedding of the word tower. This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns. If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel.</p> <p></p> <p>If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things.</p> <p></p> <p>More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.</p> <p></p> <p>After all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence. Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, \u201ctherefore the murderer was...\u201d. If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word \u201cwas\u201d, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.</p> <p></p> <p>To step through the computations, though, let's take a much simpler example. Imagine that the input includes the phrase, \u201ca fluffy blue creature roamed the verdant forest\u201d. And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns. This is what we would call a single head of attention.</p> <p></p> <p>Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context.</p> <p></p> <p>Let's go ahead and denote these embeddings with the letter e.</p> <p></p> <p>The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.</p> <p></p> <p>To be clear, we\u2019re making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing. As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.</p> <p>For the first step of this process, you might imagine each noun, like creature, asking the question, \u201chey, are there any adjectives sitting in front of me?\u201d</p> <p></p> <p>And for the words fluffy and blue, to each be able to answer, \u201cyeah, I'm an adjective and I'm in that position\u201d.</p> <p></p> <p>That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word. This query vector though has a much smaller dimension than the embedding vector, say 128.</p> <p></p> <p></p> <p>Computing this query looks like taking a certain matrix, which we'll label Wq, and multiplying it by the embedding. In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.</p> <p></p> <p>The entries of this matrix are parameters of the model, which means the true behavior is learned from data. But for our sake, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions. As to what it does to other embeddings, who knows? Right now, we're laser focused on the nouns.</p> <p></p> <p>At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings. This produces a second sequence of vectors that we call the keys. Conceptually, you want to think of the keys as potentially answering the queries. This key matrix is also full of tunable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space.</p> <p></p> <p></p> <p></p> <p></p> <p>You think of the keys as matching the queries whenever they closely align with each other. In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature.</p> <p>To measure how well each key matches each query, you compute a dot product between each possible key-query pair.</p> <p></p> <p>We can visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align.</p> <p></p> <p>For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers. In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other. So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance. So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution. We compute a softmax along each one of these columns to normalize the values.</p> <p></p> <p></p> <p></p> <p>In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values. At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.</p> <p></p> <p>We call this grid an attention pattern.</p> <p></p> <p>Now if you look at the original transformer paper, there's a really compact way that they write this all down. Here the variables Q and K represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices. This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries. A small technical detail that we didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space. Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column. As to that V term, we'll talk about it in just a second.</p> <p></p> <p></p> <p></p> <p></p> <p>It turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage.</p> <p></p> <p></p> <p></p> <p>This is really nice, because it means what would otherwise be a single training example effectively acts as many. For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.</p> <p></p> <p>What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.</p> <p></p> <p></p> <p>The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized. So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity. If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized. This process is called masking.</p> <p></p> <p></p> <p>Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size. So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.</p> <p></p> <p>Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to. For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature.</p> <p>The most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy. The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature.</p> <p></p> <p></p> <p></p> <p>So this value vector lives in the same very high-dimensional space as the embeddings.</p> <p>For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.</p> <p></p> <p>For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.</p> <p></p> <p>And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding.</p> <p></p> <p></p> <p>Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.</p> <p></p> <p>And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block.</p> <p></p> <p></p> <p>This process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.</p> <p></p> <p>These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.</p> <p></p> <p>If you look at that value matrix by contrast, the way we've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space. If true, that would mean about 150 million added parameters.</p> <p></p> <p></p> <p>And to be clear, you could do that. You could devote orders of magnitude more parameters to the value map than to the key and query. But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query.</p> <p></p> <p>The way this looks is that the value map is factored as a product of two smaller matrices. Conceptually, we should still think about the overall linear map, one with inputs and outputs, both in this larger embedding space.</p> <p></p> <p>What this means is you can think of it as mapping the large embedding vectors down to a much smaller space. This is not the conventional naming, but we to call this the \u201cvalue down\u201d matrix. The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates. We call this one the \u201cvalue up\u201d matrix, which again is not conventional. Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head.</p> <p></p> <p>As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention. Cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription.</p> <p></p> <p></p> <p>A cross-attention head looks almost identical. The only difference is that the key and query maps act on different data sets. In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another. And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.</p> <p></p> <p>All that's really left to us is to lay out the sense in which you do this many many different times. In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word. And a lot of associations might be less grammatical. If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.</p> <p></p> <p>For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings. And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.</p> <p>Everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps.</p> <p></p> <p>GPT-3 for example uses 96 attention heads inside each block. Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns. Then each head has its own distinct value matrices used to produce 96 sequences of value vectors.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.</p> <p></p> <p>So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.</p> <p></p> <p></p> <p></p> <p>This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.</p> <p></p> <p>The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning. Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.</p> <p></p> <p>We said that the value map is factored out into these two distinct matrices, which we labeled as the value down and the value up matrices. The way that we framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way. That would be a valid design.</p> <p></p> <p>But the way that you see this written in papers and the way that it's implemented in practice looks a little different. All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.</p> <p></p> <p>And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that we were labeling as the value down projection into the smaller space.</p> <p></p> <p>We saw how data flowing through a transformer doesn't just flow through a single attention block. For one thing, it also goes through these other operations called multi-layer perceptrons.</p> <p></p> <p>And then it repeatedly goes through many many copies of both of these operations. What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings. The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure. Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that.</p> <p></p> <p>Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.</p> <p></p> <p>That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total. So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.</p> <p></p> <p>A big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.</p> <p></p>"},{"location":"ai/natural_language_processing/attention_mechanism/","title":"Attention Mechanism","text":"<p>When you hear the sentence \"the ball is on the field,\" you don\u2019t assign the same importance to all 6 words. You primarily take note of the words \"ball,\" \"on,\" and \"field,\" since those are the words that are most \"important\" to you. Similarly, there is a flaw in using the final RNN hidden state as the single \"context vector\" for sequence-to-sequence models: often, different parts of an input have different levels of significance.</p> <p>Attention mechanisms make use of this observation by providing the decoder network with a look at the entire input sequence at every decoding step; the decoder can then decide what input words are important at any point in time.</p> <p>Consider a long input we would like to translate.</p> <p></p> <p>But for longer phrases, even with LSTMs, words that are input early on can be forgotten easily. And in this case, if we forget the first word, Don't, then, Don't eat the delicious looking and smelling pizza turns into eat the delicious looking and smelling pizza. The main idea of LSTMs is that they provide separate paths for long and short memories. Even with separate paths, if we have a lot of data, both paths have to carry a lot of information. This means that a word at the start of the phrase can still get lost.</p> <p> </p> <p>Note: Although there are conventions, there are no rules for how Attention should be added to an Encoder-Decoder model. So what follows is just one example.</p> <p> </p> <p>We can instead do Dot Product as well for calculating Similarity.</p> <p></p> <p>And since the score for go is higher, we want the encoding for go to have more influence on the first word that comes out of the Decoder.</p> <p></p> <p>So we can think of the output of the Softmax function as a way to determine what percentage of each encoded input word we should use when decoding.</p> <p> </p> <p>...and, lastly we add the scaled values together.</p> <p> </p> <p>...and the second output from the Decoder is EOS so we are done decoding.</p> <p>Another variation of this is where we concatenate the attention value (which is a weighted sum of encoder hidden states) with the decoder hidden state and proceed as in the non-attention encoder-decoder model.</p>"},{"location":"ai/natural_language_processing/decoder_only_transformers/","title":"Decoder-Only Transformers","text":"<p>One way to calculate similarities between the Query and the Keys is to calculate something called a Dot Product.</p> <p></p> <p></p> <p></p> <p></p> <p>Now we use the Query and Key for What to calculate the similarity with itself...</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Reusing the sets of Weights for the Query, Key and Value numbers lets the Decoder-only Transformer handle prompts that have different lengths.</p> <p></p> <p>Lastly, we need a way to use the encodings we have for each word in the prompt to generate the word that follows it and then to generate a response.</p> <p></p> <p></p> <p></p> <p>Beacause this is a Decoder-only Transformer, we need one thing that can both encode the prompt and generate the output.</p> <p>Thus, even though we are not yet generating a response, we need to include the parts that will do that.</p> <p>Also, we can compare the known input to what the model generates when we train the model.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Note: If we were training the Decoder-only Transformer, then we would use the fact that we made a mistake to modify the Weights and Biases.</p> <p>In contrast, when we are using the model to generate responses, we don't really care what words come out.</p> <p></p> <p></p> <p>Let\u2019s review.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>However, it is also important to keep track of the relationships between the input sentence and the output.</p> <p></p> <p></p> <p></p> <p>...and these two sentences have completely opposite meanings.</p> <p>So it is super important for the Decoder to keep track of the significant words in the input.</p> <p>The nice thing, is that all we have to do to add this ability to our Decoder-only Transformer is just include the prompt when we do Masked Self-Attention while generating the output.</p> <p></p> <p></p> <p></p> <p>...and run everything through the Softmax function.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>...and the Softmax function we used before.</p> <p></p> <p></p> <p></p> <p></p> <p>Now we calculate the Masked Self-Attention values.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>...and the Softmax function we used before.</p> <p></p>"},{"location":"ai/natural_language_processing/differences_between_the_basic_transformer_and_the_decoder_only_transformer/","title":"Differences between the basic Transformer and the Decoder-Only Transformer","text":"<p>Lastly, during training, a normal Transformer uses Masked Self-Attention, but only on the output. In contrast, a Decoder-only Transformer has a single unit for both encoding the input as well as generating the output. And a Decoder-only Transformer uses a single type of attention, Masked Self-Attention. And a Decoder-only Transformer uses Masked Self-Attention all the time on everything, the input and the output.</p>"},{"location":"ai/natural_language_processing/distributional_semantics_and_word2vec/","title":"Distributional semantics and Word2vec","text":""},{"location":"ai/natural_language_processing/gated_recurrent_units/","title":"Gated Recurrent Units (GRUs)","text":""},{"location":"ai/natural_language_processing/how_large_language_models_work_a_visual_intro_to_transformers/","title":"How large language models work, a visual intro to transformers","text":"<p>The initials GPT stand for Generative Pretrained Transformer. So that first word is straightforward enough, these are bots that generate new text (or some other format of output). Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training. But the last word, that's the real key piece. A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the boom in AI. </p> <p>There are many different kinds of models that you can build using transformers. </p> <p> </p> <p>And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another. </p> <p>But the variant we will focus on, which is the type that underlies tools like ChatGPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage. That prediction takes the form of a probability distribution over many different chunks of text that might follow.</p> <p></p> <p>At first glance, you might think that predicting the next word feels like a very different goal from generating new text. But once you have a prediction model like this, a simple thing you generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added. This process here of repeated prediction and sampling is essentially what's happening when you interact with ChatGPT or any of these other large language models and you see them producing one word at a time.</p> <p>Let's kick things off with a very high level preview In broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood.</p> <p>First, the input is broken up into a bunch of little pieces. These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations. If images or sound are involved, then tokens could be little patches of that image or little chunks of that sound.</p> <p></p> <p>Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece.</p> <p></p> <p>If you think of these vectors as giving coordinates in some very high dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space. This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values.</p> <p></p> <p>For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model.</p> <p></p> <p>The attention block is what's responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated. And again, whenever we use the word meaning, this is somehow entirely encoded in the entries of those vectors.</p> <p>After that, these vectors pass through a different kind of operation, a multi-layer perceptron or maybe a feed-forward layer.</p> <p></p> <p>And here the vectors don't talk to each other, they all go through the same operation in parallel.</p> <p></p> <p>The step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions.</p> <p></p> <p>All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices.</p> <p></p> <p>After that, the process essentially repeats, you go back and forth between attention blocks and multi-layer perceptron blocks, until at the very end the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence.</p> <p> </p> <p>We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next.</p> <p></p> <p>Once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over.</p> <p>Deep learning describes a class of models that have proven to scale remarkably well, and what unifies them is the same training algorithm, called backpropagation. These models follow a certain specific format.</p> <p>First, whatever model you're making, the input has to be formatted as an array of real numbers. This could mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor. You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output.</p> <p></p> <p>For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens. In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums. Typically though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product. </p> <p>For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices. </p> <p></p> <p>Those matrices in turn fall into eight different categories, and we could step through each one of those categories to understand what that type does. As we go through, it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from.</p> <p></p> <p>The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words. These columns are what determines what vector each word turns into in that first step.</p> <p></p> <p>Its values begin random, but they're going to be learned based on data. Turning words into vectors was common practice in machine learning long before transformers. The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning.</p> <p>The embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model. Using the GPT-3 numbers, the vocabulary size specifically is 50257, and again, technically this consists not of words per se, but of tokens. The embedding dimension is 12,288, and multiplying those tells us this consists of about 617 million weights. Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion.</p> <p> </p> <p>In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words. More importantly, you should think of them as having the capacity to soak in context.</p> <p>A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network, so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language.</p> <p></p> <p>Think about your own understanding of a given word. The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently. </p> <p>To be clear, in the very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings.</p> <p></p> <p>But you should think of the primary goal of this network that it flows through as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent. </p> <p></p> <p>The network can only process a fixed number of vectors at a time, known as its context size.</p> <p></p> <p>For GPT-3 it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions. This context size limits how much text the transformer can incorporate when it's making a prediction of the next word. This is why long conversations with certain chatbots, like the early versions of ChatGPT, often gave the feeling of the bot kind of losing the thread of conversation as you continued too long.</p> <p>Let\u2019s talk about what happens at the very end. Remember, the desired output is a probability distribution over all tokens that might come next. For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately preceding we see least favorite teacher, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape.</p> <p></p> <p>This involves two different steps. The first one is to use another matrix that maps the very last vector in that context to a list of 50000 values, one for each token in the vocabulary.</p> <p></p> <p>Then there's a function that normalizes this into a probability distribution, it's called Softmax. </p> <p>It might seem a little bit weird to only use this last embedding to make a prediction, when after all in that last step there are thousands of other vectors in the layer just sitting there with their own context-rich meanings.</p> <p></p> <p>This has to do with the fact that in the training process it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it.</p> <p> </p> <p>The final matrix is called the Unembedding matrix and we give it the label WU. Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process.</p> <p></p> <p>Keeping score on our total parameter count, this Unembedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension.</p> <p></p> <p>It's very similar to the embedding matrix, just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far is a little over a billion, a small but not wholly insignificant fraction of the 175 billion we'll end up with in total.</p> <p></p>"},{"location":"ai/natural_language_processing/introduction/","title":"Introduction","text":"<p>ML is a subfield of AI that involves training algorithms to learn from data, allowing them to make predictions or decisions without those being explicitly programmed. ML is driving advancements in so many different fields, such as computer vision, voice recognition, and, of course, NLP. </p> <p>NLP is a field of artificial intelligence (AI) focused on the interaction between computers and human languages. It involves using computational techniques to understand, interpret, and generate human language, making it possible for computers to understand and respond to human input naturally and meaningfully. </p> <p>With all the (impressive!) advances in NLP in the last decades, we are still nowhere close to developing learning machines that have a fraction of acquisition ability of children.</p> <p>Here are a few of the major applications of NLP, not to be exhaustive.</p> <p>Machine Translation. Perhaps one of the earliest and most successful applications and driving uses of natural language processing, MT systems learn to translate between languages and are ubiquitous in the digital world.</p> <p>Question answering and information retrieval. In NLP, question answering has tended to be related to information-seeking questions (\u201cWho is the emir of Abu Dhabi?\u201d, \u201cWhat is the process by which I can get an intern visa for the United Kingdom?\u201d). Continually broadening the scope of answerable questions, providing provenance for answers, answering questions in an interactive dialogue- this is one of the fastest-evolving research directions.</p> <p>Summarization and analysis of text. Companies want to do market research, politicians want to know peoples\u2019 opinions, individuals want summaries of complex topics in digestible form.</p> <p>One of the most significant advances in NLP has been the development of pre-trained language models, such as bidirectional encoder representations from transformers (BERTs) and generative pre-trained transformers (GPTs). These models have been trained on massive amounts of text data and can be fine-tuned for specific tasks, such as sentiment analysis or language translation. </p>"},{"location":"ai/natural_language_processing/language_models/","title":"Language Models","text":"<p>Language models compute the probability of occurrence of a number of words in a particular sequence. The probability of a sequence of \\(m\\) words \\(\\{w_1, \\ldots, w_m\\}\\) is denoted as \\(P(w_1, \\ldots, w_m)\\). Since the number of words coming before a word, \\(w_i\\), varies depending on its location in the input document, \\(P(w_1, \\ldots, w_m)\\) is usually conditioned on a window of \\(n\\) previous words rather than all previous words.</p> <p></p> <p>In machine translation, the model chooses the best word ordering for an input phrase by assigning a goodness score to each output word sequence alternative. To do so, the model may choose between different word ordering or word choice alternatives. It would achieve this objective by running all word sequence candidates through a probability function that assigns each a score. The sequence with the highest score is the output of the translation. For example, the machine would give a higher score to \"the cat is small\" compared to \"small the is cat\", and a higher score to \"walking home after school\" compared to \"walking house after school\".</p> <p>To compute the probabilities mentioned above, the count of each \\(n\\)-gram (i.e., sequence of \\(n\\) words) could be compared against the frequency of each word. This is called an \\(n\\)-gram Language Model. For instance, if the model takes bi-grams, the frequency of each bi-gram, calculated via combining a word with its previous word, would be divided by the frequency of the corresponding uni-gram. Equations 2 and 3 show this relationship for bigram and trigram models.</p> <p></p> <p>For instance, what (3) means is that \\(P(w_3 \\mid (w_1, w_2))\\) is equal to the ratio of count of all instances where the sequence \\((w_1, w_2, w_3)\\) appears to the count of all instances where the sequence \\((w_1, w_2)\\) appears. Clearly this is \\(&lt; 1\\) because the numerator is a subset of the denominator. This gives a sense of the answer to the question\u2014\"What's the probability of \\(w_3\\) occurring given that the sequence \\((w_1, w_2)\\) has already occurred?\"</p> <p>The relationship in Equation 3 focuses on making predictions based on a fixed window of context (i.e., the \\(n\\) previous words) used to predict the next word. But how long should the context be? In some cases, the window of past consecutive \\(n\\) words may not be sufficient to capture the context. For instance, consider the sentence \"As the proctor started the clock, the students opened their ___\". If the window only conditions on the previous three words \"the students opened their\", the probabilities calculated based on the corpus may suggest that the next word be \"books\"\u2014however, if \\(n\\) had been large enough to include the \"proctor\" context, the probability might have suggested \"exam\". This leads us to two main issues with \\(n\\)-gram Language Models: Sparsity and Storage.</p> <p>Sparsity problems with these models arise due to two issues. Firstly, note the numerator of Equation 3. If \\(w_1\\), \\(w_2\\) and \\(w_3\\) never appear together in the corpus, the probability of \\(w_3\\) is \\(0\\). To solve this, a small \\(d\\) could be added to the count for each word in the vocabulary. This is called smoothing. Secondly, consider the denominator of Equation 3. If \\(w_1\\) and \\(w_2\\) never occurred together in the corpus, then no probability can be calculated for \\(w_3\\). To solve this, we could condition on \\(w_2\\) alone. This is called backoff. Increasing \\(n\\) makes sparsity problems worse.</p> <p>We know that we need to store the count for all \\(n\\)-grams we saw in the corpus. As \\(n\\) increases (or the corpus size increases), the model size increases as well.</p> <p>Let\u2019s look at another language model, which is a Window-based Neural Language Model. </p> <p></p> <p>Equation 4 represents the above figure and shows the parameters of the \\(\\text{softmax}()\\) function, consisting of the standard \\(\\tanh()\\) function (i.e., the hidden layer) as well as the linear function, \\(W^{(3)}x + b^{(3)}\\), that captures all the previous \\(n\\) input word vectors.</p> <p></p> <p>\\(W^{(1)}\\), \\(W^{(2)}\\), \\(W^{(3)}\\), \\(b^{(1)}\\) and \\(b^{(2)}\\) are the parameters of the model. Note that the weight matrix \\(W^{(1)}\\) is applied to the word vectors, \\(W^{(2)}\\) is applied to the hidden layer and \\(W^{(3)}\\) is applied to the word vectors. The output distribution is a \\(\\text{softmax}\\) over the vocabulary.</p> <p></p>"},{"location":"ai/natural_language_processing/long_short_term_memory_networks/","title":"Long-Short Term Memory (LSTM) Networks","text":""},{"location":"ai/natural_language_processing/long_short_term_memory_networks/#a-friendly-introduction","title":"A friendly introduction","text":"<p>Even though this part of the LSTM unit determines what percent of Long Term Memory will be remembered, it is usually called the Forget Gate.</p> <p></p> <p>... to create a potential Long Term Memory.</p> <p> </p> <p>Remember: the tanh() activation function turns any input to a number between -1 and 1. </p> <p> </p> <p>Now the LSTM has to decide how much of this potential memory to save. And this is done using the exact same method we used earlier, when we determined what percent of Long-Term Memory to remember.</p> <p> </p> <p>Even though this part of the LSTM unit determines how we should update the Long Term Memory, it is usually called the Input Gate.</p> <p> </p> <p>Now the LSTM has to decide how much of this Potential Short-Term Memory to pass on...</p> <p></p> <p>In all three cases, we used the Sigmoid Activation Function to determine what percent the LSTM remembers.</p> <p> </p> <p>Because the New Short-Term Memory is the output of this entire LSTM unit, this stage is called the Output Gate.</p> <p> </p> <p>Then the LSTM does it's math, using the exact same Weights and Biases as before...</p> <p> </p> <p>Now that we have shown that the LSTM can correctly predict the value on Day 5 for Company A...</p> <p> </p> <p>... and that means we can unroll them more times to accommodate longer sequences of input data than a vanilla RNN.</p> <p>In practice, we actually will rarely ever use the Vanilla RNN formula. Instead, we will use what we call a Long-Short Term Memory (LSTM) RNN.</p>"},{"location":"ai/natural_language_processing/long_short_term_memory_networks/#vanilla-rnn-gradient-flow-and-the-vanishing-gradient-problem","title":"Vanilla RNN Gradient Flow and the Vanishing Gradient Problem","text":"<p>An RNN block takes in input \\(x_t\\) and previous hidden representation \\(h_{t-1}\\) and learns a transformation, which is then passed through \\(\\tanh\\) to produce the hidden representation \\(h_t\\) for the next time step and output \\(y_t\\) as shown in the equation below.</p> \\[h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t)\\] <p>The partial derivative of \\(h_t\\) with respect to \\(h_{t-1}\\) is written as:</p> \\[\\frac{\\partial h_t}{\\partial h_{t-1}} = \\tanh'(W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}\\] <p>We update the weights \\(W_{hh}\\) by getting the derivative of the loss at the very last time step \\(L_t\\) with respect to \\(W_{hh}\\):</p> \\[\\frac{\\partial L_t}{\\partial W_{hh}} = \\frac{\\partial L_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial h_{t-1}}\\ldots\\frac{\\partial h_1}{\\partial W_{hh}} = \\frac{\\partial L_t}{\\partial h_t}\\left(\\prod_{t=2}^{T}\\frac{\\partial h_t}{\\partial h_{t-1}}\\right)\\frac{\\partial h_1}{\\partial W_{hh}} = \\frac{\\partial L_t}{\\partial h_t}\\left(\\prod_{t=2}^{T}\\tanh'(W_{hh}h_{t-1} + W_{xh}x_t)\\right)W_{hh}^{T-1}\\frac{\\partial h_1}{\\partial W_{hh}}\\] <p>Vanishing gradient: We see that \\(\\tanh'(W_{hh}h_{t-1} + W_{xh}x_t)\\) will almost always be less than 1 because \\(\\tanh\\) is always between negative one and one. Thus, as \\(t\\) gets larger (i.e., longer timesteps), the gradient \\(\\frac{\\partial L_t}{\\partial W}\\) will decrease in value and get close to zero. This will lead to the vanishing gradient problem, where gradients at future time steps rarely impact gradients at the very first time step. This is problematic when we model long sequences of inputs because the updates will be extremely slow.</p> <p>Removing non-linearity (\\(\\tanh\\)): If we remove non-linearity (\\(\\tanh\\)) to solve the vanishing gradient problem, then we will be left with:</p> \\[\\frac{\\partial L_t}{\\partial W} = \\frac{\\partial L_t}{\\partial h_t}\\left(\\prod_{t=2}^{T}W_{hh}\\right)\\frac{\\partial h_1}{\\partial W} = \\frac{\\partial L_t}{\\partial h_t}W_{hh}^{T-1}\\frac{\\partial h_1}{\\partial W}\\] <p>The singular values of a matrix tell us how much the matrix can stretch or compress vectors. For a matrix \\(W_{hh}\\), the largest singular value (also called the spectral norm) is the maximum factor by which \\(W_{hh}\\) can stretch any vector. Mathematically, if \\(\\sigma_{\\max}\\) is the largest singular value of \\(W_{hh}\\), then for any vector \\(\\mathbf{v}\\), we have \\(\\|W_{hh}\\mathbf{v}\\| \\leq \\sigma_{\\max}\\|\\mathbf{v}\\|\\). When we repeatedly multiply by \\(W_{hh}\\) (as in \\(W_{hh}^{T-1}\\)), the effect depends on whether \\(\\sigma_{\\max} &gt; 1\\), \\(\\sigma_{\\max} = 1\\), or \\(\\sigma_{\\max} &lt; 1\\).</p> <p>If the largest singular value of \\(W_{hh}\\) is greater than 1, then the gradients will blow up and the model will get very large gradients coming back from future time steps. Exploding gradient often leads to getting gradients that are NaNs. If the largest singular value of \\(W_{hh}\\) is smaller than 1, then we will have the vanishing gradient problem as mentioned above.</p>"},{"location":"ai/natural_language_processing/long_short_term_memory_networks/#lstm-formulation","title":"LSTM Formulation","text":"<p>The following is the precise formulation for LSTM. On step \\(t\\), there is a hidden state \\(h_t\\) and a cell state \\(c_t\\). Both \\(h_t\\) and \\(c_t\\) are vectors of size \\(n\\). One distinction of LSTM from Vanilla RNN is that LSTM has this additional \\(c_t\\) cell state, and intuitively it can be thought of as \\(c_t\\) stores long-term information. LSTM can read, erase, and write information to and from this \\(c_t\\) cell. The way LSTM alters \\(c_t\\) cell is through three special gates: \\(i\\), \\(f\\), \\(o\\) which correspond to \"input\", \"forget\", and \"output\" gates. The values of these gates vary from closed (0) to open (1). All \\(i\\), \\(f\\), \\(o\\) gates are vectors of size \\(n\\).</p> <p>At every timestep we have an input vector \\(x_t\\), previous hidden state \\(h_{t-1}\\), previous cell state \\(c_{t-1}\\), and LSTM computes the next hidden state \\(h_t\\) and next cell state \\(c_t\\) at timestep \\(t\\) as follows:</p> \\[f_t = \\sigma(W_{hf}h_{t-1} + W_{xf}x_t)\\] \\[i_t = \\sigma(W_{hi}h_{t-1} + W_{xi}x_t)\\] \\[o_t = \\sigma(W_{ho}h_{t-1} + W_{xo}x_t)\\] \\[g_t = \\tanh(W_{hg}h_{t-1} + W_{xg}x_t)\\] <p></p> <p></p> <p>where \\(\\odot\\) is an element-wise Hadamard product. \\(g_t\\) in the above formulas is an intermediary calculation cache that's later used with \\(o\\) gate in the above formulas.</p> <p>Since all \\(f\\), \\(i\\), \\(o\\) gate vector values range from 0 to 1, because they were squashed by sigmoid function \\(\\sigma\\), when multiplied element-wise, we can see that:</p> <p>Forget Gate: Forget gate \\(f_t\\) at time step \\(t\\) controls how much information needs to be \"removed\" from the previous cell state \\(c_{t-1}\\). This forget gate learns to erase hidden representations from the previous time steps, which is why LSTM will have two hidden representations \\(h_t\\) and cell state \\(c_t\\). This \\(c_t\\) will get propagated over time and learn whether to forget the previous cell state or not.</p> <p>Input Gate: Input gate \\(i_t\\) at time step \\(t\\) controls how much information needs to be \"added\" to the next cell state \\(c_t\\) from previous hidden state \\(h_{t-1}\\) and input \\(x_t\\). Instead of \\(\\tanh\\), the \"input\" gate \\(i\\) has a sigmoid function, which converts inputs to values between zero and one. This serves as a switch, where values are either almost always zero or almost always one. This \"input\" gate decides whether to take the RNN output that is produced by the \"gate\" gate \\(g\\) and multiplies the output with input gate \\(i\\).</p> <p>Output Gate: Output gate \\(o_t\\) at time step \\(t\\) controls how much information needs to be \"shown\" as output in the current hidden state \\(h_t\\).</p> <p>The key idea of LSTM is the cell state, the horizontal line running through between recurrent timesteps. You can imagine the cell state to be some kind of highway of information passing through straight down the entire chain, with only some minor linear interactions. Thus, even when there is a bunch of LSTMs stacked together, we can get an uninterrupted gradient flow where the gradients flow back through cell states instead of hidden states \\(h\\) without vanishing in every time step. This greatly fixes the gradient vanishing/exploding problem we have outlined above.</p> <p></p> <p>From the friendly introduction's terminology, we note down the following.</p> <ul> <li> <p>Long-Term-Memory = \\(c_{t-1}\\)</p> </li> <li> <p>New Long-Term-Memory = \\(c_t\\)</p> </li> <li> <p>Short-Term-Memory = \\(h_{t-1}\\)</p> </li> <li> <p>New Short-Term-Memory = \\(h_t\\)</p> </li> <li> <p>Potential-Long-Term-Memory = \\(g_t\\)</p> </li> <li> <p>Potential-Short-Term-Memory = \\(\\tanh(\\text{New Long-Term-Memory}) = \\tanh(c_t)\\)</p> </li> </ul>"},{"location":"ai/natural_language_processing/long_short_term_memory_networks/#does-lstm-solve-the-vanishing-gradient-problem","title":"Does LSTM solve the vanishing gradient problem?","text":"<p>LSTM architecture makes it easier for the RNN to preserve information over many recurrent time steps. For example, if the forget gate is set to 1, and the input gate is set to 0, then the information of the cell state will always be preserved over many recurrent time steps. For a Vanilla RNN, in contrast, it's much harder to preserve information in hidden states in recurrent time steps by just making use of a single weight matrix.</p> <p>LSTMs do not guarantee that there is no vanishing/exploding gradient problems, but it does provide an easier way for the model to learn long-distance dependencies.</p>"},{"location":"ai/natural_language_processing/recurrent_neural_networks/","title":"Recurrent Neural Networks","text":""},{"location":"ai/natural_language_processing/recurrent_neural_networks/#a-friendly-introduction","title":"A friendly introduction","text":"<p>..we can unroll the feedback loop by making a copy of the neural network for each input value.</p> <p> </p> <p>Note: Regardless of how many times we unroll an RNN, the weights and biases are shared across every input. So no matter how many times we unroll an RNN, we don't increase the number of weights and biases that we need to train.</p>"},{"location":"ai/natural_language_processing/recurrent_neural_networks/#introduction","title":"Introduction","text":"<p>While \"vanilla\" neural networks receive a single input and produce one label for that image, there are tasks where the model produces a sequence of outputs. Recurrent Neural Networks allow us to operate over sequences of input, output, or both at the same time.</p> <p>An example of a one-to-many model is image captioning, where a fixed-sized image is given and a sequence of words that describe the content of that image is produced through an RNN.</p> <p>An example of a many-to-one task is sentiment classification in NLP, where a sequence of words of a sentence is given and then the sentiment (e.g., positive or negative) of that sentence is classified.</p> <p>An example of a many-to-many task is video captioning, where the input is a sequence of video frames and the output is a caption that describes what was in the video. Another example of a many-to-many task is machine translation in NLP, where an RNN takes a sequence of words of a sentence in English, and then this RNN is asked to produce a sequence of words of a sentence in French.</p> <p>There is also a variation of the many-to-many task, where the model generates an output at every timestep.</p> <p>In general, RNNs allow us to wire up an architecture where the prediction at every single timestep is a function of all the timesteps that have come before.</p> <p></p>"},{"location":"ai/natural_language_processing/recurrent_neural_networks/#why-are-existing-convnets-insufficient","title":"Why are existing convnets insufficient?","text":"<p>Existing convnets are insufficient to deal with tasks that have inputs and outputs with variable sequence lengths. In the example of video captioning, inputs have a variable number of frames (e.g., 10-minute and 10-hour long videos) and outputs are captions of variable length. Convnets can only take in inputs with a fixed size of width and height and cannot generalize over inputs with different sizes. In order to tackle this problem, Recurrent Neural Networks (RNNs) are introduced.</p>"},{"location":"ai/natural_language_processing/recurrent_neural_networks/#architecture","title":"Architecture","text":"<p>If an RNN model is unrolled, then there are inputs (e.g., video frames) at different timesteps shown as \\(x_1, x_2, x_3, \\ldots, x_t\\). The RNN at each timestep takes in two inputs\u2014 an input frame (\\(x_i\\)) and the previous representation of what it has seen so far (i.e., history)\u2014 to generate an output \\(y_i\\) and update its history, which will be forward propagated over time. All the RNN blocks are the same block that share the same parameters, but have different inputs and history at each timestep.</p> <p></p> <p>More precisely, an RNN can be represented as a recurrence formula of some function \\(f_W\\) with parameters \\(W\\):</p> \\[h_t = f_W(h_{t-1}, x_t)\\] <p>where at every timestep it receives some previous state as a vector \\(h_{t-1}\\) from the previous iteration at timestep \\(t-1\\) and the current input vector \\(x_t\\) to produce the current state as a vector \\(h_t\\). A fixed function \\(f_W\\) with weights \\(W\\) is applied at every single timestep, and that allows the Recurrent Neural Network to be used on sequences without having to commit to the size of the sequence because the exact same function is applied at every single timestep, no matter how long the input or output sequences are.</p> <p>In the simplest form of RNN, which is called a Vanilla RNN, there are weight matrices \\(W_{hh}\\) and \\(W_{xh}\\), where they project both the hidden state \\(h_{t-1}\\) from the previous timestep and the current input \\(x_t\\), and then those are summed and squished with a \\(\\tanh\\) function to update the hidden state \\(h_t\\) at timestep \\(t\\).</p> \\[h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t)\\] <p></p> <p>Predictions can be based on \\(h_t\\) by using another matrix projection on top of the hidden state. This is the simplest complete case in which a neural network can be wired up:</p> \\[y_t = W_{hy}h_t\\] <p></p>"},{"location":"ai/natural_language_processing/recurrent_neural_networks/#multilayer-rnns","title":"Multilayer RNNs","text":"<p>RNNs can be stacked together in multiple layers, which gives more depth, and empirically deeper architectures tend to work better.</p> <p></p> <p>In the illustration above, there are three separate RNNs each with their own set of weights. Three RNNs are stacked on top of each other, so the input of the second RNN (second RNN layer) is the vector of the hidden state vector of the first RNN (first RNN layer). All stacked RNNs are trained jointly, and the diagram represents one computational graph.</p>"},{"location":"ai/natural_language_processing/representing_words/","title":"Representing words","text":"<p>Expressing and processing the nuance and wildness of language- while achieving the strong transfer of information that language is intended to achieve- makes representing words an endlessly fascinating problem.</p> <p>Perhaps the simplest way to represent words is as independent, unrelated entities. You might think of this as a set {. . . , tea, . . . , coffee, . . . , intimidate}.</p> <p>We will refer to a word type as an element of a finite vocabulary, independent of actually observing the word in context. So, we\u2019ve just written a set of types. A word token is an instance of the type observed in some context.</p> <p>We will often be working with vectors; the conventional vector representation of independent components is the set of 1-hot, or standard basis, vectors. A couple of examples.</p> <p></p> <p>Why do we represent words as vectors? To better compute with them. And when computing with 1-hot vectors, we do achieve the crucial fact that different words are different, but alas, we encode no meaningful notion of similarity or other relationship. This is because, for example, if we take the dot product as a notion of similarity we compute a result of zero.</p> <p></p> <p>All words are equally dissimilar from each other.</p> <p>Should we represent word semantics not as one-hot vectors, but instead as a collection of features and relationships to linguistic categories and other words? WordNet annotates for synonyms, hyponyms, and other semantic relations; UniMorph annotates for morphology (subword structure) information across many languages. With such resources, one could build word vectors that look something like the following.</p> <p></p> <p>One main failure is that human-annotated resources are always lacking in vocabulary compared to methods that can draw a vocabulary from a naturally occurring text source- updating these resources is costly and they\u2019re always incomplete. Another failure is a tradeoff between dimensionality and utility of the embedding- it takes a very high-dimensional vector to represent all of these categories, and modern neural methods that tend to operate on dense vectors do not behave well with such sparse vectors.</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/","title":"Self-Attention and Transformers, a mathematical approach","text":""},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#notation-and-basics","title":"Notation and basics","text":"<p>Let \\(w_{1:n}\\) be a sequence, where each \\(w_i \\in V\\), a finite vocabulary. We'll also overload \\(w_{1:n}\\) to be a matrix of one-hot vectors, \\(w_{1:n} \\in \\mathbb{R}^{n \\times |V|}\\). We'll use \\(w \\in V\\) to represent an arbitrary vocabulary element, and \\(w_i \\in V\\) to pick out a specific indexed element of a sequence \\(w_{1:n}\\).</p> <p>We'll use the notation,</p> \\[w_t \\sim \\text{softmax}(f(w_{1:t-1})),\\] <p>to mean that under a model, \\(x_t\\) \"is drawn from\" the probability distribution defined by the right-hand-side of the tilde, \\(\\sim\\). </p> <p>When we use the softmax function (as above), we'll use it without direct reference to the dimension being normalized over, and it should be interpreted as follows. If \\(A\\) is a tensor of shape \\(\\mathbb{R}^{\\ell,d}\\), the softmax is computed as follows:</p> \\[\\text{softmax}(A)_{i,j} = \\frac{\\exp A_{i,j}}{\\sum_{j'=1}^{d} \\exp A_{i,j'}},\\] <p>for all \\(i \\in 1, \\ldots, \\ell\\), \\(j \\in 1, \\ldots, d\\), and similarly for tensors of more than two axes. That is, if we had a tensor \\(B \\in \\mathbb{R}^{m,\\ell,d}\\), we would define the softmax over the last dimension, similarly. At the risk of being verbose, we'll write it out:</p> \\[\\text{softmax}(B)_{q,i,j} = \\frac{\\exp B_{q,i,j}}{\\sum_{j'=1}^{d} \\exp B_{q,i,j'}}.\\] <p>In all of our methods, we'll assume an embedding matrix, \\(E \\in \\mathbb{R}^{d \\times |V|}\\), mapping from the vocabulary space to the hidden dimensionality \\(d\\), written as \\(Ew \\in \\mathbb{R}^d\\).</p> <p>The embedding \\(Ew_i\\) of a token in sequence \\(w_{1:n}\\) is what's known as a non-contextual representation; despite \\(w_i\\) appearing in a sequence, the representation \\(Ew_i\\) is independent of context. Since we'll almost always be working on the embedded version of \\(w_{1:n}\\), we'll let \\(x = Ew\\), and \\(x_{1:n} = w_{1:n}E^\\top \\in \\mathbb{R}^{n \\times d}\\). An overarching goal of the methods discussed in this note is to develop strong contextual representations of tokens; that is, a representation \\(h_i\\) that represents \\(w_i\\) but is a function of the entire sequence \\(x_{1:n}\\) (or a prefix \\(x_{1:i}\\), as in the case of language modeling).</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#problems-with-rnn","title":"Problems with RNN","text":"<p>There were twofold issues with the recurrent neural network form, and they both had to do with the the dependence on the sequence index (often called the dependence on \"time\").</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#parallelization-issues-with-dependence-on-the-sequence-index","title":"Parallelization issues with dependence on the sequence index","text":"<p>Modern graphics processing units (GPUs) are excellent at crunching through a lot of simple operations (like addition) in parallel. For example, when we have a matrix \\(A \\in \\mathbb{R}^{n \\times k}\\) and a matrix \\(B \\in \\mathbb{R}^{k \\times d}\\), a GPU is just blazing fast at computing \\(AB \\in \\mathbb{R}^{n \\times d}\\). The constraint of the operations occurring in parallel, however, is crucial \u2013 when computing \\(AB\\) the simplest way, we're performing a bunch of multiplies and then a bunch of sums, most of which don't depend on the output of each other. However, in a recurrent neural network, when we compute</p> \\[h_2 = \\sigma(Wh_1 + Ux_2),\\] <p>we can't compute \\(h_2\\) until we know the value of \\(h_1\\), so we can write it out as</p> \\[h_2 = \\sigma(W\\sigma(Wh_0 + Ux_1) + Ux_2).\\] <p>Likewise if we wanted to compute \\(h_3\\), we can't compute it until we know \\(h_2\\), which we can't compute until we know \\(h_1\\), etc. As the sequence gets longer, there is only so much we can parallelize the computation of the network on a GPU because of the number of serial dependencies.</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#linear-interaction-distance","title":"Linear interaction distance","text":"<p>A related issue with RNNs is the difficulty with which distant tokens in a sequence can interact with each other. By interact, we mean that the presence of one token (already observed in the past) gainfully affects the processing of another token.</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#a-minimal-self-attention-architecture","title":"A minimal self-attention architecture","text":"<p>Attention, broadly construed, is a method for taking a query, and softly looking up information in a key-value store by picking the value(s) of the key(s) most like the query. By \"picking\" and \"most like,\" we mean averaging overall values, putting more weight on those which correspond to the keys more like the query. In self-attention, we mean that we use the same elements to help us define the querys as we do the keys and values.</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#the-key-query-value-self-attention-mechanism","title":"The key-query-value self-attention mechanism","text":"<p>Consider a token \\(x_i\\) in the sequence \\(x_{1:n}\\). From it, we define a query \\(q_i = Qx_i\\), for matrix \\(Q \\in \\mathbb{R}^{d \\times d}\\). Then, for each token in the sequence \\(x_j \\in \\{x_1, \\ldots, x_n\\}\\), we define both a key and a value similarly, with two other weight matrices: \\(k_j = Kx_j\\), and \\(v_j = Vx_j\\) for \\(K \\in \\mathbb{R}^{d \\times d}\\) and \\(V \\in \\mathbb{R}^{d \\times d}\\).</p> <p>Our contextual representation \\(h_i\\) of \\(x_i\\) is a linear combination (that is, a weighted sum) of the values of the sequence,</p> \\[h_i = \\sum_{j=1}^{n} \\alpha_{ij}v_j,\\] <p>where the weights, these \\(\\alpha_{ij}\\) control the strength of contribution of each \\(v_j\\). Going back to our key-value store analogy, the \\(\\alpha_{ij}\\) softly selects what data to look up. We define these weights by computing the affinities between the keys and the query, \\(q_i^\\top k_j\\), and then computing the softmax over the sequence:</p> \\[\\alpha_{ij} = \\frac{\\exp(q_i^\\top k_j)}{\\sum_{j'=1}^{n} \\exp(q_i^\\top k_{j'})}.\\] <p>Intuitively, what we've done by this operation is take our element \\(x_i\\) and look in its own sequence \\(x_{1:n}\\) to figure out what information (in an informal sense,) from what other tokens, should be used in representing \\(x_i\\) in context. The use of matrices \\(K\\), \\(Q\\), \\(V\\) intuitively allow us to use different views of the \\(x_i\\) for the different roles of key, query, and value. We perform this operation to build \\(h_i\\) for all \\(i \\in \\{1, \\ldots, n\\}\\).</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#position-representations","title":"Position representations","text":"<p>Consider the sequence the oven cooked the bread so. This is a different sequence than the bread cooked the oven so, as you might guess. The former sentence has us making delicious bread, and the latter we might interpret as the bread somehow breaking the oven. In a recurrent neural network, the order of the sequence defines the order of the rollout, so the two sequences have different representations. In the self-attention operation, there's no built-in notion of order. The self-attention operation has no built-in notion of the sequence order.</p> <p>To see this, let's take a look at self-attention on this sequence. We have a set of vectors \\(x_{1:n}\\) for the oven cooked the bread so, which we can write as</p> \\[x_{1:n} = [x_{\\text{the}}; x_{\\text{oven}}; x_{\\text{cooked}}; x_{\\text{the}}; x_{\\text{bread}}; x_{\\text{so}}] \\in \\mathbb{R}^{6 \\times d}.\\] <p>As an example, consider performing self-attention to represent the word so in context. The weights over the context are as follows, recalling that \\(q_i = Qx_i\\) for all words, and \\(k_i = Kx_i\\) likewise:</p> \\[\\alpha_{\\text{so}} = \\text{softmax}\\left([q_{\\text{so}}^\\top k_{\\text{the}}; q_{\\text{so}}^\\top k_{\\text{oven}}; q_{\\text{so}}^\\top k_{\\text{cooked}}; q_{\\text{so}}^\\top k_{\\text{the}}; q_{\\text{so}}^\\top k_{\\text{bread}}; q_{\\text{so}}^\\top k_{\\text{so}}]\\right).\\] <p>So, the weight \\(\\alpha_{\\text{so},0}\\), the amount that we look up the first word, (by writing out the softmax) is,</p> \\[\\alpha_{\\text{so},0} = \\frac{\\exp(q_{\\text{so}}^\\top k_{\\text{the}})}{\\exp(q_{\\text{so}}^\\top k_{\\text{the}}) + \\cdots + \\exp(q_{\\text{so}}^\\top k_{\\text{bread}})}.\\] <p>So, \\(\\alpha \\in \\mathbb{R}^6\\) are our weights, and we compute the weighted average with these weights to compute \\(h_{\\text{so}}\\).</p> <p>For the reordered sentence the bread cooked the oven so, note that \\(\\alpha_{\\text{so},0}\\) is identical. The numerator hasn't changed, and the denominator hasn't changed; we've just rearranged terms in the sum. Likewise for \\(\\alpha_{\\text{so},\\text{bread}}\\) and \\(\\alpha_{\\text{so},\\text{oven}}\\), you can compute that they too are identical independent of the ordering of the sequence. This all comes back down to the two facts that (1) the representation of \\(x\\) is not position-dependent; it's just \\(Ew\\) for whatever word \\(w\\), and (2) there's no dependence on position in the self-attention operations.</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#position-representation-through-learned-embeddings","title":"Position representation through learned embeddings","text":"<p>To represent position in self-attention, you either need to (1) use vectors that are already position-dependent as inputs, or (2) change the self-attention operation itself. One common solution is a simple implementation of (1). We posit a new parameter matrix, \\(P \\in \\mathbb{R}^{N \\times d}\\), where \\(N\\) is the maximum length of any sequence that your model will be able to process.</p> <p>We then simply add embedded representation of the position of a word to its word embedding:</p> \\[\\tilde{x}_i = P_i + x_i,\\] <p>and perform self-attention as we otherwise would. Now, the self-attention operation can use the embedding \\(P_i\\) to look at the word at position \\(i\\) differently than if that word were at position \\(j\\).</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#elementwise-nonlinearity","title":"Elementwise nonlinearity","text":"<p>Imagine if we were to stack self-attention layers. Would this be sufficient for a replacement for stacked LSTM layers? Intuitively, there's one thing that's missing: the elementwise nonlinearities that we've come to expect in standard deep learning architectures. In fact, if we stack two self-attention layers, we get something that looks a lot like a single self-attention layer:</p> \\[o_i = \\sum_{j=1}^{n} \\alpha_{ij}V^{(2)}\\left(\\sum_{k=1}^{n} \\alpha_{jk}V^{(1)}x_k\\right)\\] \\[= \\sum_{k=1}^{n}\\left(\\alpha_{jk}\\sum_{j=1}^{n}\\alpha_{ij}\\right)V^{(2)}V^{(1)}x_k\\] \\[= \\sum_{k=1}^{n} \\alpha^*_{ik}V^*x_k,\\] <p>where \\(\\alpha^*_{ik} = \\alpha_{jk}\\sum_{j=1}^{n}\\alpha_{ij}\\), and \\(V^* = V^{(2)}V^{(1)}\\). So, this is just a linear combination of a linear transformation of the input, much like a single layer of self-attention! Is this good enough?</p> <p>In practice, after a layer of self-attention, it's common to apply feed-forward network independently to each word representation:</p> \\[h_{\\text{FF}} = W_2 \\text{ReLU}(W_1h_{\\text{self-attention}} + b_1) + b_2,\\] <p>where often, \\(W_1 \\in \\mathbb{R}^{5d \\times d}\\), and \\(W_2 \\in \\mathbb{R}^{d \\times 5d}\\). That is, the hidden dimension of the feed-forward network is substantially larger than the hidden dimension of the network, \\(d\\)\u2014this is done because this matrix multiply is an efficiently parallelizable operation, so it's an efficient place to put a lot of computation and parameters.</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#future-masking","title":"Future masking","text":"<p>When performing language modeling like we've seen in this course (often called autoregressive modeling), we predict a word given all words so far:</p> \\[w_t \\sim \\text{softmax}(f(w_{1:t-1})),\\] <p>where \\(f\\) is function to map a sequence to a vector in \\(\\mathbb{R}^{|V|}\\).</p> <p>One crucial aspect of this process is that we can't look at the future when predicting it\u2014otherwise the problem becomes trivial. This idea is built-in to unidirectional RNNs. If we want to use an RNN for the function \\(f\\), we can use the hidden state for word \\(w_{t-1}\\):</p> \\[w_t \\sim \\text{softmax}(h_{t-1}E)\\] \\[h_{t-1} = \\sigma(Wh_{t-2} + Ux_{t-1}),\\] <p>and by the rollout of the RNN, we haven't looked at the future. (In this case, the future is all the words \\(w_t, \\ldots, w_n\\).)</p> <p>In a Transformer, there's nothing explicit in the self-attention weight \\(\\alpha\\) that says not to look at indices \\(j &gt; i\\) when representing token \\(i\\). In practice, we enforce this constraint simply adding a large negative constant to the input to the softmax (or equivalently, setting \\(\\alpha_{ij} = 0\\) where \\(j &gt; i\\).)</p> \\[\\alpha_{ij,\\text{masked}} = \\begin{cases} \\alpha_{ij} &amp; j \\leq i \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p></p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#the-transformer","title":"The Transformer","text":"<p>The Transformer is an architecture based on self-attention that consists of stacked Blocks, each of which contains self-attention and feedforward layers, and a few other components.</p> <p></p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#multi-head-self-attention","title":"Multi-head Self-Attention","text":"<p>Intuitively, a single call of self-attention is best at picking out a single value (on average) from the input value set. It does so softly, by averaging over all of the values, but it requires a balancing game in the key-query dot products in order to carefully average two or more things.</p> <p>What we'll present now, multi-head self-attention, intuitively applies self-attention multiple times at once, each with different key, query, and value transformations of the same input, and then combines the outputs.</p> <p>For an integer number of heads \\(k\\), we define matrices \\(K^{(\\ell)}\\), \\(Q^{(\\ell)}\\), \\(V^{(\\ell)} \\in \\mathbb{R}^{d \\times d/k}\\) for \\(\\ell\\) in \\(\\{1, \\ldots, k\\}\\). We'll see why we have the dimensionality reduction to \\(d/k\\) soon. These are the key, query, and value matrices for each head. Correspondingly, we get keys, queries, and values \\(k^{(\\ell)}_{1:n}\\), \\(q^{(\\ell)}_{1:n}\\), \\(v^{(\\ell)}_{1:n}\\), as in single-head self-attention.</p> <p>We then perform self-attention with each head:</p> \\[h^{(\\ell)}_i = \\sum_{j=1}^{n} \\alpha^{(\\ell)}_{ij} v^{(\\ell)}_j\\] \\[\\alpha^{(\\ell)}_{ij} = \\frac{\\exp(q^{(\\ell)\\top}_i k^{(\\ell)}_j)}{\\sum_{j'=1}^{n} \\exp(q^{(\\ell)\\top}_i k^{(\\ell)}_{j'})}\\] <p>Note that the output \\(h^{(\\ell)}_i\\) of each head is in reduced dimension \\(d/k\\).</p> <p>Finally, we define the output of multi-head self-attention as a linear transformation of the concatenation of the head outputs, letting \\(O \\in \\mathbb{R}^{d \\times d}\\):</p> \\[h_i = O[h^{(1)}_i; \\cdots; h^{(k)}_i],\\] <p>where we concatenate the head outputs each of dimensionality \\(d \\times d/k\\) at their second axis, such that their concatenation has dimension \\(d \\times d\\).</p> <p>To understand why we have the reduced dimension of each head output, it's instructive to get a bit closer to how multi-head self-attention is implemented in code. In practice, multi-head self-attention is no more expensive than single-head due to the low-rankness of the transformations we apply.</p> <p>For a single head, recall that \\(x_{1:n}\\) is a matrix in \\(\\mathbb{R}^{n \\times d}\\). Then we can compute our value vectors as a matrix as \\(x_{1:n}V\\), and likewise our keys and queries \\(x_{1:n}K\\) and \\(x_{1:n}Q\\), all matrices in \\(\\mathbb{R}^{n \\times d}\\). To compute self-attention, we can compute our weights in matrix operations:</p> \\[\\alpha = \\text{softmax}(x_{1:n}QK^\\top x_{1:n}^\\top) \\in \\mathbb{R}^{n \\times n}\\] <p>and then compute the self-attention operation for all \\(x_{1:n}\\) via:</p> \\[h_{1:n} = \\text{softmax}(x_{1:n}QK^\\top x_{1:n}^\\top)x_{1:n}V \\in \\mathbb{R}^{n \\times d}.\\] <p>Here's a diagram showing the matrix ops:</p> <p></p> <p>When we perform multi-head self-attention in this matrix form, we first reshape \\(x_{1:n}Q\\), \\(x_{1:n}K\\), and \\(x_{1:n}V\\) each into a matrix of shape \\(\\mathbb{R}^{n,k,d/k}\\), splitting the model dimensionality into two axes, for the number of heads and the number of dimensions per head. We can then transpose the matrices to \\(\\mathbb{R}^{k,n,d/k}\\), which intuitively should look like \\(k\\) sequences of length \\(n\\) and dimensionality \\(d/k\\). This allows us to perform the batched softmax operation in parallel across the heads, using the number of heads kind of like a batch axis (and indeed in practice we'll also have a separate batch axis.) So, the total computation (except the last linear transformation to combine the heads) is the same, just distributed across the (each lower-rank) heads. Here's a diagram like the single-head diagram, demonstrating how the multi-head operation ends up much like the single-head operation.</p> <p></p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#layer-norm","title":"Layer Norm","text":"<p>One important learning aid in Transformers is layer normalization. The intuition of layer norm is to reduce uninformative variation in the activations at a layer, providing a more stable input to the next layer.</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#residual-connections","title":"Residual Connections","text":"<p>Residual connections simply add the input of a layer to the output of that layer:</p> \\[f_{\\text{residual}}(h_{1:n}) = f(h_{1:n}) + h_{1:n},\\] <p>the intuition being that (1) the gradient flow of the identity function is great (the local gradient is 1 everywhere!) so the connection allows for learning much deeper networks, and (2) it is easier to learn the difference of a function from the identity function than it is to learn the function from scratch. As simple as these seem, they're massively useful in deep learning, not just in Transformers!</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#add-norm","title":"Add &amp; Norm","text":"<p>In the Transformer diagrams you'll see the application of layer normalization and residual connection are often combined in a single visual block labeled Add &amp; Norm. Such a layer might look like:</p> \\[h_{\\text{pre-norm}} = f(\\text{LN}(h)) + h,\\] <p>where \\(f\\) is either a feed-forward operation or a self-attention operation, (this is known as pre-normalization), or like:</p> \\[h_{\\text{post-norm}} = \\text{LN}(f(h) + h),\\] <p>which is known as post-normalization. It turns out that the gradients of pre-normalization are much better at initialization, leading to much faster training [Xiong et al., 2020].</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#attention-logit-scaling","title":"Attention logit scaling","text":"<p>Another trick introduced in [Vaswani et al., 2017] they dub scaled dot product attention. The dot product part comes from the fact that we're computing dot products \\(q_i^\\top k_j\\). The intuition of scaling is that, when the dimensionality \\(d\\) of the vectors we're dotting grows large, the dot product of even random vectors (e.g., at initialization) grows roughly as \\(\\sqrt{d}\\). So, we normalize the dot products by \\(\\sqrt{d}\\) to stop this scaling:</p> \\[\\alpha = \\text{softmax}\\left(\\frac{x_{1:n}QK^\\top x_{1:n}^\\top}{\\sqrt{d}}\\right) \\in \\mathbb{R}^{n \\times n}\\]"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#transformer-encoder","title":"Transformer Encoder","text":"<p>A Transformer Encoder takes a single sequence \\(w_{1:n}\\), and performs no future masking. It embeds the sequence with \\(E\\) to make \\(x_{1:n}\\), adds the position representation, and then applies a stack of independently parameterized Encoder Blocks, each of which consisting of (1) multi-head attention and Add &amp; Norm, and (2) feed-forward and Add &amp; Norm. So, the output of each Block is the input to the next.</p> <p></p> <p>A Transformer Encoder is great in contexts where you aren't trying to generate text autoregressively (there's no masking in the encoder so each position index can see the whole sequence,) and want strong representations for the whole sequence (again, possible because even the first token can see the whole future of the sequence when building its representation).</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#transformer-decoder","title":"Transformer Decoder","text":"<p>To build a Transformer autoregressive language model, one uses a Transformer Decoder. These differ from Transformer Encoders simply by using future masking at each application of self-attention. This ensures that the informational constraint (no cheating by looking at the future!) holds throughout the architecture. Famous examples of this are GPT-2 [Radford et al., 2019], GPT-3 [Brown et al., 2020] and BLOOM [Workshop et al., 2022].</p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#transformer-encoder-decoder","title":"Transformer Encoder-Decoder","text":"<p>A Transformer encoder-decoder takes as input two sequences. The first sequence \\(x_{1:n}\\) is passed through a Transformer Encoder to build contextual representations. The second sequence \\(y_{1:m}\\) is encoded through a modified Transformer Decoder architecture in which cross-attention (which we haven't yet defined!) is applied from the encoded representation of \\(y_{1:m}\\) to the output of the Encoder. So, let's take a quick detour to discuss cross-attention; it's not too different from what we've already seen.</p> <p></p>"},{"location":"ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/#cross-attention","title":"Cross-Attention","text":"<p>Cross-attention uses one sequence to define the keys and values of self-attention, and another sequence to define the queries. You might think, hey wait, isn't that just what attention always was before we got into this self-attention business? Yeah, pretty much. So if</p> \\[h^{(x)}_{1:n} = \\text{TransformerEncoder}(w_{1:n}),\\] <p>and we have some intermediate representation \\(h^{(y)}\\) of sequence \\(y_{1:m}\\), then we let</p> \\[q_i = Qh^{(y)}_i \\quad i \\in \\{1, \\ldots, m\\}\\] \\[k_j = Kh^{(x)}_j \\quad j \\in \\{1, \\ldots, n\\}\\] \\[v_j = Vh^{(x)}_j \\quad j \\in \\{1, \\ldots, n\\},\\] <p>and compute the attention on \\(q\\), \\(k\\), \\(v\\) as we defined for self-attention. Note that in the Transformer Encoder-Decoder, cross-attention always applies to the output of the Transformer encoder.</p> <p>An encoder-decoder is used when we'd like bidirectional context on something (like an article to summarize) to build strong representations (i.e., each token can attend to all other tokens), but then generate an output according to an autoregressive decomposition as we can with a decoder. While such an architecture has been found to provide better performance than decoder-only models at modest scale [Raffel et al., 2020], it involves splitting parameters between encoder and decoder, and most of the largest Transformers are decoder-only.</p>"},{"location":"ai/natural_language_processing/seq2seq_models/","title":"Seq2Seq Models","text":""},{"location":"ai/natural_language_processing/svd_based_methods/","title":"SVD based methods","text":"<p>For this class of methods to find word embeddings (otherwise known as word vectors), we first loop over a massive dataset and accumulate word co-occurrence counts in some form of a matrix \\(X\\), and then perform Singular Value Decomposition on \\(X\\) to get a \\(USV^T\\) decomposition. We then use the rows of \\(U\\) as the word embeddings for all words in our dictionary.</p> <p>Let us discuss a few choices of \\(X\\).</p> <p>As our first attempt, we make the bold conjecture that words that are related will often appear in the same documents. For instance, \"banks\", \"bonds\", \"stocks\", \"money\", etc. are probably likely to appear together. But \"banks\", \"octopus\", \"banana\", and \"hockey\" would probably not consistently appear together. We use this fact to build a word-document matrix \\(X\\) in the following manner: loop over billions of documents and for each time word \\(i\\) appears in document \\(j\\), we add one to entry \\(X_{ij}\\). This is obviously a very large matrix and it scales with the number of documents. So perhaps we can try something better.</p> <p>In another method we count the number of times each word appears inside a window of a particular size around the word of interest. We calculate this count for all the words in corpus. We display an example below.</p> <p></p> <p>We now perform SVD on \\(X\\) to obtain \\(X = USV^T\\), observe the singular values (the diagonal entries in the resulting \\(S\\) matrix), and select the first \\(k\\) singular vectors to form \\(U_k\\) as described in the dimensionality reduction section above. The choice of \\(k\\) is based on the desired percentage variance captured.</p> <p>The Singular Value Decomposition (SVD) of a matrix \\(X \\in \\mathbb{R}^{m \\times n}\\) is given by:</p> \\[X = USV^T\\] <p>where:</p> <ul> <li> <p>\\(U \\in \\mathbb{R}^{m \\times m}\\) is an orthogonal matrix whose columns are the left singular vectors</p> </li> <li> <p>\\(S \\in \\mathbb{R}^{m \\times n}\\) is a diagonal matrix with non-negative singular values \\(\\sigma_1 \\geq  \\sigma_2 \\geq \\cdots \\geq \\sigma_{\\min(m,n)} \\geq 0\\) on the diagonal</p> </li> <li> <p>\\(V \\in \\mathbb{R}^{n \\times n}\\) is an orthogonal matrix whose columns are the right singular vectors</p> </li> </ul> <p>The singular values are ordered from largest to smallest, and they represent the importance of each corresponding dimension.</p> <p>To reduce the dimensionality of our word embeddings, we select the first \\(k\\) singular vectors. This gives us a rank-\\(k\\) approximation of \\(X\\):</p> \\[X \\approx U_k S_k V_k^T\\] <p>where:</p> <ul> <li> <p>\\(U_k \\in \\mathbb{R}^{m \\times k}\\) contains the first \\(k\\) columns of \\(U\\) (the \\(k\\) most important left singular vectors)</p> </li> <li> <p>\\(S_k \\in \\mathbb{R}^{k \\times k}\\) is a diagonal matrix containing the first \\(k\\) singular values</p> </li> <li> <p>\\(V_k \\in \\mathbb{R}^{n \\times k}\\) contains the first \\(k\\) columns of \\(V\\) (the \\(k\\) most important right singular vectors)</p> </li> </ul> <p>The rows of \\(U_k\\) then serve as our \\(k\\)-dimensional word embeddings. This rank-\\(k\\) approximation captures the most important information in \\(X\\) while reducing the dimensionality from the original space (which could be \\(m\\) or \\(n\\) dimensions) down to \\(k\\) dimensions. The choice of \\(k\\) is typically based on the desired percentage of variance captured, which can be computed as:</p> \\[\\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{\\min(m,n)} \\sigma_i^2}\\] <p>Problems with this kind of approach are outlined below:</p> <ul> <li> <p>Dynamic dimensions: The dimensions of the matrix change very often (new words are added very frequently and corpus changes in size).</p> </li> <li> <p>Sparsity: The matrix is extremely sparse since most words do not co-occur.</p> </li> <li> <p>High dimensionality: The matrix is very high dimensional in general (\\(10^6 \\times 10^6\\)).</p> </li> <li> <p>Computational cost: Quadratic cost to train (i.e., to perform SVD).</p> </li> <li> <p>Word frequency imbalance: Requires the incorporation of some hacks on \\(X\\) to account for the drastic imbalance in word frequency.</p> </li> </ul>"},{"location":"ai/natural_language_processing/the_basic_transformer/","title":"The basic Transformer","text":"<p>So these two phrases...</p> <p>'Squatch eats pizza!!! and *Pizza eats 'Squatch!!!</p> <p>... use the exact same words, but have very different meanings.</p> <p>So keeping track of word order is super important.</p> <p>We'll start by showing how to add Positional Encoding to the phrase, 'Squatch eats pizza!!!</p> <p>Note: There are a bunch of ways to do Postional Encoding, but we're going to talk about one popular method.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>... and we end up with word embeddings plus Positional Encoding for the whole sentence...</p> <p></p> <p>Thus, Positional Encoding allows a Transformer to keep track of word order.</p> <p></p> <p>...let's talk about how a Transformer keeps track of the relationships among words.</p> <p>For example, if the input sentence was this...</p> <p></p> <p></p> <p></p> <p>The good news is that Transformers have something called Self-Attention, which is a mechanism to correctly associate the word 'it' with 'pizza'. In general, Self-Attention works by seeing how similar each word is to all the words in the sentence, including itself.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>One way to calculate similarities between the Query and the Keys is to calculate something called a Dot Product.</p> <p></p> <p>...tells us Let's is much more similar to itself than it is to the word Go.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>First, the new Self-Attention values for each word contain input from all of the other words, and this helps give each word context.</p> <p></p> <p></p> <p></p> <p>And that is all we need to do encode the input for this simple Transformer.</p> <p></p> <p></p> <p></p> <p>However, this time we create embedding values for the output vocabulary, which consists of the Spanish words... </p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Note: The sets of Weights we used to calculate the Decoder's Self-Attention Query, Key and Value are different from the sets we used in the Encoder.</p> <p></p> <p>Now, so far we have talked about how Self-Attention helps the Transformer keep track of how words are related within a sentence. However, since we're translating a sentence, we need to keep track of the relationships between the input sentence and the output.</p> <p></p> <p></p> <p></p> <p>...and these two sentences have completely opposite meanings.</p> <p>So it is super important for the Decoder to keep track of the significant words in the input.</p> <p>So, the main idea of Encoder-Decoder Attention is to allow the Decoder to keep track of the significant words in the input.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Now that we know what percentage of each input word to use when determining what should be the first translated word...</p> <p>...we calculate Values for each input word.</p> <p></p> <p></p> <p></p> <p>Note: The weights we use to calculate the Queries, Keys and Values for Encoder-Decoder Attention are different from the sets of Weights we use for Self-Attention. </p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>First, we get the Word Embeddings for vamos...</p> <p>...then we add the Positional Encoding. </p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/","title":"Applications of Projections in \u211d\u207f: Orthogonal Bases of Planes and Linear Regression","text":"<p>Linear regression refers to the problem of finding a function \\(f(x) = mx + b\\) which best fits a collection of given data points \\((x_i, y_i)\\).</p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/#finding-an-orthogonal-basis-special-case","title":"Finding an orthogonal basis: special case","text":"<p>Theorem: Suppose \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) are nonzero, and not scalar multiples of each other. The vectors \\(\\mathbf{y}\\) and \\(\\mathbf{x}' = \\mathbf{x} - \\text{Proj}_{\\mathbf{y}} \\mathbf{x}\\) constitute an orthogonal basis of \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\). In particular, \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\) is 2-dimensional.</p> <p>The setup is symmetric in \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), so \\(\\{\\mathbf{x}, \\mathbf{y}' = \\mathbf{y} - \\text{Proj}_{\\mathbf{x}} \\mathbf{y}\\}\\) is also an orthogonal basis of \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\).</p> <p></p> <p>Note: This is similar to the situation of projection of \\(\\mathbf{x}\\) onto a linear subspace \\(V\\). The displacement vector between the projection and \\(\\mathbf{x}\\) is perpendicular to everything in \\(V\\). In our case, when we project \\(\\mathbf{x}\\) onto the span of \\(\\mathbf{y}\\), the resulting vector \\(\\mathbf{x}' = \\mathbf{x} - \\text{Proj}_{\\mathbf{y}} \\mathbf{x}\\) is orthogonal to \\(\\mathbf{y}\\), which means it's perpendicular to everything in the span of \\(\\mathbf{y}\\). This is why \\(\\mathbf{y}\\) and \\(\\mathbf{x}'\\) form an orthogonal basis - they are perpendicular to each other and together span the same 2-dimensional space as the original vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).</p> <p>Example: Consider the plane \\(V\\) in \\(\\mathbb{R}^3\\) through \\(0\\) spanned by the vectors</p> \\[\\mathbf{v} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 3 \\\\ 4 \\end{bmatrix}\\] <p>Imagine that this plane is a metal sheet on which an electric charge is uniformly distributed. An iron particle placed at the point \\(\\mathbf{p} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) would then be attracted to the metal sheet, and by the symmetry of the situation this particle would move straight towards the point on the plane closest to the initial position of the particle. What is that point?</p> <p>In other words, we seek to compute the projection \\(\\text{Proj}_V(\\mathbf{p})\\). To compute this, we first seek an orthogonal basis for the plane \\(V\\). By the theorem above, such an orthogonal basis is given by \\(\\mathbf{w}\\) and \\(\\mathbf{v}' = \\mathbf{v} - \\text{Proj}_{\\mathbf{w}}(\\mathbf{v})\\). We first compute \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{v})\\). This is given by</p> \\[\\text{Proj}_{\\mathbf{w}}(\\mathbf{v}) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = \\frac{3}{25} \\begin{bmatrix} 0 \\\\ 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\frac{9}{25} \\\\ \\frac{12}{25} \\end{bmatrix}\\] <p>Thus \\(\\mathbf{v}' = \\mathbf{v} - \\begin{bmatrix} 0 \\\\ \\frac{9}{25} \\\\ \\frac{12}{25} \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ \\frac{16}{25} \\\\ -\\frac{12}{25} \\end{bmatrix}\\). As a safety check, \\(\\mathbf{w}\\) and \\(\\mathbf{v}'\\) are indeed orthogonal.</p> <p>The vector \\(\\mathbf{v}'\\) is a bit ugly due to the fractions, and for the purposes of having an orthogonal basis it is harmless to replace it with a nonzero scalar multiple, such as</p> \\[\\mathbf{v}'' = 25\\mathbf{v}' = \\begin{bmatrix} 50 \\\\ 16 \\\\ -12 \\end{bmatrix}\\] <p>Since \\(\\{\\mathbf{w}, \\mathbf{v}''\\}\\) is an orthogonal basis of the plane \\(V\\), we have</p> \\[\\text{Proj}_V(\\mathbf{p}) = \\text{Proj}_V \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\text{Proj}_{\\mathbf{w}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} + \\text{Proj}_{\\mathbf{v}''} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\text{Proj}_{\\mathbf{w}}(\\mathbf{p}) + \\text{Proj}_{\\mathbf{v}''}(\\mathbf{p})\\] <p>To compute these projections, we first work out some relevant dot products:</p> \\[\\mathbf{w} \\cdot \\mathbf{w} = 25, \\quad \\mathbf{v}'' \\cdot \\mathbf{v}'' = 2900, \\quad \\mathbf{p} \\cdot \\mathbf{w} = 7, \\quad \\mathbf{p} \\cdot \\mathbf{v}'' = 54\\] <p>Hence</p> \\[\\text{Proj}_{\\mathbf{w}}(\\mathbf{p}) = \\frac{\\mathbf{p} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = \\frac{7}{25} \\begin{bmatrix} 0 \\\\ 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\frac{21}{25} \\\\ \\frac{28}{25} \\end{bmatrix}\\] \\[\\text{Proj}_{\\mathbf{v}''}(\\mathbf{p}) = \\frac{\\mathbf{p} \\cdot \\mathbf{v}''}{\\mathbf{v}'' \\cdot \\mathbf{v}''} \\mathbf{v}'' = \\frac{54}{2900} \\begin{bmatrix} 50 \\\\ 16 \\\\ -12 \\end{bmatrix} = \\begin{bmatrix} \\frac{27}{29} \\\\ \\frac{216}{725} \\\\ -\\frac{162}{725} \\end{bmatrix}\\] <p>Thus, the place on the metal sheet that the particle ends up at is</p> \\[\\text{Proj}_V \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\text{Proj}_{\\mathbf{w}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} + \\text{Proj}_{\\mathbf{v}''} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\frac{21}{25} \\\\ \\frac{28}{25} \\end{bmatrix} + \\begin{bmatrix} \\frac{27}{29} \\\\ \\frac{216}{725} \\\\ -\\frac{162}{725} \\end{bmatrix} = \\begin{bmatrix} \\frac{27}{29} \\\\ \\frac{33}{29} \\\\ \\frac{26}{29} \\end{bmatrix} \\approx \\begin{bmatrix} 0.931 \\\\ 1.138 \\\\ 0.897 \\end{bmatrix}\\] <p>Example: Let \\(\\mathbf{w}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{w}_2 = \\begin{bmatrix} 1 \\\\ -3 \\\\ 1 \\\\ 1 \\end{bmatrix}\\). Define \\(U\\) to be the collection of all 4-vectors \\(\\mathbf{u}\\) that are orthogonal to both \\(\\mathbf{w}_1\\) and \\(\\mathbf{w}_2\\). Show that \\(U\\) is a linear subspace of \\(\\mathbb{R}^4\\) by writing it as a span of finitely many vectors. Explain why \\(\\dim(U) = 2\\).</p> <p>Solution:</p> <p>First, let's understand what \\(U\\) represents. A vector \\(\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4 \\end{bmatrix}\\) belongs to \\(U\\) if and only if:</p> \\[\\mathbf{u} \\cdot \\mathbf{w}_1 = 0 \\quad \\text{and} \\quad \\mathbf{u} \\cdot \\mathbf{w}_2 = 0\\] <p>This gives us the system of equations:</p> \\[u_1 + u_2 + u_3 + u_4 = 0\\] \\[u_1 - 3u_2 + u_3 + u_4 = 0\\] <p>Subtracting the second equation from the first:</p> \\[(u_1 + u_2 + u_3 + u_4) - (u_1 - 3u_2 + u_3 + u_4) = 0 - 0\\] \\[4u_2 = 0\\] \\[u_2 = 0\\] <p>Substituting \\(u_2 = 0\\) back into the first equation:</p> \\[u_1 + 0 + u_3 + u_4 = 0\\] \\[u_1 + u_3 + u_4 = 0\\] <p>This means \\(u_1 = -u_3 - u_4\\). So any vector in \\(U\\) must have the form:</p> \\[\\mathbf{u} = \\begin{bmatrix} -u_3 - u_4 \\\\ 0 \\\\ u_3 \\\\ u_4 \\end{bmatrix} = u_3 \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + u_4 \\begin{bmatrix} -1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] <p>Let's define:</p> \\[\\mathbf{v}_1 = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] <p>Then \\(U = \\text{span}(\\mathbf{v}_1, \\mathbf{v}_2)\\), which shows that \\(U\\) is indeed a linear subspace of \\(\\mathbb{R}^4\\).</p> <p>The dimension of \\(U\\) is 2 because we found that \\(U\\) is spanned by two vectors: \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\). These vectors are linearly independent (neither is a scalar multiple of the other). Therefore, \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) is a basis for \\(U\\). Since the basis has 2 elements, \\(\\dim(U) = 2\\).</p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/#fitting-a-function-to-data","title":"Fitting a function to data","text":"<p>What does \"best fit\" mean? Informally, we want \\(f(x_i)\\) to be as close as possible to \\(y_i\\) for all \\(i\\). The error</p> \\[\\text{error}_i = y_i - (mx_i + b)\\] <p>measures in absolute value how close the line \\(y = mx + b\\) is vertically to \\((x_i, y_i)\\).</p> <p></p> <p>Suppose the line is given by the equation \\(y = mx + b\\). Suppose the \\(i\\)th data point is denoted \\((x_i, y_i)\\). The \\(i\\)th error is given by \\(\\text{error}_i = e_i = y_i - (mx_i + b)\\). These errors are shown as blue line segments in the figure.</p> <p>To be a \"good fit\" means to choose \\((m, b)\\) so that the errors are collectively small. There are many ways to specify what \"collectively small\" means. The meaning in the least squares method is this: choose \\((m, b)\\) to minimize the sum of the squares of the errors; i.e., choose \\((m, b)\\) to minimize</p> \\[\\sum_{i=1}^n (y_i - (mx_i + b))^2\\] <p>Why use the sum of squares of the errors? The errors themselves might be positive and might be negative; we want to penalize a large negative error as well as a large positive error, so squaring errors removes the sign.</p> <p>But sometimes other ways to define the \"total error\" are indeed more appropriate, such as summing the absolute values of the errors (used in computational statistics, geophysics, and the important signal processing algorithm called \"compressed sensing\"). The absolute value function is inconvenient for our purposes; e.g., from a calculus viewpoint, \\(|x|\\) has the defect relative to \\(x^2\\) that it is not differentiable at \\(x = 0\\). Always remember that we choose how to define \"total error\" for any particular application, and experience determines the appropriateness of that choice; mathematics is a creation of the human mind.</p> <p>Put the data of all \\(x\\)-values into a single \\(n\\)-vector, and the data of all \\(y\\)-values into a single \\(n\\)-vector:</p> \\[X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\quad Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\] <p>Also, define \\(\\mathbf{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\in \\mathbb{R}^n\\) to be the vector with all entries equal to 1 (analogous to \\(\\mathbf{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^n\\)), so</p> \\[mX + b\\mathbf{1} = \\begin{bmatrix} mx_1 \\\\ mx_2 \\\\ \\vdots \\\\ mx_n \\end{bmatrix} + \\begin{bmatrix} b \\\\ b \\\\ \\vdots \\\\ b \\end{bmatrix} = \\begin{bmatrix} mx_1 + b \\\\ mx_2 + b \\\\ \\vdots \\\\ mx_n + b \\end{bmatrix}\\] <p>and hence</p> \\[Y - (mX + b\\mathbf{1}) = \\begin{bmatrix} y_1 - (mx_1 + b) \\\\ y_2 - (mx_2 + b) \\\\ \\vdots \\\\ y_n - (mx_n + b) \\end{bmatrix} = \\text{\"vector of errors\"}\\] <p>Thus, since \\(\\sum_{i=1}^n v_i^2 = \\|\\mathbf{v}\\|^2\\) for any \\(\\mathbf{v} \\in \\mathbb{R}^n\\) (by definition of \\(\\|\\mathbf{v}\\|\\)!), the sum of the squares of the errors is</p> \\[\\sum_{i=1}^n (y_i - (mx_i + b))^2 = \\|Y - (mX + b\\mathbf{1})\\|^2\\] <p>So we seek \\(m\\) and \\(b\\) that minimizes the squared length of \\(Y - (mX + b\\mathbf{1})\\), which is the same as minimizing the length of that difference.</p> <p>The length \\(\\|Y - (mX + b\\mathbf{1})\\|\\) is the distance from \\(Y\\) to \\(mX + b\\mathbf{1}\\) since \"distance\" between any \\(n\\)-vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is \\(\\|\\mathbf{v} - \\mathbf{w}\\|\\) by definition. As \\(m\\) and \\(b\\) vary, the vectors of the form \\(mX + b\\mathbf{1}\\) are exactly the vectors in \\(\\text{span}(X, \\mathbf{1})\\), due to the definition of \"span\". Hence, the least-squares minimization problem for \\(n\\) data points is equivalent to the following geometric problem:</p> <p>find the vector in \\(\\text{span}(X, \\mathbf{1})\\) that is closest to the vector \\(Y \\in \\mathbb{R}^n\\).</p> <p>Our task is now an instance of finding the point of a linear subspace of \\(\\mathbb{R}^n\\) closest to a given \\(n\\)-vector.</p> <p>The vectors \\(X\\) and \\(\\mathbf{1}\\) are not scalar multiples of each other because the hypothesis that the \\(n\\) data points do not lie in a common vertical line (i.e., the \\(x_i\\)'s are not all equal to each other) says that \\(X\\) is not a scalar multiple of the nonzero vector \\(\\mathbf{1}\\).</p> <p>By using the Theorem above, an orthogonal basis of \\(\\text{span}(X, \\mathbf{1})\\) is given by \\(\\mathbf{1}\\) and \\(\\hat{X} = X - \\text{Proj}_{\\mathbf{1}}X\\) with</p> \\[\\text{Proj}_{\\mathbf{1}}(X) = \\frac{X \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}} \\mathbf{1} = \\frac{\\sum_{i=1}^n x_i \\cdot 1}{\\sum_{i=1}^n 1 \\cdot 1} \\mathbf{1} = \\frac{\\sum_{i=1}^n x_i}{n} \\mathbf{1} = \\bar{x} \\mathbf{1} = \\begin{bmatrix} \\bar{x} \\\\ \\bar{x} \\\\ \\vdots \\\\ \\bar{x} \\end{bmatrix}\\] <p>equal to the \\(n\\)-vector each of whose entries is equal to the average \\(\\bar{x}\\) of the \\(x_i\\)'s. Hence,</p> \\[\\hat{X} = X - \\text{Proj}_{\\mathbf{1}}(X) = \\begin{bmatrix} x_1 - \\bar{x} \\\\ x_2 - \\bar{x} \\\\ \\vdots \\\\ x_n - \\bar{x} \\end{bmatrix}\\] <p>is obtained from \\(X\\) by subtracting the average \\(\\bar{x}\\) from all entries.</p> <p>By applying to this span the formula for the nearest point on a linear subspace in terms of an orthogonal basis, we obtain that the closest vector to \\(Y\\) in \\(\\text{span}(X, \\mathbf{1})\\) is</p> \\[\\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}} \\mathbf{1} = \\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\bar{y} \\mathbf{1}\\] <p>where \\(\\bar{y} = (1/n) \\sum_{i=1}^n y_i\\) is the average of the \\(y_i\\)'s.</p> <p>In the expression \\(\\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\bar{y} \\mathbf{1}\\) on the right side, we can expand \\(\\hat{X}\\) in terms of \\(X\\) and \\(\\mathbf{1}\\) using the definition of \\(\\text{Proj}_{\\mathbf{1}}(X)\\) and collect terms to rewrite this as a linear combination \\(mX + b\\mathbf{1}\\) of \\(X\\) and \\(\\mathbf{1}\\). Those coefficients \\(m\\) and \\(b\\) are exactly the desired \"\\(m\\)\" and \"\\(b\\)\" for the best-fit line!</p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/#correlation-coefficient-and-quality-of-fit","title":"Correlation Coefficient and quality of fit","text":"<p>Let the best-fit line be \\(y = mx + b\\), and let \\(r\\) be the correlation coefficient for the recentered data \\((x_i - \\bar{x}, y_i - \\bar{y})\\) (whose coordinates average to 0) with associated \\(n\\)-vectors \\(\\hat{X}\\) and \\(\\hat{Y}\\). Then the role of nearness of \\(r^2\\) to 1 (or equivalently of nearness of \\(1 - r^2\\) to 0) as a measure of quality of fit is expressed by the following identity:</p> \\[\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2 (1 - r^2)\\] <p>This equation will be proven later.</p> <p>where \\(\\hat{Y}\\) is the \"recentered\" version of \\(Y\\) (subtracting \\(\\bar{y}\\) from all \\(y_i\\)'s).</p> <p>To explain the meaning of the above equation, expand out the left side (and use that \\(t^2 = |t|^2\\) for any \\(t\\)) to get</p> \\[|y_1 - (mx_1 + b)|^2 + |y_2 - (mx_2 + b)|^2 + \\cdots + |y_n - (mx_n + b)|^2\\] <p>The number \\(|y_i - (mx_i + b)|\\) is the vertical distance between the data point \\((x_i, y_i)\\) and the best fit line \\(y = mx + b\\). When \\(r^2 \\approx 1\\), the equation therefore says that these vertical distances are \"collectively small\": the sum of their squares is tiny since \\(1 - r^2\\) on the right side of the equation is small, so the data points are all close to the best fit line. When \\(r^2 \\approx 0\\) then (at least informally) the opposite happens since the right side is approximately \\(\\|\\hat{Y}\\|^2\\), which is typically quite far from 0 (even though the average of the entries in \\(\\hat{Y}\\) is 0 by design).</p> <p>Example: Sometimes a quantity \\(y\\) of interest is expected to be (approximately \"linearly\") related to a pair of quantities \\(x\\) and \\(v\\) rather than just a single quantity \\(x\\). In such cases, as a variant on linear regression, we seek three constants \\(a, b, c\\) for which</p> \\[y \\approx a + bx + cv\\] <p>as measured by data.</p> <p>Suppose we make \\(n\\) measurements of \\(x, v, y\\), yielding data points \\((x_i, v_i, y_i)\\). Let \\(X, V, Y \\in \\mathbb{R}^n\\) be the corresponding \\(n\\)-vectors for the \\(n\\) measurements of each of \\(x, v, y\\). Assume \\(W = \\text{span}(\\mathbf{1}, X, V)\\) is 3-dimensional (a reasonable assumption when neither \\(x\\) nor \\(v\\) determines the other).</p> <p>(a) Explain in words how the vector \\(\\text{Proj}_W(Y) \\in W\\) encodes a \"least squares\" choice of \\((a, b, c)\\) in terms of the data.</p> <p>(b) What is the practical difficulty in using the equation of finding a projection of a vector to compute \\(\\text{Proj}_W(Y)\\), whereas we had no difficulty in computing the analogous such projection for linear regression?</p> <p>Solution:</p> <p>(a) Least Squares Interpretation</p> <p>The vector \\(\\text{Proj}_W(Y) \\in W\\) represents the closest point in the subspace \\(W = \\text{span}(\\mathbf{1}, X, V)\\) to the data vector \\(Y\\). Since \\(W\\) is 3-dimensional, any vector in \\(W\\) can be written as a linear combination:</p> \\[\\text{Proj}_W(Y) = a\\mathbf{1} + bX + cV\\] <p>for some constants \\(a, b, c \\in \\mathbb{R}\\).</p> <p>This projection minimizes the distance \\(\\|Y - (a\\mathbf{1} + bX + cV)\\|\\), which is equivalent to minimizing the sum of squared errors:</p> \\[\\sum_{i=1}^n (y_i - (a + bx_i + cv_i))^2\\] <p>Therefore, the coefficients \\((a, b, c)\\) in the expression \\(\\text{Proj}_W(Y) = a\\mathbf{1} + bX + cV\\) represent the least squares solution to the multiple linear regression problem \\(y \\approx a + bx + cv\\).</p> <p>(b) Practical Difficulty</p> <p>The practical difficulty in computing \\(\\text{Proj}_W(Y)\\) for multiple linear regression compared to simple linear regression is the dimensionality of the subspace.</p> <p>Simple Linear Regression (2D subspace):</p> <ul> <li> <p>We had \\(W = \\text{span}(\\mathbf{1}, X)\\), a 2-dimensional subspace</p> </li> <li> <p>We could easily construct an orthogonal basis using the Gram-Schmidt process</p> </li> <li> <p>The projection formula was straightforward: \\(\\text{Proj}_W(Y) = \\text{Proj}_{\\mathbf{1}}(Y) + \\text{Proj}_{\\hat{X}}(Y)\\)</p> </li> <li> <p>We could compute this step-by-step with simple projections</p> </li> </ul> <p>Multiple Linear Regression (3D subspace):</p> <ul> <li> <p>We now have \\(W = \\text{span}(\\mathbf{1}, X, V)\\), a 3-dimensional subspace</p> </li> <li> <p>Constructing an orthogonal basis becomes more complex</p> </li> <li> <p>The Gram-Schmidt process requires more steps and can lead to numerical instability</p> </li> <li> <p>The projection formula involves more terms and becomes computationally intensive</p> </li> </ul> <p>Specific Challenges:</p> <ol> <li> <p>Orthogonal Basis Construction: We need to find three mutually orthogonal vectors spanning \\(W\\), which requires applying Gram-Schmidt to three vectors instead of two.</p> </li> <li> <p>Numerical Stability: As the dimension increases, small errors in computations can accumulate, leading to less accurate results.</p> </li> <li> <p>Computational Complexity: The projection involves more dot products and vector operations, making it computationally expensive for large datasets.</p> </li> <li> <p>Matrix Methods: For higher dimensions, it becomes more practical to use matrix methods (like QR decomposition or solving the normal equations) rather than geometric projection formulas.</p> </li> </ol> <p>This is why in practice, multiple linear regression is typically solved using matrix algebra and computational algorithms rather than the geometric projection approach, even though the geometric interpretation remains valid and insightful.</p> <p>Example: The vectors \\(\\mathbf{v} = \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{w} = \\begin{bmatrix} 11 \\\\ 5 \\\\ -10 \\\\ 1 \\end{bmatrix}\\) span a plane \\(P\\) through the origin in \\(\\mathbb{R}^4\\). Let</p> \\[L = \\left\\{\\begin{bmatrix} 4-t \\\\ 4+4t \\\\ 4-t \\\\ -7-2t \\end{bmatrix} : t \\in \\mathbb{R}\\right\\}\\] <p>be a line in \\(\\mathbb{R}^4\\).</p> <p>(a) Consider the displacement vector \\(\\mathbf{x}\\) between any two different points of \\(L\\) (all such displacements are scalar multiples of each other since \\(L\\) is a line). Show that \\(\\mathbf{x}\\) belongs to \\(P\\); this is described in words by saying \\(L\\) is parallel to \\(P\\).</p> <p>Solution:</p> <p>Let's take two different points on the line \\(L\\) by choosing two different values of \\(t\\). Let's use \\(t = 0\\) and \\(t = 1\\):</p> <ul> <li> <p>Point 1 (when \\(t = 0\\)): \\(\\begin{bmatrix} 4 \\\\ 4 \\\\ 4 \\\\ -7 \\end{bmatrix}\\)</p> </li> <li> <p>Point 2 (when \\(t = 1\\)): \\(\\begin{bmatrix} 3 \\\\ 8 \\\\ 3 \\\\ -9 \\end{bmatrix}\\)</p> </li> </ul> <p>The displacement vector between these two points is:</p> \\[\\mathbf{x} = \\begin{bmatrix} 3 \\\\ 8 \\\\ 3 \\\\ -9 \\end{bmatrix} - \\begin{bmatrix} 4 \\\\ 4 \\\\ 4 \\\\ -7 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix}\\] <p>We need to find scalars \\(a, b \\in \\mathbb{R}\\) such that:</p> \\[\\mathbf{x} = a\\mathbf{v} + b\\mathbf{w}\\] <p>This gives us the system of equations:</p> \\[\\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix} = a\\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} + b\\begin{bmatrix} 11 \\\\ 5 \\\\ -10 \\\\ 1 \\end{bmatrix}\\] <p>Which expands to:</p> \\[-1 = 2a + 11b \\quad \\text{(1)}\\] \\[4 = -a + 5b \\quad \\text{(2)}\\] \\[-1 = -a - 10b \\quad \\text{(3)}\\] \\[-2 = a + b \\quad \\text{(4)}\\] <p>Let's solve equations (1) and (2) first:</p> <p>From equation (2): \\(4 = -a + 5b\\), so \\(a = 5b - 4\\)</p> <p>Substitute into equation (1):</p> \\[-1 = 2(5b - 4) + 11b = 10b - 8 + 11b = 21b - 8\\] \\[21b = 7\\] \\[b = \\frac{1}{3}\\] <p>Now substitute \\(b = \\frac{1}{3}\\) back to find \\(a\\):</p> \\[a = 5\\left(\\frac{1}{3}\\right) - 4 = \\frac{5}{3} - 4 = \\frac{5}{3} - \\frac{12}{3} = -\\frac{7}{3}\\] <p>Let's check if \\(a = -\\frac{7}{3}\\) and \\(b = \\frac{1}{3}\\) satisfy all four equations:</p> <p>Equation (1): \\(2a + 11b = 2\\left(-\\frac{7}{3}\\right) + 11\\left(\\frac{1}{3}\\right) = -\\frac{14}{3} + \\frac{11}{3} = -\\frac{3}{3} = -1\\) \u2713</p> <p>Equation (2): \\(-a + 5b = -\\left(-\\frac{7}{3}\\right) + 5\\left(\\frac{1}{3}\\right) = \\frac{7}{3} + \\frac{5}{3} = \\frac{12}{3} = 4\\) \u2713</p> <p>Equation (3): \\(-a - 10b = -\\left(-\\frac{7}{3}\\right) - 10\\left(\\frac{1}{3}\\right) = \\frac{7}{3} - \\frac{10}{3} = -\\frac{3}{3} = -1\\) \u2713</p> <p>Equation (4): \\(a + b = -\\frac{7}{3} + \\frac{1}{3} = -\\frac{6}{3} = -2\\) \u2713</p> <p>Since we found scalars \\(a = -\\frac{7}{3}\\) and \\(b = \\frac{1}{3}\\) such that:</p> \\[\\mathbf{x} = -\\frac{7}{3}\\mathbf{v} + \\frac{1}{3}\\mathbf{w}\\] <p>This proves that the displacement vector \\(\\mathbf{x} = \\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix}\\) belongs to the plane \\(P = \\text{span}(\\mathbf{v}, \\mathbf{w})\\).</p> <p>This means that the line \\(L\\) is parallel to the plane \\(P\\). In \\(\\mathbb{R}^4\\), just as in \\(\\mathbb{R}^3\\), a line is parallel to a plane if the direction vector of the line (which is a scalar multiple of any displacement vector between two points on the line) lies in the plane.</p> <p>(b) Whenever one has a linear subspace \\(V\\) of \\(\\mathbb{R}^n\\) and a line \\(\\ell\\) in \\(\\mathbb{R}^n\\) (possibly not through the origin) that is parallel to \\(V\\), it is a fact (not difficult to show, but you may take it on faith) that all points in \\(\\ell\\) have the same distance to \\(V\\). That is, for every point \\(\\mathbf{y} \\in \\ell\\) and the point \\(\\mathbf{y}' \\in V\\) nearest to \\(\\mathbf{y}\\), the distance \\(\\|\\mathbf{y} - \\mathbf{y}'\\|\\) is the same regardless of which \\(\\mathbf{y}\\) on \\(\\ell\\) we consider. Taking \\(V\\) and \\(\\ell\\) to be \\(P\\) and \\(L\\) above, compute the common distance \\(\\|\\mathbf{y} - \\mathbf{y}'\\|\\) (since it is independent of \\(\\mathbf{y}\\), you may pick whatever you consider to be the most convenient point \\(\\mathbf{y}\\) in \\(L\\) to do the calculation).</p> <p>Solution:</p> <p>Since all points on the line \\(L\\) have the same distance to the plane \\(P\\), we can choose the most convenient point. Let's use the point when \\(t = 0\\): \\(\\mathbf{y} = \\begin{bmatrix} 4 \\\\ 4 \\\\ 4 \\\\ -7 \\end{bmatrix}\\).</p> <p>The distance from \\(\\mathbf{y}\\) to the plane \\(P\\) is the distance from \\(\\mathbf{y}\\) to its projection onto \\(P\\). To find this projection, we need an orthogonal basis for \\(P\\).</p> <p>Step 1: Find an orthogonal basis for \\(P\\)</p> <p>Using the theorem from earlier, we can construct an orthogonal basis for \\(P = \\text{span}(\\mathbf{v}, \\mathbf{w})\\):</p> <p>Let \\(\\mathbf{v}_1 = \\mathbf{v} = \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\)</p> <p>Then \\(\\mathbf{v}_2 = \\mathbf{w} - \\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w})\\)</p> <p>First, compute \\(\\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w})\\):</p> \\[\\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w}) = \\frac{\\mathbf{w} \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1} \\mathbf{v}_1\\] \\[\\mathbf{w} \\cdot \\mathbf{v}_1 = 11(2) + 5(-1) + (-10)(-1) + 1(1) = 22 - 5 + 10 + 1 = 28\\] \\[\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = 2^2 + (-1)^2 + (-1)^2 + 1^2 = 4 + 1 + 1 + 1 = 7\\] <p>So:</p> \\[\\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w}) = \\frac{28}{7} \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} = 4 \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ -4 \\\\ -4 \\\\ 4 \\end{bmatrix}\\] <p>Therefore:</p> \\[\\mathbf{v}_2 = \\mathbf{w} - \\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w}) = \\begin{bmatrix} 11 \\\\ 5 \\\\ -10 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 8 \\\\ -4 \\\\ -4 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 9 \\\\ -6 \\\\ -3 \\end{bmatrix}\\] <p>Step 2: Compute the projection of \\(\\mathbf{y}\\) onto \\(P\\)</p> <p>Using the orthogonal basis \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\), the projection is:</p> \\[\\text{Proj}_P(\\mathbf{y}) = \\frac{\\mathbf{y} \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1} \\mathbf{v}_1 + \\frac{\\mathbf{y} \\cdot \\mathbf{v}_2}{\\mathbf{v}_2 \\cdot \\mathbf{v}_2} \\mathbf{v}_2\\] <p>Compute the dot products:</p> \\[\\mathbf{y} \\cdot \\mathbf{v}_1 = 4(2) + 4(-1) + 4(-1) + (-7)(1) = 8 - 4 - 4 - 7 = -7\\] \\[\\mathbf{y} \\cdot \\mathbf{v}_2 = 4(3) + 4(9) + 4(-6) + (-7)(-3) = 12 + 36 - 24 + 21 = 45\\] \\[\\mathbf{v}_2 \\cdot \\mathbf{v}_2 = 3^2 + 9^2 + (-6)^2 + (-3)^2 = 9 + 81 + 36 + 9 = 135\\] <p>So:</p> \\[\\text{Proj}_P(\\mathbf{y}) = \\frac{-7}{7} \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} + \\frac{45}{135} \\begin{bmatrix} 3 \\\\ 9 \\\\ -6 \\\\ -3 \\end{bmatrix}\\] \\[= -1 \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} + \\frac{1}{3} \\begin{bmatrix} 3 \\\\ 9 \\\\ -6 \\\\ -3 \\end{bmatrix}\\] \\[= \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ -1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 3 \\\\ -2 \\\\ -1 \\end{bmatrix}\\] \\[= \\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix}\\] <p>Step 3: Compute the distance</p> <p>The distance from \\(\\mathbf{y}\\) to the plane \\(P\\) is:</p> \\[\\|\\mathbf{y} - \\text{Proj}_P(\\mathbf{y})\\| = \\left\\|\\begin{bmatrix} 4 \\\\ 4 \\\\ 4 \\\\ -7 \\end{bmatrix} - \\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix}\\right\\| = \\left\\|\\begin{bmatrix} 5 \\\\ 0 \\\\ 5 \\\\ -5 \\end{bmatrix}\\right\\|\\] \\[\\|\\mathbf{y} - \\text{Proj}_P(\\mathbf{y})\\| = \\sqrt{5^2 + 0^2 + 5^2 + (-5)^2} = \\sqrt{25 + 0 + 25 + 25} = \\sqrt{75} = \\sqrt{25 \\times 3} = 5\\sqrt{3}\\] <p>Answer: The common distance from any point on the line \\(L\\) to the plane \\(P\\) is \\(\\sqrt{75}\\).</p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/#orthogonal-basis-formula-and-relation-of-correlation-coefficient-to-best-fit-lines","title":"Orthogonal basis formula and relation of correlation coefficient to best fit lines","text":"<p>In this section we prove some results discussed earlier.</p> <p>Theorem: Suppose \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) are nonzero, and not scalar multiples of each other. The vectors \\(\\mathbf{y}\\) and \\(\\mathbf{x}' = \\mathbf{x} - \\text{Proj}_{\\mathbf{y}} \\mathbf{x}\\) constitute an orthogonal basis of \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\). In particular, \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\) is 2-dimensional.</p> <p>The setup is symmetric in \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), so \\(\\{\\mathbf{x}, \\mathbf{y}' = \\mathbf{y} - \\text{Proj}_{\\mathbf{x}} \\mathbf{y}\\}\\) is also an orthogonal basis of \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\).</p> <p>Proof: Write \\(\\mathbf{x}' = \\mathbf{x} - \\text{Proj}_{\\mathbf{y}} \\mathbf{x}\\).</p> \\[\\mathbf{x}' \\cdot \\mathbf{y} = \\left(\\mathbf{x} - \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\mathbf{y} \\cdot \\mathbf{y}}\\mathbf{y}\\right) \\cdot \\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{y} - \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\mathbf{y} \\cdot \\mathbf{y}}\\mathbf{y} \\cdot \\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{y} - \\mathbf{x} \\cdot \\mathbf{y} = 0.\\] <p>Next, \\(\\mathbf{y}\\) is not zero (we have assumed this). Also, \\(\\mathbf{x}'\\) is not zero: if it were zero then \\(\\mathbf{x} = \\text{Proj}_{\\mathbf{y}}(\\mathbf{x})\\), yet such a projection is always a scalar multiple of \\(\\mathbf{y}\\) and we have assumed \\(\\mathbf{x}\\) is not a scalar multiple of \\(\\mathbf{y}\\). Therefore \\(\\{\\mathbf{x}', \\mathbf{y}\\}\\) is a pair of nonzero orthogonal vectors belonging to \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\) by design (note that \\(\\mathbf{y} = 0 \\cdot \\mathbf{x} + 1 \\cdot \\mathbf{y}\\)), and they exhaust that span since we can also write each of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) as linear combinations of \\(\\mathbf{x}'\\) and \\(\\mathbf{y}\\): \\(\\mathbf{x} = \\mathbf{x}' + \\text{Proj}_{\\mathbf{y}}(\\mathbf{x}) = \\mathbf{x}' + ((\\mathbf{x} \\cdot \\mathbf{y})/(\\mathbf{y} \\cdot \\mathbf{y}))\\mathbf{y}\\) and \\(\\mathbf{y} = 0 \\cdot \\mathbf{x}' + 1 \\cdot \\mathbf{y}\\). Since any collection of pairwise orthogonal nonzero vectors is a basis for its span, we conclude that \\(\\{\\mathbf{x}', \\mathbf{y}\\}\\) is an orthogonal basis of \\(\\text{span}(\\mathbf{x}', \\mathbf{y}) = \\text{span}(\\mathbf{x}, \\mathbf{y})\\).</p> <p>Now suppose we are given \\(n\\) data points \\((x_i, y_i)\\), assembled into \\(n\\)-vectors</p> \\[X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad \\text{and} \\quad Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\] <p>Earlier, we described the relationship between the correlation coefficient \\(r\\) for the recentered data (corresponding to the \\(n\\)-vectors \\(\\hat{X}\\) and \\(\\hat{Y}\\)) and the line of best fit. Let's restate that in terms of \\(r^2\\), which we expressed as the formula</p> \\[r^2 = \\frac{(\\hat{X} \\cdot \\hat{Y})^2}{\\|\\hat{X}\\|^2\\|\\hat{Y}\\|^2} = \\frac{(\\hat{X} \\cdot \\hat{Y})^2}{(\\hat{X} \\cdot \\hat{X})(\\hat{Y} \\cdot \\hat{Y})}\\] <p>We stated that \\(r^2\\) is near 0 when the line of best fit is a bad fit, and near 1 when it is a good fit (note that this could happen either when \\(r\\) is near 1, or when \\(r\\) is near \u22121). We made the role of \\(r^2\\) as a measure of quality of fit precise. Here is the derivation of \\(\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2 (1 - r^2)\\).</p> <p>Proof: We know that the closest vector to \\(Y\\) in \\(\\text{span}(X, \\mathbf{1})\\) is</p> \\[\\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}} \\mathbf{1} = \\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\bar{y} \\mathbf{1}\\] <p>where \\(\\bar{y} = (1/n) \\sum_{i=1}^n y_i\\) is the average of the \\(y_i\\)'s.</p> \\[Y - (mX + b\\mathbf{1}) = Y - \\left(\\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X} + \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}}\\mathbf{1}\\right) = \\left(Y - \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}}\\mathbf{1}\\right) - \\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X}\\] <p>where \\(Y - \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}}\\mathbf{1} = Y - \\bar{y}\\mathbf{1}\\) is indeed equal to \\(\\hat{Y}\\).</p> <p>Note that \\(\\hat{Y} \\cdot \\hat{X} = Y \\cdot \\hat{X}\\) because the difference \\(\\hat{Y} - Y = -\\bar{y}\\mathbf{1}\\) is orthogonal to \\(\\hat{X}\\).</p> <p>To understand why \\(\\hat{Y} \\cdot \\hat{X} = Y \\cdot \\hat{X}\\), let's examine the orthogonality of \\(\\hat{Y} - Y = -\\bar{y}\\mathbf{1}\\) to \\(\\hat{X}\\). Recall that \\(\\hat{X} = X - \\bar{x}\\mathbf{1}\\), which means \\(\\hat{X}\\) is the vector \\(X\\) with the mean \\(\\bar{x}\\) subtracted from each component. We need to show that \\((-\\bar{y}\\mathbf{1}) \\cdot \\hat{X} = 0\\). This is:</p> \\[(-\\bar{y}\\mathbf{1}) \\cdot \\hat{X} = -\\bar{y}\\mathbf{1} \\cdot (X - \\bar{x}\\mathbf{1}) = -\\bar{y}(\\mathbf{1} \\cdot X) + \\bar{y}\\bar{x}(\\mathbf{1} \\cdot \\mathbf{1})\\] <p>But \\(\\mathbf{1} \\cdot X = \\sum_{i=1}^n x_i = n\\bar{x}\\) and \\(\\mathbf{1} \\cdot \\mathbf{1} = n\\).</p> <p>So: \\(-\\bar{y}(\\mathbf{1} \\cdot X) + \\bar{y}\\bar{x}(\\mathbf{1} \\cdot \\mathbf{1}) = -\\bar{y}(n\\bar{x}) + \\bar{y}\\bar{x}(n) = -n\\bar{x}\\bar{y} + n\\bar{x}\\bar{y} = 0\\)</p> \\[\\hat{Y} \\cdot \\hat{X} = (Y + (\\hat{Y} - Y)) \\cdot \\hat{X} = Y \\cdot \\hat{X} + (\\hat{Y} - Y) \\cdot \\hat{X} = Y \\cdot \\hat{X} + 0 = Y \\cdot \\hat{X}\\] <p>Putting this into the numerator of the final coefficient on the right side yields</p> \\[Y - (mX + b\\mathbf{1}) = \\hat{Y} - \\frac{\\hat{Y} \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X} = \\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}\\] <p>The vectors \\(\\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}\\) and \\(\\text{Proj}_{\\hat{X}}\\hat{Y}\\) are perpendicular to each other. Therefore, by the Pythagorean Theorem in \\(\\mathbb{R}^n\\), we have</p> \\[\\|\\hat{Y}\\|^2 = \\|(\\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}) + \\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2 = \\|\\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2 + \\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2\\] <p>so \\(\\|\\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2 = \\|\\hat{Y}\\|^2 - \\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2\\). But the vector difference on the left side is exactly \\(Y - (mX + b\\mathbf{1})\\), so</p> \\[\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2 - \\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2\\] <p>Finally, using the definition of \\(\\text{Proj}_{\\hat{X}}\\hat{Y}\\), we have</p> \\[\\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2 = \\left(\\frac{\\hat{Y} \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X}\\right) \\cdot \\left(\\frac{\\hat{Y} \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X}\\right) = \\left(\\frac{\\hat{Y} \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\right)^2\\hat{X} \\cdot \\hat{X} = \\frac{(\\hat{Y} \\cdot \\hat{X})^2}{\\hat{X} \\cdot \\hat{X}} = r^2(\\hat{Y} \\cdot \\hat{Y})\\] <p>so plugging into \\(\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2 - \\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2\\) yields \\(\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2(1 - r^2)\\), which is exactly the desired identity.</p>"},{"location":"math/linear_algebra/basis_and_orthogonality/","title":"Basis and orthogonality","text":"<p>A basis for a nonzero linear subspace \\(V\\) in \\(\\mathbb{R}^n\\) is a spanning set for \\(V\\) consisting of exactly \\(\\dim(V)\\) vectors.</p> <p>If \\(\\dim(V) = 2\\) then a basis for \\(V\\) consists of any v, w for which \\(\\text{span}(v,w) = V\\).</p> <p>One basis of \\(\\mathbb{R}^3\\) is given by \\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\), \\(\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\\), \\(\\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\); this is called the standard basis of \\(\\mathbb{R}^3\\). But many other triples of vectors are also a basis of \\(\\mathbb{R}^3\\).</p> <p>Although we have a way to figure out the dimension of the span of 2 or 3 nonzero vectors, we have to confront the reality that for the span of 4 or more nonzero vectors in \\(\\mathbb{R}^n\\) it becomes rather cumbersome to figure out the dimension via algebra alone; we need another way.</p> <p>A collection of vectors v\\(_1\\), . . . , v\\(_k\\) in \\(\\mathbb{R}^n\\) is called orthogonal if \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) whenever \\(i \\neq j\\). In words, the vectors are all perpendicular to one another.</p> <p>If v\\(_1\\), . . . , v\\(_k\\) is an orthogonal collection of nonzero vectors in \\(\\mathbb{R}^n\\) then it is a basis for \\(\\text{span}(v_1, \\ldots, v_k)\\). In particular, \\(\\text{span}(v_1, \\ldots, v_k)\\) then has dimension \\(k\\) and we call v\\(_1\\), . . . , v\\(_k\\) an orthogonal basis for its span (a single nonzero vector is always an orthogonal basis for its span!).</p> <p>The span of a collection of \\(k\\) vectors in \\(\\mathbb{R}^n\\) has dimension at most \\(k\\) (e.g., three vectors in \\(\\mathbb{R}^3\\) lying in a common plane through 0 have span with dimension less than 3). Orthogonality is a useful way to guarantee that \\(k\\) given nonzero \\(n\\)-vectors have a \\(k\\)-dimensional span.</p> <p>Example: Consider the span \\(V\\) of the following three vectors in \\(\\mathbb{R}^5\\):</p> \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\end{bmatrix}\\] <p>This collection of three vectors is not orthogonal, since, for example, \\(\\mathbf{v}_1 \\cdot \\mathbf{v}_2 = 1 + 0 + 6 + 0 + 3 = 10\\). We can show that \\(\\dim(V) = 3\\), so the triple \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) is a basis of \\(V\\) (if \\(\\dim(V) = 2\\), then the triple would not be a basis of V, just a regular spanning set), but not an orthogonal basis of \\(V\\).</p> <p>Note: There is a systematic process for finding an orthogonal basis for the span of \\(k\\) vectors in \\(\\mathbb{R}^n\\) called the \"Gram\u2013Schmidt process\".</p> <p>Every nonzero linear subspace of \\(\\mathbb{R}^n\\) has an orthogonal basis.</p> <p>There is an especially convenient type of orthogonal basis for a nonzero linear subspace of \\(\\mathbb{R}^n\\). A collection of vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\) is called orthonormal if they are orthogonal to each other and in addition they are all unit vectors; that is, \\(\\mathbf{v}_i \\cdot \\mathbf{v}_i = 1\\) for all \\(i\\) (ensuring \\(\\|\\mathbf{v}_i\\| = \\sqrt{\\mathbf{v}_i \\cdot \\mathbf{v}_i} = \\sqrt{1} = 1\\) for all \\(i\\)).</p> <p>Any orthonormal collection of vectors is a basis of its span.</p> <p>For any \\(n\\) the analogous orthonormal collection of \\(n\\) vectors \\(\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\) in \\(\\mathbb{R}^n\\) can be written down (i.e., \\(\\mathbf{e}_i\\) has its \\(i\\)th entry equal to \\(1\\) and all other entries are \\(0\\)), and this spans \\(\\mathbb{R}^n\\); it is called the standard basis of \\(\\mathbb{R}^n\\), and shows \\(\\dim(\\mathbb{R}^n) = n\\) (as we expect). In particular, (with \\(V = \\mathbb{R}^n\\)), every linear subspace of \\(\\mathbb{R}^n\\) has dimension at most \\(n\\) and the only \\(n\\)-dimensional one is \\(\\mathbb{R}^n\\) itself (as geometric intuition may suggest).</p> <p>In the special case \\(n = 3\\), the vectors \\(\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3 \\in \\mathbb{R}^3\\) are often respectively denoted as \\(\\mathbf{i}, \\mathbf{j}, \\mathbf{k}\\) in physics and engineering contexts.</p> <p>Example: The triple</p> \\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\end{bmatrix}, \\quad \\begin{bmatrix} -6 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} -2 \\\\ -25 \\\\ 13 \\end{bmatrix}\\] <p>is an orthogonal basis for \\(\\mathbb{R}^3\\). How can one \"see\" this? One can check by hand that it is an orthogonal collection of vectors, so this collection of \\(3\\) nonzero vectors must be a basis of its span by, and hence its span has dimension \\(3\\).</p>"},{"location":"math/linear_algebra/basis_and_orthogonality/#fourier-formula","title":"Fourier formula","text":"<p>If \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) are nonzero vectors in \\(\\mathbb{R}^n\\), by definition any vector \\(\\mathbf{v} \\in \\text{span}(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k)\\) can be written as a linear combination</p> \\[\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i\\] <p>for some scalars \\(c_1, \\ldots, c_k\\). If the collection of \\(\\mathbf{v}_i\\)'s is orthogonal, we can actually solve for the \\(c_i\\)'s in terms of \\(\\mathbf{v}\\) by the following slick technique that has useful generalizations throughout mathematics (with Fourier series, special function theory, and so on).</p> <p>For instance, if we form the dot product against \\(\\mathbf{v}_1\\) then we obtain</p> \\[\\mathbf{v} \\cdot \\mathbf{v}_1 = c_1(\\mathbf{v}_1 \\cdot \\mathbf{v}_1) + c_2(\\mathbf{v}_2 \\cdot \\mathbf{v}_1) + c_3(\\mathbf{v}_3 \\cdot \\mathbf{v}_1) + \\cdots = c_1(\\mathbf{v}_1 \\cdot \\mathbf{v}_1),\\] <p>where the tremendous cancellation at the final equality is precisely due to the orthogonality of the collection of \\(\\mathbf{v}_i\\)'s. Since \\(\\mathbf{v}_1\\) is nonzero, so \\(\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = \\|\\mathbf{v}_1\\|^2\\) is nonzero, we can now divide by it at both ends of our string of equalities above to obtain</p> \\[\\frac{\\mathbf{v} \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1} = c_1.\\] <p>In this way we have solved for \\(c_1\\)!</p> <p>The same procedure works likewise to solve for each \\(c_i\\) via forming dot products against \\(\\mathbf{v}_i\\), yielding the general formula</p> \\[c_i = \\frac{\\mathbf{v} \\cdot \\mathbf{v}_i}{\\mathbf{v}_i \\cdot \\mathbf{v}_i}\\] <p>for each \\(i\\). Substituting back into the right side of the equation for \\(\\mathbf{v}\\), we obtain the following result.</p> <p>Theorem (Fourier formula). For any orthogonal collection of nonzero vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\) and vector \\(\\mathbf{v}\\) in their span,</p> \\[\\mathbf{v} = \\sum_{i=1}^k \\frac{\\mathbf{v} \\cdot \\mathbf{v}_i}{\\mathbf{v}_i \\cdot \\mathbf{v}_i} \\mathbf{v}_i.\\] <p>In particular, if the \\(\\mathbf{v}_i\\)'s are all unit vectors (so \\(\\mathbf{v}_i \\cdot \\mathbf{v}_i = 1\\) for all \\(i\\)) then \\(\\mathbf{v} = \\sum_{i=1}^k (\\mathbf{v} \\cdot \\mathbf{v}_i)\\mathbf{v}_i\\).</p> <p>Example: For the orthonormal basis \\(\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3, \\mathbf{e}_4\\) of \\(\\mathbb{R}^4\\) and any \\(\\mathbf{v} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} \\in \\mathbb{R}^4\\), the coefficients \\(\\mathbf{v} \\cdot \\mathbf{e}_i\\) work out as follows:</p> \\[\\mathbf{v} \\cdot \\mathbf{e}_1 = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = a_1\\] <p>and similarly \\(\\mathbf{v} \\cdot \\mathbf{e}_i = a_i\\) for each \\(i = 1, 2, 3, 4\\). Thus (since \\(\\mathbf{e}_i \\cdot \\mathbf{e}_i = 1\\)), \\(\\mathbf{v} = \\sum_{i=1}^4 (\\mathbf{v} \\cdot \\mathbf{e}_i)\\mathbf{e}_i = \\sum_{i=1}^4 a_i\\mathbf{e}_i\\). Unpacking the summation notation, this is just asserting</p> \\[\\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} = a_1\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + a_2\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + a_3\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + a_4\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\\] <p>which can be directly verified by hand since the right side is exactly</p> \\[\\begin{bmatrix} a_1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ a_2 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ a_3 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ a_4 \\end{bmatrix}.\\] <p>In other words, the Fourier formula in the special case that \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) is the orthonormal basis \\(\\{\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\}\\) of \\(\\mathbb{R}^n\\) is precisely the familiar fact that any vector in \\(\\mathbb{R}^n\\) can be decomposed as the sum of its \"components\" along the various coordinate directions. This is neither surprising nor perhaps particularly interesting, so we next give a more \"typical\" example.</p> <p>Example: Consider the span \\(V\\) of the following three vectors in \\(\\mathbb{R}^5\\):</p> \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\end{bmatrix}\\] <p>Consider the following three nonzero vectors in \\(V\\), which form an orthogonal basis for their span:</p> \\[\\mathbf{w}_1 = \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{w}_2 = -2\\mathbf{v}_1 + 3\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 3 \\\\ 0 \\\\ -4 \\\\ 7 \\end{bmatrix}, \\quad \\mathbf{w}_3 = -9\\mathbf{v}_1 - 24\\mathbf{v}_2 + 75\\mathbf{v}_3 = \\begin{bmatrix} -33 \\\\ 201 \\\\ -75 \\\\ 132 \\\\ -6 \\end{bmatrix}\\] <p>Consider the vector</p> \\[\\mathbf{v} = 2\\mathbf{v}_1 - \\mathbf{v}_2 + \\mathbf{v}_3 = \\begin{bmatrix} 2 \\\\ 0 \\\\ 6 \\\\ 4 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\\\ 6 \\\\ 0 \\end{bmatrix}\\] <p>in \\(V\\). Since \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}\\) is a basis of \\(V\\), we know that there is some expression of the form</p> \\[\\mathbf{v} = c_1\\mathbf{w}_1 + c_2\\mathbf{w}_2 + c_3\\mathbf{w}_3\\] <p>for unknown scalars \\(c_1, c_2, c_3\\). What are these scalars? A brute-force approach would be to write everything out as explicit vectors to obtain</p> \\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\\\ 6 \\\\ 0 \\end{bmatrix} = \\mathbf{v} = c_1\\mathbf{w}_1 + c_2\\mathbf{w}_2 + c_3\\mathbf{w}_3 = c_1\\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix} + c_2\\begin{bmatrix} 1 \\\\ 3 \\\\ 0 \\\\ -4 \\\\ 7 \\end{bmatrix} + c_3\\begin{bmatrix} -33 \\\\ 201 \\\\ -75 \\\\ 132 \\\\ -6 \\end{bmatrix} = \\begin{bmatrix} c_1 + c_2 - 33c_3 \\\\ 3c_2 + 201c_3 \\\\ 3c_1 - 75c_3 \\\\ 2c_1 - 4c_2 + 132c_3 \\\\ c_1 + 7c_2 - 6c_3 \\end{bmatrix},\\] <p>and then equate corresponding vector entries on the left and right sides to get a huge system of \\(5\\) equations in \\(3\\) unknowns. We can entirely bypass that by computing dot products for our specific \\(\\mathbf{v}\\)!</p> <p>To carry this out, we use the explicit descriptions of \\(\\mathbf{v}\\) and the \\(\\mathbf{w}_i\\)'s to compute</p> \\[\\mathbf{v} \\cdot \\mathbf{w}_1 = 25, \\quad \\mathbf{v} \\cdot \\mathbf{w}_2 = -17, \\quad \\mathbf{v} \\cdot \\mathbf{w}_3 = 861,\\] <p>so the Fourier formula says for this particular \\(\\mathbf{v}\\) that</p> \\[\\mathbf{v} = \\frac{25}{15}\\mathbf{w}_1 - \\frac{17}{75}\\mathbf{w}_2 + \\frac{861}{64575}\\mathbf{w}_3 = \\frac{5}{3}\\mathbf{w}_1 - \\frac{17}{75}\\mathbf{w}_2 + \\frac{1}{75}\\mathbf{w}_3.\\] <p>That's it! This is the expression for \\(\\mathbf{v}\\) as a linear combination of the orthogonal basis \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}\\) of \\(V\\).</p>"},{"location":"math/linear_algebra/planes_in_r3/","title":"Planes in \\(\\mathbb{R}^3\\)","text":"<p>The collection of points \\((x, y, z)\\) in \\(\\mathbb{R}^3\\) satisfying an equation of the form</p> \\[ax + by + cz = d\\] <p>with at least one of the constants \\(a\\), \\(b\\), or \\(c\\) nonzero, is a plane in \\(\\mathbb{R}^3\\). Note that although the equation \\(x = 0\\) on \\(\\mathbb{R}^2\\) defines a line (the \\(y\\)-axis, consisting of points \\((0, y)\\)), the \"same\" equation \\(x = 0\\) on \\(\\mathbb{R}^3\\) defines a plane, namely the vertical \\(yz\\)-plane consisting of points \\((0, y, z)\\).</p>"},{"location":"math/linear_algebra/planes_in_r3/#lines-in-mathbbr3","title":"Lines in \\(\\mathbb{R}^3\\)","text":"<p>In \\(\\mathbb{R}^3\\), a line can be represented in several forms:</p> <p>Parametric form (most common): \\(\\mathbf{r}(t) = \\mathbf{r}_0 + t\\mathbf{v}\\) where \\(\\mathbf{r}_0\\) is a point on the line and \\(\\mathbf{v}\\) is a direction vector.</p> <p>As an example, consider a line that passes through the point \\((1, 0, -2)\\) and has direction vector \\((3, 1, 4)\\). The parametric equation is:</p> \\[\\mathbf{r}(t) = (1, 0, -2) + t(3, 1, 4) = (1 + 3t, 0 + t, -2 + 4t)\\] <p>This means any point on the line has coordinates \\((1 + 3t, t, -2 + 4t)\\) for some real number \\(t\\). For example:</p> <ul> <li> <p>When \\(t = 0\\): \\((1, 0, -2)\\) (the base point)</p> </li> <li> <p>When \\(t = 1\\): \\((4, 1, 2)\\)</p> </li> <li> <p>When \\(t = -1\\): \\((-2, -1, -6)\\)</p> </li> </ul> <p>Parametric form through two points: Given two points \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\), the line passing through them has parametric equation:</p> \\[\\mathbf{r}(t) = \\mathbf{p} + t(\\mathbf{q} - \\mathbf{p}) = (1-t)\\mathbf{p} + t\\mathbf{q}\\] <p>This form uses the direction vector \\(\\mathbf{q} - \\mathbf{p}\\) and parameter \\(t\\) ranges from 0 to 1 to give all points between \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\).</p> <p>Example: For points \\(\\mathbf{p} = (1, 2, 3)\\) and \\(\\mathbf{q} = (4, 1, 0)\\), the line equation is:</p> \\[\\mathbf{r}(t) = (1, 2, 3) + t(3, -1, -3) = (1 + 3t, 2 - t, 3 - 3t)\\] <p>Symmetric form (when all components of \\(\\mathbf{v}\\) are nonzero): \\(\\frac{x - x_0}{v_1} = \\frac{y - y_0}{v_2} = \\frac{z - z_0}{v_3}\\)</p> <p>In the symmetric form:</p> <ul> <li> <p>\\((x_0, y_0, z_0)\\) is a point on the line (the base point)</p> </li> <li> <p>\\((v_1, v_2, v_3)\\) are the components of the direction vector \\(\\mathbf{v}\\)</p> </li> </ul> <p>So if we have a line with parametric form \\(\\mathbf{r}(t) = \\mathbf{r}_0 + t\\mathbf{v}\\), then \\((x_0, y_0, z_0) = \\mathbf{r}_0\\) and \\((v_1, v_2, v_3) = \\mathbf{v}\\).</p> <p>For example, if a line passes through the point \\((2, -1, 3)\\) and has direction vector \\((1, 2, -1)\\), then the symmetric form would be:</p> \\[\\frac{x - 2}{1} = \\frac{y - (-1)}{2} = \\frac{z - 3}{-1}\\] <p>This form eliminates the parameter \\(t\\) and gives a direct relationship between the coordinates, but it only works when all components of the direction vector are nonzero (to avoid division by zero).</p> <p>Intersection of two planes:  \\(\\begin{cases} a_1x + b_1y + c_1z = d_1 \\\\ a_2x + b_2y + c_2z = d_2 \\end{cases}\\)</p> <p>The key difference from \\(\\mathbb{R}^2\\) is that in \\(\\mathbb{R}^3\\), a single linear equation \\(ax + by + cz = d\\) defines a plane, not a line. To define a line in \\(\\mathbb{R}^3\\), you need either a parametric equation with one parameter, the intersection of two planes (two linear equations), or, a point and a direction vector.</p>"},{"location":"math/linear_algebra/planes_in_r3/#forms-of-planes","title":"Forms of Planes","text":"<p>Planes in \\(\\mathbb{R}^3\\) can be represented in several different forms, each useful for different purposes:</p> <p>1. Point-normal form: \\((x - x_0, y - y_0, z - z_0) \\cdot \\mathbf{n} = 0\\)</p> <p>This form uses a point \\((x_0, y_0, z_0)\\) on the plane and a normal vector \\(\\mathbf{n} = (a, b, c)\\) perpendicular to the plane.</p> <p></p> <p>Example: A plane (shown above) passing through the point \\((0, 1, 1)\\) with normal vector \\((3, -2, 1)\\) has equation:</p> \\[(x - 0, y - 1, z - 1) \\cdot (3, -2, 1) = 0\\] \\[3(x - 0) - 2(y - 1) + 1(z - 1) = 0\\] \\[3x - 2y + z + 1 = 0\\] <p>From the coefficients we again read off that (3, -2, 1) is a normal vector to the plane (not to the individual points in the plane, but rather to differences between such points). This is no surprise, in view of how the plane was originally defined.</p> <p>2. General form: \\(ax + by + cz + d = 0\\)</p> <p>This is the standard form where \\((a, b, c)\\) is a normal vector to the plane and \\(d\\) determines the position of the plane in space.</p> <p>Example: The plane \\(2x - 3y + 4z = 12\\) has normal vector \\((2, -3, 4)\\) and can be rewritten as \\(2x - 3y + 4z - 12 = 0\\).</p> <p>The plane \\(ax + by + cz + d = 0\\) is at a distance of \\(\\frac{|d|}{\\sqrt{a^2 + b^2 + c^2}}\\) from the origin. All planes with the same normal vector \\((a, b, c)\\) but different \\(d\\) values are parallel to each other.</p> <p>If you think of the normal vector \\((a,b,c)\\) as pointing in a fixed direction, then \\(d\\) tells you \"how far along that direction\" the plane is located. For example, the planes \\(2x - 3y + 4z = 0\\), \\(2x - 3y + 4z = 5\\), and \\(2x - 3y + 4z = -3\\) all have the same normal vector \\((2, -3, 4)\\) but are at different distances from the origin.</p> <p>3. Parametric form: \\(\\mathbf{r}(s,t) = \\mathbf{r}_0 + s\\mathbf{v}_1 + t\\mathbf{v}_2\\)</p> <p>This form uses a point \\(\\mathbf{r}_0\\) on the plane and two non-parallel direction vectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) that lie in the plane.</p> <p>Example: A plane through the point \\((1, 0, 2)\\) with direction vectors \\((1, 1, 0)\\) and \\((0, 1, 1)\\) has parametric equation: \\(\\mathbf{r}(s,t) = (1, 0, 2) + s(1, 1, 0) + t(0, 1, 1) = (1 + s, s + t, 2 + t)\\)</p> <p>An advantage of the parametric form is that as we independently vary values of the parameters \\(t\\) and \\(t\u2032\\), the vectors we get in the parametric form are guaranteed to lie exactly on the plane. For instance, if we want to trace out some path in the plane, then by varying the values of \\(t\\) and \\(t\u2032\\) continuously we trace out a continuous curve exactly in the plan.</p> <p></p> <p>The parametric description for planes and its analogues for more complicated surfaces (such as a sphere, a cylinder, etc.) is quite useful in computer graphics to generate the image of a path of motion lying exactly on a specific surface. For such applications a parametric form is far more useful than the general form; as with the parametric form we do not have to solve for anything.</p> <p>4. Three-point form: Using three non-collinear points</p> <p>Given three points \\((x_1, y_1, z_1)\\), \\((x_2, y_2, z_2)\\), and \\((x_3, y_3, z_3)\\), the plane equation is: \\(\\begin{vmatrix} x - x_1 &amp; y - y_1 &amp; z - z_1 \\\\ x_2 - x_1 &amp; y_2 - y_1 &amp; z_2 - z_1 \\\\ x_3 - x_1 &amp; y_3 - y_1 &amp; z_3 - z_1 \\end{vmatrix} = 0\\)</p> <p>Example: For points \\((1, 0, 0)\\), \\((0, 1, 0)\\), and \\((0, 0, 1)\\), the plane equation becomes:</p> \\[\\begin{vmatrix} x - 1 &amp; y &amp; z \\\\ -1 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 \\end{vmatrix} = 0\\] \\[(x - 1)(1 \\cdot 1 - 0 \\cdot 0) - y(-1 \\cdot 1 - 0 \\cdot (-1)) + z(-1 \\cdot 0 - 1 \\cdot (-1)) = 0\\] \\[(x - 1) + y + z = 0\\] \\[x + y + z = 1\\] <p>Note: We claim that the only way three different points can be on a common line in space is when the difference vectors from one of them to the other two lie along the same or opposite directions. This corresponds to those two difference vectors being scalar multiples of each other (a positive scalar when pointing in the same direction, and a negative scalar when pointing in opposite directions).</p> <p>5. Intercept form: \\(\\frac{x}{a} + \\frac{y}{b} + \\frac{z}{c} = 1\\)</p> <p>This form shows the intercepts of the plane with the coordinate axes: \\((a, 0, 0)\\), \\((0, b, 0)\\), and \\((0, 0, c)\\).</p> <p>Example: The plane \\(\\frac{x}{3} + \\frac{y}{2} + \\frac{z}{6} = 1\\) has intercepts at \\((3, 0, 0)\\), \\((0, 2, 0)\\), and \\((0, 0, 6)\\).</p> <p>Each form has its advantages. The Point-normal form is useful for finding distance from a point to a plane. The General form is standard for solving systems of equations. The Parametric form is convenient for generating points on the plane. Three-point form is natural when given three points. The Intercept form provides immediate geometric insight.</p>"},{"location":"math/linear_algebra/projections/","title":"Projections","text":"<p>For many applications in engineering, physics, and data-related problems in all scientific fields (genetics, economics, neuroscience, computer science, etc.) it is essential to go far beyond \\(\\mathbb{R}^3\\) and solve problems involving distance minimization to subspaces in \\(\\mathbb{R}^n\\) for any \\(n\\):</p> <p>If \\(V\\) is a linear subspace in \\(\\mathbb{R}^n\\) and \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is some point, then what point in \\(V\\) is closest to \\(\\mathbf{x}\\)? This closest point will be called the projection of \\(\\mathbf{x}\\) into \\(V\\).</p>"},{"location":"math/linear_algebra/projections/#the-closest-point-to-a-line","title":"The closest point to a line","text":"<p>The way we will solve the general problem of distance minimization from a point in \\(\\mathbb{R}^n\\) to a linear subspace \\(V\\) involves \"assembling\" a collection of solutions to distance minimization to certain \\(1\\)-dimensional linear subspaces of \\(V\\), or in other words, solving distance minimization problems to a collection of lines through \\(0\\) in \\(\\mathbb{R}^n\\).</p> <p>Since minimizing distance to lines will be the foundation for the general case, we begin by focusing on this special case. Consider a line \\(L\\) in \\(\\mathbb{R}^n\\) through \\(0\\), so \\(L = \\text{span}(\\mathbf{w}) = \\{c\\mathbf{w} : c \\in \\mathbb{R}\\}\\) where \\(\\mathbf{w} \\in \\mathbb{R}^n\\) is some nonzero vector. For any point \\(\\mathbf{x} \\in \\mathbb{R}^n\\), we want to show that there is a unique point in \\(L\\) closest to \\(\\mathbf{x}\\), and to actually give a formula for how to compute this nearest point to \\(\\mathbf{x}\\) in \\(L\\). </p> <p>Here is the fundamental idea: although our task (necessary for many applications!) takes place in \\(\\mathbb{R}^n\\) with completely general (and possibly huge) \\(n\\), we look at a low-dimensional instance of the problem in the hope that the low-dimensional case will suggest some feature that has a chance to adapt to the general situation. This balancing of insight from pictures in low-dimensional cases alongside algebraic work and geometric language developed for \\(\\mathbb{R}^n\\) with general \\(n\\) is an important part of linear algebra, giving visual insight into \\(\\mathbb{R}^n\\) for big \\(n\\). This doesn't justify results in \\(\\mathbb{R}^n\\) for general \\(n\\), but it inspires what we should expect and/or try to prove is true.</p> <p>Let's look at the case \\(n = 2\\) as shown in the figure below.</p> <p></p> <p>The key insight suggested by the figure above is that the point on the line \\(L\\) that is closest to \\(\\mathbf{x}\\) has another characterization (that in turn will allow us to compute it): it is the one point on \\(L\\) for which the displacement vector to \\(\\mathbf{x}\\) (the dotted line segment joining it to \\(\\mathbf{x}\\) as in the figure above) is perpendicular to \\(L\\), or equivalently is perpendicular to \\(\\mathbf{w}\\). Visually, the perpendicular direction to \\(L\\) from \\(\\mathbf{x}\\) is the \"most direct\" route. Or put another way, you may convince yourself by drawing some pictures that any deviation from perpendicularity entails a longer path from \\(\\mathbf{x}\\) to the line \\(L\\).</p> <p>To summarize, the figure above suggests a workable idea: the point \\(c\\mathbf{w} \\in L\\) for which \\(\\|\\mathbf{x} - c\\mathbf{w}\\|\\) is minimal should also have the property that \\(\\mathbf{x} - c\\mathbf{w}\\) is orthogonal to everything in \\(L\\). Although this idea is suggested by the picture in \\(\\mathbb{R}^2\\), as written it makes equally good sense in \\(\\mathbb{R}^n\\) for any \\(n\\) whatsoever. But is it true? And even once we know it is true, how can we exploit this property of the nearest point to \\(\\mathbf{x}\\) on \\(L\\) to actually compute this nearest point?</p> <p>The informal reasoning above may have already convinced you that the distance is minimized precisely when the displacement vector is orthogonal to \\(L\\).</p> <p>Method I (algebraic). In accordance with the idea inspired by the figure above in the case \\(n = 2\\), for general \\(n\\) we look for a scalar \\(c\\) for which \\(\\mathbf{x} - c\\mathbf{w}\\) is orthogonal to every vector in \\(L\\). The points of \\(L = \\text{span}(\\mathbf{w})\\) are those of the form \\(a\\mathbf{w}\\) for scalars \\(a\\), so we seek \\(c\\) making \\((\\mathbf{x} - c\\mathbf{w}) \\cdot (a\\mathbf{w}) = 0\\) for every scalar \\(a\\). The dot product has the property that \\((\\mathbf{x} - c\\mathbf{w}) \\cdot (a\\mathbf{w}) = a((\\mathbf{x} - c\\mathbf{w}) \\cdot \\mathbf{w})\\), so actually it suffices to make sure that \\((\\mathbf{x} - c\\mathbf{w}) \\cdot \\mathbf{w} = 0\\). We can use the further properties of the dot product to rewrite this as</p> \\[0 = (\\mathbf{x} - c\\mathbf{w}) \\cdot \\mathbf{w} = \\mathbf{x} \\cdot \\mathbf{w} - (c\\mathbf{w}) \\cdot \\mathbf{w}.\\] <p>We can rearrange this expression to write it as \\(c(\\mathbf{w} \\cdot \\mathbf{w}) = \\mathbf{x} \\cdot \\mathbf{w}\\). But \\(\\mathbf{w} \\cdot \\mathbf{w} = \\|\\mathbf{w}\\|^2 &gt; 0\\) (since \\(\\mathbf{w} \\neq 0\\)), so it makes sense to divide both sides by \\(\\mathbf{w} \\cdot \\mathbf{w}\\) to obtain that \\(c = \\frac{\\mathbf{x} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}}\\). This is the coefficient we had previously regarded as unknown! </p> <p>To summarize, we have shown through algebra and the properties of dot products that there is exactly one point in the line \\(L = \\text{span}(\\mathbf{w})\\) through \\(0\\) in \\(\\mathbb{R}^n\\) whose difference from \\(\\mathbf{x}\\) is orthogonal to everything in \\(L\\): it is \\(\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w}\\). We have not yet actually shown that this scalar multiple of \\(\\mathbf{w}\\) on \\(L\\) is closer to \\(\\mathbf{x}\\) than every other vector in \\(L\\), but if we believe the orthogonality insight inspired by the \\(2\\)-dimensional picture in the figure above then this must be that closest point. As a bonus, we have obtained an explicit formula for it!</p> <p>Method II (geometric). We next use some plane geometry via the figure above to obtain the same formula for the closest point. Strictly speaking, this argument only applies when \\(n = 2\\), but you might find that it gives the formula some visual meaning that is somehow lacking in the purely algebraic work in Method I.</p> <p>There is nothing to be done if \\(\\mathbf{x} \\cdot \\mathbf{w} = 0\\) (in that case \\(\\mathbf{x}\\) is perpendicular to \\(L\\) and we are thereby convinced that \\(0\\) is the closest point, as is also given by the desired formula). Hence, we can suppose \\(\\mathbf{x} \\neq 0\\) and the angle \\(\\theta\\) between \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\) satisfies either \\(0\u00b0 &lt; \\theta &lt; 90\u00b0\\) or \\(90\u00b0 &lt; \\theta &lt; 180\u00b0\\).</p> <p>The case of acute \\(\\theta\\) is shown in the figure above, whereas if \\(\\theta\\) is obtuse then the point we seek would be in the direction of \\(-\\mathbf{w}\\) (rather than in the direction of \\(\\mathbf{w}\\)). If \\(\\theta\\) is acute, as in the figure above, then by basic trigonometry, the leg along \\(L\\) for the right triangle as shown has length \\(\\|\\mathbf{x}\\| \\cos(\\theta)\\) and it points in the direction of the unit vector \\(\\mathbf{w}/\\|\\mathbf{w}\\|\\). This says that the endpoint on \\(L\\) of the dotted segment is the vector</p> \\[(\\|\\mathbf{x}\\| \\cos(\\theta)) \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}.\\] <p>But \\(\\cos(\\theta) = (\\mathbf{x} \\cdot \\mathbf{w})/(\\|\\mathbf{x}\\|\\|\\mathbf{w}\\|)\\), so plugging this into the equation above yields the desired formula since \\(\\|\\mathbf{w}\\|^2 = \\mathbf{w} \\cdot \\mathbf{w}\\). The case when \\(90\u00b0 &lt; \\theta &lt; 180\u00b0\\) goes very similarly, except now \\(\\cos(\\theta) &lt; 0\\) (so the endpoint on \\(L\\) of the dotted segment is in the direction of the opposite unit vector \\(-\\mathbf{w}/\\|\\mathbf{w}\\|\\)) and we have to work with the length \\(\\|\\mathbf{x}\\| |\\cos(\\theta)| = -\\|\\mathbf{x}\\| \\cos(\\theta)\\). Putting these together, the two signs cancel and we get the desired formula again.</p> <p>Proposition. Let \\(L = \\text{span}(\\mathbf{w}) = \\{c\\mathbf{w} : c \\in \\mathbb{R}\\}\\) be a \\(1\\)-dimensional linear subspace of \\(\\mathbb{R}^n\\) (so \\(\\mathbf{w} \\neq 0\\)), a \"line\". Choose any point \\(\\mathbf{x} \\in \\mathbb{R}^n\\). There is exactly one point in \\(L\\) closest to \\(\\mathbf{x}\\), and it is given by the scalar multiple</p> \\[\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w}\\] <p>of \\(\\mathbf{w}\\). This is called \"the projection of \\(\\mathbf{x}\\) into \\(\\text{span}(\\mathbf{w})\\)\"; we denote it by the symbol \\(\\text{Proj}_{\\mathbf{w}} \\mathbf{x}\\).</p> <p>Note: This can be seen as (dot product of \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\)), which is a scalar, times the unit vector in direction of \\(\\mathbf{w}\\).</p> <p>Example: Consider \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}\\). The numbers \\(1, 3, 4\\) represent the amount of \\(\\mathbf{v}\\) that points along the \\(x, y, z\\)-axes respectively. More precisely, \\(\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\mathbf{e}_1\\) is the component of \\(\\mathbf{v}\\) along the \\(x\\)-axis line, \\(\\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\end{bmatrix} = 3\\mathbf{e}_2\\) is the component of \\(\\mathbf{v}\\) along the \\(y\\)-axis line, and \\(\\begin{bmatrix} 0 \\\\ 0 \\\\ 4 \\end{bmatrix} = 4\\mathbf{e}_3\\) is the component of \\(\\mathbf{v}\\) along the \\(z\\)-axis line. These are the closest points to \\(\\mathbf{v}\\) on the \\(x\\)-, \\(y\\)- and \\(z\\)-axes, respectively.</p> <p>In terms of this data, we want to compute the projection of \\(\\mathbf{v}\\) on some line pointing with some other direction: if \\(\\mathbf{w}\\) is a (nonzero) vector along this new direction, we want to compute \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{v})\\). The key point is that the formula for \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{x})\\) behaves well for any linear combination of any \\(n\\)-vectors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_k\\): the projection of a linear combination of the \\(\\mathbf{x}_i\\)'s is equal to the corresponding linear combination of the projections.</p> <p>For example, with \\(k = 2\\) it says \\(\\text{Proj}_{\\mathbf{w}}(5\\mathbf{x}_1 - 7\\mathbf{x}_2) = 5 \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_1) - 7 \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_2)\\) and likewise with \\(5\\) and \\(-7\\) replaced by any two scalars. The reason this works is an algebraic calculation:</p> \\[\\text{Proj}_{\\mathbf{w}}(c_1\\mathbf{x}_1 + \\cdots + c_k\\mathbf{x}_k) = \\frac{(c_1\\mathbf{x}_1 + \\cdots + c_k\\mathbf{x}_k) \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = \\frac{c_1(\\mathbf{x}_1 \\cdot \\mathbf{w}) + \\cdots + c_k(\\mathbf{x}_k \\cdot \\mathbf{w})}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = c_1 \\frac{\\mathbf{x}_1 \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} + \\cdots + c_k \\frac{\\mathbf{x}_k \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = c_1 \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_1) + \\cdots + c_k \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_k).\\] <p>Applying this to the expression \\(\\mathbf{v} = \\mathbf{e}_1 + 3\\mathbf{e}_2 + 4\\mathbf{e}_3\\) yields \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{v}) = \\text{Proj}_{\\mathbf{w}}(\\mathbf{e}_1) + 3 \\text{Proj}_{\\mathbf{w}}(\\mathbf{e}_2) + 4 \\text{Proj}_{\\mathbf{w}}(\\mathbf{e}_3)\\).</p>"},{"location":"math/linear_algebra/projections/#projection-onto-a-general-subspace","title":"Projection onto a general subspace","text":"<p>Let's look at the case of a plane \\(V\\) through the origin in \\(\\mathbb{R}^3\\) equipped with a choice of orthogonal basis \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) of this plane. In the figure below, we draw the typical situation, indicating with the notation \\(\\text{Proj}_V(\\mathbf{x})\\) the point in \\(V\\) closest to \\(\\mathbf{x}\\). The first geometric insight, similar to our experience with lines, is that since this nearest point should have displacement vector to \\(\\mathbf{x}\\) that is the \"most direct\" route to \\(V\\) from \\(\\mathbf{x}\\), the displacement should involve \"no tilting\" relative to any direction within \\(V\\).</p> <p></p> <p>If you think about it, hopefully it seems plausible that if \\(\\mathbf{v} \\in V\\) makes the displacement \\(\\mathbf{x} - \\mathbf{v}\\) perpendicular to everything in \\(V\\) then \\(\\mathbf{v}\\) should be the point in \\(V\\) for which the direction of the displacement \\(\\mathbf{x} - \\mathbf{v}\\) is the \"most direct\" route from \\(\\mathbf{x}\\) to \\(V\\), making \\(\\mathbf{v}\\) the point in \\(V\\) nearest to \\(\\mathbf{x}\\).</p> <p>Theorem (Orthogonal Projection Theorem, version I). For any \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and linear subspace \\(V\\) of \\(\\mathbb{R}^n\\), there is a unique \\(\\mathbf{v}\\) in \\(V\\) closest to \\(\\mathbf{x}\\). In symbols, \\(\\|\\mathbf{x} - \\mathbf{v}\\| &lt; \\|\\mathbf{x} - \\mathbf{v}'\\|\\) for all \\(\\mathbf{v}'\\) in \\(V\\) with \\(\\mathbf{v}' \\neq \\mathbf{v}\\). This \\(\\mathbf{v}\\) is called the projection of \\(\\mathbf{x}\\) onto \\(V\\), and is denoted \\(\\text{Proj}_V(\\mathbf{x})\\); see the figure above. The projection \\(\\text{Proj}_V(\\mathbf{x})\\) is also the only vector \\(\\mathbf{v} \\in V\\) with the property that the displacement \\(\\mathbf{x} - \\mathbf{v}\\) is perpendicular to \\(V\\) (i.e., \\(\\mathbf{x} - \\mathbf{v}\\) is perpendicular to every vector in \\(V\\)).</p> <p>If \\(V\\) is nonzero then for any orthogonal basis \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) of \\(V\\) we have</p> \\[\\text{Proj}_V(\\mathbf{x}) = \\text{Proj}_{\\mathbf{v}_1}(\\mathbf{x}) + \\text{Proj}_{\\mathbf{v}_2}(\\mathbf{x}) + \\cdots + \\text{Proj}_{\\mathbf{v}_k}(\\mathbf{x}),\\] <p>where \\(\\text{Proj}_{\\mathbf{v}_i}(\\mathbf{x}) = \\frac{\\mathbf{x} \\cdot \\mathbf{v}_i}{\\mathbf{v}_i \\cdot \\mathbf{v}_i} \\mathbf{v}_i\\). For \\(\\mathbf{x} \\in V\\) we have \\(\\text{Proj}_V(\\mathbf{x}) = \\mathbf{x}\\) \u2013 the point in \\(V\\) closest to \\(\\mathbf{x}\\) is itself! \u2013 so the equation above for \\(\\mathbf{x} \\in V\\) recovers the Fourier formula!</p> <p>Theorem (Orthogonal Projection Theorem, version II). If \\(V\\) is a linear subspace of \\(\\mathbb{R}^n\\) then every vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) can be uniquely expressed as a sum</p> \\[\\mathbf{x} = \\mathbf{v} + \\mathbf{v}'\\] <p>with \\(\\mathbf{v} \\in V\\) and \\(\\mathbf{v}'\\) orthogonal to everything in \\(V\\). Explicitly, \\(\\mathbf{v} = \\text{Proj}_V(\\mathbf{x})\\) and \\(\\mathbf{v}' = \\mathbf{x} - \\text{Proj}_V(\\mathbf{x})\\).</p> <p>Since the \\(\\mathbf{v}_i\\)'s span \\(V\\), the point \\(\\mathbf{v} \\in V\\) closest to \\(\\mathbf{x}\\) can be written in the form \\(\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i\\) for some unknown coefficients \\(c_i\\). We are going to see that the perpendicularity of \\(\\mathbf{x} - \\mathbf{v}\\) to everything in \\(V\\) forces \\(c_i = (\\mathbf{x} \\cdot \\mathbf{v}_i)/(\\mathbf{v}_i \\cdot \\mathbf{v}_i)\\) for every \\(i\\). But then \\(c_i\\mathbf{v}_i\\) is exactly the formula for \\(\\text{Proj}_{\\mathbf{v}_i}(\\mathbf{x})\\), so we would obtain \\(\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i = \\sum_{i=1}^k \\text{Proj}_{\\mathbf{v}_i}(\\mathbf{x})\\) as asserted in the equation above.</p> <p>How can we show that the coefficients \\(c_i\\) are really given by the ratios \\((\\mathbf{x} \\cdot \\mathbf{v}_i)/(\\mathbf{v}_i \\cdot \\mathbf{v}_i)\\)? For this we have to do some algebra (rather than geometry): since \\(\\mathbf{x} - \\mathbf{v}\\) is perpendicular to everything in \\(V\\), it is in particular perpendicular to every \\(\\mathbf{v}_j\\), so</p> \\[0 = (\\mathbf{x} - \\mathbf{v}) \\cdot \\mathbf{v}_j = \\mathbf{x} \\cdot \\mathbf{v}_j - \\mathbf{v} \\cdot \\mathbf{v}_j\\] <p>for every \\(j\\). This says \\(\\mathbf{x} \\cdot \\mathbf{v}_j = \\mathbf{v} \\cdot \\mathbf{v}_j\\) for every \\(j\\). But \\(\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i\\) with some unknown \\(c_i\\)'s, so</p> \\[\\mathbf{v} \\cdot \\mathbf{v}_j = \\sum_{i=1}^k (c_i\\mathbf{v}_i) \\cdot \\mathbf{v}_j = \\sum_{i=1}^k c_i(\\mathbf{v}_i \\cdot \\mathbf{v}_j),\\] <p>and the terms for \\(i \\neq j\\) all vanish since \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) whenever \\(i \\neq j\\) (as \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) is an orthogonal basis of \\(V\\)!). In other words, \\(\\mathbf{v} \\cdot \\mathbf{v}_j = c_j(\\mathbf{v}_j \\cdot \\mathbf{v}_j)\\) for every \\(j\\). But we have seen that \\(\\mathbf{x} \\cdot \\mathbf{v}_j = \\mathbf{v} \\cdot \\mathbf{v}_j\\), so</p> \\[\\mathbf{x} \\cdot \\mathbf{v}_j = c_j(\\mathbf{v}_j \\cdot \\mathbf{v}_j)\\] <p>for every \\(j\\). We can divide by \\(\\mathbf{v}_j \\cdot \\mathbf{v}_j\\) since this is nonzero (it is equal to \\(\\|\\mathbf{v}_j\\|^2 &gt; 0\\), as \\(\\mathbf{v}_j \\neq 0\\)), so we thereby obtain the formula \\(c_j = (\\mathbf{x} \\cdot \\mathbf{v}_j)/(\\mathbf{v}_j \\cdot \\mathbf{v}_j)\\) for every \\(j\\), as desired.</p>"},{"location":"math/linear_algebra/span_subspaces_and_dimension/","title":"Span, subspaces, and dimension","text":""},{"location":"math/linear_algebra/span_subspaces_and_dimension/#span-and-linear-subspaces","title":"Span and linear subspaces","text":"<p>Consider a plane \\(P\\) in \\(\\mathbb{R}^3\\) passing through 0 = \\((0, 0, 0)\\). We want to express mathematically the idea that \\(P\\) is \"flat\" with \"two degrees of freedom\". Choose two other points in \\(P\\), denoted v and w, that do not lie on a common line through 0.</p> <p></p> <p>We can get to any point on \\(P\\) by starting at 0 and walking first some specific distance in the v-direction, then some specific distance in the w-direction. Symbolically,</p> \\[P = \\{\\text{all vectors of the form } a\\mathbf{v} + b\\mathbf{w}, \\text{ for scalars } a, b\\}\\] <p>\\(a &lt; 0\\) and \\(b &lt; 0\\) correspond to walking \"backwards\" relative to the directions of v and w respectively. The description as vectors \\(a\\mathbf{v} + b\\mathbf{w}\\) for varying scalars \\(a\\) and \\(b\\) is a way of encoding the flatness of \\(P\\) with two degrees of freedom.</p> <p>In other words, any vector that we can obtain from v and w repeatedly using the vector operations (addition and scalar multiplication) in any order is actually of the form \\(a\\mathbf{v} + b\\mathbf{w}\\) for some scalars \\(a\\), \\(b\\).</p> <p>Thus, the right side of the symbolic equation gives a parametric form of the plane through the 3 points 0, v, w and describes all vectors created from v, w using vector operations. If we instead allow the nonzero 3-vectors v, w to lie on a common line through 0, which is to say w is a scalar multiple of v, then the right side describes a line through 0 rather than a plane (as shown in the figure below).</p> <p></p> <p>The span of vectors v\\(_1\\), . . . , v\\(_k\\) in \\(\\mathbb{R}^n\\) is the collection of all vectors in \\(\\mathbb{R}^n\\) that one can obtain from v\\(_1\\), . . . , v\\(_k\\) by repeatedly using addition and scalar multiplication. In symbols,</p> \\[\\text{span}(v_1, \\ldots, v_k) = \\{\\text{all } n\\text{-vectors } \\mathbf{x} \\text{ of the form } c_1\\mathbf{v}_1 + \\cdots + c_k\\mathbf{v}_k\\}\\] <p>where \\(c_1\\), . . . , \\(c_k\\) are arbitrary scalars.</p> <p>In \\(\\mathbb{R}^3\\), for \\(k = 2\\) and nonzero v\\(_1\\), v\\(_2\\) not multiples of each other, this recovers the parametric form of a plane through \\(P = 0\\). In general, the span of a collection of finitely many \\(n\\)-vectors is the collection of all the \\(n\\)-vectors one can reach from those given \\(n\\)-vectors by forming linear combinations in every possible way.</p> <p>This is a very new kind of concept- considering such a collection of \\(n\\)-vectors all at the same time. But it is ultimately no different than how we may visualize a plane in our head yet it consists of a lot of different points. The span of two nonzero \\(n\\)-vectors that are not scalar multiples of each other should be visualized as a \"plane\" through 0 in \\(\\mathbb{R}^n\\).</p> <p>Example: Let's show that the set \\(U\\) of 4-vectors \\(\\begin{bmatrix} x \\\\ y \\\\ z \\\\ w \\end{bmatrix}\\) that are perpendicular to v = \\(\\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\\\ 7 \\end{bmatrix}\\) is a span of three 4-vectors (the same type of calculation as what we are about to do shows that the set of vectors in \\(\\mathbb{R}^n\\) perpendicular to any fixed nonzero vector in \\(\\mathbb{R}^n\\) is a span of \\(n-1\\) nonzero \\(n\\)-vectors). This is a \"higher-dimensional\" analogue of our visual experience in \\(\\mathbb{R}^3\\) that the collection of vectors in \\(\\mathbb{R}^3\\) perpendicular to a given nonzero vector is a plane through the origin (and hence is the span of two nonzero vectors in \\(\\mathbb{R}^3\\)).</p> <p>As a first step, we write out the condition of perpendicularity using the dot product: \\(2x + 3y + z + 7w = 0\\)</p> <p>Now we solve for \\(w\\) (say) to get \\(w = -(2/7)x - (3/7)y - z/7\\). Thus, points of \\(U\\) are precisely</p> \\[\\begin{bmatrix} x \\\\ y \\\\ z \\\\ w \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ z \\\\ -(2/7)x - (3/7)y - z/7 \\end{bmatrix} = x\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ -2/7 \\end{bmatrix}_{\\mathbf{a}} + y\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -3/7 \\end{bmatrix}_{\\mathbf{b}} + z\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -1/7 \\end{bmatrix}_{\\mathbf{c}}\\] <p>with arbitrary scalars \\(x\\), \\(y\\), \\(z\\). This shows that the vectors perpendicular to v are precisely those in the span of the vectors a, b, c that appear on the right side. This shows that \\(U\\) is the span of a, b, c in \\(\\mathbb{R}^4\\).</p> <p>We have noted that a line in \\(\\mathbb{R}^2\\) or \\(\\mathbb{R}^3\\) passing through 0 and a plane in \\(\\mathbb{R}^3\\) passing through 0 each arise as a span of one or two vectors. But lines and planes not passing through 0 are not a span of any collection of vectors! The reason is that the span of any collection of \\(n\\)-vectors always contains 0, by setting all coefficients \\(c_1\\), . . . , \\(c_k\\) in the span definition to be 0, since \\(0\\mathbf{v}_1 + 0\\mathbf{v}_2 + \\cdots + 0\\mathbf{v}_k = 0\\).</p> <p>If \\(V\\) is the span of some finite collection of vectors in \\(\\mathbb{R}^n\\) then there are generally many different collections of vectors v\\(_1\\), . . . , v\\(_k\\) in \\(\\mathbb{R}^n\\) with span equal to \\(V\\). For example, given one parametric form of a plane in \\(\\mathbb{R}^3\\) through 0 we get lots of others by rotating the resulting parallelogram grid around the origin in that plane by any nonzero angle we like. To refer to a span while suppressing the mention of any specific choice of vectors v\\(_1\\), . . . , v\\(_k\\) that create the span, some new terminology is convenient.</p> <p>A linear subspace of \\(\\mathbb{R}^n\\) is a subset of \\(\\mathbb{R}^n\\) that is the span of a finite collection of vectors in \\(\\mathbb{R}^n\\) (this is also referred to as a subspace, dropping the word \"linear\"). If \\(V\\) is a linear subspace of \\(\\mathbb{R}^n\\), a spanning set for \\(V\\) is a collection of \\(n\\)-vectors v\\(_1\\), . . . , v\\(_k\\) whose span equals \\(V\\) (a linear subspace has lots of spanning sets, akin to tiling a floor by parallelogram tiles in many ways).</p> <p>Planes and lines in \\(\\mathbb{R}^3\\) passing through 0 are the visual examples to keep in mind when you hear the phrase \"linear subspace\". You may wonder: what is the difference between a linear subspace and a span? There is no difference, but saying \"span\" emphasizes the input \u2013 a specific finite list v\\(_1\\), . . . , v\\(_k\\) and the dynamic process of forming their linear combinations \u2013 whereas saying \"linear subspace\" emphasizes the output collection \\(V\\) of \\(n\\)-vectors without choosing a specific v\\(_1\\), . . . , v\\(_k\\) whose span is \\(V\\). It is far more important to know that \\(V\\) can be obtained as a span of some list v\\(_1\\), . . . , v\\(_k\\) rather than to pick a specific such list.</p> <p>Proposition. If \\(V\\) is a linear subspace in \\(\\mathbb{R}^n\\) then for any vectors \\(\\mathbf{x}_1\\), . . . , \\(\\mathbf{x}_m \\in V\\) and scalars \\(a_1\\), . . . , \\(a_m\\) the linear combination \\(a_1\\mathbf{x}_1 + \\cdots + a_m\\mathbf{x}_m\\) also lies in \\(V\\). In words: all linear combinations of \\(n\\)-vectors chosen from a linear subspace of \\(\\mathbb{R}^n\\) belong to that same subspace.</p> <p>Some illustrations below of which are linear subspaces and which are not. Only the blue and green planes are linear subspaces by definition as well as the proposition above.</p> <p> </p>"},{"location":"math/linear_algebra/span_subspaces_and_dimension/#dimension","title":"Dimension","text":"<p>We would like to define the \"dimension\" of a linear subspace in a way that generalizes our familiar concept of dimension: a thread or a line is 1-dimensional, a sheet of paper is 2-dimensional, the world around us is 3-dimensional.</p> <p>In particular, we may informally say: the \"dimension\" of an object \\(X\\) tells us how many different numbers are needed to locate a point in \\(X\\).</p> <p>To turn this into something unambiguous and useful, we now focus on the case of a linear subspace \\(V\\) of \\(\\mathbb{R}^n\\), where vector algebra will provide a way to make that informal idea precise. The \"dimension\" of \\(V\\) will be, intuitively, the number of independent directions in \\(V\\). In other words, it will tell us how many numbers we need in order to specify a vector v in \\(V\\).</p> <p>More precisely, recall that by the definition of \"linear subspace\", \\(V\\) is the span of some finite collection of vectors v\\(_1\\), . . . , v\\(_k \\in \\mathbb{R}^n\\). That is, for any v \\(\\in V\\) we can write</p> \\[\\mathbf{v} = c_1\\mathbf{v}_1 + \\cdots + c_k\\mathbf{v}_k\\] <p>for some scalars \\(c_1\\), . . . , \\(c_k\\), so to determine v it is enough to tell us \\(k\\) numbers\u2013 the scalars \\(c_1\\), . . . , \\(c_k\\). But \\(V\\) can have another spanning set consisting of a different number of vectors.</p> <p>The span \\(V\\) of two nonzero vectors in \\(\\mathbb{R}^3\\) could be a line (such as if the two vectors point in the same or opposite directions), in which case \\(V\\) is also spanned by just one of those two vectors (e.g., the second vector is redundant).</p> <p>Similarly, the span of three nonzero vectors in \\(\\mathbb{R}^3\\) could be a plane in special circumstances (or even a line in especially degenerate circumstances).</p> <p>In both such cases, the initial spanning set has some redundancy. To define \"dimension\" for \\(V\\), we want to use ways of spanning \\(V\\) that (in a sense we need to make precise) don't have redundancy.</p> <p>Let \\(V\\) be a nonzero linear subspace of some \\(\\mathbb{R}^n\\). The dimension of \\(V\\), denoted as \\(\\dim(V)\\), is defined to be the smallest number of vectors needed to span \\(V\\). We define \\(\\dim(\\{0\\}) = 0\\).</p> <p>For \\(k \\geq 2\\), consider a collection v\\(_1\\), . . . , v\\(_k\\) of vectors spanning a linear subspace \\(V\\) in \\(\\mathbb{R}^n\\). We have \\(\\dim(V) = k\\) precisely when \"there is no redundancy\": each v\\(_i\\) is not a linear combination of the others, or in other words removing it from the list destroys the spanning property.</p> <p>Equivalently, \\(\\dim(V) &lt; k\\) precisely when \"there is redundancy\": some v\\(_i\\) is a linear combination of the others, or in other words removing some v\\(_i\\) from the list does not affect the span. If some v\\(_i\\) vanishes then it is a linear combination of the others and hence can be dropped from the span, so \\(\\dim(V) &lt; k\\).</p> <p>If \\(V\\) and \\(W\\) are linear subspaces of \\(\\mathbb{R}^n\\) with \\(W\\) contained in \\(V\\) (i.e., every vector in \\(W\\) also belongs to \\(V\\), much like a line inside a plane in \\(\\mathbb{R}^3\\)) then \\(\\dim(W) \\leq \\dim(V)\\), and equality holds precisely when \\(W = V\\).</p>"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/","title":"Vector geometry in \\(\\mathbb{R}^n\\) and correlation coefficients","text":""},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#angles","title":"Angles","text":"<p>The angle \\(0\u00b0 \\leq \\theta \\leq 180\u00b0\\) between nonzero 2-vectors a = \\((a_1, a_2)\\) and b = \\((b_1, b_2)\\) satisfies</p> \\[\\cos \\theta = \\frac{a_1b_1 + a_2b_2}{\\|a\\|\\|b\\|}\\] <p>The angle \\(0\u00b0 \\leq \\theta \\leq 180\u00b0\\) between two nonzero 3-vectors a = \\((a_1, a_2, a_3)\\) and b = \\((b_1, b_2, b_3)\\) satisfies</p> \\[\\cos \\theta = \\frac{a_1b_1 + a_2b_2 + a_3b_3}{\\|a\\|\\|b\\|}\\] <p>The preceding in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) motivates how to define appropriate concepts with \\(n\\)-vectors for any \\(n\\).</p> <p>Consider \\(n\\)-vectors x = \\(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\) and y = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\).</p> <p>(i) The dot product of x and y is defined to be the scalar</p> \\[x \\cdot y = x_1y_1 + x_2y_2 + \\cdots + x_ny_n = \\sum_{i=1}^n x_iy_i\\] <p>The dot product is only defined if the two vectors are \\(n\\)-vectors for the same value of \\(n\\).</p> <p>(ii) The angle \\(\\theta\\) between two nonzero \\(n\\)-vectors x, y is defined by the formula</p> \\[\\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\|\\|y\\|}\\] <p>with \\(0\u00b0 \\leq \\theta \\leq 180\u00b0\\). For emphasis: x and y must be nonzero \\(n\\)-vectors for a common \\(n\\).</p> <p>(iii) When \\(x \\cdot y = 0\\) (same as \\(\\theta = 90\u00b0\\) if x, y \u2260 0), we say x and y are perpendicular; the word orthogonal is often used for this (\"orthog\u014dnios\" is ancient Greek for \"right-angled\"), though only rarely at the U.S. Supreme Court.</p> <p>Always remember that the dot product of vectors is a scalar (it is not a vector).</p> <p>The notion of angle is a definition in \\(\\mathbb{R}^n\\) for general \\(n\\): it is motivated by the case when \\(n = 3\\), but for general \\(n\\) there is nothing to \"physically justify\". The real content making this definition for general \\(n\\) is that (as you will learn with experience) this notion of angle behaves like our visual experience in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) and so provides useful visual guidance with \\(n\\)-vectors for any \\(n\\).</p> <p>Whenever we speak of an angle between two lines through the origin, there is always an ambiguity (when they're not perpendicular) of whether we want the acute angle between them or the (supplementary) obtuse angle between them. This corresponds to the fact that when we set it up as a vector problem, we have to choose a direction along each line (coming out of the intersection point). Depending on the choice, we will get the acute or the obtuse angle.</p>"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#properties-of-dot-products","title":"Properties of dot products","text":"<p>For any \\(n\\)-vectors v, w, w\\(_1\\), and w\\(_2\\), the following hold:</p> <p>(i) \\(v \\cdot w = w \\cdot v\\),</p> <p>(ii) \\(v \\cdot v = \\|v\\|^2\\),</p> <p>(iii) \\(v \\cdot (cw) = c(v \\cdot w)\\) for any scalar \\(c\\), and \\(v \\cdot (w_1 + w_2) = v \\cdot w_1 + v \\cdot w_2\\).</p> <p>(iii\u2032) Combining both rules in (iii), for any scalars \\(c_1\\), \\(c_2\\) we have</p> \\[v \\cdot (c_1w_1 + c_2w_2) = c_1(v \\cdot w_1) + c_2(v \\cdot w_2)\\]"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#pythagorean-theorem-in-mathbbrn-and-the-cauchyschwarz-inequality","title":"Pythagorean Theorem in \\(\\mathbb{R}^n\\) and the Cauchy\u2013Schwarz Inequality","text":"<p>As an application of the dot product rules, we can establish a version of the Pythagorean Theorem for \\(n\\)-vectors with any \\(n\\) (not just \\(n = 2\\)) and we can show that the subtlety lurking in the definition of \"angle\" between \\(n\\)-vectors is really not a problem at all.</p> <p>Theorem (Pythagoras). If \\(n\\)-vectors v\\(_1\\) and v\\(_2\\) are nonzero and perpendicular (i.e., at an angle of \\(90\u00b0\\)) then</p> \\[\\|v_1 + v_2\\|^2 = \\|v_1\\|^2 + \\|v_2\\|^2 \\] <p>Proof. Expand the left side as a dot product:</p> \\[(v_1 + v_2) \\cdot (v_1 + v_2) = v_1 \\cdot (v_1 + v_2) + v_2 \\cdot (v_1 + v_2) = v_1 \\cdot v_1 + v_1 \\cdot v_2 + v_2 \\cdot v_1 + v_2 \\cdot v_2\\] <p>We've used the rules for dot products at each step to expand. But the common value \\(v_1 \\cdot v_2\\) and \\(v_2 \\cdot v_1\\) is 0 because v\\(_1\\) and v\\(_2\\) are assumed to be perpendicular (which means by definition that their dot product equals 0). Thus, the right side equals \\(v_1 \\cdot v_1 + v_2 \\cdot v_2 = \\|v_1\\|^2 + \\|v_2\\|^2\\), as we wanted. \\(\\square\\)</p> <p>The motivation for our definitions of perpendicularity and more generally angle between vectors in \\(\\mathbb{R}^n\\) and length of vectors in \\(\\mathbb{R}^n\\) for general \\(n\\) (especially \\(n &gt; 3\\)) came from our knowledge of how things work in \\(\\mathbb{R}^2\\) based on knowing the Pythagorean Theorem in plane geometry. Making up definitions of words cannot ever replace the work involved in proving a real theorem.</p> <p>Since we now have several good properties of dot products in hand, we can establish a fact that is needed to confirm that our definition of \"angle\" between nonzero \\(n\\)-vectors makes sense for any \\(n\\):</p> <p>Theorem (Cauchy\u2013Schwarz Inequality). For \\(n\\)-vectors v, w, we have</p> \\[-\\|v\\| \\|w\\| \\leq v \\cdot w \\leq \\|v\\| \\|w\\|\\] <p>(or equivalently the absolute value \\(|v \\cdot w|\\) is at most \\(\\|v\\| \\|w\\|\\)). Moreover, one of the inequalities is an equality precisely when one of v or w is a scalar multiple of the other.</p> <p>Proof. If v = 0 or w = 0 then everything is clear (note that 0 is a scalar multiple of any \\(n\\)-vector: multiply it by the scalar 0), so now we assume v, w \u2260 0. The idea of the proof is to explore how the length of v + \\(x\\)w depends on \\(x\\). This is most conveniently done by analyzing the squared-length, which is a dot product:</p> \\[\\|v + xw\\|^2 = (v + xw) \\cdot (v + xw)\\] <p>Using the linearity properties of dot products, we have</p> \\[(v + xw) \\cdot (v + xw) = (v + xw) \\cdot v + x((v + xw) \\cdot w) = v \\cdot v + (xw) \\cdot v + x((v \\cdot w) + x(w \\cdot w)) = v \\cdot v + x(w \\cdot v) + x(v \\cdot w) + x^2(w \\cdot w)\\] <p>But \\(w \\cdot v = v \\cdot w\\), so combining the middle two terms yields:</p> \\[\\|v + xw\\|^2 = \\|v\\|^2 + 2(v \\cdot w)x + \\|w\\|^2x^2\\] <p>The squared length of a vector is always \u2265 0, and it equals 0 precisely when the vector equals 0. But the vector v + \\(x\\)w equals 0 for some value \\(x = c\\) precisely when v = \\(-c\\)w, which is to say v is a scalar multiple of w, and that is the same as w being a scalar multiple of v (since the scalar multiplier can be brought to the other side as its reciprocal as long as the scalar cannot be 0, and indeed such a scalar cannot be 0 since we have arranged that v, w \u2260 0). So we just need to analyze what it means that the quadratic polynomial in \\(x\\) given by</p> \\[q(x) = \\|w\\|^2x^2 + 2(v \\cdot w)x + \\|v\\|^2\\] <p>is always non-negative, and determine when this polynomial does actually attain the value 0 for some value of \\(x\\).</p> <p>Let's review when a quadratic polynomial \\(ax^2 + bx + c\\) with positive leading coefficient (such as \\(a = \\|w\\|^2\\) for \\(q(x)\\) above) is \u2265 0 everywhere. This happens precisely when its concave-up parabolic graph lies entirely on one side of the \\(x\\)-axis (possibly touching the \\(x\\)-axis at one point), which is exactly the situation that the graph does not cross the \\(x\\)-axis at two different points. This is exactly the situation when the output of the quadratic formula does not yield two different real numbers. The opposite case of having two different real roots occurs exactly when the \"\\(b^2 - 4ac\\)\" part of the quadratic formula inside the square-root is &gt; 0, so in our situation we must have the exactly opposite situation: \\(b^2 - 4ac \\leq 0\\), with equality happening precisely when there is a real root.</p> <p>Applying the preceding review with \\(a = \\|w\\|^2\\), \\(b = 2(v \\cdot w)\\), \\(c = \\|v\\|^2\\) for \\(q(x)\\), we get</p> \\[(2(v \\cdot w))^2 - 4\\|w\\|^2\\|v\\|^2 \\leq 0\\] <p>with equality happening exactly when v and w are scalar multiples of each other. Bringing the second term on the left over to the other side, we conclude that</p> \\[(2(v \\cdot w))^2 \\leq 4\\|w\\|^2\\|v\\|^2\\] <p>with equality precisely when v and w are scalar multiples of each other. Dividing each side by 4, this is the same as the inequality</p> \\[|v \\cdot w|^2 \\leq (\\|w\\| \\|v\\|)^2\\] <p>so taking square roots of both sides gives what we want. \\(\\square\\)</p>"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#the-correlation-coefficient","title":"The correlation coefficient","text":"<p>Given data points \\((x_1, y_1)\\), . . . , \\((x_n, y_n)\\), it is often useful to seek a line which gives a \"best fit\" to this collection of points.</p> <p>The problem of finding a \"best fit\" line to some data is called linear regression. But at a more basic level we may seek a measure of the extent to which it is reasonable to try to find a line that could be regarded as a good fit to the data (setting aside what that specific line may be). There is a widely used measure of whether one should seek such a line: this measure is called the correlation coefficient of the data points.</p> <p>Consider \\(n\\) data points \\((x_1, y_1)\\), \\((x_2, y_2)\\), . . . , \\((x_n, y_n)\\) in \\(\\mathbb{R}^2\\). Assume they don't all lie on a common vertical line nor on a common horizontal line (i.e., the \\(x_i\\)'s are not all equal to each other, and the \\(y_i\\)'s are not all equal to each other, so in particular X, Y \u2260 0).</p> <p>In the above setup, assume furthermore that the averages \\(\\bar{x} = \\frac{1}{n}\\sum x_i\\) and \\(\\bar{y} = \\frac{1}{n}\\sum y_i\\) of the \\(x\\)-coordinates and of the \\(y\\)-coordinates both equal 0. The correlation coefficient \\(r\\) between the \\(x_i\\)'s and \\(y_i\\)'s is defined to be the cosine of the angle between X and Y, or equivalently between the unit vectors \\(\\frac{X}{\\|X\\|}\\) and \\(\\frac{Y}{\\|Y\\|}\\):</p> \\[r = \\text{cosine of the angle between X and Y} = \\frac{X \\cdot Y}{\\|X\\|\\|Y\\|} = \\frac{X}{\\|X\\|} \\cdot \\frac{Y}{\\|Y\\|}\\] <p></p> <p>Intuition: This definition makes perfect geometric sense. When the data points \\((x_i, y_i)\\) are centered (so their averages are 0), we can think of X and Y as vectors in \\(\\mathbb{R}^n\\) representing the \\(x\\)-coordinates and \\(y\\)-coordinates respectively. </p> <p>The correlation coefficient \\(r\\) measures how well the data points align along a line through the origin. When \\(r = 1\\), the vectors X and Y point in the same direction, meaning the data points lie perfectly on a line with positive slope. When \\(r = -1\\), the vectors point in opposite directions, meaning the data points lie perfectly on a line with negative slope. When \\(r = 0\\), the vectors are perpendicular, meaning there's no linear relationship between the variables.</p> <p>You may be bothered by the assumption that the averages \\(\\bar{x}\\) and \\(\\bar{y}\\) of the coordinates of the data both equal 0, since in practice it is rarely satisfied. What is done in real-world problems is that the data is recentered: we replace \\(x_i\\) with \\(\\hat{x}_i = x_i - \\bar{x}\\) and replacing \\(y_i\\) with \\(\\hat{y}_i = y_i - \\bar{y}\\). Such subtraction of the averages makes \"center of mass\" move to \\((0, 0)\\) (i.e., \\(\\hat{\\bar{x}}, \\hat{\\bar{y}} = 0\\)).</p> <p>Often people work with \\(r^2\\), which is always non-negative. This is</p> \\[r^2 = \\frac{(X \\cdot Y)^2}{\\|X\\|^2\\|Y\\|^2}\\] <p>it is near 0 when there is little correlation, and near 1 when there's a strong linear relationship (without specifying the sign of the slope: \\(r\\) may be near 1 or near \\(-1\\)).</p> <p>Don't confuse the value of \\(r\\) with the slope of a \"best-fit line\"! The nearness of \\(r^2\\) to 1 (or of \\(r\\) to \\(\\pm 1\\)) is a measure of quality of fit. The actual slope of the best-fit line (which could be any real number at all) has nothing whatsoever to do with the value of \\(r\\) (which is always between \\(-1\\) and 1).</p> <p>Correlation coefficients go hand in hand with linear regression (finding a \"best fit\" line for data) and help one to understand how meaningful the results of a linear regression are.</p> <p>Note: Let's see why the correlation coefficient equals 1 precisely when the points \\((x_i, y_i)\\) all lie exactly on a line \\(y = mx\\) whose slope \\(m\\) is positive. We assume as always that the data doesn't all lie on a common vertical line or a common horizontal line, and that the averages \\(\\bar{x}\\) and \\(\\bar{y}\\) equal 0. By then replacing \\(y_i\\) with \\(-y_i\\) everywhere, it would follow that the correlation coefficient equals \\(-1\\) precisely when the points \\((x_i, y_i)\\) all lie exactly on a line \\(y = mx\\) whose slope \\(m\\) is negative.</p> <p>Note that X, Y \u2260 0 since we assumed the data points aren't on a common horizontal line and aren't on a common vertical line. We want to show that the correlation coefficient is 1 precisely when Y = \\(m\\)X for some \\(m &gt; 0\\).</p> <p>But the correlation coefficient is the cosine of the angle between the nonzero vectors X and Y, so the correlation coefficient is equal to 1 precisely when the angle between X and Y is \\(0\u00b0\\). The angle \\(\\theta\\) between the (nonzero) vectors X and Y is \\(0\u00b0\\) precisely when Y = \\(m\\)X for some \\(m &gt; 0\\).</p>"},{"location":"math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/","title":"Vectors, vector addition, and scalar multiplication","text":"<p>Differential equations provided the mathematical framework for many of the advances of the 20th century, but linear algebra (the algebra and geometry of vectors and matrices in arbitrary dimensions) is the mathematical tool par excellence (alongside statistics) for the systematic analysis and management of the data-driven tasks of the 21st century. Even for modern applications of differential equations, linear algebra far beyond 3 dimensions is an important tool.</p> <p>Also, a good understanding of multivariable differential calculus requires first learning some ideas and computational techniques in linear algebra.</p>"},{"location":"math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/#vectors-and-their-linear-combinations","title":"Vectors and their linear combinations","text":"<p>In the physical sciences, vectors are used to represent quantities that have both a magnitude and a direction. Examples of such quantities include displacement, velocity, force, and angular momentum.</p> <p>In data science, economics, and many industrial applications of mathematics, vectors are used to keep track of collections of numerical data. This type of example is much more varied than the examples arising from natural sciences, and nearly always \\(n\\) is very large.</p> <p>The sum v + w of two vectors is defined precisely when v and w are \\(n\\)-vectors for the same \\(n\\). In that case, we define their sum by the rule</p> \\[\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} + \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} = \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\\\ \\vdots \\\\ v_n + w_n \\end{bmatrix}\\] <p>We multiply a scalar \\(c\\) against an \\(n\\)-vector v = \\(\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\) by the rule </p> \\[c\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} cv_1 \\\\ cv_2 \\\\ \\vdots \\\\ cv_n \\end{bmatrix}\\] <p></p> <p>As in the above for \\(n = 3\\), the vector u + v is represented by the diagonal arrow in the parallelogram with one vertex at the origin and two edges given by u and v. This description of vector addition for \\(n = 2\\) and \\(n = 3\\) is called the parallelogram law.</p> <p>We define subtraction as we did addition, or equivalently:</p> \\[v - w = v + (-1)w\\] <p>A linear combination of two \\(n\\)-vectors v,w is an \\(n\\)-vector \\(av + bw\\) for scalars \\(a, b\\). More generally, a linear combination of \\(k\\) such \\(n\\)-vectors v\\(_1\\), v\\(_2\\), . . . , v\\(_k\\) is \\(a_1\\)v\\(_1\\) + \\(a_2\\)v\\(_2\\) + \u00b7 \u00b7 \u00b7 + \\(a_k\\)v\\(_k\\) for scalars \\(a_1\\), \\(a_2\\), . . . , \\(a_k\\). In physical sciences, this is often called a \"superposition\" of v\\(_1\\), . . . , v\\(_k\\).</p> <p>Example: Suppose that T\\(_{2001}\\), T\\(_{2002}\\), . . . , T\\(_{2016}\\) are 365-vectors that describe the daily average temperatures in Palo Alto (say in Celsius) in years 2001, 2002, . . . , 2016 (let's ignore February 29 in leap years). Then \\(\\frac{1}{16}\\)(T\\(_{2001}\\) + \u00b7 \u00b7 \u00b7 + T\\(_{2016}\\)) is a 365-vector that tells us, for each given day, the average temperature in the years 2001\u20132016. For example, the first entry of this vector is the average January 1 temperature during this period.</p> <p>A special type of linear combination that arises in applications such as linear programming, weighted averages, and probability theory is convex combination: in the case of two \\(n\\)-vectors v and w, this means a linear combination of the form \\((1 - t)\\)v + \\(tw\\) = v + \\(t\\)(w - v) with \\(0 \\leq t \\leq 1\\). This adds to v a portion (given by \\(t\\)) of the displacement from v to w. It has the geometric interpretation (for \\(n = 2, 3\\)) of being a point on the line segment between the tips of v and w; e.g., it is v when \\(t = 0\\), it is the midpoint when \\(t = \\frac{1}{2}\\), and it is w when \\(t = 1\\).</p> <p></p> <p>For any \\(n\\)-vectors v\\(_1\\), . . . , v\\(_k\\), a convex combination of them means a linear combination \\(t_1\\)v\\(_1\\) + \u00b7 \u00b7 \u00b7 + \\(t_k\\)v\\(_k\\) for which all \\(t_j \\geq 0\\) and the sum of the coefficients is equal to 1; that is, \\(t_1\\) + \u00b7 \u00b7 \u00b7 + \\(t_k\\) = 1. When the coefficients are all equal, which is to say every \\(t_j\\) is equal to \\(\\frac{1}{k}\\), this is the average (sometimes called the centroid) of the vectors.</p> <p>In linear algebra, the phrase \"point in \\(\\mathbb{R}^n\\)\" means exactly the same thing as \"\\(n\\)-vector\" (as well as \"vector\", when we don't need to specify \\(n\\)). The mental image for a given situation may suggest a preference between the words \"point\" and \"vector\", such as \"displacement vector\" or \"closest point\", but there is absolutely no difference in the meanings of these words in linear algebra. You might imagine that a \"point\" is the tip of an arrow emanating from 0, or that a \"vector\" is a directed line segment with specified endpoints.</p> <p>In physics and engineering, a special \"cross product\" of 3-vectors (the output of which is also a 3-vector) shows up a lot. This has no analogue in \\(\\mathbb{R}^n\\) for \\(n \\neq 3\\), and it behaves very differently from products of numbers; e.g., it is neither commutative nor associative!</p>"},{"location":"math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/#length-for-vectors-and-distance-in-mathbbrn","title":"Length for vectors and distance in \\(\\mathbb{R}^n\\)","text":"<p>The length or magnitude of an \\(n\\)-vector v = \\(\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\), denoted \\(\\|v\\|\\), is the number</p> \\[\\|v\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\geq 0\\] <p>Note that the length is a scalar, and \\(\\|-v\\| = \\|v\\|\\) (in accordance with the visualization of \\(-v\\) as \"a copy of v pointing in the opposite direction\") because signs disappear when squaring each \\(-v_j\\).</p> <p>If \\(c\\) is any scalar then \\(\\|cv\\| = |c|\\|v\\|\\) (i.e., if we multiply a vector by \\(c\\) then the length scales by the factor \\(|c|\\)). For example, \\((-5)\\)v has length \\(5\\|v\\|\\).</p> <p>In other references, you may see \\(\\|v\\|\\) called the \"norm\" of v.</p> <p>The distance between two \\(n\\)-vectors x, y is defined to be \\(\\|x - y\\|\\).</p> <p>In general it also equals \\(\\|y - x\\|\\) since y - x = \\(-(x - y)\\) and any vector has the same length as its negative, so the order of subtraction doesn't matter.</p> <p>There is no \"physical justification\" to be given when \\(n &gt; 3\\). What is important is that (i) for \\(n = 2, 3\\) we convince ourselves that it is the usual notion of distance, and (ii) for general \\(n\\) it satisfies reasonable properties for a notion of \"distance\" to provide helpful geometric insight.</p> <p>The zero vector in \\(\\mathbb{R}^n\\) is 0 = \\(\\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\), and a unit vector is a vector with length 1.</p> <p>Always \\(\\|v\\| \\geq 0\\), and \\(\\|v\\| = 0\\) precisely when v = 0. If v \u2260 0 then dividing v by its length (i.e., multiplying by the scalar \\(\\frac{1}{\\|v\\|} &gt; 0\\)) yields a unit vector \"pointing in the same direction\" as v.</p> <p>In general the length of \\(cv\\) for \\(c &gt; 0\\) is \\(c\\|v\\|\\), so in order that \\(cv\\) be a unit vector the condition is precisely that \\(c\\|v\\| = 1\\), which is to say \\(c = \\frac{1}{\\|v\\|}\\). In other words, the scalar multiple \\(\\frac{1}{\\|v\\|}\\)v of v is indeed the unique unit vector pointing in the same direction as v (in the opposite direction we have the unit vector \\(-\\frac{1}{\\|v\\|}\\)v).</p>"},{"location":"math/multivariate_calculus/multivariable_functions_level_sets_and_contour_plots/","title":"Multivariable functions, level sets, and contour plots","text":""},{"location":"math/multivariate_calculus/multivariable_functions_level_sets_and_contour_plots/#basic-terminology","title":"Basic terminology","text":"<p>Functions of more than one variable are called multivariable functions.</p> <p>In particular a function from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\), typically denoted as</p> \\[f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\] <p>takes vectors in \\(\\mathbb{R}^n\\) as input and gives vectors in \\(\\mathbb{R}^m\\) as output. Keep in mind that a function assigns to each input a single output, but it is fine if two inputs yield the same output (e.g., \\(f(\\mathbf{x}) = \\|\\mathbf{x}\\| = f(-\\mathbf{x})\\)). </p> <p>A scalar-valued function is a function \\(\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) (that is to say, with \\(m = 1\\)). In other words, a scalar-valued function gives real number outputs.</p> <p>Example: The problem of finding a best-fit line involves minimizing a scalar-valued function \\(E : \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) of the vector \\((m, b) \\in \\mathbb{R}^2\\) (or more concretely, \\(E\\) is an \\(\\mathbb{R}\\)-valued function of two variables \\(m\\) and \\(b\\)). We choose \\((m, b)\\) to minimize the sum of the squares of the errors; i.e., choose \\((m, b)\\) to minimize the scalar-valued function</p> \\[E(m, b) = \\sum_{i=1}^{n} (y_i - (mx_i + b))^2\\] <p>Example: Addition and multiplication are scalar-valued functions \\(\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\):</p> \\[A(x_1, x_2) = x_1 + x_2, \\quad M(x_1, x_2) = x_1x_2\\] <p>Definition: A vector-valued function is a function \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) with general \\(m \\geq 1\\). In other words, a vector-valued function gives output considered as vectors in some \\(\\mathbb{R}^m\\).</p> <p>A vector-valued function \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) can be expressed in terms of \\(m\\) scalar-valued component functions or coordinate functions \\(f_1, \\ldots, f_m : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), defined by the expressions</p> \\[f(\\mathbf{x}) = \\begin{pmatrix} f_1(\\mathbf{x}) \\\\ \\vdots \\\\ f_m(\\mathbf{x}) \\end{pmatrix} = (f_1(\\mathbf{x}), \\ldots, f_m(\\mathbf{x}))\\] <p>(depending on whether we consider the output to be a \"vector\" or a \"point\"), with each \\(f_j\\) a scalar-valued function.</p> <p>We can write the output of \\(f\\) on the input \\(\\mathbf{x} \\in \\mathbb{R}^n\\) in (at least) three ways:</p> \\[f(\\mathbf{x}) = f\\left(\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\\right) = f(x_1, \\ldots, x_n),\\] <p>depending on whether we want to keep things compact, emphasize that the input to \\(f\\) is considered as a vector in \\(\\mathbb{R}^n\\), or emphasize that the output of \\(f\\) depends on \\(n\\) real-number inputs (the coordinates of the point or vector \\(\\mathbf{x}\\)).</p> <p>Example: Consider a small object flying through the air. At any given time \\(t\\), its position in space is a point \\(\\mathbf{x}(t) = (x(t), y(t), z(t)) \\in \\mathbb{R}^3\\) and its velocity (a vector pointing in the direction of motion with magnitude equal to the speed) is some</p> \\[\\mathbf{v}(t) = \\begin{pmatrix} v_1(t) \\\\ v_2(t) \\\\ v_3(t) \\end{pmatrix} \\in \\mathbb{R}^3,\\] <p>so both position and velocity are \\(\\mathbb{R}^3\\)-valued functions of time \\(t \\in \\mathbb{R}\\). In other words, we have vector-valued functions</p> \\[\\mathbf{x} : \\mathbb{R} \\rightarrow \\mathbb{R}^3, \\quad \\mathbf{v} : \\mathbb{R} \\rightarrow \\mathbb{R}^3.\\] <p>Example: Vector-valued functions can encode ways to manipulate vectors geometrically. For example, the function \\(T : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) given by</p> \\[T\\left(\\begin{pmatrix} x \\\\ y \\end{pmatrix}\\right) = \\begin{pmatrix} -y \\\\ x \\end{pmatrix}\\] <p>is a rotation.</p>"},{"location":"math/multivariate_calculus/multivariable_functions_level_sets_and_contour_plots/#composition","title":"Composition","text":"<p>For functions \\(f : \\mathbb{R} \\rightarrow \\mathbb{R}\\) and \\(g : \\mathbb{R} \\rightarrow \\mathbb{R}\\), the composition \\(f \\circ g : \\mathbb{R} \\rightarrow \\mathbb{R}\\) is defined by \\((f \\circ g)(x) = f(g(x))\\). As an illustration, the function \\(h(x) = \\sin(x^2)\\) is the composition \\(f \\circ g\\) for \\(g(x) = x^2\\) and \\(f(u) = \\sin(u)\\).</p> <p>Just as with functions \\(\\mathbb{R} \\rightarrow \\mathbb{R}\\), we can form the composition of vector-valued functions.</p> <p>Definition. If \\(g : \\mathbb{R}^n \\rightarrow \\mathbb{R}^p\\) and \\(f : \\mathbb{R}^p \\rightarrow \\mathbb{R}^m\\) are multivariable functions (note that \\(g\\) has output belonging to \\(\\mathbb{R}^p\\) on which \\(f\\) is applied), we can form a new composite function: take an input in \\(\\mathbb{R}^n\\); first apply \\(g\\) to it, and then apply \\(f\\):</p> \\[\\text{input } \\mathbf{x} \\in \\mathbb{R}^n \\xrightarrow{g} \\mathbb{R}^p \\xrightarrow{f} \\mathbb{R}^m\\] <p>As a shorthand, we write this new function as \\(f \\circ g\\); the symbol \\(\\circ\\) is read as \"composed with.\" In symbols, the new function is given by</p> \\[(f \\circ g)(\\mathbf{x}) = (f \\text{ applied to } g(\\mathbf{x})) = f(g(\\mathbf{x}))\\] <p>Example: Consider the functions \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) and \\(g : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) defined by</p> \\[f(u, v) = (uv, u + v), \\quad g(x, y) = (e^{xy}, x - y)\\] <p>So \\(f_1(u, v) = uv\\), \\(f_2(u, v) = u + v\\), \\(g_1(x, y) = e^{xy}\\), \\(g_2(x, y) = x - y\\). Then \\(f \\circ g : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) evaluated at \\((x, y) \\in \\mathbb{R}^2\\) equals</p> \\[(f \\circ g)(x, y) = f(g(x, y)) = f(e^{xy}, x - y) = (e^{xy}(x - y), e^{xy} + x - y).\\] <p>In this example, the composite function \\(g \\circ f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) also makes sense. Its value on input \\((u, v) \\in \\mathbb{R}^2\\) is</p> \\[(g \\circ f)(u, v) = g(f(u, v)) = g(uv, u + v) = (e^{uv(u+v)}, uv - (u + v)) = (e^{u^2v+uv^2}, uv - u - v).\\] <p>Observe that in this case, \\(f \\circ g\\) and \\(g \\circ f\\) are very different functions (just look at the formulas we have computed for each). The order of composition matters (familiar for scalar functions: \\(\\sin(x^2) \\neq \\sin(x)^2\\) as functions).</p> <p>Example: For the functions \\(g : \\mathbb{R} \\rightarrow \\mathbb{R}^3\\) and \\(f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^2\\) defined by</p> \\[g(t) = (t, \\cos(t), \\sin(t)), \\quad f(x, y, z) = (y, z),\\] <p>\\(g\\) can be visualized as the path of a particle moving on a helix on the cylinder \\(y^2 + z^2 = 1\\) of radius 1 around the \\(x\\)-axis, and \\(f\\) is the projection onto the \\(yz\\)-plane. Then \\(f \\circ g : \\mathbb{R} \\rightarrow \\mathbb{R}^2\\) is given by</p> \\[(f \\circ g)(t) = f(t, \\cos(t), \\sin(t)) = (\\cos(t), \\sin(t)),\\] <p>the path of the particle's \"shadow\" in the \\(yz\\)-plane moving counterclockwise around a circle of radius 1 in the \\(yz\\)-plane.</p> <p></p>"},{"location":"math/multivariate_calculus/multivariable_functions_level_sets_and_contour_plots/#graphs-level-sets-and-contour-plots","title":"Graphs, level sets, and contour plots","text":"<p>In our experience with functions \\(f(x)\\) of one variable, it can be quite helpful to visualize the function graphically.</p> <p>Much as the graph of a 1-variable function \\(f(x)\\) is the subset of \\(\\mathbb{R}^2\\) defined as</p> \\[\\text{Graph}(f) = \\{(x, y) \\in \\mathbb{R}^2 : y = f(x)\\},\\] <p>for an \\(n\\)-variable function \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) its graph is a subset of \\(\\mathbb{R}^{n+1}\\) defined as follows:</p> <p>Definition: The graph of \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is the subset of \\(\\mathbb{R}^{n+1}\\) (not \\(\\mathbb{R}^n\\)!) defined as</p> \\[\\text{Graph}(f) = \\{(x_1, \\cdots, x_n, z) \\in \\mathbb{R}^{n+1} : z = f(x_1, \\cdots, x_n)\\}.\\] <p></p> <p>Example: Let's work out the graph of the function \\(f(x, y) = \\sqrt{1 - x^2 - y^2}\\).</p> <p>We can only take the square root of a nonnegative number, so we require \\(1 - x^2 - y^2 \\geq 0\\), or equivalently \\(x^2 + y^2 \\leq 1\\).</p> <p>This is the disk \\(D\\) centered at the origin in \\(\\mathbb{R}^2\\) with radius 1. The graph of \\(f\\) is therefore</p> \\[\\text{Graph}(f) = \\{(x, y, z) \\in \\mathbb{R}^3 : (x, y) \\in D, z = f(x, y)\\} = \\{(x, y, \\sqrt{1 - x^2 - y^2}) : (x, y) \\in D\\}\\] <p>Notice that since \\(z = \\sqrt{1 - x^2 - y^2}\\), we then have \\(x^2 + y^2 + z^2 = 1\\) with \\(z \\geq 0\\). This graph is the upper hemisphere of the sphere in \\(\\mathbb{R}^3\\) with radius 1 centered at \\((0, 0, 0)\\).</p> <p></p> <p>If you are hiking in a park and you get a \u201ccontour map\u201d (or \u201ccontour plot\u201d) of the terrain you are about to hike in, it may look something like below.</p> <p></p> <p>The curves on the left in the figure above indicate where the terrain is at a fixed level. For example, the curve that is labeled by 400 represents where the terrain has an altitude of 400 feet. The mathematical way to think about this is to consider the altitude function,</p> \\[z = A(x, y).\\] <p>The set of points \\((x, y)\\) where \\(A(x, y) = 400\\) is the curve on the contour map labeled 400. The set of points \\((x, y)\\) where \\(A(x, y) = 600\\) is the curve on the contour map labeled 600. In general, the set of points \\((x, y)\\) where \\(A(x, y) = c\\) is called the level curve of the function \\(A\\) at level \\(c\\) (and is also called a level set, or sometimes even a contour line even though it generally looks nothing at all like a line).</p> <p>The contour map consisting of a collection of level curves is very helpful in visualizing the altitude function \\(z = A(x, y)\\), and gives us a good understanding of the terrain. Of course, the contour map doesn't show the level curves \\(A(x, y) = c\\) for every \\(c\\); that would be impossible. Rather, the contour map shows these level curves for \"enough\" values of \\(c\\) that one can get a sense of the hilliness of the terrain for practical purposes. Between level curves drawn for values \\(c_1 &lt; c_2\\) are level curves for intermediate values of \\(c\\) that are omitted for clarity.</p> <p>Definition: Let \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) be a function. For any \\(c \\in \\mathbb{R}\\), the level set of \\(f\\) at level \\(c\\) is the set of points \\((x_1, \\ldots, x_n) \\in \\mathbb{R}^n\\) for which \\(f(x_1, \\ldots, x_n) = c\\). It is also called the \\(c\\)-level set of \\(f\\).</p> <p>If \\(f\\) is a function \\(\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) of 2 variables then a contour plot of \\(f\\) is a picture in \\(\\mathbb{R}^2\\) that depicts the level sets of \\(f\\) for many different values of \\(c\\) (often values with some common difference for \"consecutive\" level sets, such as a common difference of 10, or 4, or 1, or 0.2, etc).</p> <p>Note: The website Desmos plots level curves \\(g(x, y) = c\\) for varying \\(c\\). To explore surface graphs, try CalcPlot3D or GeoGebra.</p> <p>For example, here are surface graphs drawn using GeoGebra of the functions \\(f(x, y) = 2xy\\) and \\(f(x, y) = sin(x)\\)</p> <p> </p>"},{"location":"math/multivariate_calculus/multivariable_functions_level_sets_and_contour_plots/#exercises","title":"Exercises","text":"<p>1(a). Consider the set \\(S = \\{(x, y, z) \\in \\mathbb{R}^3 : x^3 + z^3 + 3y^2z^3 + 5xy = 0\\}\\).</p> <p>Give functions \\(f, h : \\mathbb{R}^3 \\rightarrow \\mathbb{R}\\) for which \\(S\\) is a level set of both \\(f(x, y, z)\\) and \\(h(x, y, z)\\).</p> <p>Solution: One straightforward choice is to define \\(f\\) directly from the equation defining \\(S\\):</p> \\[f(x, y, z) = x^3 + z^3 + 3y^2z^3 + 5xy\\] <p>Then \\(S\\) is the \\(0\\)-level set of \\(f\\), since \\(S = \\{(x, y, z) \\in \\mathbb{R}^3 : f(x, y, z) = 0\\}\\).</p> <p>For \\(h\\), we can choose any function that has the same zero set. For example:</p> \\[h(x, y, z) = 2(x^3 + z^3 + 3y^2z^3 + 5xy) = 2f(x, y, z)\\] <p>Then \\(h(x, y, z) = 0\\) if and only if \\(f(x, y, z) = 0\\), so \\(S\\) is also the \\(0\\)-level set of \\(h\\).</p> <p>Alternatively, we could choose \\(h(x, y, z) = (x^3 + z^3 + 3y^2z^3 + 5xy)^2\\), which also has \\(S\\) as its \\(0\\)-level set, since a square equals zero if and only if the base equals zero.</p> <p>1(b). By solving for \\(z\\) in terms of \\(x\\) and \\(y\\), give a function \\(g : \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) for which \\(S\\) is the graph of \\(g\\).</p> <p>Solution: Starting from the equation \\(x^3 + z^3 + 3y^2z^3 + 5xy = 0\\), we can factor out \\(z^3\\):</p> \\[x^3 + z^3(1 + 3y^2) + 5xy = 0\\] <p>Since \\(1 + 3y^2 \\geq 1 &gt; 0\\) for all \\(y \\in \\mathbb{R}\\), we can solve for \\(z^3\\):</p> \\[z = \\sqrt[3]{\\frac{-x^3 - 5xy}{1 + 3y^2}}\\] <p>Therefore, we can define the function \\(g : \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) by:</p> \\[g(x, y) = \\sqrt[3]{\\frac{-x^3 - 5xy}{1 + 3y^2}}\\] <p>Then \\(S\\) is the graph of \\(g\\), since \\(S = \\{(x, y, z) \\in \\mathbb{R}^3 : z = g(x, y)\\}\\).</p> <p>2. Consider the function \\(g : \\mathbb{R} \\rightarrow \\mathbb{R}^2\\) defined by \\(g(t) = \\left(\\frac{e^t + e^{-t}}{2}, \\frac{e^t - e^{-t}}{2}\\right)\\).</p> <p>Every point in the output of \\(g\\) lies on the hyperbola \\(x^2 - y^2 = 1\\). Are all points in the hyperbola \\(\\{(x, y) \\in \\mathbb{R}^2 : x^2 - y^2 = 1\\}\\) in the output of \\(g\\)? If \"yes\" then explain why, and if \"no\" then explain why a specific point on the hyperbola is not in the output.</p> <p>Solution: The answer is no. </p> <p>First, let's verify that every point in the output of \\(g\\) lies on the hyperbola. For \\(g(t) = \\left(\\frac{e^t + e^{-t}}{2}, \\frac{e^t - e^{-t}}{2}\\right)\\), we have:</p> \\[x^2 - y^2 = \\left(\\frac{e^t + e^{-t}}{2}\\right)^2 - \\left(\\frac{e^t - e^{-t}}{2}\\right)^2 = \\frac{(e^t + e^{-t})^2 - (e^t - e^{-t})^2}{4}\\] <p>Expanding the squares,</p> \\[= \\frac{e^{2t} + 2 + e^{-2t} - (e^{2t} - 2 + e^{-2t})}{4} = \\frac{4}{4} = 1\\] <p>So indeed, every point in the output of \\(g\\) satisfies \\(x^2 - y^2 = 1\\).</p> <p>However, not all points on the hyperbola are in the output of \\(g\\).  </p> <p>A point that is not in the output is \\((-\\sqrt{2}, -1)\\) on the hyperbola. We have \\((-\\sqrt{2})^2 - (-1)^2 = 1\\), so it's on the hyperbola. But since \\(\\frac{e^t + e^{-t}}{2} \\geq 1\\) for all \\(t\\), we cannot have the first coordinate equal to \\(-\\sqrt{2} &lt; 0\\). Therefore, \\((-\\sqrt{2}, -1)\\) is not in the output of \\(g\\).</p> <p>3(a). Let \\(S\\) be a level set \\(\\{(x, y, z) \\in \\mathbb{R}^3 : f(x, y, z) = c\\}\\) in \\(\\mathbb{R}^3\\). If \\(S\\) is also the graph \\(\\{(x, y, z) \\in \\mathbb{R}^3 : (x, y) \\in D, z = g(x, y)\\}\\) of a function \\(g : D \\rightarrow \\mathbb{R}\\) on some region \\(D\\) in \\(\\mathbb{R}^2\\), explain why \\(S\\) meets each vertical line \\(\\{(a, b, t) : t \\in \\mathbb{R}\\}\\) (for \\((a, b) \\in \\mathbb{R}^2\\)) in at most one point. A vertical line in \\(\\mathbb{R}^3\\) is a line parallel to the \\(z\\)-axis. For a fixed point \\((a, b) \\in \\mathbb{R}^2\\), the vertical line through \\((a, b)\\) is the set \\(\\{(a, b, t) : t \\in \\mathbb{R}\\}\\)\u2014 all points with the same \\(x\\) and \\(y\\) coordinates \\((a, b)\\), but with \\(z\\) (denoted here as \\(t\\)) varying over all real numbers.</p> <p>Solution: Since \\(S\\) is the graph of the function \\(g : D \\rightarrow \\mathbb{R}\\), we have \\(S = \\{(x, y, z) \\in \\mathbb{R}^3 : (x, y) \\in D, z = g(x, y)\\}\\).</p> <p>Consider a vertical line \\(\\{(a, b, t) : t \\in \\mathbb{R}\\}\\) for some fixed \\((a, b) \\in \\mathbb{R}^2\\). </p> <p>If \\((a, b) \\notin D\\), then no point on this vertical line is in \\(S\\) (since \\(S\\) only contains points whose first two coordinates \\((x, y)\\) are in \\(D\\)). So the intersection is empty (zero points).</p> <p>If \\((a, b) \\in D\\), then by the definition of \\(S\\) as a graph, there is exactly one point in \\(S\\) with first two coordinates \\((a, b)\\), namely the point \\((a, b, g(a, b))\\). This point lies on the vertical line \\(\\{(a, b, t) : t \\in \\mathbb{R}\\}\\) (specifically when \\(t = g(a, b)\\)). </p> <p>Since \\(g\\) is a function, it assigns exactly one value \\(g(a, b)\\) to each input \\((a, b) \\in D\\). Therefore, there cannot be two different points in \\(S\\) with the same first two coordinates \\((a, b)\\), which means the vertical line can intersect \\(S\\) in at most one point.</p> <p>3(b). For the sphere \\(S = \\{(x, y, z) \\in \\mathbb{R}^3 : x^2 + y^2 + z^2 = 4\\}\\) of radius 2 centered at the origin, explain both algebraically and geometrically why \\(S\\) violates the \"vertical line test\" in (a), so \\(S\\) is not the graph of a function.</p> <p>Solution: For a point \\((a, b, z)\\) on a vertical line to also be on the sphere \\(S\\), we need:</p> \\[a^2 + b^2 + z^2 = 4\\] <p>Solving for \\(z\\):</p> \\[z^2 = 4 - a^2 - b^2\\] <p>There are two solutions:</p> \\[z = \\sqrt{4 - a^2 - b^2} \\quad \\text{and} \\quad z = -\\sqrt{4 - a^2 - b^2}\\] <p>This means the vertical line intersects the sphere in two distinct points: \\((a, b, \\sqrt{4 - a^2 - b^2})\\) and \\((a, b, -\\sqrt{4 - a^2 - b^2})\\).</p> <p>Since the vertical line intersects \\(S\\) in more than one point, the sphere violates the \"vertical line test\" from part (a). Therefore, \\(S\\) cannot be the graph of a function \\(g(x, y)\\).</p> <p>The sphere \\(x^2 + y^2 + z^2 = 4\\) is a closed surface centered at the origin. Consider any vertical line (parallel to the \\(z\\)-axis) that passes through a point \\((a, b)\\) inside the sphere where \\(a^2 + b^2 &lt; 4\\). </p> <p>Geometrically, this vertical line will pierce through the sphere, intersecting it at two points. Specifically, the line intersects the sphere at the point on the \"lower\" hemisphere (where \\(z &lt; 0\\)). And the line intersects the sphere at the point on the \"upper\" hemisphere (where \\(z &gt; 0\\)). This violates the requirement that a graph of a function must intersect each vertical line in at most one point.</p> <p>Note: Contrast with the upper hemisphere: Now consider the upper hemisphere \\(S_{\\text{upper}} = \\{(x, y, z) \\in \\mathbb{R}^3 : x^2 + y^2 + z^2 = 4, z \\geq 0\\}\\).</p> <p>For a vertical line through a point \\((a, b)\\) where \\(a^2 + b^2 &lt; 4\\), the equation \\(a^2 + b^2 + z^2 = 4\\) still gives two solutions: \\(z = \\sqrt{4 - a^2 - b^2}\\) and \\(z = -\\sqrt{4 - a^2 - b^2}\\). However, since \\(S_{\\text{upper}}\\) only includes points with \\(z \\geq 0\\), only the positive solution \\(z = \\sqrt{4 - a^2 - b^2}\\) is in \\(S_{\\text{upper}}\\).</p> <p>Therefore, each vertical line through a point \\((a, b)\\) with \\(a^2 + b^2 &lt; 4\\) intersects \\(S_{\\text{upper}}\\) in exactly one point: \\((a, b, \\sqrt{4 - a^2 - b^2})\\). This satisfies the \"vertical line test\" from part (a).</p> <p>Indeed, the upper hemisphere \\(S_{\\text{upper}}\\) is the graph of the function \\(g : D \\rightarrow \\mathbb{R}\\) defined by \\(g(x, y) = \\sqrt{4 - x^2 - y^2}\\), where \\(D = \\{(x, y) \\in \\mathbb{R}^2 : x^2 + y^2 \\leq 4\\}\\). For a vertical line through a point \\((a, b)\\) where \\(a^2 + b^2 &gt; 4\\), the equation \\(a^2 + b^2 + z^2 = 4\\) would require \\(z^2 = 4 - a^2 - b^2 &lt; 0\\), which has no real solutions. Therefore, such a vertical line does not intersect \\(S_{\\text{upper}}\\) at all (zero points), which also satisfies the \"at most one point\" requirement.</p> <p>4. Consider the function \\(g(v, w) = v^2 - w^2\\). For \\(c &gt; 0\\), check that \\(g(v/\\sqrt{c}, w/\\sqrt{c}) = 1\\) precisely when \\(g(v, w) = c\\). Explain why the level set \\(g(v, w) = c\\) is the same as scaling up the level set \\(g(v, w) = 1\\) by the factor \\(\\sqrt{c}\\).</p> <p>Solution: First, we verify that \\(g(v/\\sqrt{c}, w/\\sqrt{c}) = 1\\) precisely when \\(g(v, w) = c\\):</p> \\[g\\left(\\frac{v}{\\sqrt{c}}, \\frac{w}{\\sqrt{c}}\\right) = \\left(\\frac{v}{\\sqrt{c}}\\right)^2 - \\left(\\frac{w}{\\sqrt{c}}\\right)^2 = \\frac{v^2}{c} - \\frac{w^2}{c} = \\frac{v^2 - w^2}{c} = \\frac{g(v, w)}{c}\\] <p>Therefore, \\(g(v/\\sqrt{c}, w/\\sqrt{c}) = 1\\) if and only if \\(\\frac{g(v, w)}{c} = 1\\), which is equivalent to \\(g(v, w) = c\\).</p> <p>Now, suppose \\((v, w)\\) is a point on the level set \\(g(v, w) = c\\). Then by the above, we have \\(g(v/\\sqrt{c}, w/\\sqrt{c}) = 1\\), which means \\((v/\\sqrt{c}, w/\\sqrt{c})\\) is a point on the level set \\(g(v, w) = 1\\).</p> <p>Conversely, if \\((p, q)\\) is a point on the level set \\(g(p, q) = 1\\), then \\(g(\\sqrt{c} \\cdot p, \\sqrt{c} \\cdot q) = c\\), which means \\((\\sqrt{c} \\cdot p, \\sqrt{c} \\cdot q)\\) is a point on the level set \\(g(v, w) = c\\).</p> <p>The transformation \\((p, q) \\mapsto (\\sqrt{c} \\cdot p, \\sqrt{c} \\cdot q)\\) is precisely scaling by the factor \\(\\sqrt{c}\\) in both coordinates. Therefore, the level set \\(g(v, w) = c\\) is obtained by scaling the level set \\(g(v, w) = 1\\) by the factor \\(\\sqrt{c}\\).</p> <p>5. Prove that the graph of \\(f(x, y) = x^2 - y\\) is the 2-level set of some function \\(F(x, y, z)\\).</p> <p>Solution: The graph of \\(f(x, y) = x^2 - y\\) is the set:</p> \\[\\text{Graph}(f) = \\{(x, y, z) \\in \\mathbb{R}^3 : z = x^2 - y\\}\\] <p>Define the function \\(F : \\mathbb{R}^3 \\rightarrow \\mathbb{R}\\) by:</p> \\[F(x, y, z) = z - (x^2 - y) + 2 = z - x^2 + y + 2\\] <p>Then \\(F(x, y, z) = 2\\) if and only if:</p> \\[z - x^2 + y + 2 = 2\\] \\[z - x^2 + y = 0\\] \\[z = x^2 - y\\] <p>Therefore, the 2-level set of \\(F\\) is:</p> \\[\\{(x, y, z) \\in \\mathbb{R}^3 : F(x, y, z) = 2\\} = \\{(x, y, z) \\in \\mathbb{R}^3 : z = x^2 - y\\} = \\text{Graph}(f)\\] <p>Note: In general, the graph of any function \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) can be expressed as a level set. For the graph \\(\\{(x_1, \\ldots, x_n, z) : z = f(x_1, \\ldots, x_n)\\}\\), we can define \\(F(x_1, \\ldots, x_n, z) = z - f(x_1, \\ldots, x_n) + c\\) for any constant \\(c\\), and the graph will be the \\(c\\)-level set of \\(F : \\mathbb{R}^{n+1} \\rightarrow \\mathbb{R}\\).</p>"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/","title":"The TL;DR version of Derivatives","text":""},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#scalar-case","title":"Scalar case","text":"<p>You are probably familiar with the concept of a derivative in the scalar case.</p> <p>Given a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\), the derivative of \\(f\\) at a point \\(x \\in \\mathbb{R}\\) is defined as:</p> \\[f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\\] <p>Derivatives are a way to measure change. In the scalar case, the derivative of the function \\(f\\) at the point \\(x\\) tells us how much the function \\(f\\) changes as the input \\(x\\) changes by a small amount \\(\\varepsilon\\):</p> \\[f(x + \\varepsilon) \\approx f(x) + \\varepsilon f'(x)\\] <p>For ease of notation we will commonly assign a name to the output of \\(f\\), say \\(y = f(x)\\), and write \\(\\frac{\\partial y}{\\partial x}\\) for the derivative of \\(y\\) with respect to \\(x\\). This notation emphasizes that \\(\\frac{\\partial y}{\\partial x}\\) is the rate of change between the variables \\(x\\) and \\(y\\); concretely if \\(x\\) were to change by \\(\\varepsilon\\) then \\(y\\) will change by approximately \\(\\varepsilon \\frac{\\partial y}{\\partial x}\\).</p> <p>We can write this relationship as</p> \\[x \\to x + \\Delta x \\implies y \\to y + \\frac{\\partial y}{\\partial x} \\Delta x\\] <p>You should read this as saying \"changing \\(x\\) to \\(x + \\Delta x\\) implies that \\(y\\) will change to approximately \\(y + \\Delta x \\frac{\\partial y}{\\partial x}\\)\". This notation is nonstandard, but it emphasizes the relationship between changes in \\(x\\) and changes in \\(y\\).</p>"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#chain-rule","title":"Chain rule","text":"<p>The chain rule tells us how to compute the derivative of the composition of functions. In the scalar case suppose that \\(f, g: \\mathbb{R} \\to \\mathbb{R}\\) and \\(y = f(x)\\), \\(z = g(y)\\); then we can also write \\(z = (g \\circ f)(x)\\), or draw the following computational graph:</p> \\[x \\xrightarrow{f} y \\xrightarrow{g} z\\] <p>The (scalar) chain rule tells us that</p> \\[\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}\\] <p>This equation makes intuitive sense. The derivatives \\(\\frac{\\partial z}{\\partial y}\\) and \\(\\frac{\\partial y}{\\partial x}\\) give:</p> \\[x \\to x + \\Delta x \\implies y \\to y + \\frac{\\partial y}{\\partial x} \\Delta x\\] \\[y \\to y + \\Delta y \\implies z \\to z + \\frac{\\partial z}{\\partial y} \\Delta y\\] <p>Combining these two rules lets us compute the effect of \\(x\\) on \\(z\\): if \\(x\\) changes by \\(\\Delta x\\) then \\(y\\) will change by \\(\\frac{\\partial y}{\\partial x}\\Delta x\\), so we have \\(\\Delta y = \\frac{\\partial y}{\\partial x}\\Delta x\\). If \\(y\\) changes by \\(\\Delta y\\) then \\(z\\) will change by \\(\\frac{\\partial z}{\\partial y}\\Delta y = \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x}\\Delta x\\) which is exactly what the chain rule tells us.</p>"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#gradient-vector-in-scalar-out","title":"Gradient: Vector in, scalar out","text":"<p>This same intuition carries over into the vector case. Now suppose that \\(f: \\mathbb{R}^N \\to \\mathbb{R}\\) takes a vector as input and produces a scalar. The derivative of \\(f\\) at the point \\(\\mathbf{x} \\in \\mathbb{R}^N\\) is now called the gradient, and it is defined as:</p> \\[\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\lim_{\\mathbf{h} \\to \\mathbf{0}} \\frac{f(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x})}{\\|\\mathbf{h}\\|}\\] <p>Now the gradient \\(\\nabla_{\\mathbf{x}} f(\\mathbf{x}) \\in \\mathbb{R}^N\\) is a vector, with the same intuition as the scalar case. If we set \\(y = f(\\mathbf{x})\\) then we have the relationship</p> \\[\\mathbf{x} \\to \\mathbf{x} + \\Delta \\mathbf{x} \\implies y \\to y + \\frac{\\partial y}{\\partial \\mathbf{x}} \\cdot \\Delta \\mathbf{x}\\] <p>The formula changes a bit from the scalar case to account for the fact that \\(\\mathbf{x}\\), \\(\\Delta \\mathbf{x}\\), and \\(\\frac{\\partial y}{\\partial \\mathbf{x}}\\) are now vectors in \\(\\mathbb{R}^N\\) while \\(y\\) is a scalar. In particular when multiplying \\(\\frac{\\partial y}{\\partial \\mathbf{x}}\\) by \\(\\Delta \\mathbf{x}\\) we use the dot product, which combines two vectors to give a scalar.</p> <p>One nice outcome of this formula is that it gives meaning to the individual elements of the gradient \\(\\frac{\\partial y}{\\partial \\mathbf{x}}\\). Suppose that \\(\\Delta \\mathbf{x}\\) is the \\(i\\)th basis vector \\(\\mathbf{e}_i\\), so that the \\(i\\)th coordinate of \\(\\Delta \\mathbf{x}\\) is 1 and all other coordinates of \\(\\Delta \\mathbf{x}\\) are 0. </p> <p>Then the dot product \\(\\frac{\\partial y}{\\partial \\mathbf{x}} \\cdot \\Delta \\mathbf{x}\\) here is simply the \\(i\\)th coordinate of \\(\\frac{\\partial y}{\\partial \\mathbf{x}}\\). Thus the \\(i\\)th coordinate of \\(\\frac{\\partial y }{\\partial \\mathbf{x}}\\) times \\(\\varepsilon\\) tells us the amount by which \\(y\\) will change if we move \\(\\mathbf{x}\\) by a small amount \\(\\varepsilon\\) along the \\(i\\)th coordinate axis (meaning, if we change only the \\(i\\)th component of \\(\\mathbf{x}\\) by \\(\\varepsilon\\), where \\(\\varepsilon\\) is the \\(i\\)th component of \\(\\Delta \\mathbf{x}\\)).</p> <p>This means that we can also view the gradient \\(\\frac{\\partial y}{\\partial \\mathbf{x}}\\) as a vector of partial derivatives:</p> \\[\\frac{\\partial y}{\\partial \\mathbf{x}} = \\left(\\frac{\\partial y}{\\partial x_1}, \\frac{\\partial y}{\\partial x_2}, \\ldots, \\frac{\\partial y}{\\partial x_N}\\right)\\] <p>where \\(x_i\\) is the \\(i\\)th coordinate of the vector \\(\\mathbf{x}\\), which is a scalar, so each partial derivative \\(\\frac{\\partial y}{\\partial x_i}\\) is also a scalar.</p>"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#jacobian-vector-in-vector-out","title":"Jacobian: Vector in, Vector out","text":"<p>Now suppose that \\(f: \\mathbb{R}^N \\to \\mathbb{R}^M\\) takes a vector as input and produces a vector as output. Then the derivative of \\(f\\) at a point \\(\\mathbf{x}\\), also called the Jacobian, is the \\(M \\times N\\) matrix of partial derivatives. If we again set \\(\\mathbf{y} = f(\\mathbf{x})\\) then we can write:</p> \\[\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial y_1}{\\partial x_N} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial y_M}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial y_M}{\\partial x_N} \\end{pmatrix}\\] <p>The Jacobian tells us the relationship between each element of \\(\\mathbf{x}\\) and each element of \\(\\mathbf{y}\\): the \\((i, j)\\)-th element of \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is equal to \\(\\frac{\\partial y_i}{\\partial x_j}\\), so it tells us the amount by which \\(y_i\\) will change if \\(x_j\\) is changed by a small amount.</p> <p>Just as in the previous cases, the Jacobian tells us the relationship between changes in the input and changes in the output:</p> \\[\\mathbf{x} \\to \\mathbf{x} + \\Delta \\mathbf{x} \\implies \\mathbf{y} \\to \\mathbf{y} + \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x}\\] <p>Here \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is a \\(M \\times N\\) matrix and \\(\\Delta \\mathbf{x}\\) is an \\(N\\)-dimensional vector, so the product \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x}\\) is a matrix-vector multiplication resulting in an \\(M\\)-dimensional vector.</p> <p>It's worth noting that each row of the Jacobian matrix \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is actually a gradient! Specifically, the \\(i\\)th row of \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is the gradient of the scalar function \\(y_i\\) with respect to \\(\\mathbf{x}\\):</p> \\[\\text{Row } i \\text{ of } \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\nabla_{\\mathbf{x}} y_i = \\left(\\frac{\\partial y_i}{\\partial x_1}, \\frac{\\partial y_i}{\\partial x_2}, \\ldots, \\frac{\\partial y_i}{\\partial x_N}\\right)\\] <p>This makes sense because \\(y_i\\) is a scalar function of the vector \\(\\mathbf{x}\\), so its gradient is an \\(N\\)-dimensional vector. When we stack all \\(M\\) of these gradient vectors as rows, we get the \\(M \\times N\\) Jacobian matrix.</p> <p>This insight also explains why the matrix-vector multiplication \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x}\\) works the way it does. Since each row of the Jacobian is a gradient, the multiplication is equivalent to computing the dot product of each gradient with \\(\\Delta \\mathbf{x}\\):</p> \\[\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x} = \\begin{pmatrix} \\nabla_{\\mathbf{x}} y_1 \\cdot \\Delta \\mathbf{x} \\\\ \\nabla_{\\mathbf{x}} y_2 \\cdot \\Delta \\mathbf{x} \\\\ \\vdots \\\\ \\nabla_{\\mathbf{x}} y_M \\cdot \\Delta \\mathbf{x} \\end{pmatrix}\\] <p>In other words, the Jacobian-vector product is just a stack of gradient-vector products! Each component of the result tells us how much the corresponding output component \\(y_i\\) changes when we move \\(\\mathbf{x}\\) by \\(\\Delta \\mathbf{x}\\).</p> <p>For example, if \\(M = 2\\) and \\(N = 3\\), we might have:</p> \\[\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\begin{pmatrix} 2 &amp; 1 &amp; 0 \\\\ 0 &amp; 3 &amp; 1 \\end{pmatrix}, \\quad \\Delta \\mathbf{x} = \\begin{pmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\end{pmatrix}\\] <p>Then:</p> \\[\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x} = \\begin{pmatrix} 2 &amp; 1 &amp; 0 \\\\ 0 &amp; 3 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 2(0.1) + 1(0.2) + 0(0.3) \\\\ 0(0.1) + 3(0.2) + 1(0.3) \\end{pmatrix} = \\begin{pmatrix} 0.4 \\\\ 0.9 \\end{pmatrix}\\] <p>The chain rule can be extended to the vector case using Jacobian matrices. Suppose that \\(f: \\mathbb{R}^N \\to \\mathbb{R}^M\\) and \\(g: \\mathbb{R}^M \\to \\mathbb{R}^K\\). Let \\(\\mathbf{x} \\in \\mathbb{R}^N\\), \\(\\mathbf{y} \\in \\mathbb{R}^M\\), and \\(\\mathbf{z} \\in \\mathbb{R}^K\\) with \\(\\mathbf{y} = f(\\mathbf{x})\\) and \\(\\mathbf{z} = g(\\mathbf{y})\\), so we have the same computational graph as the scalar case:</p> \\[\\mathbf{x} \\xrightarrow{f} \\mathbf{y} \\xrightarrow{g} \\mathbf{z}\\] <p>The chain rule also has the same form as the scalar case:</p> \\[\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\] <p>However now each of these terms is a matrix: \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}\\) is a \\(K \\times M\\) matrix, \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is a \\(M \\times N\\) matrix, and \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\) is a \\(K \\times N\\) matrix; the multiplication of \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}\\) and \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is matrix multiplication.</p>"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#generalized-jacobian-tensor-in-tensor-out","title":"Generalized Jacobian: Tensor in, Tensor out","text":"<p>Just as a vector is a one-dimensional list of numbers and a matrix is a two-dimensional grid of numbers, a tensor is a \\(D\\)-dimensional grid of numbers. Many operations in deep learning accept tensors as inputs and produce tensors as outputs. For example an image is usually represented as a three-dimensional grid of numbers, where the three dimensions correspond to the height, width, and color channels (red, green, blue) of the image. We must therefore develop a derivative that is compatible with functions operating on general tensors.</p> <p>Suppose now that \\(f: \\mathbb{R}^{N_1 \\times \\cdots \\times N_{D_x}} \\to \\mathbb{R}^{M_1 \\times \\cdots \\times M_{D_y}}\\). Then the input to \\(f\\) is a \\(D_x\\)-dimensional tensor of shape \\(N_1 \\times \\cdots \\times N_{D_x}\\), and the output of \\(f\\) is a \\(D_y\\)-dimensional tensor of shape \\(M_1 \\times \\cdots \\times M_{D_y}\\). If \\(\\mathbf{y} = f(\\mathbf{x})\\) then the derivative \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is a generalized Jacobian, which is an object with shape</p> \\[(M_1 \\times \\cdots \\times M_{D_y}) \\times (N_1 \\times \\cdots \\times N_{D_x})\\] <p>Note that we have separated the dimensions of \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) into two groups: the first group matches the dimensions of \\(\\mathbf{y}\\) and the second group matches the dimensions of \\(\\mathbf{x}\\). With this grouping, we can think of the generalized Jacobian as generalization of a matrix, where each \"row\" has the same shape as \\(\\mathbf{y}\\) and each \"column\" has the same shape as \\(\\mathbf{x}\\).</p> <p>Now if we let \\(\\mathbf{i} \\in \\mathbb{Z}^{D_y}\\) and \\(\\mathbf{j} \\in \\mathbb{Z}^{D_x}\\) be vectors of integer indices, then we can write</p> \\[\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{\\mathbf{i},\\mathbf{j}} = \\frac{\\partial y_{\\mathbf{i}}}{\\partial x_{\\mathbf{j}}}\\] <p>For example, if \\(\\mathbf{y}\\) is a \\(2 \\times 3\\) tensor and \\(\\mathbf{x}\\) is a \\(4 \\times 2\\) tensor, then \\(\\mathbf{i} = (i_1, i_2)\\) where \\(i_1 \\in \\{0,1\\}\\) and \\(i_2 \\in \\{0,1,2\\}\\), and \\(\\mathbf{j} = (j_1, j_2)\\) where \\(j_1 \\in \\{0,1,2,3\\}\\) and \\(j_2 \\in \\{0,1\\}\\). So we might have:</p> \\[\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,2),(3,0)} = \\frac{\\partial y_{(1,2)}}{\\partial x_{(3,0)}}\\] <p>This tells us how the element at position \\((1,2)\\) in \\(\\mathbf{y}\\) changes with respect to the element at position \\((3,0)\\) in \\(\\mathbf{x}\\).</p> <p>In this equation note that \\(y_{\\mathbf{i}}\\) and \\(x_{\\mathbf{j}}\\) are scalars, so the derivative \\(\\frac{\\partial y_{\\mathbf{i}}}{\\partial x_{\\mathbf{j}}}\\) is also a scalar. Using this notation we see that like the standard Jacobian, the generalized Jacobian tells us the relative rates of change between all elements of \\(\\mathbf{x}\\) and all elements of \\(\\mathbf{y}\\).</p> <p>The generalized Jacobian gives the same relationship between inputs and outputs as before:</p> \\[\\mathbf{x} \\to \\mathbf{x} + \\Delta \\mathbf{x} \\implies \\mathbf{y} \\to \\mathbf{y} + \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x}\\] <p>The difference is that now \\(\\Delta \\mathbf{x}\\) is a tensor of shape \\(N_1 \\times \\cdots \\times N_{D_x}\\) and \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is a generalized matrix of shape \\((M_1 \\times \\cdots \\times M_{D_y}) \\times (N_1 \\times \\cdots \\times N_{D_x})\\). The product \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x}\\) is therefore a generalized matrix-vector multiply, which results in a tensor of shape \\(M_1 \\times \\cdots \\times M_{D_y}\\).</p> <p>The generalized matrix-vector multiply follows the same algebraic rules as a traditional matrix-vector multiply:</p> \\[\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x}\\right)_{\\mathbf{i}} = \\sum_{\\mathbf{j}} \\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{\\mathbf{i},\\mathbf{j}} (\\Delta \\mathbf{x})_{\\mathbf{j}} = \\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{\\mathbf{i},:} \\cdot \\Delta \\mathbf{x}\\] <p>For example, continuing with our \\(2 \\times 3\\) output tensor \\(\\mathbf{y}\\) and \\(4 \\times 2\\) input tensor \\(\\mathbf{x}\\), the generalized Jacobian \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) has shape \\((2 \\times 3) \\times (4 \\times 2)\\). </p> <p>Let's say we have specific values for the Jacobian entries at position \\((1,1)\\):</p> \\[\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(0,0)} = 2, \\quad \\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(0,1)} = 3\\] \\[\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(1,0)} = 1, \\quad \\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(1,1)} = 4\\] \\[\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(2,0)} = 5, \\quad \\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(2,1)} = 2\\] \\[\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(3,0)} = 3, \\quad \\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(3,1)} = 1\\] <p>And suppose:</p> \\[\\Delta \\mathbf{x} = \\begin{pmatrix} 0.1 &amp; 0.2 \\\\ 0.3 &amp; 0.4 \\\\ 0.5 &amp; 0.6 \\\\ 0.7 &amp; 0.8 \\end{pmatrix}\\] <p>Let's use the general formula.</p> \\[\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x}\\right)_{(1,1)} = \\sum_{\\mathbf{j}} \\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),\\mathbf{j}} (\\Delta \\mathbf{x})_{\\mathbf{j}}\\] <p>Here, \\(\\mathbf{j}\\) iterates over all input positions \\((j_1, j_2)\\) where \\(j_1 \\in \\{0,1,2,3\\}\\) and \\(j_2 \\in \\{0,1\\}\\):</p> <ul> <li> <p>\\(\\mathbf{j} = (0,0)\\): \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(0,0)} (\\Delta \\mathbf{x})_{(0,0)} = 2 \\times 0.1 = 0.2\\)</p> </li> <li> <p>\\(\\mathbf{j} = (0,1)\\): \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(0,1)} (\\Delta \\mathbf{x})_{(0,1)} = 3 \\times 0.2 = 0.6\\)</p> </li> <li> <p>\\(\\mathbf{j} = (1,0)\\): \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(1,0)} (\\Delta \\mathbf{x})_{(1,0)} = 1 \\times 0.3 = 0.3\\)</p> </li> <li> <p>\\(\\mathbf{j} = (1,1)\\): \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(1,1)} (\\Delta \\mathbf{x})_{(1,1)} = 4 \\times 0.4 = 1.6\\)</p> </li> <li> <p>\\(\\mathbf{j} = (2,0)\\): \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(2,0)} (\\Delta \\mathbf{x})_{(2,0)} = 5 \\times 0.5 = 2.5\\)</p> </li> <li> <p>\\(\\mathbf{j} = (2,1)\\): \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(2,1)} (\\Delta \\mathbf{x})_{(2,1)} = 2 \\times 0.6 = 1.2\\)</p> </li> <li> <p>\\(\\mathbf{j} = (3,0)\\): \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(3,0)} (\\Delta \\mathbf{x})_{(3,0)} = 3 \\times 0.7 = 2.1\\)</p> </li> <li> <p>\\(\\mathbf{j} = (3,1)\\): \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{(1,1),(3,1)} (\\Delta \\mathbf{x})_{(3,1)} = 1 \\times 0.8 = 0.8\\)</p> </li> </ul> <p>Summing all these terms gives us \\(0.2 + 0.6 + 0.3 + 1.6 + 2.5 + 1.2 + 2.1 + 0.8 = 9.3\\).</p> <p>Thus, </p> \\[\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Delta \\mathbf{x}\\right)_{(1,1)} =  9.3\\] <p>The only difference is that the indices \\(\\mathbf{i}\\) and \\(\\mathbf{j}\\) are not scalars; instead they are vectors of indices. In the equation above the term \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{\\mathbf{i},:}\\) is the \\(\\mathbf{i}\\)th \"row\" of the generalized matrix \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\), which is a tensor with the same shape as \\(\\mathbf{x}\\). We have also used the convention that the dot product between two tensors of the same shape is an elementwise product followed by a sum, identical to the dot product between vectors.</p> <p>The chain rule also looks the same in the case of tensor-valued functions. Suppose that \\(\\mathbf{y} = f(\\mathbf{x})\\) and \\(\\mathbf{z} = g(\\mathbf{y})\\), where \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) have the same shapes as above and \\(\\mathbf{z}\\) has shape \\(K_1 \\times \\cdots \\times K_{D_z}\\). Now the chain rule looks the same as before:</p> \\[\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\] <p>The difference is that now \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}\\) is a generalized matrix of shape \\((K_1 \\times \\cdots \\times K_{D_z}) \\times (M_1 \\times \\cdots \\times M_{D_y})\\), and \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is a generalized matrix of shape \\((M_1 \\times \\cdots \\times M_{D_y}) \\times (N_1 \\times \\cdots \\times N_{D_x})\\); the product \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) is a generalized matrix-matrix multiply, resulting in an object of shape \\((K_1 \\times \\cdots \\times K_{D_z}) \\times (N_1 \\times \\cdots \\times N_{D_x})\\). Like the generalized matrix-vector multiply defined above, the generalized matrix-matrix multiply follows the same algebraic rules as the traditional matrix-matrix multiply:</p> \\[\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)_{\\mathbf{i},\\mathbf{j}} = \\sum_{\\mathbf{k}} \\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}\\right)_{\\mathbf{i},\\mathbf{k}} \\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{\\mathbf{k},\\mathbf{j}} = \\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}\\right)_{\\mathbf{i},:} \\cdot \\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{:,\\mathbf{j}}\\] <p>In this equation the indices \\(\\mathbf{i}\\), \\(\\mathbf{j}\\), \\(\\mathbf{k}\\) are vectors of indices, and the terms \\(\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}\\right)_{\\mathbf{i},:}\\) and \\(\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)_{:,\\mathbf{j}}\\) are the \\(\\mathbf{i}\\)th \"row\" of \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}\\) and the \\(\\mathbf{j}\\)th \"column\" of \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) respectively.</p>"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#some-tips-and-tricks","title":"Some tips and tricks","text":""},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#simplify-simplify-simplify","title":"Simplify, simplify, simplify","text":"<p>Much of the confusion in taking derivatives involving arrays stems from trying to do too many things at once. These \"things\" include taking derivatives of multiple components simultaneously, taking derivatives in the presence of summation notation, and applying the chain rule.</p>"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#expanding-notation-into-explicit-sums-and-equations-for-each-component","title":"Expanding notation into explicit sums and equations for each component","text":"<p>In order to simplify a given calculation, it is often useful to write out the explicit formula for a single scalar element of the output in terms of nothing but scalar variables. Once one has an explicit formula for a single scalar element of the output in terms of other scalar values, then one can use the calculus that you used as a beginner, which is much easier than trying to do matrix math, summations, and derivatives all at the same time.</p> <p>Example: Suppose we have a column vector \\(\\mathbf{y}\\) of length \\(C\\) that is calculated by forming the product of a matrix \\(W\\) that is \\(C\\) rows by \\(D\\) columns with a column vector \\(\\mathbf{x}\\) of length \\(D\\):</p> \\[\\mathbf{y} = W\\mathbf{x} \\quad\\] <p>Suppose we are interested in the derivative of \\(\\mathbf{y}\\) with respect to \\(\\mathbf{x}\\). A full characterization of this derivative requires the (partial) derivatives of each component of \\(\\mathbf{y}\\) with respect to each component of \\(\\mathbf{x}\\), which in this case will contain \\(C \\times D\\) values since there are \\(C\\) components in \\(\\mathbf{y}\\) and \\(D\\) components of \\(\\mathbf{x}\\).</p> <p>Let's start by computing one of these, say, the 3rd component of \\(\\mathbf{y}\\) with respect to the 7th component of \\(\\mathbf{x}\\). That is, we want to compute</p> \\[\\frac{\\partial y_3}{\\partial x_7}\\] <p>which is just the derivative of one scalar with respect to another.</p> <p>The first thing to do is to write down the formula for computing \\(y_3\\) so we can take its derivative. From the definition of matrix-vector multiplication, the value \\(y_3\\) is computed by taking the dot product between the 3rd row of \\(W\\) and the vector \\(\\mathbf{x}\\):</p> \\[y_3 = \\sum_{j=1}^{D} W_{3,j} x_j \\quad\\] <p>At this point, we have reduced the original matrix equation to a scalar equation. This makes it much easier to compute the desired derivatives.</p>"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#completing-the-derivative-the-jacobian-matrix","title":"Completing the derivative: the Jacobian matrix","text":"<p>Recall that our original goal was to compute the derivatives of each component of \\(\\mathbf{y}\\) with respect to each component of \\(\\mathbf{x}\\), and we noted that there would be \\(C \\times D\\) of these. They can be written out as a matrix in the following form:</p> \\[\\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} &amp; \\frac{\\partial y_1}{\\partial x_2} &amp; \\frac{\\partial y_1}{\\partial x_3} &amp; \\cdots &amp; \\frac{\\partial y_1}{\\partial x_D} \\\\ \\frac{\\partial y_2}{\\partial x_1} &amp; \\frac{\\partial y_2}{\\partial x_2} &amp; \\frac{\\partial y_2}{\\partial x_3} &amp; \\cdots &amp; \\frac{\\partial y_2}{\\partial x_D} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial y_C}{\\partial x_1} &amp; \\frac{\\partial y_C}{\\partial x_2} &amp; \\frac{\\partial y_C}{\\partial x_3} &amp; \\cdots &amp; \\frac{\\partial y_C}{\\partial x_D} \\end{pmatrix}\\] <p>In this particular case, this is called the Jacobian matrix, but this terminology is not too important for our purposes.</p> <p>Notice that for the equation \\(\\mathbf{y} = W\\mathbf{x}\\), the partial of \\(y_3\\) with respect to \\(x_7\\) was simply given by \\(W_{3,7}\\). If you go through the same process for other components, you will find that, for all \\(i\\) and \\(j\\),</p> \\[\\frac{\\partial y_i}{\\partial x_j} = W_{i,j}\\] <p>This means that the matrix of partial derivatives is</p> \\[\\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} &amp; \\frac{\\partial y_1}{\\partial x_2} &amp; \\frac{\\partial y_1}{\\partial x_3} &amp; \\cdots &amp; \\frac{\\partial y_1}{\\partial x_D} \\\\ \\frac{\\partial y_2}{\\partial x_1} &amp; \\frac{\\partial y_2}{\\partial x_2} &amp; \\frac{\\partial y_2}{\\partial x_3} &amp; \\cdots &amp; \\frac{\\partial y_2}{\\partial x_D} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial y_C}{\\partial x_1} &amp; \\frac{\\partial y_C}{\\partial x_2} &amp; \\frac{\\partial y_C}{\\partial x_3} &amp; \\cdots &amp; \\frac{\\partial y_C}{\\partial x_D} \\end{pmatrix} = \\begin{pmatrix} W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; \\cdots &amp; W_{1,D} \\\\ W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; \\cdots &amp; W_{2,D} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ W_{C,1} &amp; W_{C,2} &amp; W_{C,3} &amp; \\cdots &amp; W_{C,D} \\end{pmatrix}\\] <p>This, of course, is just \\(W\\) itself.</p> <p>Thus, after all this work, we have concluded that for \\(\\mathbf{y} = W\\mathbf{x}\\), we have</p> \\[\\frac{d\\mathbf{y}}{d\\mathbf{x}} = W\\]"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#row-vectors-instead-of-column-vectors","title":"Row vectors instead of column vectors","text":"<p>It is important in working with different neural networks packages to pay close attention to the arrangement of weight matrices, data matrices, and so on. For example, if a data matrix \\(X\\) contains many different vectors, each of which represents an input, is each data vector a row or column of the data matrix \\(X\\)?</p> <p>In the previous example, we worked with a vector \\(\\mathbf{x}\\) that was a column vector. However, you should also be able to use the same basic ideas when \\(\\mathbf{x}\\) is a row vector.</p> <p>Let \\(\\mathbf{y}\\) be a row vector with \\(C\\) components computed by taking the product of another row vector \\(\\mathbf{x}\\) with \\(D\\) components and a matrix \\(W\\) that is \\(D\\) rows by \\(C\\) columns:</p> \\[\\mathbf{y} = \\mathbf{x}W\\] <p>Importantly, despite the fact that \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\) have the same number of components as before, the shape of \\(W\\) is the transpose of the shape that we used before for \\(W\\).</p> <p>In this case, you will see, by writing</p> \\[y_3 = \\sum_{j=1}^{D} x_j W_{j,3}\\] <p>that</p> \\[\\frac{\\partial y_3}{\\partial x_7} = W_{7,3}\\] <p>Notice that the indexing into \\(W\\) is the opposite from what it was in the first example.</p> <p>However, when we assemble the full Jacobian matrix, we can still see that in this case as well,</p> \\[\\frac{d\\mathbf{y}}{d\\mathbf{x}} = W\\]"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#dealing-with-more-than-two-dimensions","title":"Dealing with more than two dimensions","text":"<p>Let's consider another closely related problem, that of computing \\(\\frac{d\\mathbf{y}}{dW}\\):</p> <p>In this case, \\(\\mathbf{y}\\) varies along one coordinate while \\(W\\) varies along two coordinates. Thus, the entire derivative is most naturally contained in a three-dimensional array.</p> <p>Let's again compute a scalar derivative between one component of \\(\\mathbf{y}\\), say \\(y_3\\) and one component of \\(W\\), say \\(W_{7,8}\\). Let's start with the same basic setup in which we write down an equation for \\(y_3\\) in terms of other scalar components. Now we would like an equation that expresses \\(y_3\\) in terms of scalar values, and shows the role that \\(W_{7,8}\\) plays in its computation.</p> <p>However, what we see is that \\(W_{7,8}\\) plays no role in the computation of \\(y_3\\), since</p> \\[y_3 = x_1 W_{1,3} + x_2 W_{2,3} + \\cdots + x_D W_{D,3} \\quad\\] <p>In other words,</p> \\[\\frac{\\partial y_3}{\\partial W_{7,8}} = 0\\] <p>However, the partials of \\(y_3\\) with respect to elements of the 3rd column of \\(W\\) will certainly be non-zero. For example, the derivative of \\(y_3\\) with respect to \\(W_{2,3}\\) is given by</p> \\[\\frac{\\partial y_3}{\\partial W_{2,3}} = x_2 \\quad\\] <p>as can be easily seen by examining the equation for \\(y_3\\).</p> <p>In general, when the index of the \\(\\mathbf{y}\\) component is equal to the second index of \\(W\\), the derivative will be non-zero, but will be zero otherwise. We can write:</p> \\[\\frac{\\partial y_j}{\\partial W_{i,j}} = x_i\\] <p>but the other elements of the 3-d array will be 0. If we let \\(F\\) represent the 3d array representing the derivative of \\(\\mathbf{y}\\) with respect to \\(W\\), where</p> \\[F_{i,j,k} = \\frac{\\partial y_i}{\\partial W_{j,k}}\\] <p>then</p> \\[F_{i,j,i} = x_j\\] <p>but all other entries of \\(F\\) are zero.</p> <p>Finally, if we define a new two-dimensional array \\(G\\) as</p> \\[G_{i,j} = F_{i,j,i}\\] <p>we can see that all of the information we need about \\(F\\) can be stored in \\(G\\), and that the non-trivial portion of \\(F\\) is really two-dimensional, not three-dimensional.</p> <p>Representing the important part of derivative arrays in a compact way is critical to efficient implementations of neural networks.</p>"},{"location":"math/multivariate_calculus/the_tl_dr_version_of_derivatives/#multiple-data-points","title":"Multiple data points","text":"<p>Let's assume that each individual \\(\\mathbf{x}\\) is a row vector of length \\(D\\), and that \\(X\\) is a two-dimensional array with \\(N\\) rows and \\(D\\) columns. \\(W\\) will be a matrix with \\(D\\) rows and \\(C\\) columns. \\(Y\\), given by</p> \\[Y = XW\\] <p>will also be a matrix, with \\(N\\) rows and \\(C\\) columns. Thus, each row of \\(Y\\) will give a row vector associated with the corresponding row of the input \\(X\\).</p> <p>Sticking to our technique of writing down an expression for a given component of the output, we have</p> \\[Y_{i,j} = \\sum_{k=1}^{D} X_{i,k} W_{k,j}\\] <p>We can see immediately from this equation that among the derivatives</p> \\[\\frac{\\partial Y_{a,b}}{\\partial X_{c,d}}\\] <p>they are all zero unless \\(a = c\\). That is, since each component of \\(Y\\) is computed using only the corresponding row of \\(X\\), derivatives of components between different rows of \\(Y\\) and \\(X\\) are all zero.</p> <p>Furthermore, we can see that</p> \\[\\frac{\\partial Y_{i,j}}{\\partial X_{i,k}} = W_{k,j}\\] <p>doesn't depend at all upon which row of \\(Y\\) and \\(X\\) we are comparing.</p> <p>In fact, the matrix \\(W\\) holds all of these partials as it is\u2014 we just have to remember to index into it according to the equation above to obtain the specific partial derivative that we want.</p> <p>If we let \\(Y_{i,:}\\) be the \\(i\\)th row of \\(Y\\) and let \\(X_{i,:}\\) be the \\(i\\)th row of \\(X\\), then we see that</p> \\[\\frac{\\partial Y_{i,:}}{\\partial X_{i,:}} = W\\] <p>which is a simple generalization of our previous result.</p>"},{"location":"math/probability/beta_distributions/","title":"Beta Distributions","text":"<p>The beta distribution is a family of continuous probability distributions defined on the interval [0, 1] or (0, 1). It's particularly useful for modeling random variables that represent proportions, probabilities, or percentages- anything that naturally falls between 0 and 1.</p> <p>Think of it this way: if you're trying to model a random variable as something like \"the probability of success in a trial\" or \"the proportion of people who like a new product,\" the beta distribution is often your go-to choice because it's specifically designed for values between 0 and 1.</p>"},{"location":"math/probability/beta_distributions/#pdf","title":"PDF","text":"<p>The probability density function (PDF) of the beta distribution, for \\(0 \\leq x \\leq 1\\) or \\(0 &lt; x &lt; 1\\), and shape parameters \\(\\alpha\\), \\(\\beta &gt; 0\\), is a power function of the variable \\(x\\) and of its reflection \\((1-x)\\) as follows:</p> \\[f(x;\\alpha,\\beta) = \\mathrm{constant} \\cdot x^{\\alpha-1}(1-x)^{\\beta-1} = \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\] <p>The beta function, \\(\\mathrm{B}\\), is a normalization constant to ensure that the total probability is 1.</p>"},{"location":"math/probability/beta_distributions/#case-study-biased-coin-selection","title":"Case study: Biased coin selection","text":"<p>Let's illustrate the beta distribution with a concrete example involving biased coins.</p> <p>Setup: We have 11 coins with different bias probabilities:</p> <ul> <li> <p>Coin 0: P(H) = 0.0</p> </li> <li> <p>Coin 1: P(H) = 0.1  </p> </li> <li> <p>Coin 2: P(H) = 0.2</p> </li> <li> <p>Coin 3: P(H) = 0.3</p> </li> <li> <p>Coin 4: P(H) = 0.4</p> </li> <li> <p>Coin 5: P(H) = 0.5</p> </li> <li> <p>Coin 6: P(H) = 0.6</p> </li> <li> <p>Coin 7: P(H) = 0.7</p> </li> <li> <p>Coin 8: P(H) = 0.8</p> </li> <li> <p>Coin 9: P(H) = 0.9</p> </li> <li> <p>Coin 10: P(H) = 1.0</p> </li> </ul> <p>We randomly select one coin and flip it 10 times, observing 7 heads and 3 tails.</p> <p>Question: What's the probability that we selected each of the 11 coins?</p> <p>Solution: Let \\(X\\) be the random variable representing the probability of heads for the selected coin. We want to find \\(P(X = p_i)\\) for each \\(p_i \\in \\{0, 0.1, 0.2, ..., 1.0\\}\\).</p> <p>Initial probabilities: Since we randomly selected a coin, each coin has equal probability: \\(P(X = p_i) = \\frac{1}{11}\\) for all \\(i\\).</p> <p>Probability of observing 7 heads: Given that we observed 7 heads out of 10 flips, the probability of this outcome for each coin is:</p> \\[P(\\text{7 heads out of 10} | X = p_i) = \\binom{10}{7} p_i^7 (1-p_i)^3\\] <p>Updated probabilities: Using Bayes' theorem, the probability that we selected coin \\(i\\) given the observed data is:</p> \\[P(X = p_i | \\text{7 heads out of 10}) = \\frac{P(\\text{7 heads out of 10} | X = p_i) \\cdot P(X = p_i)}{\\sum_{j=0}^{10} P(\\text{7 heads out of 10} | X = p_j) \\cdot P(X = p_j)}\\] <p>Since each coin was equally likely initially, we can simplify:</p> \\[P(X = p_i | \\text{7 heads out of 10}) = \\frac{p_i^7 (1-p_i)^3}{\\sum_{j=0}^{10} p_j^7 (1-p_j)^3}\\] <p>Let's calculate the numerator for each coin:</p> Coin P(H) \\(p_i^7 (1-p_i)^3\\) 0 0.0 \\(0^7 \\cdot 1^3 = 0\\) 1 0.1 \\(0.1^7 \\cdot 0.9^3 = 0.000000729\\) 2 0.2 \\(0.2^7 \\cdot 0.8^3 = 0.00001024\\) 3 0.3 \\(0.3^7 \\cdot 0.7^3 = 0.000250047\\) 4 0.4 \\(0.4^7 \\cdot 0.6^3 = 0.001179648\\) 5 0.5 \\(0.5^7 \\cdot 0.5^3 = 0.000976563\\) 6 0.6 \\(0.6^7 \\cdot 0.4^3 = 0.001194393\\) 7 0.7 \\(0.7^7 \\cdot 0.3^3 = 0.000250047\\) 8 0.8 \\(0.8^7 \\cdot 0.2^3 = 0.00001024\\) 9 0.9 \\(0.9^7 \\cdot 0.1^3 = 0.000000729\\) 10 1.0 \\(1^7 \\cdot 0^3 = 0\\) <p>Total sum: \\(\\sum_{j=0}^{10} p_j^7 (1-p_j)^3 \\approx 0.003872\\)</p> <p>Normalized probabilities:</p> <ul> <li> <p>P(X = 0.6) = \\(\\frac{0.001194393}{0.003872} \\approx 0.308\\) (30.8%)</p> </li> <li> <p>P(X = 0.4) = \\(\\frac{0.001179648}{0.003872} \\approx 0.305\\) (30.5%)</p> </li> <li> <p>P(X = 0.5) = \\(\\frac{0.000976563}{0.003872} \\approx 0.252\\) (25.2%)</p> </li> <li> <p>All other coins have negligible probability</p> </li> </ul> <p>After observing 7 heads out of 10 flips, we're most confident that we selected:</p> <ol> <li> <p>Coin 6 (P(H) = 0.6) with ~30.8% probability</p> </li> <li> <p>Coin 4 (P(H) = 0.4) with ~30.5% probability  </p> </li> <li> <p>Coin 5 (P(H) = 0.5) with ~25.2% probability</p> </li> </ol> <p>The beta distribution would give us a continuous version of this discrete problem, allowing us to consider any probability between 0 and 1, not just the 11 discrete values.</p> <p>Let's extend this to 101 coins with probabilities from 0 to 1 in steps of 0.01:</p> <p>Setup: We have 101 coins with probabilities:</p> <ul> <li>Coin 0: P(H) = 0.00</li> <li>Coin 1: P(H) = 0.01  </li> <li>Coin 2: P(H) = 0.02</li> <li>...</li> <li>Coin 50: P(H) = 0.50</li> <li>...</li> <li>Coin 99: P(H) = 0.99</li> <li>Coin 100: P(H) = 1.00</li> </ul> <p>We randomly select one coin and flip it 10 times, observing 7 heads and 3 tails.</p> <p>Solution: The same approach applies, but now we have 101 coins instead of 11.</p> <p>Updated probabilities: </p> \\[P(X = p_i | \\text{7 heads out of 10}) = \\frac{p_i^7 (1-p_i)^3}{\\sum_{j=0}^{100} p_j^7 (1-p_j)^3}\\] <p>The most likely coins after observing 7 heads out of 10 flips are:</p> Coin P(H) \\(p_i^7 (1-p_i)^3\\) Probability 70 0.70 \\(0.70^7 \\cdot 0.30^3 = 0.000250047\\) ~25.0% 69 0.69 \\(0.69^7 \\cdot 0.31^3 = 0.000248\\) ~24.8% 71 0.71 \\(0.71^7 \\cdot 0.29^3 = 0.000248\\) ~24.8% 68 0.68 \\(0.68^7 \\cdot 0.32^3 = 0.000240\\) ~24.0% 72 0.72 \\(0.72^7 \\cdot 0.28^3 = 0.000240\\) ~24.0% <p>This discrete problem with 101 coins closely approximates the continuous beta distribution. If we used a continuous beta distribution with:</p> <ul> <li> <p>\u03b1 = 7 + 1 = 8 (number of successes + 1)</p> </li> <li> <p>\u03b2 = 3 + 1 = 4 (number of failures + 1)</p> </li> </ul> <p>The beta distribution Beta(8, 4) would have:</p> <ul> <li> <p>Mean: \\(\\frac{8}{8+4} = \\frac{2}{3} \\approx 0.667\\)</p> </li> <li> <p>Mode: \\(\\frac{8-1}{8+4-2} = \\frac{7}{10} = 0.7\\)</p> </li> </ul> <p>Notice how the discrete results cluster around 0.7, which matches the mode of the continuous beta distribution!</p> <p>As we increase the number of discrete coins (from 11 to 101 to 1001, etc.), the discrete probability distribution approaches the continuous beta distribution.</p> <p>Let's see what happens when we flip the selected coin 100 times and observe 70 heads and 30 tails:</p> <p>Setup: Same 101 coins with probabilities from 0.00 to 1.00 in steps of 0.01.</p> <p>Updated probabilities: </p> \\[P(X = p_i | \\text{70 heads out of 100}) = \\frac{p_i^{70} (1-p_i)^{30}}{\\sum_{j=0}^{100} p_j^{70} (1-p_j)^{30}}\\] <p>The most likely coins after observing 70 heads out of 100 flips are:</p> Coin P(H) \\(p_i^{70} (1-p_i)^{30}\\) Probability 70 0.70 \\(0.70^{70} \\cdot 0.30^{30}\\) ~99.9% 69 0.69 \\(0.69^{70} \\cdot 0.31^{30}\\) ~0.1% 71 0.71 \\(0.71^{70} \\cdot 0.29^{30}\\) ~0.1% 68 0.68 \\(0.68^{70} \\cdot 0.32^{30}\\) ~0.0% 72 0.72 \\(0.72^{70} \\cdot 0.28^{30}\\) ~0.0% <p>Comparison: 10 flips vs 100 flips:</p> Scenario Most Likely Coin Probability Confidence 7 heads out of 10 Coin 70 (0.70) ~25% Low 70 heads out of 100 Coin 70 (0.70) ~99.9% Very High <p>With 70 successes and 30 failures, the corresponding beta distribution is Beta(71, 31):</p> <ul> <li> <p>Mean: \\(\\frac{71}{71+31} = \\frac{71}{102} \\approx 0.696\\)</p> </li> <li> <p>Mode: \\(\\frac{71-1}{71+31-2} = \\frac{70}{100} = 0.7\\)</p> </li> </ul> <p>Key insights:</p> <ol> <li> <p>More data = more confidence: With 100 flips, we're 99.9% confident it's coin 70, compared to only 25% confidence with 10 flips.</p> </li> <li> <p>Precision increases: The probability mass concentrates much more tightly around the true value.</p> </li> <li> <p>Beta distribution reflects this: Beta(71, 31) has a much sharper peak than Beta(8, 4), showing how more data leads to more precise estimates.</p> </li> <li> <p>Law of large numbers: As we get more data, our estimate converges to the true probability (0.7 in this case).</p> </li> </ol> <p>An animation of the beta distribution for different values of its parameters:</p> <p></p>"},{"location":"math/probability/central_limit_theorem/","title":"Central Limit Theorem","text":""},{"location":"math/probability/central_limit_theorem/#a-simplified-galton-board","title":"A simplified Galton Board","text":"<p>This is a Galton board. Maybe you've seen one before\u2014 it's a popular demonstration of how, even when a single event is chaotic and random with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed.</p> <p></p> <p>Let's take a model version of this. </p> <p></p> <p>In this model, we will assume that each ball falls directly onto a certain central peg and that it has a 50-50 probability of bouncing to the left or to the right. </p> <p></p> <p>We'll think of each of those outcomes as either adding one or subtracting one from its position. Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead center on the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right.</p> <p> </p> <p>Here, there are five different rows of pegs, so the little hopping ball makes five different random choices between plus one and minus one. </p> <p></p> <p>We can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one.</p> <p></p> <p>We might label all of the different buckets with the sum that they represent. As we repeat this, we're looking at different possible sums for those five random numbers.</p> <p></p> <p>Let it be emphasized that the goal right now is not to accurately model physics\u2014 the goal is to give a simple example to illustrate the central limit theorem. For that purpose, idealized though this might be, it actually provides a really good example.</p> <p>If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other (as if they're all ghosts), then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is.</p> <p> </p> <p>In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket. If you do want to think that through, you'll find it very reminiscent of Pascal's triangle.</p> <p></p> <p>The basic idea of the central limit theorem is that if you increase the size of that sum (for example, here would mean increasing the number of rows of pegs for each ball to bounce off), then the distribution that describes where that sum is going to fall looks more and more like a bell curve.</p> <p> </p>"},{"location":"math/probability/central_limit_theorem/#the-general-idea","title":"The general idea","text":"<p>It's actually worth taking a moment to write down that general idea. </p> <p>The setup is that we have a random variable. We'll call that random number \\(X\\). What we're doing is taking multiple different samples of that variable \\(X\\) (or we could take different random variables that do the exact same thing, like \\(X1\\), \\(X2\\),.. that are independent and are the exact same function, i.e., identical) and adding them all together. On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results. The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve. That's it- that is the general idea.</p> <p></p>"},{"location":"math/probability/central_limit_theorem/#dice-simulations","title":"Dice simulations","text":"<p>Usually if you think of rolling a die, you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that. We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds.</p> <p>To better illustrate what the central limit theorem is all about, let's run four of these simulations in parallel, where on the upper left we're doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time.</p> <p></p> <p>Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve\u2014 it looks a lot more reminiscent of the one we started with, skewed towards the left. But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric. It has the lump in the middle and fade towards the tails shape of a bell curve.</p> <p> </p> <p>And let it be emphasized again: you can start with any different distribution.</p> <p>Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise. How many samples do we need before we can be sure that what we're looking at is representative of the true distribution?</p>"},{"location":"math/probability/central_limit_theorem/#the-true-distributions-for-sums","title":"The true distributions for sums","text":"<p>Let's get a little more theoretical and show the precise shape these distributions will take on in the long run. The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1/6th.</p> <p></p> <p>For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram drawn, you can conveniently think about by going through all the different diagonals. Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets.</p> <p></p> <p>That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this. </p> <p></p> <p>Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die. </p> <p></p> <p>So just to be crystal clear on what's being represented here: if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution being drawn represents how likely you are to see various different sums (this is equivalent to the convolution of the die's probability mass function with itself). Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case. </p> <p></p> <p>So if we compute what the distributions for these sums look like for larger and larger sums, it looks more and more like a bell curve.</p> <p></p> <p>But before we get to that, let's make a couple of simple observations. For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat. You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation.</p> <p></p>"},{"location":"math/probability/central_limit_theorem/#mean-variance-and-standard-deviation","title":"Mean, variance, and standard deviation","text":"<p>Looking back at our sequence of distributions, let's talk about the mean and standard deviation. If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if it's said that the mean of the next one is 2 times mu. That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die. Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth. The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction.</p> <p></p> <p>A little more challenging, but very important, is to describe how the standard deviation changes. The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances. The main thing to highlight is how it's the variance that adds\u2014 it's not the standard deviation that adds. </p> <p></p> <p>So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation.</p> <p></p> <p>For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on and so forth.</p> <p></p> <p>This, as mentioned, is very important. It means that even though our distributions are getting spread out, they're not spreading out all that quickly\u2014 they only do so in proportion to the square root of the size of the sum.</p> <p>As we prepare to make a more quantitative description of the central limit theorem, the core intuition to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to one. </p> <p> </p> <p>And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment.</p> <p></p> <p>And let it be said one more time: the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all one, we still approach that same universal shape, which is kind of mind-boggling.</p>"},{"location":"math/probability/central_limit_theorem/#unpacking-the-gaussian-formula","title":"Unpacking the Gaussian formula","text":"<p>The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay. </p> <p> </p> <p>To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value. </p> <p></p> <p>That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions.</p> <p></p> <p>This gives us the basic bell curve shape. Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves.</p> <p> </p> <p>And a quick thing to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation.</p> <p> </p> <p>And in that sense, the number e is not really all that special for our formula. We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant. Make it a 2, same family of curves. Make it a 3, same family of curves. The reason we use e is that it gives that constant a very readable meaning.</p> <p></p> <p>Or rather, if we reconfigure things a little bit so that the exponent looks like negative 1/2 times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution. And that's very nice.</p> <p> </p> <p>But before we can interpret this as a probability distribution, we need the area under the curve to be 1.</p> <p></p> <p>As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi.</p> <p></p> <p>For our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want. </p> <p></p> <p>Throwing back in the constants we had earlier, the one half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2. </p> <p></p> <p>So we also need to divide out by that in order to make sure it has an area of 1, and combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi.</p> <p> </p> <p>This, finally, is a valid probability distribution. As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1. </p> <p> </p> <p>The special case where sigma equals 1 has a specific name\u2014 we call it the standard normal distribution.</p> <p></p> <p>And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution. </p> <p> </p> <p>So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi.</p> <p>Let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like. When you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation. </p> <p></p> <p>And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size.</p> <p></p> <p>So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do. You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution.</p> <p> </p>"},{"location":"math/probability/central_limit_theorem/#formal-definition","title":"Formal Definition","text":"<p>Before stating the formal definition of the Central Limit Theorem, it's important to understand what i.i.d. means. This stands for \"independent and identically distributed\" and refers to a collection of random variables that satisfy two key properties:</p> <ol> <li> <p>Independent: The outcome of one variable doesn't influence the outcome of any other variable</p> </li> <li> <p>Identically distributed: All variables follow the same probability distribution</p> </li> </ol> <p>In our dice example, each die roll is independent of the others, and each die follows the same distribution (whether fair or weighted), making the sequence of die rolls i.i.d.</p> <p>Now, let \\(X_1, X_2, X_3, \\ldots\\) be i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Here, \\(\\mu\\) and \\(\\sigma\\) describe the underlying distribution that each individual \\(X_i\\) follows:</p> <ul> <li> <p>\\(\\mu = E[X_i]\\) is the expected value (mean) of any single random variable \\(X_i\\)</p> </li> <li> <p>\\(\\sigma^2 = \\text{Var}(X_i)\\) is the variance of any single random variable \\(X_i\\)</p> </li> <li> <p>\\(\\sigma = \\sqrt{\\sigma^2}\\) is the standard deviation of any single random variable \\(X_i\\)</p> </li> </ul> <p>For example, if we're rolling a fair die, then \\(\\mu = 3.5\\) and \\(\\sigma = 1.71\\). These are the mean and standard deviation of a single die roll.</p> <p>The sample mean \\(\\bar{X}_n\\) is defined as:</p> \\[\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i = \\frac{X_1 + X_2 + \\cdots + X_n}{n}\\] <p>This represents the average of the first \\(n\\) observations. For example, if we roll a die 100 times and get outcomes \\(X_1, X_2, \\ldots, X_{100}\\), then \\(\\bar{X}_{100}\\) would be the average of those 100 die rolls.</p> <p>The CLT states that for large \\(n\\), the distribution of \\(\\bar{X}_n\\) after standardization approaches a standard normal distribution. By standardization, we mean that we subtract \\(\\mu\\) (the mean of any random variable \\(X_i\\)) and divide by \\(\\sigma/\\sqrt{n}\\) (the standard deviation of the sample mean \\(\\bar{X}_n\\)).</p> <p>Theorem (Central Limit Theorem): As \\(n \\to \\infty\\),</p> \\[\\sqrt{n} \\cdot \\frac{\\bar{X}_n - \\mu}{\\sigma} \\to N(0, 1) \\text{ in distribution}\\] <p>This means that the standardized sample mean converges in distribution to a standard normal random variable as the sample size grows large.</p>"},{"location":"math/probability/conditional_distributions/","title":"Conditional Distributions","text":"<p>Conditional distributions describe the probability distribution of one random variable given that another random variable takes on a specific value. They are fundamental to understanding how random variables relate to each other.</p>"},{"location":"math/probability/conditional_distributions/#discrete-case","title":"Discrete Case","text":"<p>For two discrete random variables \\(X\\) and \\(Y\\), the conditional probability mass function of \\(X\\) given \\(Y = y\\) is defined as:</p> \\[p_{X|Y}(x|y) = \\frac{p_{X,Y}(x,y)}{p_Y(y)}\\] <p>where \\(p_Y(y) &gt; 0\\).</p>"},{"location":"math/probability/conditional_distributions/#continuous-case","title":"Continuous Case","text":"<p>For two continuous random variables \\(X\\) and \\(Y\\), the conditional probability density function of \\(X\\) given \\(Y = y\\) is defined as:</p> \\[f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\\] <p>where \\(f_Y(y) &gt; 0\\).</p>"},{"location":"math/probability/conditional_distributions/#interpretation","title":"Interpretation","text":"<p>The conditional distribution \\(p_{X|Y}(x|y)\\) or \\(f_{X|Y}(x|y)\\) represents:</p> <ul> <li> <p>The probability (or probability density) that \\(X = x\\) given that we know \\(Y = y\\)</p> </li> <li> <p>How the distribution of \\(X\\) changes when we condition on different values of \\(Y\\)</p> </li> </ul> <p>The conditional distribution formulas work analogously to the familiar conditional probability formula:</p> \\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\] <p>In the discrete case, we have:</p> \\[p_{X|Y}(x|y) = \\frac{p_{X,Y}(x,y)}{p_Y(y)}\\] <p>This is exactly analogous to:</p> \\[P(X = x | Y = y) = \\frac{P(X = x, Y = y)}{P(Y = y)}\\] <p>The joint PMF \\(p_{X,Y}(x,y)\\) represents \\(P(X = x, Y = y)\\), and the marginal PMF \\(p_Y(y)\\) represents \\(P(Y = y)\\).</p> <p>For continuous variables, the PDF values themselves are not probabilities, but the ratio of PDFs works the same way:</p> \\[f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\\] <p>Think of it this way:</p> <ol> <li>Joint PDF \\(f_{X,Y}(x,y)\\) tells us how \"dense\" the probability is at point \\((x,y)\\)</li> <li> <p>Marginal PDF \\(f_Y(y)\\) tells us how \"dense\" the probability is along the entire line \\(Y = y\\)</p> </li> <li> <p>Conditional PDF \\(f_{X|Y}(x|y)\\) tells us how \"dense\" the probability is at \\(X = x\\) when we're restricted to \\(Y = y\\)</p> </li> </ol> <p>Example: Consider a chicken that lays eggs according to a Poisson process. Let \\(N\\) be the total number of eggs laid, where \\(N \\sim \\text{Poisson}(\\lambda)\\). Each egg has a probability \\(p\\) of hatching. Let \\(X\\) be the number of eggs that hatch, so \\(X|N \\sim \\text{Binomial}(N,p)\\). Let \\(Y\\) be the number of eggs that don't hatch, so \\(X + Y = N\\). Find the joint distribution of \\(X\\) and \\(Y\\), and determine if they are independent.</p> <p>We are given:  - \\(N \\sim \\text{Poisson}(\\lambda)\\): Total eggs laid</p> <ul> <li> <p>\\(X|N \\sim \\text{Binomial}(N,p)\\): Eggs that hatch, given \\(N\\) eggs</p> </li> <li> <p>\\(Y = N - X\\): Eggs that don't hatch</p> </li> <li> <p>We need to find \\(p_{X,Y}(x,y)\\)</p> </li> </ul> <p>Let's find the joint distribution. Since \\(X + Y = N\\), we can write:</p> \\[p_{X,Y}(x,y) = P(X = x, Y = y) = P(X = x, N = x + y)\\] <p>Using the law of total probability and the conditional distribution:</p> \\[p_{X,Y}(x,y) = P(X = x|N = x + y) \\cdot P(N = x + y)\\] <p>Given \\(X|N \\sim \\text{Binomial}(N,p)\\):</p> \\[P(X = x|N = x + y) = \\binom{x + y}{x} p^x (1-p)^y\\] <p>And \\(N \\sim \\text{Poisson}(\\lambda)\\):</p> \\[P(N = x + y) = \\frac{e^{-\\lambda} \\lambda^{x + y}}{(x + y)!}\\] <p>Therefore:</p> \\[p_{X,Y}(x,y) = \\binom{x + y}{x} p^x (1-p)^y \\cdot \\frac{e^{-\\lambda} \\lambda^{x + y}}{(x + y)!}\\] <p>Simplifying:</p> \\[p_{X,Y}(x,y) = \\frac{(x + y)!}{x! y!} p^x (1-p)^y \\cdot \\frac{e^{-\\lambda} \\lambda^{x + y}}{(x + y)!}\\] \\[p_{X,Y}(x,y) = \\frac{e^{-\\lambda} \\lambda^{x + y} p^x (1-p)^y}{x! y!}\\] \\[p_{X,Y}(x,y) = \\frac{e^{-\\lambda} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] <p>Let's find the marginal distributions.</p> \\[p_X(x) = \\sum_{y=0}^{\\infty} p_{X,Y}(x,y) = \\sum_{y=0}^{\\infty} \\frac{e^{-\\lambda} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] \\[p_X(x) = \\frac{e^{-\\lambda} (\\lambda p)^x}{x!} \\sum_{y=0}^{\\infty} \\frac{(\\lambda(1-p))^y}{y!}\\] \\[p_X(x) = \\frac{e^{-\\lambda} (\\lambda p)^x}{x!} e^{\\lambda(1-p)} = \\frac{e^{-\\lambda p} (\\lambda p)^x}{x!}\\] <p>This shows \\(X \\sim \\text{Poisson}(\\lambda p)\\).</p> <p>Similarly:</p> \\[p_Y(y) = \\frac{e^{-\\lambda(1-p)} (\\lambda(1-p))^y}{y!}\\] <p>So \\(Y \\sim \\text{Poisson}(\\lambda(1-p))\\).</p> <p>If \\(X\\) and \\(Y\\) were independent, then:</p> \\[p_{X,Y}(x,y) = p_X(x) \\cdot p_Y(y)\\] <p>Let's check:</p> \\[p_X(x) \\cdot p_Y(y) = \\frac{e^{-\\lambda p} (\\lambda p)^x}{x!} \\cdot \\frac{e^{-\\lambda(1-p)} (\\lambda(1-p))^y}{y!}\\] \\[p_X(x) \\cdot p_Y(y) = \\frac{e^{-\\lambda p - \\lambda(1-p)} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] \\[p_X(x) \\cdot p_Y(y) = \\frac{e^{-\\lambda} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] <p>This equals \\(p_{X,Y}(x,y)\\), so \\(X\\) and \\(Y\\) are independent!</p> <p>The joint distribution is:</p> \\[p_{X,Y}(x,y) = \\frac{e^{-\\lambda} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] <p>And \\(X\\) and \\(Y\\) are independent random variables, each following Poisson distributions with parameters \\(\\lambda p\\) and \\(\\lambda(1-p)\\) respectively.</p> <p>Note: It's crucial to distinguish between \\(X|N \\sim \\text{Binomial}(N,p)\\) (conditional distribution where \\(N\\) is random) and \\(X \\sim \\text{Binomial}(N,p)\\) (unconditional distribution where \\(N\\) would be fixed). The conditional notation is essential here since \\(N\\) is a random variable.</p>"},{"location":"math/probability/conditional_expectation/","title":"Conditional Expectation","text":""},{"location":"math/probability/conditional_expectation/#the-two-envelope-paradox","title":"The Two Envelope Paradox","text":"<p>The two envelope paradox is a famous probability puzzle that challenges our intuition about conditional expectation and reveals subtle issues in reasoning about random variables.</p> <p>You are given two envelopes. One contains twice as much money as the other. You pick one envelope at random, open it, and see that it contains $X. You are then given the choice to either:</p> <ol> <li>Keep the envelope you have (containing $X)</li> <li>Switch to the other envelope</li> </ol> <p>The question: Should you switch?</p> <p>At first glance, it seems like switching should be beneficial.</p> <p>The other envelope contains either $X/2 or $2X, each with probability 1/2. The expected value of switching is:</p> \\[E[\\text{other envelope}] = \\frac{1}{2} \\cdot \\frac{X}{2} + \\frac{1}{2} \\cdot 2X = \\frac{X}{4} + X = \\frac{5X}{4}\\] <p>Since \\(\\frac{5X}{4} &gt; X\\), you should always switch!</p> <p>The paradox: This reasoning suggests you should always switch, regardless of which envelope you initially chose. But this is clearly wrong- if you always switch, you're back to a 50-50 choice!</p> <p>The error lies in assuming that the other envelope contains \\(X/2\\) or \\(2X\\) with equal probability 1/2.</p> <p>Let's be more careful about the random variables involved.</p> <p>Let \\(A\\) be the amount in the first envelope, and \\(2A\\) be the amount in the second envelope. The envelopes are chosen randomly, so:</p> <ul> <li> <p>\\(P(\\text{you pick first}) = \\frac{1}{2}\\)</p> </li> <li> <p>\\(P(\\text{you pick second}) = \\frac{1}{2}\\)</p> </li> </ul> <p>Case 1: You picked the first envelope (contains \\(A\\))</p> <ul> <li> <p>The other envelope contains \\(2A\\)</p> </li> <li> <p>If you switch, you gain \\(A\\)</p> </li> </ul> <p>Case 2: You picked the second envelope (contains \\(2A\\))</p> <ul> <li> <p>The other envelope contains \\(A\\)</p> </li> <li> <p>If you switch, you lose \\(A\\)</p> </li> </ul> <p>Expected gain from switching:</p> \\[E[\\text{gain from switching}] = \\frac{1}{2} \\cdot A + \\frac{1}{2} \\cdot (-A) = 0\\] <p>Conclusion: There is no expected gain from switching. The correct strategy is to be indifferent between keeping and switching.</p> <p>The original reasoning made a subtle error by treating \\(X\\) as if it were independent of which envelope you chose. But \\(X\\) is not independent- it depends on whether you picked the first or second envelope.</p>"},{"location":"math/probability/conditional_expectation/#definition-of-conditional-expectation","title":"Definition of Conditional Expectation","text":"<p>Conditional expectation is the expected value of a random variable given that we know the value of another random variable. It's a fundamental concept in probability theory that allows us to make predictions based on partial information.</p>"},{"location":"math/probability/conditional_expectation/#discrete-case","title":"Discrete Case","text":"<p>For discrete random variables \\(X\\) and \\(Y\\), the conditional expectation of \\(X\\) given \\(Y = y\\) is:</p> \\[E[X | Y = y] = \\sum_x x \\cdot P(X = x | Y = y) = \\sum_x x \\cdot \\frac{P(X = x, Y = y)}{P(Y = y)}\\] <p>Key properties:</p> <ul> <li> <p>\\(E[X | Y = y]\\) is a function of \\(y\\)</p> </li> <li> <p>It represents the average value of \\(X\\) when we know \\(Y = y\\)</p> </li> <li> <p>If \\(X\\) and \\(Y\\) are independent, then \\(E[X | Y = y] = E[X]\\) for all \\(y\\)</p> </li> </ul>"},{"location":"math/probability/conditional_expectation/#continuous-case","title":"Continuous Case","text":"<p>For continuous random variables \\(X\\) and \\(Y\\) with joint density \\(f_{X,Y}(x,y)\\) and marginal density \\(f_Y(y)\\), the conditional expectation of \\(X\\) given \\(Y = y\\) is:</p> \\[E[X | Y = y] = \\int_{-\\infty}^{\\infty} x \\cdot f_{X|Y}(x|y) \\, dx = \\int_{-\\infty}^{\\infty} x \\cdot \\frac{f_{X,Y}(x,y)}{f_Y(y)} \\, dx\\] <p>where \\(f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\\) is the conditional density of \\(X\\) given \\(Y = y\\).</p>"},{"location":"math/probability/conditional_expectation/#the-random-variable-ex-y-and-its-intuitive-meaning","title":"The Random Variable \\(E[X | Y]\\) and its intuitive meaning","text":"<p>Conditional expectation is itself a random variable! We write \\(E[X | Y]\\) to denote the random variable that takes the value \\(E[X | Y = y]\\) when \\(Y = y\\).</p> <p>Conditional expectation answers the question: \"What is the average value of \\(X\\) when we know that \\(Y\\) takes a specific value?\"</p> <p>Example: If \\(X\\) is your test score and \\(Y\\) is the number of hours you studied:</p> <ul> <li> <p>\\(E[X | Y = 0]\\) = average test score for students who didn't study</p> </li> <li> <p>\\(E[X | Y = 10]\\) = average test score for students who studied 10 hours</p> </li> <li> <p>\\(E[X | Y]\\) = a function that tells you the expected test score for any amount of study time</p> </li> </ul> <p>Example: Let \\(X \\sim N(0, 1)\\) and \\(Y = X^2\\). What is \\(E[Y | X]\\)?</p> <p>Since \\(Y = X^2\\), when we know the value of \\(X\\), we know exactly what \\(Y\\) is. Therefore:</p> \\[E[Y | X = x] = E[X^2 | X = x] = E[x^2 | X = x] = x^2\\] <p>This makes intuitive sense: if we know \\(X = x\\), then we know \\(Y = x^2\\) with certainty, so the expected value of \\(Y\\) given \\(X = x\\) is simply \\(x^2\\).</p> <p>Key insight: When \\(Y\\) is a function of \\(X\\), the conditional expectation \\(E[Y | X]\\) is just that function evaluated at \\(X\\). In this case, \\(E[Y | X] = X^2\\).</p> <p>It's important to distinguish between \\(X^2\\) and \\(x^2\\). \\(X^2\\) is a random variable- it represents the square of the random variable \\(X\\). Since \\(X\\) can take different values, \\(X^2\\) can also take different values. \\(x^2\\) is a specific number - it's the square of a particular observed value \\(x\\) of the random variable \\(X\\).</p> <p>In our example:</p> <ul> <li> <p>\\(X \\sim N(0,1)\\) - \\(X\\) is a random variable that can take any real value</p> </li> <li> <p>\\(X^2\\) - This is also a random variable (the square of \\(X\\))</p> </li> <li> <p>\\(x\\) - This is a specific observed value of \\(X\\) (like \\(x = 1.5\\) or \\(x = -0.3\\))</p> </li> <li> <p>\\(x^2\\) - This is the square of that specific value (like \\(1.5^2 = 2.25\\) or \\((-0.3)^2 = 0.09\\))</p> </li> </ul> <p>When we write \\(E[Y | X = x]\\), we're asking: \"What's the expected value of \\(Y\\) given that \\(X\\) takes the specific value \\(x\\)?\" Since \\(Y = X^2\\), when \\(X = x\\), we know \\(Y = x^2\\) (a specific number). So \\(E[Y | X = x] = E[x^2 | X = x] = x^2\\)</p> <p>Now let's compute \\(E[X | Y]\\) for the same example. This is more interesting because knowing \\(Y = y\\) doesn't completely determine \\(X\\)- there are two possible values of \\(X\\) that give \\(Y = y\\).</p> <p>If \\(Y = y\\), then \\(X = \\pm\\sqrt{y}\\) (since \\(Y = X^2\\)).</p> <p>Since \\(X \\sim N(0,1)\\) is symmetric about 0, both \\(+\\sqrt{y}\\) and \\(-\\sqrt{y}\\) are equally likely when \\(Y = y\\).</p> \\[E[X | Y = y] = \\sqrt{y} \\cdot P(X = \\sqrt{y} | Y = y) + (-\\sqrt{y}) \\cdot P(X = -\\sqrt{y} | Y = y)\\] <p>Since \\(X \\sim N(0,1)\\) is symmetric about 0:</p> <ul> <li> <p>\\(P(X = \\sqrt{y}) = P(X = -\\sqrt{y})\\) (by symmetry)</p> </li> <li> <p>Therefore: \\(P(X = \\sqrt{y} | Y = y) = P(X = -\\sqrt{y} | Y = y)\\)</p> </li> </ul> <p>Since these are the only two possibilities when \\(Y = y\\), and they're equal:</p> \\[P(X = \\sqrt{y} | Y = y) = P(X = -\\sqrt{y} | Y = y) = \\frac{1}{2}\\] <p>When we condition on \\(Y = y\\), we're essentially asking: \"Given that \\(X^2 = y\\), what's the probability that \\(X = \\sqrt{y}\\) vs \\(X = -\\sqrt{y}\\)?\"</p> <p>Since the normal distribution is symmetric about 0, the probability density at \\(+\\sqrt{y}\\) is exactly the same as the probability density at \\(-\\sqrt{y}\\). When we condition on \\(Y = y\\), we're restricting ourselves to these two equally likely points.</p> \\[E[X | Y = y] = \\sqrt{y} \\cdot \\frac{1}{2} + (-\\sqrt{y}) \\cdot \\frac{1}{2} = \\frac{\\sqrt{y} - \\sqrt{y}}{2} = 0\\] <p>Result: \\(E[X | Y = y] = 0\\) for all \\(y &gt; 0\\).</p> <p>Key insight: Even though knowing \\(Y\\) gives us information about the magnitude of \\(X\\) (since \\(|X| = \\sqrt{Y}\\)), it doesn't give us information about the sign of \\(X\\). Due to the symmetry of the normal distribution, the positive and negative values cancel out, giving an expected value of 0.</p> <p>Example: Let's consider a more complex example involving continuous random variables and uniform distributions.</p> <p>We have a stick of length \\(L\\). First break: Break the stick at a point \\(Y\\) chosen uniformly at random along its length. This means \\(Y \\sim \\text{Uniform}(0, L)\\). Second break: Take the remaining piece of the stick (which has length \\(Y\\)) and break it at a point \\(X\\) chosen uniformly at random along this remaining length.</p> <p>Question: What is \\(E[X | Y]\\)?</p> <p>Given that \\(Y = y\\), we know:</p> <ul> <li> <p>The remaining piece has length \\(y\\)</p> </li> <li> <p>\\(X\\) is chosen uniformly at random from \\([0, y]\\)</p> </li> <li> <p>Therefore: \\(X | Y = y \\sim \\text{Uniform}(0, y)\\)</p> </li> </ul> <p>For a uniform distribution on \\([0, y]\\), the expected value is the midpoint:</p> \\[E[X | Y = y] = \\frac{0 + y}{2} = \\frac{y}{2}\\] <p>Result: \\(E[X | Y] = \\frac{Y}{2}\\)</p> <p>This makes perfect sense! If we know the remaining piece has length \\(y\\), then the expected position of the second break is exactly in the middle of that piece, which is \\(\\frac{y}{2}\\).</p> <p>Let's verify this:</p> <p>Step 1: Find \\(E[X]\\) directly</p> <ul> <li> <p>\\(Y \\sim \\text{Uniform}(0, L)\\), so \\(E[Y] = \\frac{L}{2}\\)</p> </li> <li> <p>\\(E[X] = E[E[X | Y]] = E\\left[\\frac{Y}{2}\\right] = \\frac{E[Y]}{2} = \\frac{L}{4}\\) (we will prove later why \\(E[X] = E[E[X | Y]] = E\\left[\\frac{Y}{2}\\right]\\), but it is a property)</p> </li> </ul> <p>Step 2: Find \\(E[X]\\) using the joint distribution</p> <ul> <li>Calculate the joint density</li> </ul> \\[f_{X,Y}(x,y) = f_{X|Y}(x|y) \\cdot f_Y(y) = \\frac{1}{y} \\cdot \\frac{1}{L} = \\frac{1}{Ly}\\] <p>The joint density of \\((X, Y)\\) is \\(f_{X,Y}(x,y) = \\frac{1}{Ly}\\) for \\(0 \\leq x \\leq y \\leq L\\)</p> <ul> <li>\\(E[X] = \\int_0^L \\int_0^y x \\cdot \\frac{1}{Ly} \\, dx \\, dy = \\int_0^L \\frac{y}{2L} \\, dy = \\frac{L}{4}\\)</li> </ul> <p>Both methods give the same result, confirming our calculation is correct.</p>"},{"location":"math/probability/conditional_expectation/#general-properties","title":"General Properties","text":"<p>1. Taking out what's known</p> <p>One of the most important properties of conditional expectation is the ability to \"take out what's known\":</p> \\[E[h(X)Y | X] = h(X)E[Y | X]\\] <p>where \\(h(X)\\) is any function of \\(X\\).</p> <p>Intuitive explanation: If we know the value of \\(X\\), then \\(h(X)\\) is just a constant (not random), so we can factor it out of the conditional expectation.</p> <p>Proof for discrete case:</p> \\[E[h(X)Y | X = x] = \\sum_y h(x) \\cdot y \\cdot P(Y = y | X = x) = h(x) \\sum_y y \\cdot P(Y = y | X = x) = h(x)E[Y | X = x]\\] <p>Proof for continuous case:</p> \\[E[h(X)Y | X = x] = \\int_{-\\infty}^{\\infty} h(x) \\cdot y \\cdot f_{Y|X}(y|x) \\, dy = h(x) \\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) \\, dy = h(x)E[Y | X = x]\\] <p>Examples:</p> <ul> <li> <p>\\(E[X^2Y | X] = X^2E[Y | X]\\) (since \\(X^2\\) is a function of \\(X\\))</p> </li> <li> <p>\\(E[\\sin(X)Y | X] = \\sin(X)E[Y | X]\\) (since \\(\\sin(X)\\) is a function of \\(X\\))</p> </li> <li> <p>\\(E[3Y | X] = 3E[Y | X]\\) (since 3 is a constant function of \\(X\\))</p> </li> </ul> <p>2. Independence property</p> <p>If \\(X\\) and \\(Y\\) are independent, then:</p> \\[E[Y | X] = E[Y]\\] <p>Intuitive explanation: If \\(X\\) and \\(Y\\) are independent, then knowing the value of \\(X\\) provides no information about \\(Y\\). Therefore, the conditional expectation of \\(Y\\) given \\(X\\) is the same as the unconditional expectation of \\(Y\\).</p> <p>Proof for discrete case:</p> \\[E[Y | X = x] = \\sum_y y \\cdot P(Y = y | X = x) = \\sum_y y \\cdot P(Y = y) = E[Y]\\] <p>where the second equality uses the fact that \\(P(Y = y | X = x) = P(Y = y)\\) when \\(X\\) and \\(Y\\) are independent.</p> <p>Proof for continuous case:</p> \\[E[Y | X = x] = \\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) \\, dy = \\int_{-\\infty}^{\\infty} y \\cdot f_Y(y) \\, dy = E[Y]\\] <p>where the second equality uses the fact that \\(f_{Y|X}(y|x) = f_Y(y)\\) when \\(X\\) and \\(Y\\) are independent.</p> <p>Examples:</p> <ul> <li> <p>If \\(X \\sim N(0,1)\\) and \\(Y \\sim N(0,1)\\) are independent, then \\(E[Y | X] = E[Y] = 0\\)</p> </li> <li> <p>If \\(X\\) is the number of heads in 10 coin flips and \\(Y\\) is the temperature tomorrow, and they're independent, then \\(E[Y | X] = E[Y]\\)</p> </li> </ul> <p>3. Iterated expectation (Adam's Law)</p> <p>The expectation of a conditional expectation equals the original expectation:</p> \\[E[E[Y | X]] = E[Y]\\] <p>Proof for discrete case:</p> \\[E[E[Y | X]] = \\sum_x E[Y | X = x] \\cdot P(X = x) = \\sum_x \\left(\\sum_y y \\cdot P(Y = y | X = x)\\right) \\cdot P(X = x)\\] \\[= \\sum_x \\sum_y y \\cdot P(Y = y | X = x) \\cdot P(X = x) = \\sum_x \\sum_y y \\cdot P(X = x, Y = y) = \\sum_y y \\cdot P(Y = y) = E[Y]\\] <p>Proof for continuous case:</p> \\[E[E[Y | X]] = \\int_{-\\infty}^{\\infty} E[Y | X = x] \\cdot f_X(x) \\, dx = \\int_{-\\infty}^{\\infty} \\left(\\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) \\, dy\\right) f_X(x) \\, dx\\] \\[= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) f_X(x) \\, dy \\, dx = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} y \\cdot f_{X,Y}(x,y) \\, dy \\, dx = E[Y]\\] <p>Examples:</p> <ul> <li> <p>In our stick-breaking example: \\(E[E[X | Y]] = E\\left[\\frac{Y}{2}\\right] = \\frac{E[Y]}{2} = \\frac{L}{4} = E[X]\\)</p> </li> <li> <p>If \\(Y\\) is test score and \\(X\\) is study hours: \\(E[E[Y | X]] = E[Y]\\) (the average test score across all students)</p> </li> </ul> <p>Why this matters: This property is fundamental to many probability calculations. It allows us to compute expectations by first conditioning on another variable, then taking the expectation of the conditional expectation. It's often easier to compute \\(E[Y | X]\\) first, then take its expectation, rather than computing \\(E[Y]\\) directly.</p> <p>4. Residual property</p> \\[E[(Y - E[Y | X])h(X)] = 0\\] <p>Proof:</p> \\[E[(Y - E[Y | X])h(X)] = E[Y \\cdot h(X) - E[Y | X] \\cdot h(X)]\\] \\[= E[Y \\cdot h(X)] - E[E[Y | X] \\cdot h(X)]\\] \\[= E[Y \\cdot h(X)] - E[h(X) \\cdot E[Y | X]]\\] <p>Now, using the \"taking out what's known\" property:</p> \\[E[h(X) \\cdot E[Y | X]] = E[h(X) \\cdot E[Y | X]] = E[E[h(X) \\cdot Y | X]] = E[h(X) \\cdot Y]\\] <p>Therefore:</p> \\[E[(Y - E[Y | X])h(X)] = E[Y \\cdot h(X)] - E[h(X) \\cdot Y] = 0\\] <p>Correlation interpretation:</p> <p>Since \\(E[(Y - E[Y | X])h(X)] = 0\\), we have:</p> \\[\\text{Cov}(Y - E[Y | X], h(X)) = E[(Y - E[Y | X])h(X)] - E[Y - E[Y | X]] \\cdot E[h(X)] = 0 - 0 \\cdot E[h(X)] = 0\\] <p>This shows that the residual \\(Y - E[Y | X]\\) is uncorrelated with any function \\(h(X)\\) of \\(X\\).</p> <p>Intuitive explanation: The residual \\(Y - E[Y | X]\\) represents the part of \\(Y\\) that cannot be predicted from \\(X\\). Since we've already extracted all the information that \\(X\\) can provide about \\(Y\\) (in the form of \\(E[Y | X]\\)), the remaining part should be uncorrelated with any function of \\(X\\).</p> <p>Examples:</p> <ul> <li> <p>\\(E[(Y - E[Y | X])X] = 0\\) (residual is uncorrelated with \\(X\\))</p> </li> <li> <p>\\(E[(Y - E[Y | X])X^2] = 0\\) (residual is uncorrelated with \\(X^2\\))</p> </li> <li> <p>\\(E[(Y - E[Y | X])\\sin(X)] = 0\\) (residual is uncorrelated with \\(\\sin(X)\\))</p> </li> </ul> <p>Geometric interpretation (Projection): The conditional expectation \\(E[Y | X]\\) is the projection of \\(Y\\) onto the space of all functions of \\(X\\), shown below as a plane. This is because in a certain sense, \\(E[Y | X]\\) is the function of \\(X\\) that is closest to \\(Y\\); we say that \\(E[Y | X]\\) is the projection of \\(Y\\) into the space of all functions of \\(X\\).The residual \\(Y - E[Y | X]\\) is orthogonal to the plane: it's perpendicular to (uncorrelated with) any function of \\(X\\).</p> <p></p> <p>The \"line\" from \\(Y\\) to \\(E[Y | X]\\) in the figure is orthogonal (perpendicular) to the \"plane\", since any other route from \\(Y\\) to \\(E[Y | X]\\) would be longer. This orthogonality turns out to be the geometric interpretation of the residual property.</p>"},{"location":"math/probability/conditional_expectation/#conditional-expectation-as-the-best-predictor","title":"Conditional Expectation as the Best Predictor","text":"<p>We can think of \\(E[Y | X]\\) as a prediction for \\(Y\\) based on \\(X\\). This is an extremely common problem in statistics: predict or estimate the future observations or unknown parameters based on data. The projection interpretation of conditional expectation implies that \\(E[Y | X]\\) is the best predictor of \\(Y\\) based on \\(X\\), in the sense that it is the function of \\(X\\) with the lowest mean squared error (expected squared difference between \\(Y\\) and the prediction of \\(Y\\)).</p> <p>Mathematical statement: For any function \\(g(X)\\) of \\(X\\):</p> \\[E[(Y - E[Y | X])^2] \\leq E[(Y - g(X))^2]\\] <p>Proof: Let \\(g(X)\\) be any function of \\(X\\). Then:</p> \\[E[(Y - g(X))^2] = E[(Y - E[Y | X] + E[Y | X] - g(X))^2]\\] \\[= E[(Y - E[Y | X])^2] + E[(E[Y | X] - g(X))^2] + 2E[(Y - E[Y | X])(E[Y | X] - g(X))]\\] <p>The cross term is zero because \\(E[Y | X] - g(X)\\) is a function of \\(X\\), and we know that \\(Y - E[Y | X]\\) is uncorrelated with any function of \\(X\\). Therefore:</p> \\[E[(Y - g(X))^2] = E[(Y - E[Y | X])^2] + E[(E[Y | X] - g(X))^2] \\geq E[(Y - E[Y | X])^2]\\] <p>The equality holds if and only if \\(g(X) = E[Y | X]\\) (almost surely).</p> <p>Why this matters: This shows that conditional expectation is not just a convenient mathematical tool, but the optimal predictor in the mean squared error sense. This is why it's so fundamental to statistics, machine learning, and any field that involves prediction.</p>"},{"location":"math/probability/conditional_expectation/#case-study-linear-regression","title":"Case study: Linear Regression","text":"<p>An extremely widely used method for data analysis in statistics is linear regression. In its most basic form, the linear regression model uses a single explanatory variable \\(X\\) to predict a response variable \\(Y\\), and it assumes that the conditional expectation of \\(Y\\) is linear in \\(X\\):</p> \\[E[Y | X] = a + bX\\] <p>Show that an equivalent way to express this is to write</p> \\[Y = a + bX + \\epsilon\\] <p>where \\(\\epsilon\\) is an r.v. (called the error) with \\(E[\\epsilon | X] = 0\\).</p> <p>Solution: Let \\(Y = a + bX + \\epsilon\\), with \\(E[\\epsilon | X] = 0\\). Then by linearity:</p> \\[E[Y | X] = E[a | X] + E[bX | X] + E[\\epsilon | X] = a + bX\\] <p>Conversely, suppose that \\(E[Y | X] = a + bX\\), and define:</p> \\[\\epsilon = Y - (a + bX)\\] <p>Then \\(Y = a + bX + \\epsilon\\), with:</p> \\[E[\\epsilon | X] = E[Y | X] - E[a + bX | X] = E[Y | X] - (a + bX) = 0\\]"},{"location":"math/probability/conditional_probability/","title":"Conditional Probability","text":""},{"location":"math/probability/conditional_probability/#independent-events","title":"Independent Events","text":"<p>Two events \\(A\\) and \\(B\\) are independent if the occurrence of one event does not affect the probability of the occurrence of the other event.</p> <p>Mathematical Definition: Events \\(A\\) and \\(B\\) are independent if and only if:</p> \\[P(A \\cap B) = P(A) \\cdot P(B)\\] <p>Independence means that knowing whether event \\(B\\) occurred gives us no information about whether event \\(A\\) occurred, and vice versa.</p> <p>Example 1: coin flips Event \\(A\\): First coin flip is heads. Event \\(B\\): Second coin flip is heads. These events are independent because the result of the first flip doesn't affect the second flip. \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{1}{2}\\), \\(P(A \\cap B) = \\frac{1}{4} = \\frac{1}{2} \\cdot \\frac{1}{2}\\) \u2713</p> <p>Example 2: drawing cards (with replacement) Event \\(A\\): First card drawn is red. Event \\(B\\): Second card drawn is red (with replacement). These events are independent because we replace the first card. \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{1}{2}\\), \\(P(A \\cap B) = \\frac{1}{4} = \\frac{1}{2} \\cdot \\frac{1}{2}\\) \u2713</p> <p>Example 3: drawing cards (without replacement) - NOT independent Event \\(A\\): First card drawn is red. Event \\(B\\): Second card drawn is red (without replacement). These events are NOT independent because drawing a red card first affects the probability of drawing red second.</p> <p>It's crucial to understand the difference between independent events and disjoint (mutually exclusive) events.</p> <p>Independent Events:</p> <ul> <li> <p>Can occur together: \\(P(A \\cap B) = P(A) \\cdot P(B) &gt; 0\\) (if both \\(P(A) &gt; 0\\) and \\(P(B) &gt; 0\\))</p> </li> <li> <p>Knowledge of one event doesn't affect the probability of the other</p> </li> <li> <p>Example: Rolling a die twice - getting a 6 on the first roll and a 6 on the second roll</p> </li> </ul> <p>Disjoint Events:</p> <ul> <li> <p>Cannot occur together: \\(P(A \\cap B) = 0\\)</p> </li> <li> <p>If one event occurs, the other cannot occur</p> </li> <li> <p>Example: Getting heads and tails on a single coin flip</p> </li> </ul> <p>Key insight: If two events \\(A\\) and \\(B\\) are disjoint with \\(P(A) &gt; 0\\) and \\(P(B) &gt; 0\\), then they are NOT independent. This is because knowing that \\(A\\) occurred tells us that \\(B\\) definitely did not occur.</p> <p>Three events \\(A\\), \\(B\\), and \\(C\\) are mutually independent if and only if all of the following conditions hold:</p> <p>Pairwise independence: \\(P(A \\cap B) = P(A) \\cdot P(B)\\). \\(P(A \\cap C) = P(A) \\cdot P(C)\\). \\(P(B \\cap C) = P(B) \\cdot P(C)\\)</p> <p>Triple independence: \\(P(A \\cap B \\cap C) = P(A) \\cdot P(B) \\cdot P(C)\\)</p> <p>Important: Pairwise independence alone is not sufficient for mutual independence. All four conditions must be satisfied.</p> <p>How should you update your beliefs/probabilities based on new evidence? This is a pretty broad question.</p>"},{"location":"math/probability/conditional_probability/#definition-of-conditional-probability","title":"Definition of Conditional Probability","text":"<p>Conditional probability is the probability of an event \\(A\\) occurring given that another event \\(B\\) has already occurred.</p> <p>Mathematical Definition: The conditional probability of event \\(A\\) given event \\(B\\) is:</p> \\[P(A | B) = \\frac{P(A \\cap B)}{P(B)}\\] <p>provided that \\(P(B) &gt; 0\\).</p> <p>\\(P(A | B)\\) is read as \"the probability of \\(A\\) given \\(B\\)\". It represents the updated probability of \\(A\\) after we know that \\(B\\) has occurred. We restrict our sample space to only those outcomes where \\(B\\) occurs.</p> <p>Example: Consider drawing cards from a standard deck. Let \\(A\\) = \"card is an Ace\". Let \\(B\\) = \"card is red\"</p> <p>\\(P(A | B) = \\frac{P(\\text{card is red Ace})}{P(\\text{card is red})} = \\frac{2/52}{26/52} = \\frac{2}{26} = \\frac{1}{13}\\)</p> <p>This makes sense: among the 26 red cards, only 2 are Aces.</p> <p>Key Properties:</p> <ul> <li> <p>\\(0 \\leq P(A | B) \\leq 1\\)</p> </li> <li> <p>\\(P(B | B) = 1\\) (if \\(P(B) &gt; 0\\))</p> </li> <li> <p>If \\(A\\) and \\(B\\) are independent, then \\(P(A | B) = P(A)\\)</p> </li> </ul>"},{"location":"math/probability/conditional_probability/#joint-probability","title":"Joint Probability","text":"<p>Joint probability is the probability that two or more events occur simultaneously.</p>"},{"location":"math/probability/conditional_probability/#two-events","title":"Two events","text":"<p>For events \\(A\\) and \\(B\\), the joint probability is:</p> \\[P(A \\cap B) = P(A \\text{ and } B)\\] <p>Properties:</p> <ul> <li> <p>\\(0 \\leq P(A \\cap B) \\leq 1\\)</p> </li> <li> <p>\\(P(A \\cap B) = P(B \\cap A)\\) (commutative)</p> </li> <li> <p>\\(P(A \\cap B) = P(A | B) \\cdot P(B) = P(B | A) \\cdot P(A)\\) (multiplication rule)</p> </li> </ul>"},{"location":"math/probability/conditional_probability/#three-events","title":"Three events","text":"<p>For events \\(A\\), \\(B\\), and \\(C\\), the joint probability is:</p> \\[P(A \\cap B \\cap C) = P(A \\text{ and } B \\text{ and } C)\\] <p>Properties:</p> <ul> <li> <p>\\(0 \\leq P(A \\cap B \\cap C) \\leq 1\\)</p> </li> <li> <p>\\(P(A \\cap B \\cap C) = P(B \\cap A \\cap C) = P(C \\cap A \\cap B)\\) (commutative)</p> </li> <li> <p>\\(P(A \\cap B \\cap C) = P(A | B \\cap C) \\cdot P(B | C) \\cdot P(C)\\) (chain rule)</p> </li> </ul> <p>This can be shown as follows:</p> <p>\\(P(A | B \\cap C) = \\frac{P(A \\cap B \\cap C)}{P(B \\cap C)}\\).</p> <p>Thus, \\(P(A \\cap B \\cap C) = P(A | B \\cap C) \\cdot P(B \\cap C)\\).</p> <p>Applying multiplication rule to \\(P(B \\cap C)\\), we get: \\(P(B \\cap C) = P(B | C) \\cdot P(C)\\)</p> <p>Substituting into the equation for joint probability, \\(P(A \\cap B \\cap C) = P(A | B \\cap C) \\cdot P(B | C) \\cdot P(C)\\).</p> <p>This derivation shows how the chain rule naturally extends the multiplication rule to multiple events.</p>"},{"location":"math/probability/conditional_probability/#n-events","title":"n events","text":"<p>For events \\(A_1, A_2, \\ldots, A_n\\), the joint probability is:</p> \\[P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) = P(A_1 \\text{ and } A_2 \\text{ and } \\cdots \\text{ and } A_n)\\] <p>Properties:</p> <ul> <li> <p>\\(0 \\leq P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) \\leq 1\\)</p> </li> <li> <p>\\(P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) = P(A_1 | A_2 \\cap \\cdots \\cap A_n) \\cdot P(A_2 | A_3 \\cap \\cdots \\cap A_n) \\cdots P(A_{n-1} | A_n) \\cdot P(A_n)\\) (chain rule)</p> </li> <li> <p>If events are mutually independent, then \\(P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) = P(A_1) \\cdot P(A_2) \\cdots P(A_n)\\)</p> </li> </ul>"},{"location":"math/probability/conditional_probability/#law-of-total-probability","title":"Law of Total Probability","text":"<p>The law of total probability is a fundamental rule that allows us to calculate the probability of an event by considering all possible ways it can occur.</p> <p>We're breaking down the probability of \\(A\\) by considering how it can occur in each of the different scenarios \\(B_i\\). Since \\(\\{B_1, B_2, \\ldots, B_n\\}\\) is a partition of the sample space, we can write:</p> \\[A = A \\cap S = A \\cap \\left(\\bigcup_{i=1}^n B_i\\right) = \\bigcup_{i=1}^n (A \\cap B_i)\\] <p>Since the \\(B_i\\)'s are mutually exclusive, the events \\((A \\cap B_i)\\) are also mutually exclusive. Therefore:</p> \\[P(A) = P\\left(\\bigcup_{i=1}^n (A \\cap B_i)\\right) = \\sum_{i=1}^n P(A \\cap B_i)\\] <p>By the definition of conditional probability, \\(P(A \\cap B_i) = P(A | B_i) \\cdot P(B_i)\\) for each \\(i\\).</p> <p>Substituting into the previous equation:</p> \\[P(A) = \\sum_{i=1}^n P(A | B_i) \\cdot P(B_i)\\] <p>Mathematical Definition: For any event \\(A\\) and a partition \\(\\{B_1, B_2, \\ldots, B_n\\}\\) of the sample space (i.e., events that are mutually exclusive and exhaustive), we have:</p> \\[P(A) = \\sum_{i=1}^n P(A | B_i) \\cdot P(B_i)\\] <p>where \\(B_i \\cap B_j = \\emptyset\\) for all \\(i \\neq j\\) (mutually exclusive) and \\(\\bigcup_{i=1}^n B_i = S\\) (exhaustive - their union is the entire sample space)</p> <p>Example: Consider two urns. Urn 1 contains 3 red balls and 2 blue balls. Urn 2 contains 1 red ball and 4 blue balls. You randomly select an urn (each with probability \\(\\frac{1}{2}\\)) and then draw a ball from that urn. What is the probability of drawing a red ball?</p> <p>Let \\(A\\) = \"draw a red ball\". Let \\(B_1\\) = \"select Urn 1\". Let \\(B_2\\) = \"select Urn 2\". Then, \\(P(B_1) = \\frac{1}{2}\\), \\(P(B_2) = \\frac{1}{2}\\). \\(P(A | B_1) = \\frac{3}{5}\\) (3 red out of 5 balls in Urn 1). \\(P(A | B_2) = \\frac{1}{5}\\) (1 red out of 5 balls in Urn 2)</p> <p>By the law of total probability:</p> \\[P(A) = P(A | B_1) \\cdot P(B_1) + P(A | B_2) \\cdot P(B_2) = \\frac{3}{5} \\cdot \\frac{1}{2} + \\frac{1}{5} \\cdot \\frac{1}{2} = \\frac{3}{10} + \\frac{1}{10} = \\frac{4}{10} = \\frac{2}{5}\\]"},{"location":"math/probability/conditional_probability/#prior-and-posterior-probabilities","title":"Prior and Posterior Probabilities","text":"<p>In many probability problems, we distinguish between prior and posterior probabilities, which represent our beliefs before and after observing evidence.</p> <p>Prior Probability: The probability of an event before we observe any evidence. This represents our initial belief or knowledge about the event.</p> <p>Posterior Probability: The probability of an event after we observe evidence. This is our updated belief after incorporating new information.</p> <p>If we have Event \\(H\\) (hypothesis) and Event \\(E\\) (evidence), then:</p> <ul> <li> <p>Prior: \\(P(H)\\) is the prior probability of hypothesis \\(H\\)</p> </li> <li> <p>Posterior: \\(P(H | E)\\) is the posterior probability of hypothesis \\(H\\) given evidence \\(E\\)</p> </li> </ul>"},{"location":"math/probability/conditional_probability/#conditional-independence","title":"Conditional Independence","text":"<p>Conditional independence is a concept where two events are independent given knowledge of a third event, even if they might not be independent without that knowledge.</p> <p>Mathematical Definition: Events \\(A\\) and \\(B\\) are conditionally independent given event \\(C\\) if and only if:</p> \\[P(A \\cap B | C) = P(A | C) \\cdot P(B | C)\\] <p>provided that \\(P(C) &gt; 0\\).</p> <p>Equivalently, this can be written as:</p> \\[P(A | B \\cap C) = P(A | C)\\] <p>This means that knowing both \\(B\\) and \\(C\\) gives us no more information about \\(A\\) than knowing just \\(C\\) alone.</p> <p>Example: Consider drawing two cards from a standard deck without replacement.</p> <p>Let \\(A\\) = \"first card is red\". \\(B\\) = \"second card is red\". \\(C\\) = \"both cards are the same color\"</p> <p>Check regular independence:</p> <ul> <li> <p>\\(P(A) = \\frac{26}{52} = \\frac{1}{2}\\)</p> </li> <li> <p>\\(P(B) = \\frac{26}{52} = \\frac{1}{2}\\) (by symmetry)</p> </li> <li> <p>\\(P(A \\cap B) = \\frac{26 \\times 25}{52 \\times 51} = \\frac{25}{102}\\)</p> </li> </ul> <p>Since \\(P(A \\cap B) = \\frac{25}{102} \\neq \\frac{1}{4} = P(A) \\cdot P(B)\\), events \\(A\\) and \\(B\\) are not independent.</p> <p>But let's modify this: suppose we draw cards with replacement. Then:</p> <ul> <li> <p>\\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{1}{2}\\), \\(P(A \\cap B) = \\frac{1}{4}\\)</p> </li> <li> <p>So \\(A\\) and \\(B\\) are independent</p> </li> </ul> <p>Check conditional independence given \\(C\\): Given that both cards are the same color, we know they're either both red or both black.</p> <ul> <li> <p>\\(P(A | C) = P(\\text{first red} | \\text{both same color}) = \\frac{1}{2}\\)</p> </li> <li> <p>\\(P(B | C) = P(\\text{second red} | \\text{both same color}) = \\frac{1}{2}\\)</p> </li> <li> <p>\\(P(A \\cap B | C) = P(\\text{both red} | \\text{both same color}) = \\frac{1}{2}\\)</p> </li> </ul> <p>Since \\(P(A \\cap B | C) = \\frac{1}{2} \\neq \\frac{1}{4} = P(A | C) \\cdot P(B | C)\\), events \\(A\\) and \\(B\\) are not conditionally independent given \\(C\\).</p>"},{"location":"math/probability/continuous_distributions/","title":"Continuous Distributions","text":"<p>Continuous distributions are probability distributions for random variables that can take on any value in a continuous range (typically an interval of real numbers). Unlike discrete distributions, continuous random variables have probability density functions (PDFs) rather than probability mass functions (PMFs).</p> <p>For a continuous random variable \\(X\\), the probability of \\(X\\) taking any specific value is exactly 0:</p> \\[P(X = x) = 0 \\quad \\text{for any specific value } x\\] <p>Since \\(X\\) can take uncountably many values in a continuous range, the probability of any single value must be 0. Otherwise, the total probability would exceed 1.</p> <p>Consequence: We cannot use probability mass functions (PMFs) like we do for discrete random variables, because \\(P(X = x)\\) is always 0. Instead, we need probability density functions (PDFs) to describe the distribution.</p>"},{"location":"math/probability/continuous_distributions/#probability-density-function-pdf","title":"Probability Density Function (PDF)","text":"<p>Definition: A function \\(f_X(x)\\) such that \\(P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx\\) for all \\(a\\) and \\(b\\).</p> <p>When \\(a = b\\), the interval \\([a, b]\\) becomes a single point, and we have:</p> \\[P(a \\leq X \\leq a) = P(X = a) = \\int_a^a f_X(x) dx = 0\\] <p>This confirms our earlier statement that \\(P(X = x) = 0\\) for any specific value \\(x\\) in a continuous distribution. The integral over a single point (which has zero length) is always 0.</p> <p>Properties: </p> <ul> <li> <p>\\(f_X(x) \\geq 0\\) for all \\(x\\)</p> </li> <li> <p>\\(\\int_{-\\infty}^{\\infty} f_X(x) dx = 1\\)</p> </li> </ul> <p>What does \\(f_X(x)\\) actually mean?</p> <p>The PDF \\(f_X(x)\\) represents the probability density at point \\(x\\). To understand this, consider a small interval around \\(x\\):</p> \\[P(x - \\frac{\\epsilon}{2} \\leq X \\leq x + \\frac{\\epsilon}{2}) = \\int_{x - \\frac{\\epsilon}{2}}^{x + \\frac{\\epsilon}{2}} f_X(t) dt\\] <p>For very small \\(\\epsilon\\), this integral is approximately:</p> \\[P(x - \\frac{\\epsilon}{2} \\leq X \\leq x + \\frac{\\epsilon}{2}) \\approx f_X(x) \\cdot \\epsilon\\] \\[f_X(x) \\approx \\frac{P(x - \\frac{\\epsilon}{2} \\leq X \\leq x + \\frac{\\epsilon}{2})}{\\epsilon}\\] <p>Interpretation: \\(f_X(x)\\) tells us how much probability \"mass\" is concentrated around the point \\(x\\). The probability of falling in a small interval around \\(x\\) is approximately \\(f_X(x)\\) times the length of that interval. Think of probability density like physical density:</p> <ul> <li> <p>Probability mass = \\(P(x - \\frac{\\epsilon}{2} \\leq X \\leq x + \\frac{\\epsilon}{2})\\) (the \"amount\" of probability)</p> </li> <li> <p>Volume = \\(\\epsilon\\) (the \"size\" of the interval)</p> </li> <li> <p>Density = \\(f_X(x)\\) (how \"concentrated\" the probability is)</p> </li> </ul> <p>Just as \\(\\text{density} = \\frac{\\text{mass}}{\\text{volume}}\\) in physics, we have:</p> \\[\\text{probability density} = \\frac{\\text{probability mass}}{\\text{interval length}}\\] <p>This explains why \\(f_X(x)\\) can be greater than 1 - it's not a probability, but a density!</p> <p>Key insight: While \\(P(X = x) = 0\\), the density \\(f_X(x)\\) tells us how likely \\(X\\) is to fall near \\(x\\) relative to other points.</p>"},{"location":"math/probability/continuous_distributions/#cumulative-distribution-function-cdf","title":"Cumulative Distribution Function (CDF)","text":"<p>The Cumulative Distribution Function (CDF) of a continuous random variable \\(X\\) is defined as:</p> \\[F_X(x) = P(X \\leq x) = \\int_{-\\infty}^x f_X(t) dt\\] <p>What it represents: \\(F_X(x)\\) gives the probability that \\(X\\) takes a value less than or equal to \\(x\\). Here, \\(f_X(t)\\) is the probability density function (PDF) of \\(X\\).</p> <p>Properties</p> <ol> <li> <p>Non-decreasing: \\(F_X(x_1) \\leq F_X(x_2)\\) whenever \\(x_1 \\leq x_2\\)</p> </li> <li> <p>Limits: \\(\\lim_{x \\to -\\infty} F_X(x) = 0\\) and \\(\\lim_{x \\to \\infty} F_X(x) = 1\\)</p> </li> <li> <p>Right-continuous: \\(F_X(x) = \\lim_{h \\to 0^+} F_X(x + h)\\)</p> </li> <li> <p>Probability interpretation: \\(P(a &lt; X \\leq b) = F_X(b) - F_X(a)\\)</p> </li> </ol> <p>Since the CDF is the integral of the PDF, we can recover the PDF by differentiating the CDF:</p> \\[f_X(x) = \\frac{d}{dx} F_X(x) = F_X'(x)\\] <p>Example: If \\(F_X(x) = 1 - e^{-x}\\) for \\(x \\geq 0\\) (and \\(F_X(x) = 0\\) for \\(x &lt; 0\\)), then:</p> \\[f_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx}(1 - e^{-x}) = e^{-x}\\] <p>This gives us the PDF: \\(f_X(x) = e^{-x}\\) for \\(x \\geq 0\\) (and \\(f_X(x) = 0\\) for \\(x &lt; 0\\)).</p> <p>Key insight: The PDF tells us where the CDF is changing rapidly (high density) versus slowly (low density).</p> <p>Why CDFs are useful</p> <ol> <li> <p>Probability calculations: Easy to find \\(P(X \\leq x)\\) or \\(P(a &lt; X \\leq b)\\)</p> </li> <li> <p>Distribution comparison: Can compare distributions by plotting CDFs</p> </li> <li> <p>Quantiles: The \\(p\\)-th quantile \\(x_p\\) satisfies \\(F_X(x_p) = p\\)</p> </li> </ol>"},{"location":"math/probability/continuous_distributions/#expectation-of-a-continuous-random-variable","title":"Expectation of a Continuous Random Variable","text":"<p>The expectation (or expected value) of a continuous random variable \\(X\\) is defined as:</p> \\[E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) dx\\] <p>What it represents: \\(E[X]\\) is the \"center of mass\" or \"average value\" of the distribution, representing the long-run average if we were to sample from this distribution many times. Here, \\(f_X(x)\\) is the probability density function (PDF) of \\(X\\).</p> <p>Think of the PDF as a \"weight distribution\" along the real line:</p> <ul> <li> <p>\\(f_X(x)\\): How much \"weight\" (probability density) is at point \\(x\\)</p> </li> <li> <p>\\(x \\cdot f_X(x)\\): The \"weighted position\" at point \\(x\\)</p> </li> <li> <p>\\(\\int_{-\\infty}^{\\infty} x \\cdot f_X(x) dx\\): The total \"center of mass\" of all the weight</p> </li> </ul>"},{"location":"math/probability/continuous_distributions/#variance","title":"Variance","text":"<p>The variance of a random variable measures how spread out the distribution is around its mean. It's defined as the expected squared deviation from the mean.</p> <p>For any random variable \\(X\\) (discrete or continuous):</p> \\[\\text{Var}(X) = E[(X - E[X])^2]\\] <p>Why not other measures of deviation?</p> <p>Problem 1: \\(E[X - E[X]]\\)</p> <p>This would always equal 0 because:</p> \\[E[X - E[X]] = E[X] - E[E[X]] = E[X] - E[X] = 0\\] <p>The average deviation from the mean is always 0, so this tells us nothing about spread.</p> <p>Problem 2: \\(E[|X - E[X]|]\\) (Mean Absolute Deviation)</p> <p>While this measures spread, it has mathematical disadvantages:</p> <ul> <li> <p>Non-differentiable: The absolute value function isn't smooth, making calculus difficult</p> </li> <li> <p>Harder to work with: Properties like additivity are more complex</p> </li> </ul> <p>Why \\(E[(X - E[X])^2]\\) is perfect:</p> <ol> <li> <p>Always positive: \\((X - E[X])^2 \\geq 0\\) for all \\(X\\), so variance is always non-negative</p> </li> <li> <p>Mathematically tractable: Squaring gives smooth, differentiable functions</p> </li> <li> <p>Additivity: Variance of sum of independent variables equals sum of variances</p> </li> <li> <p>Theoretical elegance: Leads to beautiful results in probability theory</p> </li> <li> <p>Statistical properties: Optimal for many statistical procedures</p> </li> </ol> <p>Alternative formula (often easier to compute):</p> \\[\\text{Var}(X) = E[X^2] - (E[X])^2\\] <p>Discrete Case</p> <p>For a discrete random variable \\(X\\) with PMF \\(p_X(x)\\):</p> \\[\\text{Var}(X) = \\sum_x (x - E[X])^2 \\cdot p_X(x) = \\sum_x x^2 \\cdot p_X(x) - (E[X])^2\\] <p>Example: For a Bernoulli random variable \\(X \\sim \\text{Bernoulli}(p)\\):</p> <ul> <li> <p>\\(E[X] = p\\)</p> </li> <li> <p>\\(E[X^2] = 0^2 \\cdot (1-p) + 1^2 \\cdot p = p\\)</p> </li> <li> <p>\\(\\text{Var}(X) = E[X^2] - (E[X])^2 = p - p^2 = p(1-p)\\)</p> </li> </ul> <p>Continuous Case</p> <p>For a continuous random variable \\(X\\) with PDF \\(f_X(x)\\):</p> \\[\\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - E[X])^2 \\cdot f_X(x) dx = \\int_{-\\infty}^{\\infty} x^2 \\cdot f_X(x) dx - (E[X])^2\\] <p>Example: For an exponential random variable \\(X \\sim \\text{Exponential}(\\lambda)\\):</p> <ul> <li> <p>\\(E[X] = \\frac{1}{\\lambda}\\)</p> </li> <li> <p>\\(E[X^2] = \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = \\frac{2}{\\lambda^2}\\) (using integration by parts)</p> </li> <li> <p>\\(\\text{Var}(X) = E[X^2] - (E[X])^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\\)</p> </li> </ul>"},{"location":"math/probability/continuous_distributions/#standard-deviation","title":"Standard Deviation","text":"<p>The standard deviation of a random variable \\(X\\) is the square root of its variance:</p> \\[\\sigma_X = \\sqrt{\\text{Var}(X)} = \\sqrt{E[(X - E[X])^2]}\\] <p>What it represents: Standard deviation measures spread in the same units as the original random variable, making it more interpretable than variance.</p> <p>Variance has units that are the square of the original units. For example, if \\(X\\) measures height in meters, \\(\\text{Var}(X)\\) is in square meters. If \\(X\\) measures time in seconds, \\(\\text{Var}(X)\\) is in square seconds. Standard deviation has the same units as \\(X\\).</p>"},{"location":"math/probability/continuous_distributions/#uniform-distribution","title":"Uniform Distribution","text":"<p>The uniform distribution is the simplest continuous distribution, where every value in an interval has equal probability density.</p> <p>A random variable \\(X\\) follows a uniform distribution on the interval \\([a, b]\\) (denoted \\(X \\sim \\text{Uniform}(a, b)\\)) if its PDF is:</p> \\[f_X(x) = \\begin{cases}  \\frac{1}{b-a} &amp; \\text{if } a \\leq x \\leq b \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>Parameters:</p> <ul> <li> <p>\\(a\\): Lower bound of the interval</p> </li> <li> <p>\\(b\\): Upper bound of the interval (\\(b &gt; a\\))</p> </li> <li> <p>Support: \\(X\\) takes values in \\([a, b]\\)</p> </li> </ul> <p>Every point in \\([a, b]\\) has the same probability density. The PDF is a horizontal line (rectangle). If you randomly pick a point from \\([a, b]\\), every point is equally likely</p> <p>Examples:</p> <ul> <li> <p>Random number generation between 0 and 1</p> </li> <li> <p>Random angle selection (0 to 2\u03c0)</p> </li> <li> <p>Random time selection within an hour</p> </li> <li> <p>Random position selection along a line segment</p> </li> </ul> <p>The cumulative distribution function is:</p> \\[F_X(x) = \\begin{cases} 0 &amp; \\text{if } x &lt; a \\\\ \\frac{x-a}{b-a} &amp; \\text{if } a \\leq x \\leq b \\\\ 1 &amp; \\text{if } x &gt; b \\end{cases}\\] <p>\\(F_X(x)\\) increases linearly from 0 to 1 as \\(x\\) goes from \\(a\\) to \\(b\\).</p> <p>Expectation:</p> \\[E[X] = \\int_a^b x \\cdot \\frac{1}{b-a} dx = \\frac{1}{b-a} \\int_a^b x dx = \\frac{1}{b-a} \\cdot \\frac{b^2 - a^2}{2} = \\frac{a + b}{2}\\] <p>Variance:</p> \\[\\text{Var}(X) = E[X^2] - (E[X])^2\\] <p>First, calculate \\(E[X^2]\\):</p> \\[E[X^2] = \\int_a^b x^2 \\cdot \\frac{1}{b-a} dx = \\frac{1}{b-a} \\cdot \\frac{b^3 - a^3}{3} = \\frac{b^3 - a^3}{3(b-a)}\\] <p>Then:</p> \\[\\text{Var}(X) = \\frac{b^3 - a^3}{3(b-a)} - \\left(\\frac{a + b}{2}\\right)^2 = \\frac{(b-a)^2}{12}\\] <p>Standard deviation:</p> \\[\\sigma_X = \\frac{b-a}{2\\sqrt{3}}\\]"},{"location":"math/probability/continuous_distributions/#universality-of-the-uniform-distribution","title":"Universality of the Uniform Distribution","text":"<p>Given a \\(\\text{Uniform}(0, 1)\\) random variable, we can construct a random variable with any continuous distribution we want.</p> <p>We call this the universality of the Uniform, because it tells us the Uniform is a universal starting point for building random variables with other distributions.</p> <p>Theorem (Universality of the Uniform). Let \\(F\\) be a CDF which is a continuous function and strictly increasing on the support of the distribution. This ensures that the inverse function \\(F^{-1}\\) exists, as a function from \\((0, 1)\\) to \\(\\mathbb{R}\\). We then have the following results:</p> <ol> <li> <p>Let \\(U \\sim \\text{Unif}(0, 1)\\) and \\(X = F^{-1}(U)\\). Then \\(X\\) is a random variable with CDF \\(F\\).</p> </li> <li> <p>Let \\(X\\) be a random variable with CDF \\(F\\). Then \\(F(X) \\sim \\text{Unif}(0, 1)\\).</p> </li> </ol> <p>Part 1 is the inverse CDF method we discussed earlier - it shows how to generate any distribution from uniform.</p> <p>Part 2 is the probability integral transform - it shows that applying any CDF to its own random variable gives a uniform distribution.</p> <p>Let's make sure we understand what each part of the theorem is saying. The first part says that if we start with \\(U \\sim \\text{Unif}(0, 1)\\) and a CDF \\(F\\), then we can create a random variable whose CDF is \\(F\\) by plugging \\(U\\) into the inverse CDF \\(F^{-1}\\). Since \\(F^{-1}\\) is a function (known as the quantile function), \\(U\\) is a random variable, and a function of a random variable is a random variable, \\(F^{-1}(U)\\) is a random variable; universality of the Uniform says its CDF is \\(F\\).</p> <p>The second part of the theorem goes in the reverse direction, starting from a random variable \\(X\\) whose CDF is \\(F\\) and then creating a \\(\\text{Unif}(0, 1)\\) random variable. Again, \\(F\\) is a function, \\(X\\) is a random variable, and a function of a random variable is a random variable, so \\(F(X)\\) is a random variable. Since any CDF is between 0 and 1 everywhere, \\(F(X)\\) must take values between 0 and 1. Universality of the Uniform says that the distribution of \\(F(X)\\) is Uniform on \\((0, 1)\\).</p> <p>The second part of universality of the Uniform involves plugging a random variable \\(X\\) into its own CDF \\(F\\). This may seem strangely self-referential, but it makes sense because \\(F\\) is just a function (that satisfies the properties of a valid CDF), and a function of a random variable is a random variable. There is a potential notational confusion, however: \\(F(x) = P(X \\leq x)\\) by definition, but it would be incorrect to say \"\\(F(X) = P(X \\leq X) = 1\\)\". Rather, we should first find an expression for the CDF as a function of \\(x\\), then replace \\(x\\) with \\(X\\) to obtain a random variable. For example, if the CDF of \\(X\\) is \\(F(x) = 1 - e^{-x}\\) for \\(x &gt; 0\\), then \\(F(X) = 1 - e^{-X}\\).</p> <p>Proof.</p> <p>Let \\(U \\sim \\text{Unif}(0, 1)\\) and \\(X = F^{-1}(U)\\). For all real \\(x\\),</p> \\[P(X \\leq x) = P(F^{-1}(U) \\leq x) = P(U \\leq F(x)) = F(x);\\] <p>Why does \\(P(F^{-1}(U) \\leq x) = P(U \\leq F(x))\\) hold?</p> <p>This is a key step that uses the properties of inverse functions. Since \\(F\\) is strictly increasing, we have:</p> \\[F^{-1}(U) \\leq x \\quad \\text{if and only if} \\quad U \\leq F(x)\\] <p>If the inverse function \\(F^{-1}\\) applied to \\(U\\) gives a value \\(\\leq x\\), then \\(U\\) must be \\(\\leq F(x)\\). This is because \\(F^{-1}\\) \"undoes\" what \\(F\\) does, so the inequality reverses when we apply \\(F\\) to both sides.</p> <p>This shows the two events are equivalent, so their probabilities are equal.</p> <p>For the last equality, we used the fact that \\(P(U \\leq u) = u\\) for \\(u \\in (0, 1)\\).</p> <p>This is the fundamental property of the uniform distribution on \\((0, 1)\\). Since \\(U \\sim \\text{Unif}(0, 1)\\), the probability that \\(U\\) falls in any interval \\([0, u]\\) is exactly the length of that interval, which is \\(u\\).</p> <p>In our proof: We had \\(P(U \\leq F(x))\\), and since \\(F(x)\\) is a value between 0 and 1 (because \\(F\\) is a CDF), this equals exactly \\(F(x)\\) by the uniform distribution property.</p> <p>Let \\(X\\) have CDF \\(F\\), and find the CDF of \\(Y = F(X)\\). Since \\(Y\\) takes values in \\((0, 1)\\), \\(P(Y \\leq y)\\) equals 0 for \\(y \\leq 0\\) and equals 1 for \\(y \\geq 1\\). For \\(y \\in (0, 1)\\),</p> \\[P(Y \\leq y) = P(F(X) \\leq y) = P(X \\leq F^{-1}(y)) = F(F^{-1}(y)) = y.\\] <p>Thus \\(Y\\) has the \\(\\text{Unif}(0, 1)\\) CDF.</p> <p>Example: A large number of students take a certain exam, graded on a scale from 0 to 100. Let \\(X\\) be the score of a random student. Continuous distributions are easier to deal with here, so let's approximate the discrete distribution of scores using a continuous distribution. Suppose that \\(X\\) is continuous, with a CDF \\(F\\) that is strictly increasing. In reality, there are only finitely many students and only finitely many possible scores, but a continuous distribution may be a good approximation.</p> <p>Suppose that the median score on the exam is 60, i.e., half of the students score above 60 and the other half score below 60 (a convenient aspect of assuming a continuous distribution is that we don't need to worry about how many students had scores equal to 60). That is, \\(F(60) = 1/2\\); or, equivalently, \\(F^{-1}(1/2) = 60\\).</p> <p>If Fred scores a 72 on the exam, then his percentile is the fraction of students who score below a 72. This is \\(F(72)\\), which is some number in \\((1/2, 1)\\) since 72 is above the median. In general, a student with score \\(x\\) has percentile \\(F(x)\\). Going the other way, if we start with a percentile, say 0.95, then \\(F^{-1}(0.95)\\) is the score that has that percentile. A percentile is also called a quantile, which is why \\(F^{-1}\\) is called the quantile function. The function \\(F\\) converts scores to quantiles, and the function \\(F^{-1}\\) converts quantiles to scores.</p> <p>The strange operation of plugging \\(X\\) into its own CDF now has a natural interpretation: \\(F(X)\\) is the percentile attained by a random student. It often happens that the distribution of scores on an exam looks very non-Uniform. For example, there is no reason to think that 10% of the scores are between 70 and 80, even though \\((70, 80)\\) covers 10% of the range of possible scores.</p> <p>On the other hand, the distribution of percentiles of the students is Uniform: the universality property says that \\(F(X) \\sim \\text{Unif}(0, 1)\\). For example, 50% of the students have a percentile of at least 0.5. Universality of the Uniform is expressing the fact that 10% of the students have a percentile between 0 and 0.1, 10% of the students have a percentile between 0.1 and 0.2, 10% of the students have a percentile between 0.2 and 0.3, and so on\u2014a fact that is clear from the definition of percentile.</p> <p>Example: The Logistic CDF is</p> \\[F(x) = \\frac{e^x}{1 + e^x}, \\quad x \\in \\mathbb{R}\\] <p>Suppose we have \\(U \\sim \\text{Unif}(0, 1)\\) and wish to generate a Logistic random variable. Part 1 of the universality property says that \\(F^{-1}(U) \\sim \\text{Logistic}\\), so we first invert the CDF to get \\(F^{-1}\\):</p> \\[F^{-1}(u) = \\log\\left(\\frac{u}{1 - u}\\right)\\] <p>Then we plug in \\(U\\) for \\(u\\):</p> \\[F^{-1}(U) = \\log\\left(\\frac{U}{1 - U}\\right)\\] <p>Therefore \\(\\log\\left(\\frac{U}{1-U}\\right) \\sim \\text{Logistic}\\).</p> <p>We can verify directly that \\(\\log\\left(\\frac{U}{1-U}\\right)\\) has the required CDF: start from the definition of CDF, do some algebra to isolate \\(U\\) on one side of the inequality, and then use the CDF of the Uniform distribution. Let's work through these calculations once for practice:</p> \\[P\\left(\\log\\left(\\frac{U}{1 - U}\\right) \\leq x\\right) = P\\left(\\frac{U}{1 - U} \\leq e^x\\right) = P(U \\leq e^x(1 - U))\\] \\[= P\\left(U \\leq \\frac{e^x}{1 + e^x}\\right) = \\frac{e^x}{1 + e^x}\\] <p>which is indeed the Logistic CDF.</p> <p>We can also use simulation to visualize how universality of the Uniform works. To this end, we generate 1 million \\(\\text{Unif}(0, 1)\\) random samples. We then transform each of these values \\(u\\) into \\(\\log\\left(\\frac{u}{1-u}\\right)\\); if the universality of the Uniform is correct, the transformed numbers should follow a Logistic distribution.</p>"},{"location":"math/probability/convolution/","title":"Convolution","text":""},{"location":"math/probability/convolution/#introduction","title":"Introduction","text":"<p>Consider two different lists of numbers, or perhaps two different functions, and think about all the ways one might combine those two lists to get a new list of numbers, or combine the two functions to get a new function. One simple approach is to add them together term by term. Likewise with functions, one can add all the corresponding outputs. </p> <p></p> <p>Similarly, one could multiply the two lists term by term and do the same thing with the functions.</p> <p></p> <p>But there's another kind of combination just as fundamental as both of those, yet much less commonly discussed, known as a convolution. Unlike the previous two cases, it's not something that's merely inherited from an operation one can do to numbers. It's something genuinely new for the context of lists of numbers or combining functions.</p> <p></p> <p>Convolutions appear everywhere\u2014 they are ubiquitous in image processing, they're a core construct in the theory of probability, they're used extensively in solving differential equations, and one context where they've almost certainly been encountered, if not by this name, is multiplying two polynomials together.</p>"},{"location":"math/probability/convolution/#motivation-rolling-dice","title":"Motivation: Rolling Dice","text":"<p>Let's begin with probability, and in particular one of the simplest examples that most people have thought about at some point in their life: rolling a pair of dice and figuring out the chances of seeing various different sums.</p> <p>Each of the two dice has six different possible outcomes, which gives a total of 36 distinct possible pairs of outcomes. </p> <p></p> <p>By examining all of them, one can count how many pairs have a given sum. Arranging all the pairs in a grid reveals that all pairs with a constant sum lie along different diagonals. Simply counting how many exist on each of those diagonals tells us how likely one is to see a particular sum.</p> <p></p> <p>But can one think of other ways to visualize the same question? Other images that can help think about all the distinct pairs that have a given sum?</p> <p>One approach is to picture these two different sets of possibilities each in a row, but flip around the second row. That way all the different pairs which add up to seven line up vertically. If we slide that bottom row all the way to the right, then the unique pair that adds up to two (snake eyes) are the only ones that align. If we shift it over one unit to the right, the pairs which align are the two different pairs that add up to three.</p> <p></p> <p>In general, different offset values of this lower array (which was flipped around first) reveal all the distinct pairs that have a given sum.</p> <p> </p> <p>As far as probability questions go, this still isn't especially interesting because all we're doing is counting how many outcomes there are in each of these categories. But that is with the implicit assumption that there's an equal chance for each of these faces to come up.</p> <p></p> <p>But what if we have a special set of dice that's not uniform? Maybe the blue die has its own set of numbers describing the probabilities for each face coming up, and the red die has its own unique distinct set of numbers. </p> <p></p> <p>In that case, if one wanted to figure out, say, the probability of seeing a 2, one would multiply the probability that the blue die is a 1 times the probability that the red die is a 1. For the chances of seeing a 3, one looks at the two distinct pairs where that's possible, and again multiplies the corresponding probabilities and then adds those two products together. Similarly, the chances of seeing a 4 involves multiplying together three different pairs of possibilities and adding them all together.</p> <p></p> <p>In the spirit of setting up some formulas, let's name these top probabilities \\(a_1, a_2, a_3\\), and so on, and name the bottom ones \\(b_1, b_2, b_3\\), and so on.</p> <p> </p> <p>In general, this process where we're taking two different arrays of numbers, flipping the second one around, and then lining them up at various different offset values, taking a bunch of pairwise products and adding them up\u2014 that's one of the fundamental ways to think about what a convolution is. To spell it out more exactly, through this process, we just generated probabilities for seeing 2, 3, 4, on and on up to 12, and we got them by mixing together one list of values, \\(a\\), and another list of values, \\(b\\). In the lingo, we'd say the convolution of those two sequences gives us this new sequence\u2014 the new sequence of 11 values, each of which looks like some sum of pairwise products.</p> <p></p> <p>If one prefers, another way to think about the same operation is to first create a table of all the pairwise products, and then add up along all these diagonals. Again, that's a way of mixing together these two sequences of numbers to get a new sequence of 11 numbers. It's the same operation as the sliding windows thought, just another perspective.</p> <p></p> <p>Putting a little notation to it, here's how one might see it written down. The convolution of \\(a\\) and \\(b\\), denoted with this little asterisk (\\(*\\)), is a new list, and the \\(n\\)th element of that list looks like a sum, and that sum goes over all different pairs of indices, \\(i\\) and \\(j\\), so that the sum of those indices is equal to \\(n\\).</p> <p></p> <p>Convolution is a mathematical operation on two functions. In our dice example, we had:</p> <ul> <li> <p>Blue die: Random variable \\(X\\) with PMF \\(P(X = i) = a_i\\) for \\(i = 1, 2, \\ldots, 6\\)</p> </li> <li> <p>Red die: Random variable \\(Y\\) with PMF \\(P(Y = j) = b_j\\) for \\(j = 1, 2, \\ldots, 6\\)</p> </li> </ul> <p>Here, we performed a convolution of their PMFs. </p> <p>The resulting distribution's PMF (\\(f_Z\\)) is the convolution of the input distributions' PMFs (\\(f_X\\) and \\(f_Y\\)).</p> <p>For a fixed value \\(x\\) of \\(X\\), \\(Y\\) must take the value \\(z-x\\). Since \\(X\\) and \\(Y\\) are independent, the joint probability mass at \\((x,z-x)\\) is the product of their individual masses: \\(f_X(x) \\cdot f_Y(z-x)\\).</p> <p>Take the sum of these products over all possible values of \\(x\\) to find the total probability mass when \\(Z\\) is equal to \\(z\\).</p> <ul> <li>Convolution: \\(Z = X + Y\\) with PMF \\(P(Z = z)\\) for \\(z = 2, 3, \\ldots, 12\\)</li> </ul> <p></p>"},{"location":"math/probability/convolution/#definition","title":"Definition","text":"<p>A convolution is a sum of independent random variables. We often add independent random variables because the sum is a useful summary of an experiment (in \\(n\\) Bernoulli trials, we may only care about the total number of successes), and because sums lead to averages, which are also useful (in \\(n\\) Bernoulli trials, the proportion of successes).</p> <p>The main task is to determine the distribution of \\(Z = X + Y\\), where \\(X\\) and \\(Y\\) are independent random variables whose distributions are known.</p> <p>The distribution of \\(Z\\) is found using a convolution sum or integral. As we'll see, a convolution sum is nothing more than the law of total probability, conditioning on the value of either \\(X\\) or \\(Y\\); a convolution integral is analogous.</p> <p>Let \\(X\\) and \\(Y\\) be independent random variables and \\(Z = X + Y\\) be their sum.</p> <p>Discrete Case: If \\(X\\) and \\(Y\\) are discrete, then the PMF of \\(Z\\) is:</p> \\[P(Z = z) = \\sum_x P(Y = z - x)P(X = x) = \\sum_y P(X = z - y)P(Y = y)\\] <p>Continuous Case: If \\(X\\) and \\(Y\\) are continuous, then the PDF of \\(Z\\) is:</p> \\[f_Z(z) = \\int_{-\\infty}^{\\infty} f_Y(z - x)f_X(x)dx = \\int_{-\\infty}^{\\infty} f_X(z - y)f_Y(y)dy\\] <p>Proof: For the discrete case, we use the Law of Total Probability (LOTP), conditioning on \\(X\\):</p> \\[P(Z = z) = \\sum_x P(X + Y = z|X = x)P(X = x) = \\sum_x P(Y = z - x|X = x)P(X = x) = \\sum_x P(Y = z - x)P(X = x)\\] <p>The last equality follows from the independence of \\(X\\) and \\(Y\\). Conditioning on \\(Y\\) instead, we obtain the second formula for the PMF of \\(Z\\).</p>"},{"location":"math/probability/convolution/#moving-averages","title":"Moving Averages","text":"<p>Convolution has been demonstrated in one case where it serves as a natural and desirable operation\u2014adding up two probability distributions. Moving away from probabilities, another common example is the moving average. Consider a long list of numbers and a smaller list of numbers that all add up to 1. </p> <p></p> <p>In this case, there is a small list of 5 values that are all equal to 1/5. When applying the sliding window convolution process, and ignoring what happens at the very beginning, once the smaller list of values entirely overlaps with the larger one, each term in this convolution has a clear meaning.</p> <p> </p> <p>At each iteration, the process multiplies each of the values from the data by 1/5 and adds them all together, which means taking an average of the data inside this small window. </p> <p> </p> <p>Overall, the process produces a smoothed version of the original data. </p> <p></p> <p>This can be modified by starting with a different small list of numbers, and as long as that small list adds up to 1, it can still be interpreted as a moving average.</p> <p></p> <p>In the example shown here, that moving average would give more weight towards the central value. This also results in a smoothed version of the data.</p> <p></p>"},{"location":"math/probability/convolution/#image-processing","title":"Image Processing","text":"<p>A two-dimensional analog of convolution provides an algorithm for blurring images. </p> <p></p> <p>A small 3\u00d73 grid of values marches along the original image. When zooming in, each value is 1/9, and at each iteration, each value is multiplied by the corresponding pixel it sits on top of. </p> <p></p> <p>In computer science, colors are represented as vectors of three values representing the red, green, and blue components. When multiplying all these values by 1/9 and adding them together, the result is an average along each color channel, and the corresponding pixel for the image on the right is defined to be that sum. </p> <p></p> <p>The overall effect, as this process is applied to every single pixel on the image, is that each pixel bleeds into all of its neighbors, creating a blurrier version than the original.</p> <p></p> <p>In technical terms, the image on the right is a convolution of the original image with a small grid of values. More precisely, it is the convolution with a 180-degree rotated version of that small grid of values. While this distinction does not matter when the grid is symmetric, it is worth noting that the definition of convolution, as inherited from the pure mathematics context, always involves flipping around the second array.</p> <p></p> <p>Blurring is far from the only application of this concept. Consider a small grid of values that involves some positive numbers on the left and some negative numbers on the right, colored blue and red respectively. Take a moment to predict what effect this will have on the final image.</p> <p> </p> <p>In this case, the image is considered grayscale instead of colored, so each pixel is represented by one number instead of three.</p> <p></p> <p>One thing worth noticing is that as this convolution is performed, negative values can result. For example, at a certain point, if zooming in, the left half of the small grid sits entirely on top of black pixels, which have a value of zero, but the right half of negative values all sit on top of white pixels, which have a value of one. </p> <p> </p> <p>When multiplying corresponding terms and adding them together, the results will be very negative. The way this is displayed in the image on the right is to color negative values red and positive values blue.</p> <p></p> <p>Another thing to notice is that when on a patch that is all the same color, everything goes to zero, since the sum of the values in the small grid is zero. This is very different from the previous example where the sum of the small grid was one, which allowed interpretation as a moving average and hence a blur.</p> <p></p> <p>Overall, this process basically detects wherever there is variation in the pixel value as one moves from left to right, providing a way to pick up on all the vertical edges in the image.</p> <p></p> <p>This smaller grid is often called a kernel, and the beauty lies in how just by choosing a different kernel, different image processing effects can be achieved\u2014 not just blurring or edge detection, but also things like sharpening. </p> <p></p> <p>For those familiar with convolutional neural networks, the idea is to use data to determine what the kernels should be in the first place, as determined by whatever the neural network wants to detect.</p> <p></p> <p>Convolutions as a pure mathematical operation always produce an array that is bigger than the two arrays that are started with, at least assuming one of them does not have a length of one. In certain computer science contexts, it is often desirable to deliberately truncate that output.</p> <p>Another thing worth highlighting is that in the computer science context, the notion of flipping around the kernel before letting it march across the original often feels unusual and unnecessary, but again, note that this is inherited from the pure mathematics context, where, as seen with the probabilities, it is an incredibly natural thing to do.</p>"},{"location":"math/probability/convolution/#polynomial-multiplication","title":"Polynomial Multiplication","text":"<p>Recall that with the probability example, another way to think about convolution was to create a table of all the pairwise products and then add up those pairwise products along the diagonals. There is nothing specific to probability about this approach. Any time two different lists of numbers are convolved, this method can be used. Create a multiplication table with all pairwise products, and then each sum along the diagonal corresponds to one of the final outputs.</p> <p></p> <p>One context where this view is especially natural is when multiplying together two polynomials. For example, take the small grid already established and replace the top terms with 1, 2x, and 3x\u00b2, and replace the other terms with 4, 5x, and 6x\u00b2. Now, consider what it means when creating all these different pairwise products between the two lists.</p> <p></p> <p>What is being done is essentially expanding out the full product of the two polynomials written down, and then when adding up along the diagonal, that corresponds to collecting all like terms. </p> <p></p> <p>This is quite elegant. Expanding a polynomial and collecting like terms is exactly the same process as convolution.</p> <p>This allows for something quite interesting, because consider what is being stated here. The statement is that if two different functions are taken and multiplied together, which is a simple pointwise operation, that is the same thing as if the coefficients were first extracted from each one of those, assuming they are polynomials, and then a convolution of those two lists of coefficients was taken.</p> <p></p>"},{"location":"math/probability/covariance_and_correlation/","title":"Covariance and Correlation","text":"<p>Covariance and correlation are fundamental measures that describe the relationship between two random variables. They help us understand how variables change together and the strength and direction of their linear relationship.</p>"},{"location":"math/probability/covariance_and_correlation/#covariance","title":"Covariance","text":"<p>The covariance between two random variables \\(X\\) and \\(Y\\) is defined as:</p> \\[\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\] <p>The covariance can also be expressed as:</p> \\[\\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\\] <p>This form is often more convenient for calculations.</p>"},{"location":"math/probability/covariance_and_correlation/#interpretation-and-properties","title":"Interpretation and Properties","text":"<ul> <li> <p>Positive Covariance: When \\(X\\) tends to be above its mean, \\(Y\\) also tends to be above its mean OR when \\(X\\) tends to be below its mean, \\(Y\\) also tends to be below its mean</p> </li> <li> <p>Negative Covariance: When \\(X\\) tends to be above its mean, \\(Y\\) tends to be below its mean OR vice versa</p> </li> <li> <p>Variance as a special case: \\(\\text{Cov}(X, X) = \\text{Var}(X)\\)</p> </li> <li> <p>Symmetry: \\(\\text{Cov}(X, Y) = \\text{Cov}(Y, X)\\)</p> </li> <li> <p>Independence: If \\(X\\) and \\(Y\\) are independent, this means \\(\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]\\). This results in \\(\\text{Cov}(X, Y) = 0\\), as expected</p> </li> <li> <p>\\(\\text{Cov}(X, c) = 0\\) where \\(c\\) is a constant</p> </li> <li> <p>\\(\\text{Cov}(cX, Y) = c \\cdot \\text{Cov}(X, Y)\\) where \\(c\\) is a constant</p> </li> <li> <p>\\(\\text{Cov}(X, Y + Z) = \\text{Cov}(X, Y) + \\text{Cov}(X, Z)\\)</p> </li> <li> <p>\\(\\text{Cov}(X + Y, Z + W) = \\text{Cov}(X, Z) + \\text{Cov}(X, W) + \\text{Cov}(Y, Z) + \\text{Cov}(Y, W)\\)</p> </li> <li> <p>For constants \\(a_1, a_2, \\ldots, a_n\\) and \\(b_1, b_2, \\ldots, b_m\\):</p> </li> </ul> \\[\\text{Cov}\\left(\\sum_{i=1}^n a_i X_i, \\sum_{j=1}^m b_j Y_j\\right) = \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j \\text{Cov}(X_i, Y_j)\\] <ul> <li>\\(\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2\\text{Cov}(X_1, X_2)\\):</li> </ul> <p>Using the fact that \\(\\text{Var}(X) = \\text{Cov}(X, X)\\):</p> \\[\\begin{align} \\text{Var}(X_1 + X_2) &amp;= \\text{Cov}(X_1 + X_2, X_1 + X_2) \\\\ &amp;= \\text{Cov}(X_1, X_1) + \\text{Cov}(X_1, X_2) + \\text{Cov}(X_2, X_1) + \\text{Cov}(X_2, X_2) \\\\ &amp;= \\text{Var}(X_1) + \\text{Cov}(X_1, X_2) + \\text{Cov}(X_2, X_1) + \\text{Var}(X_2) \\\\ &amp;= \\text{Var}(X_1) + \\text{Var}(X_2) + 2\\text{Cov}(X_1, X_2) \\end{align}\\] <p>\\(\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2)\\) is true when \\(\\text{Cov}(X_1, X_2) = 0\\) (which means when \\(X_1\\) and \\(X_2\\) are uncorrelated). In particular, if \\(X_1\\) and \\(X_2\\) are independent, then \\(\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2)\\). Note that \\(X_1\\) and \\(X_2\\) can be dependent and \\(\\text{Cov}(X_1, X_2) = 0\\). But when \\(X_1\\) and \\(X_2\\) are independent, it is always true that \\(\\text{Cov}(X_1, X_2) = 0\\).</p> <p>General Case - Variance of sum of n Random Variables:</p> <p>For \\(n\\) random variables \\(X_1, X_2, \\ldots, X_n\\):</p> \\[\\text{Var}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\text{Var}(X_i) + 2\\sum_{1 \\leq i &lt; j \\leq n} \\text{Cov}(X_i, X_j)\\] <p>Note: Zero Covariance does not imply independence</p> <p>Let \\(Z \\sim N(0, 1)\\) be a standard normal random variable, and define:</p> <ul> <li> <p>\\(X = Z\\)</p> </li> <li> <p>\\(Y = Z^2\\)</p> </li> </ul> <p>Covariance calculation:</p> \\[\\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = \\mathbb{E}[Z \\cdot Z^2] - \\mathbb{E}[Z]\\mathbb{E}[Z^2]\\] <p>Since \\(Z \\sim N(0, 1)\\):</p> <ul> <li> <p>\\(\\mathbb{E}[Z] = 0\\)</p> </li> <li> <p>\\(\\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2 = 1 + 0 = 1\\)</p> </li> <li> <p>\\(\\mathbb{E}[Z^3] = 0\\) (odd moments of standard normal are zero)</p> </li> </ul> <p>Therefore: \\(\\text{Cov}(X, Y) = 0 - 0 \\cdot 1 = 0\\)</p> <p>Dependence: \\(X\\) and \\(Y\\) are clearly dependent because knowing \\(X = Z\\) completely determines \\(Y = Z^2\\). For example, if \\(X = 2\\), then \\(Y\\) must be 4.</p>"},{"location":"math/probability/covariance_and_correlation/#correlation","title":"Correlation","text":"<p>The correlation coefficient (or Pearson correlation) between two random variables \\(X\\) and \\(Y\\) is defined as:</p> \\[\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X) \\text{Var}(Y)}} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\] <p>where \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X\\) and \\(Y\\) respectively.</p>"},{"location":"math/probability/covariance_and_correlation/#interpretation-and-properties_1","title":"Interpretation and Properties","text":"<ul> <li> <p>Range: \\(-1 \\leq \\rho_{X,Y} \\leq 1\\)</p> </li> <li> <p>Scale Invariance: \\(\\rho_{aX + b, cY + d} = \\rho_{X, Y}\\) for \\(a, c &gt; 0\\)</p> </li> <li> <p>\\(\\rho_{X,Y} = 1\\) when \\(Y = aX + b\\) with \\(a &gt; 0\\)</p> </li> <li> <p>\\(\\rho_{X,Y} = -1\\) when \\(Y = aX + b\\) with \\(a &lt; 0\\)</p> </li> <li> <p>\\(\\rho = 1\\): Perfect positive linear relationship</p> </li> <li> <p>\\(\\rho = -1\\): Perfect negative linear relationship  </p> </li> <li> <p>\\(\\rho = 0\\): No linear relationship</p> </li> <li> <p>\\(|\\rho| &gt; 0.7\\): Strong linear relationship</p> </li> <li> <p>\\(0.3 &lt; |\\rho| &lt; 0.7\\): Moderate linear relationship</p> </li> <li> <p>\\(|\\rho| &lt; 0.3\\): Weak linear relationship</p> </li> <li> <p>The correlation coefficient is essentially a normalized version of the covariance:</p> </li> </ul> \\[\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\]"},{"location":"math/probability/covariance_and_correlation/#connection-to-linear-algebra","title":"Connection to Linear Algebra","text":"<p>The correlation coefficient has a beautiful geometric interpretation in terms of the angle between vectors in \\(\\mathbb{R}^n\\):</p> <p>For centered data: If we have \\(n\\) observations \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\), and we center the data by subtracting means:</p> <ul> <li> <p>\\(\\mathbf{x} = (x_1 - \\bar{x}, x_2 - \\bar{x}, \\ldots, x_n - \\bar{x})\\)</p> </li> <li> <p>\\(\\mathbf{y} = (y_1 - \\bar{y}, y_2 - \\bar{y}, \\ldots, y_n - \\bar{y})\\)</p> </li> </ul> <p>Then the correlation coefficient equals the cosine of the angle between these centered vectors:</p> \\[\\rho_{X,Y} = \\cos \\theta = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}\\] <p>Key insights:</p> <ul> <li> <p>\\(\\rho = 1\\) corresponds to \\(\\theta = 0\u00b0\\) (vectors point in same direction)</p> </li> <li> <p>\\(\\rho = -1\\) corresponds to \\(\\theta = 180\u00b0\\) (vectors point in opposite directions)  </p> </li> <li> <p>\\(\\rho = 0\\) corresponds to \\(\\theta = 90\u00b0\\) (vectors are orthogonal)</p> </li> <li> <p>The correlation measures how \"aligned\" the centered data vectors are in \\(\\mathbb{R}^n\\)</p> </li> </ul>"},{"location":"math/probability/expectation/","title":"Expectation","text":"<p>Computing Averages: two approaches</p> <p>Let's explore how to compute averages using two different methods, which will help build intuition for expectation.</p> <p>Method 1: Arithmetic Mean (summation divided by n)</p> <p>Formula: \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\)</p> <p>Example: Consider the numbers \\(2, 5, 8, 8, 8, 11, 14\\)</p> <p>Calculation:</p> \\[\\bar{x} = \\frac{1}{7} \\sum_{i=1}^7 x_i = \\frac{1}{7}(2 + 5 + 8 + 8 + 8 + 11 + 14) = \\frac{56}{7} = 8\\] <p>Method 2: Weighted Sum</p> <p>Formula: \\(\\bar{x} = \\sum_{i=1}^k w_i x_i\\) where \\(\\sum_{i=1}^k w_i = 1\\) and \\(k\\) is the number of unique values</p> <p>Same example: Unique values \\(2, 5, 8, 11, 14\\) with weights based on frequency:</p> <ul> <li> <p>\\(w_1 = \\frac{1}{7}\\) (for value 2, appears 1 time)</p> </li> <li> <p>\\(w_2 = \\frac{1}{7}\\) (for value 5, appears 1 time)  </p> </li> <li> <p>\\(w_3 = \\frac{3}{7}\\) (for value 8, appears 3 times)</p> </li> <li> <p>\\(w_4 = \\frac{1}{7}\\) (for value 11, appears 1 time)</p> </li> <li> <p>\\(w_5 = \\frac{1}{7}\\) (for value 14, appears 1 time)</p> </li> </ul> <p>Verification: \\(\\frac{1}{7} + \\frac{1}{7} + \\frac{3}{7} + \\frac{1}{7} + \\frac{1}{7} = \\frac{7}{7} = 1\\) \u2713</p> <p>Calculation:</p> \\[\\bar{x} = \\sum_{i=1}^5 w_i x_i = \\frac{1}{7}(2) + \\frac{1}{7}(5) + \\frac{3}{7}(8) + \\frac{1}{7}(11) + \\frac{1}{7}(14)\\] \\[= \\frac{2}{7} + \\frac{5}{7} + \\frac{24}{7} + \\frac{11}{7} + \\frac{14}{7} = \\frac{56}{7} = 8\\] <p>The weighted average equals the arithmetic mean because the weights reflect the actual frequency of each value in the data.</p>"},{"location":"math/probability/expectation/#definition-of-expectation","title":"Definition of Expectation","text":"<p>Now we're ready to define the expectation (or expected value) of a discrete random variable. The key insight is that the weights in our weighted sum become the probabilities of each value.</p> <p>For a discrete random variable \\(X\\) with possible values \\(x_1, x_2, \\ldots, x_k\\) and probability mass function \\(P(X = x_i) = p_i\\), the expectation is defined as:</p> \\[E[X] = \\sum_{i=1}^k x_i \\cdot P(X = x_i) = \\sum_{i=1}^k x_i \\cdot p_i\\] <p>Why use probabilities as weights? Because we want to assign higher weights to values that are more likely to occur.</p> <p>Example: Consider a random variable \\(X\\) representing the outcome of a biased die:</p> <ul> <li> <p>\\(P(X = 1) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 2) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 3) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 4) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 5) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 6) = 0.5\\) (50% chance)</p> </li> </ul> <p>Expectation calculation:</p> \\[E[X] = 1(0.1) + 2(0.1) + 3(0.1) + 4(0.1) + 5(0.1) + 6(0.5)\\] \\[= 0.1 + 0.2 + 0.3 + 0.4 + 0.5 + 3.0 = 4.5\\] <p>Expectation is a weighted average where the weights are the probabilities of each possible value. Weights are probabilities \\(P(X = x_i)\\) that sum to 1. The expectation gives us a single number that summarizes the \"center\" of a random variable's distribution.</p>"},{"location":"math/probability/expectation/#expectation-of-a-bernoulli-random-variable","title":"Expectation of a Bernoulli Random Variable","text":"<p>Let's compute the expectation of a Bernoulli random variable \\(X \\sim \\text{Bernoulli}(p)\\).</p> <p>A Bernoulli random variable \\(X\\) takes only two values:</p> <ul> <li> <p>\\(X = 1\\) with probability \\(p\\) (success)</p> </li> <li> <p>\\(X = 0\\) with probability \\(1-p\\) (failure)</p> </li> </ul> <p>Using the definition of expectation:</p> \\[E[X] = \\sum_{i=1}^k x_i \\cdot P(X = x_i)\\] <p>For Bernoulli, we have \\(k = 2\\) possible values:</p> \\[E[X] = 0 \\cdot P(X = 0) + 1 \\cdot P(X = 1)\\] \\[E[X] = 0 \\cdot (1-p) + 1 \\cdot p\\] \\[E[X] = 0 + p = p\\] <p>The expectation of a Bernoulli random variable is \\(p\\):</p> \\[E[X] = p \\quad \\text{where } X \\sim \\text{Bernoulli}(p)\\] <p>Why does this make sense?</p> <ul> <li> <p>If \\(p = 0.8\\) (80% chance of success), we expect to see 1 about 80% of the time</p> </li> <li> <p>The long-run average of many Bernoulli trials will be approximately \\(p\\)</p> </li> <li> <p>Since \\(X\\) only takes values 0 and 1, the expectation represents the \"proportion of successes\"</p> </li> </ul>"},{"location":"math/probability/expectation/#expectation-of-a-binomial-random-variable","title":"Expectation of a Binomial Random Variable","text":"<p>Now let's compute the expectation of a binomial random variable \\(X \\sim \\text{Binomial}(n, p)\\).</p> <p>A binomial random variable \\(X\\) represents the number of successes in \\(n\\) independent Bernoulli trials, each with success probability \\(p\\).</p> <p>Possible values: \\(X\\) takes values in \\(\\{0, 1, 2, \\ldots, n\\}\\). PMF: \\(P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k}\\) for \\(k = 0, 1, 2, \\ldots, n\\)</p> <p>Using the definition of expectation:</p> \\[E[X] = \\sum_{k=0}^n k \\cdot P(X = k) = \\sum_{k=0}^n k \\cdot \\binom{n}{k} p^k(1-p)^{n-k}\\] \\[E[X] = \\sum_{k=0}^n k \\cdot \\binom{n}{k} p^k(1-p)^{n-k}\\] <p>We can use this identity:</p> \\[k \\cdot \\binom{n}{k} = k \\cdot \\frac{n!}{k!(n-k)!} = \\frac{n!}{(k-1)!(n-k)!} = n \\cdot \\frac{(n-1)!}{(k-1)!(n-k)!} = n \\cdot \\binom{n-1}{k-1}\\] \\[E[X] = \\sum_{k=0}^n n \\cdot \\binom{n-1}{k-1} p^k(1-p)^{n-k}\\] \\[E[X] = n \\cdot p \\cdot \\sum_{k=0}^n \\binom{n-1}{k-1} p^{k-1}(1-p)^{n-k}\\] <p>Let \\(j = k-1\\), so \\(k = j+1\\). When \\(k = 0\\), \\(j = -1\\); when \\(k = n\\), \\(j = n-1\\).</p> <p>Note: The term with \\(j = -1\\) contributes 0 because \\(\\binom{n-1}{-1} = 0\\) (combinatorial coefficients are 0 for negative indices). So we can adjust the range to start from \\(j = 0\\):</p> \\[E[X] = n \\cdot p \\cdot \\sum_{j=0}^{n-1} \\binom{n-1}{j} p^j(1-p)^{(n-1)-j}\\] <p>The sum \\(\\sum_{j=0}^{n-1} \\binom{n-1}{j} p^j(1-p)^{(n-1)-j}\\) is exactly the binomial expansion of \\((p + (1-p))^{n-1} = 1^{n-1} = 1\\).</p> \\[E[X] = n \\cdot p \\cdot 1 = np\\] <p>The expectation of a binomial random variable is \\(np\\):</p> \\[E[X] = np \\quad \\text{where } X \\sim \\text{Binomial}(n, p)\\] <p>Why does this make sense?</p> <ul> <li> <p>\\(n\\) trials: We perform \\(n\\) independent Bernoulli trials</p> </li> <li> <p>\\(p\\) probability: Each trial has success probability \\(p\\)</p> </li> <li> <p>Expected successes: We expect \\(p\\) proportion of trials to succeed</p> </li> <li> <p>Total expectation: \\(n \\cdot p\\) total expected successes</p> </li> </ul> <p>Example: If we flip a fair coin (\\(p = 0.5\\)) 100 times (\\(n = 100\\)):</p> <ul> <li> <p>\\(E[X] = 100 \\cdot 0.5 = 50\\)</p> </li> <li> <p>Interpretation: We expect about 50 heads in 100 flips</p> </li> </ul>"},{"location":"math/probability/expectation/#linearity-of-expectation","title":"Linearity of Expectation","text":"<p>Linearity of expectation is one of the most powerful and useful properties in probability theory. It states that expectation is a linear operator, regardless of whether the random variables are independent or not.</p> <p>For any random variables \\(X\\) and \\(Y\\) (discrete or continuous) and any constants \\(a\\) and \\(b\\):</p> \\[E[aX + bY] = aE[X] + bE[Y]\\] <p>Key insight: Linearity of expectation holds even when \\(X\\) and \\(Y\\) are dependent!</p> <p>Binomial Distribution Revisited</p> <p>If \\(X \\sim \\text{Binomial}(n, p)\\), we can think of \\(X\\) as the sum of \\(n\\) independent Bernoulli\\((p)\\) random variables:</p> \\[X = B_1 + B_2 + \\cdots + B_n\\] <p>where each \\(B_i \\sim \\text{Bernoulli}(p)\\).</p> <p>By linearity:</p> \\[E[X] = E[B_1 + B_2 + \\cdots + B_n] = E[B_1] + E[B_2] + \\cdots + E[B_n] = p + p + \\cdots + p = np\\] <p>This gives us the same result as our direct calculation, but much more simply!</p>"},{"location":"math/probability/expectation/#expectation-of-a-hypergeometric-random-variable","title":"Expectation of a Hypergeometric Random Variable","text":"<p>Now let's compute the expectation of a hypergeometric random variable \\(X \\sim \\text{Hypergeometric}(N, K, n)\\).</p> <p>A hypergeometric random variable \\(X\\) represents the number of \"success\" items when drawing \\(n\\) items without replacement from a population of \\(N\\) items, where \\(K\\) items are \"successes\".</p> <p>Possible values: \\(X\\) takes values in \\(\\{0, 1, 2, \\ldots, \\min(K, n)\\}\\). PMF: \\(P(X = k) = \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\)</p> <p>Using the definition of expectation:</p> \\[E[X] = \\sum_{k=0}^{\\min(K,n)} k \\cdot P(X = k) = \\sum_{k=0}^{\\min(K,n)} k \\cdot \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\] \\[E[X] = \\sum_{k=0}^{\\min(K,n)} k \\cdot \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\] <p>Use the identity \\(k \\cdot \\binom{K}{k} = K \\cdot \\binom{K-1}{k-1}\\)</p> <p>This identity comes from:</p> \\[k \\cdot \\binom{K}{k} = k \\cdot \\frac{K!}{k!(K-k)!} = \\frac{K!}{(k-1)!(K-k)!} = K \\cdot \\frac{(K-1)!}{(k-1)!(K-k)!} = K \\cdot \\binom{K-1}{k-1}\\] \\[E[X] = \\sum_{k=0}^{\\min(K,n)} K \\cdot \\binom{K-1}{k-1} \\cdot \\frac{\\binom{N-K}{n-k}}{\\binom{N}{n}}\\] \\[E[X] = K \\cdot \\sum_{k=0}^{\\min(K,n)} \\binom{K-1}{k-1} \\cdot \\frac{\\binom{N-K}{n-k}}{\\binom{N}{n}}\\] <p>Let \\(j = k-1\\), so \\(k = j+1\\). When \\(k = 0\\), \\(j = -1\\); when \\(k = \\min(K,n)\\), \\(j = \\min(K,n)-1\\).</p> <p>Note: The term with \\(j = -1\\) contributes 0 because \\(\\binom{K-1}{-1} = 0\\). So we can adjust the range to start from \\(j = 0\\):</p> \\[E[X] = K \\cdot \\sum_{j=0}^{\\min(K-1,n-1)} \\binom{K-1}{j} \\cdot \\frac{\\binom{N-K}{n-(j+1)}}{\\binom{N}{n}}\\] <p>The sum \\(\\sum_{j=0}^{\\min(K-1,n-1)} \\binom{K-1}{j} \\cdot \\binom{N-K}{n-(j+1)}\\) represents the total number of ways to choose \\(n-1\\) items from \\(N-1\\) items (since we're choosing \\(j\\) from \\(K-1\\) and \\(n-1-j\\) from \\(N-K\\)).</p> <p>This equals \\(\\binom{N-1}{n-1}\\).</p> \\[E[X] = K \\cdot \\frac{\\binom{N-1}{n-1}}{\\binom{N}{n}} = K \\cdot \\frac{n}{N} = n \\cdot \\frac{K}{N}\\] <p>The expectation of a hypergeometric random variable is \\(n \\cdot \\frac{K}{N}\\):</p> \\[E[X] = n \\cdot \\frac{K}{N} \\quad \\text{where } X \\sim \\text{Hypergeometric}(N, K, n)\\] <p>Example: If we have a population of 100 items with 30 successes, and we draw 20 items:</p> <ul> <li> <p>\\(N = 100\\), \\(K = 30\\), \\(n = 20\\)</p> </li> <li> <p>\\(E[X] = 20 \\cdot \\frac{30}{100} = 20 \\cdot 0.3 = 6\\)</p> </li> <li> <p>Interpretation: We expect about 6 successes in our sample of 20</p> </li> </ul> <p>This result connects beautifully to the binomial expectation:</p> <ul> <li> <p>Binomial: \\(E[X] = np\\) (with replacement, constant probability)</p> </li> <li> <p>Hypergeometric: \\(E[X] = n \\cdot \\frac{K}{N}\\) (without replacement, changing probability)</p> </li> <li> <p>Key difference: \\(\\frac{K}{N}\\) vs. \\(p\\) - the proportion of successes in the population</p> </li> </ul>"},{"location":"math/probability/expectation/#proof-of-linearity-of-expectation","title":"Proof of Linearity of Expectation","text":"<p>Let's prove that expectation is a linear operator: \\(E[aX + bY] = aE[X] + bE[Y]\\) for any random variables \\(X\\) and \\(Y\\) and constants \\(a\\) and \\(b\\).</p> <p>Step 1: Start with the definition</p> <p>Important: This step introduces a new concept - the joint expectation of random variables, which we haven't discussed yet in this document.</p> <p>The expectation of a function of two random variables is defined as:</p> \\[E[g(X,Y)] = \\sum_{x,y} g(x,y) \\cdot P(X = x, Y = y)\\] <p>This is called the joint expectation because it involves the joint probability distribution \\(P(X = x, Y = y)\\) of both random variables together.</p> <p>In our case, \\(g(X,Y) = aX + bY\\), so:</p> \\[E[aX + bY] = \\sum_{x,y} (ax + by) \\cdot P(X = x, Y = y)\\] <p>Step 2: Distribute the sum</p> \\[E[aX + bY] = \\sum_{x,y} ax \\cdot P(X = x, Y = y) + \\sum_{x,y} by \\cdot P(X = x, Y = y)\\] <p>Step 3: Factor out constants</p> \\[E[aX + bY] = a \\sum_{x,y} x \\cdot P(X = x, Y = y) + b \\sum_{x,y} y \\cdot P(X = x, Y = y)\\] <p>Step 4: Use the law of total probability</p> <p>For any event \\(A\\), the law of total probability states:</p> \\[P(A) = \\sum_B P(A \\cap B) = \\sum_B P(A, B)\\] <p>In our derivation, we're using this to \"marginalize out\" one variable:</p> <p>For the first sum:</p> \\[\\sum_{x,y} x \\cdot P(X = x, Y = y) = \\sum_x x \\sum_y P(X = x, Y = y) = \\sum_x x \\cdot P(X = x) = E[X]\\] <p>Here, we're summing over all possible \\(y\\) values for each fixed \\(x\\), which gives us \\(P(X = x)\\) by the law of total probability.</p> <p>For the second sum:</p> \\[\\sum_{x,y} y \\cdot P(X = x, Y = y) = \\sum_y y \\sum_x P(X = x, Y = y) = \\sum_y y \\cdot P(Y = y) = E[Y]\\] <p>Here, we're summing over all possible \\(x\\) values for each fixed \\(y\\), which gives us \\(P(Y = y)\\) by the law of total probability.</p> <p>Step 5: Final result</p> \\[E[aX + bY] = aE[X] + bE[Y]\\]"},{"location":"math/probability/exponential_distribution/","title":"Exponential Distribution","text":"<p>The Exponential distribution is a continuous probability distribution that describes the time between events in a Poisson process. It is characterized by its memoryless property and is fundamental in reliability theory, queuing theory, and survival analysis.</p> <p>A continuous random variable \\(X\\) has an exponential distribution with parameter \\(\\lambda &gt; 0\\) if its PDF is:</p> \\[f_X(x) = \\begin{cases} \\lambda e^{-\\lambda x} &amp; \\text{if } x \\geq 0 \\\\ 0 &amp; \\text{if } x &lt; 0 \\end{cases}\\] <p>We write this as \\(X \\sim \\text{Exponential}(\\lambda)\\) or \\(X \\sim \\text{Exp}(\\lambda)\\).</p> <p>Parameters:</p> <ul> <li>\\(\\lambda\\): rate parameter (events per unit time)</li> </ul>"},{"location":"math/probability/exponential_distribution/#memory-less-property","title":"Memory-less property","text":"<p>The exponential distribution is memoryless, meaning:</p> \\[P(X &gt; s + t \\mid X &gt; s) = P(X &gt; t)\\] <p>If you've already waited \\(s\\) units of time, the probability of waiting an additional \\(t\\) units is the same as if you were starting fresh.</p> <p>Proof:</p> \\[P(X &gt; s + t \\mid X &gt; s) = \\frac{P(X &gt; s + t \\text{ AND } X &gt; s)}{P(X &gt; s)}\\] <p>Since \\(s + t &gt; s\\), if \\(X &gt; s + t\\), then automatically \\(X &gt; s\\). Therefore:</p> \\[P(X &gt; s + t \\text{ AND } X &gt; s) = P(X &gt; s + t)\\] <p>Using the CDF of the exponential distribution:</p> \\[P(X &gt; s + t) = 1 - P(X \\leq s + t) = 1 - (1 - e^{-\\lambda(s + t)}) = e^{-\\lambda(s + t)}\\] \\[P(X &gt; s) = 1 - P(X \\leq s) = 1 - (1 - e^{-\\lambda s}) = e^{-\\lambda s}\\] \\[P(X &gt; s + t \\mid X &gt; s) = \\frac{P(X &gt; s + t)}{P(X &gt; s)} = \\frac{e^{-\\lambda(s + t)}}{e^{-\\lambda s}}\\] \\[\\frac{e^{-\\lambda(s + t)}}{e^{-\\lambda s}} = e^{-\\lambda(s + t) + \\lambda s} = e^{-\\lambda t} = P(X &gt; t)\\] <p>Therefore:</p> \\[P(X &gt; s + t \\mid X &gt; s) = P(X &gt; t)\\] <p>This proves the memoryless property!</p> <p>Example: If a light bulb has been working for 100 hours, the probability it works for another 50 hours is the same as the probability a new bulb works for 50 hours.</p>"},{"location":"math/probability/exponential_distribution/#connection-to-poisson-process","title":"Connection to Poisson Process","text":"<p>The exponential distribution describes the inter-arrival times in a Poisson process:</p> <ul> <li> <p>Poisson process: Events occur at a constant average rate \\(\\lambda\\)</p> </li> <li> <p>Exponential distribution: Time between consecutive events</p> </li> <li> <p>Relationship: If events occur at rate \\(\\lambda\\), then inter-arrival times follow \\(\\text{Exp}(\\lambda)\\)</p> </li> </ul> <p>Theorem: In a Poisson process with rate \\(\\lambda\\), the inter-arrival times (time between consecutive events) are independent and identically distributed exponential random variables with parameter \\(\\lambda\\).</p> <p>Proof: Let \\(T_1, T_2, T_3, \\ldots\\) be the inter-arrival times. We need to show that each \\(T_i \\sim \\text{Exp}(\\lambda)\\).</p> <p>In a Poisson process, events occur continuously over time. We need multiple random variables because:</p> <ol> <li>\\(T_1\\): Time from start (time 0) until the first event occurs</li> <li>\\(T_2\\): Time from the first event until the second event occurs  </li> <li>\\(T_3\\): Time from the second event until the third event occurs</li> <li>And so on...: Each \\(T_i\\) represents the time between the \\((i-1)\\)th and \\(i\\)th events</li> </ol> <p>Each \\(T_i\\) represents a different time interval between consecutive events. Since events occur randomly, each of these time intervals is itself a random variable.</p> <p>The first event occurs at time \\(T_1\\). The probability that no events occur in time interval \\([0, t]\\) is:</p> \\[P(T_1 &gt; t) = P(\\text{No events in } [0, t])\\] <p>Since the number of events in \\([0, t]\\) follows \\(\\text{Poisson}(\\lambda t)\\):</p> \\[P(T_1 &gt; t) = P(\\text{Poisson}(\\lambda t) = 0) = \\frac{(\\lambda t)^0}{0!} e^{-\\lambda t} = e^{-\\lambda t}\\] <p>Therefore:</p> \\[P(T_1 \\leq t) = 1 - P(T_1 &gt; t) = 1 - e^{-\\lambda t}\\] <p>This is exactly the CDF of \\(\\text{Exp}(\\lambda)\\), so \\(T_1 \\sim \\text{Exp}(\\lambda)\\).</p> <p>Key insight: Each \\(T_i\\) follows the same exponential distribution \\(\\text{Exp}(\\lambda)\\) because:</p> <ol> <li> <p>Stationary increments: The Poisson process has the same behavior regardless of when we start observing</p> </li> <li> <p>Memoryless property: The exponential distribution \"forgets\" how long we've been waiting</p> </li> <li> <p>Independent increments: Each time interval is independent of previous intervals</p> </li> </ol> <p>Example: Consider a Poisson process with rate \\(\\lambda = 2\\) events per hour.</p> <p>Poisson aspect: Number of events in 3 hours follows \\(\\text{Poisson}(2 \\times 3) = \\text{Poisson}(6)\\).</p> <p>Exponential aspect: Time between consecutive events follows \\(\\text{Exp}(2)\\).</p> <p>Verification: </p> <ul> <li> <p>Expected events in 3 hours: \\(E[\\text{Poisson}(6)] = 6\\)</p> </li> <li> <p>Expected time between events: \\(E[\\text{Exp}(2)] = \\frac{1}{2}\\) hour</p> </li> <li> <p>Consistency: \\(\\frac{3 \\text{ hours}}{6 \\text{ events}} = \\frac{1}{2} \\text{ hour per event}\\) \u2713</p> </li> </ul>"},{"location":"math/probability/exponential_distribution/#cdf-of-the-exponential-distribution","title":"CDF of the Exponential Distribution","text":"<p>The CDF of \\(X \\sim \\text{Exp}(\\lambda)\\) is:</p> \\[F_X(x) = \\begin{cases} 1 - e^{-\\lambda x} &amp; \\text{if } x \\geq 0 \\\\ 0 &amp; \\text{if } x &lt; 0 \\end{cases}\\] <p>Derivation: </p> \\[F_X(x) = \\int_0^x \\lambda e^{-\\lambda t} \\, dt = \\lambda \\int_0^x e^{-\\lambda t} \\, dt = \\lambda \\left[-\\frac{1}{\\lambda} e^{-\\lambda t}\\right]_0^x = 1 - e^{-\\lambda x}\\]"},{"location":"math/probability/exponential_distribution/#expectation-and-variance","title":"Expectation and Variance","text":"<p>Expectation</p> \\[E[X] = \\frac{1}{\\lambda}\\] <p>Proof:</p> \\[E[X] = \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} \\, dx\\] <p>Using integration by parts with \\(u = x\\) and \\(dv = \\lambda e^{-\\lambda x} \\, dx\\):</p> \\[E[X] = \\left[-x e^{-\\lambda x}\\right]_0^{\\infty} - \\int_0^{\\infty} (-e^{-\\lambda x}) \\, dx\\] <p>The boundary term evaluates to 0:</p> <ul> <li> <p>At \\(x = 0\\): \\(-0 \\cdot e^0 = 0\\)</p> </li> <li> <p>At \\(x = \\infty\\): \\(-x \\cdot e^{-\\lambda x} \\to 0\\) (exponential decay dominates)</p> </li> </ul> <p>Therefore:</p> \\[E[X] = \\int_0^{\\infty} e^{-\\lambda x} \\, dx = \\left[-\\frac{1}{\\lambda} e^{-\\lambda x}\\right]_0^{\\infty} = \\frac{1}{\\lambda}\\] <p>Variance</p> \\[\\text{Var}(X) = \\frac{1}{\\lambda^2}\\] <p>Proof:</p> \\[\\text{Var}(X) = E[X^2] - (E[X])^2 = E[X^2] - \\frac{1}{\\lambda^2}\\] <p>We need to calculate \\(E[X^2]\\):</p> \\[E[X^2] = \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} \\, dx\\] <p>Using integration by parts twice:</p> \\[E[X^2] = \\frac{2}{\\lambda^2}\\] <p>Therefore:</p> \\[\\text{Var}(X) = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\\] <p>Standard Deviation</p> \\[\\sigma_X = \\frac{1}{\\lambda}\\] <p>Note: For the exponential distribution, the mean equals the standard deviation.</p>"},{"location":"math/probability/independence_of_random_variables/","title":"Independence of Random Variables","text":"<p>Independence is one of the most fundamental and important concepts in probability theory. It allows us to simplify complex calculations, understand the structure of random phenomena, and make powerful assumptions that lead to elegant mathematical results.</p> <p>Two random variables \\(X\\) and \\(Y\\) are independent if and only if their joint probability distribution factors into the product of their individual distributions.</p> <p>For discrete random variables:</p> \\[P(X = x, Y = y) = P(X = x) \\cdot P(Y = y) \\quad \\text{for all } x, y\\] <p>For continuous random variables:</p> \\[f_{X,Y}(x, y) = f_X(x) \\cdot f_Y(y) \\quad \\text{for all } x, y\\] <p>For n random variables:</p> <p>A collection of random variables \\(X_1, X_2, \\ldots, X_n\\) is mutually independent if and only if their joint CDF factors into the product of their individual CDFs:</p> \\[F_{X_1, X_2, \\ldots, X_n}(x_1, x_2, \\ldots, x_n) = F_{X_1}(x_1) \\cdot F_{X_2}(x_2) \\cdots F_{X_n}(x_n) = \\prod_{i=1}^n F_{X_i}(x_i) \\quad \\text{for all } x_1, x_2, \\ldots, x_n\\] <p>Equivalently, for continuous random variables, the joint PDF factors:</p> \\[f_{X_1, X_2, \\ldots, X_n}(x_1, x_2, \\ldots, x_n) = f_{X_1}(x_1) \\cdot f_{X_2}(x_2) \\cdots f_{X_n}(x_n) = \\prod_{i=1}^n f_{X_i}(x_i) \\quad \\text{for all } x_1, x_2, \\ldots, x_n\\] <p>And for discrete random variables, the joint PMF factors:</p> \\[P(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n) = P(X_1 = x_1) \\cdot P(X_2 = x_2) \\cdots P(X_n = x_n) = \\prod_{i=1}^n P(X_i = x_i) \\quad \\text{for all } x_1, x_2, \\ldots, x_n\\] <p>Independence means that knowing the value of one random variable gives you no information about the value of the other. The random variables are completely unrelated in their behavior.</p>"},{"location":"math/probability/indicator_random_variables/","title":"Indicator Random Variables","text":"<p>Indicator random variables are one of the most powerful and elegant tools in probability theory. They provide a bridge between probability and expectation, making complex problems surprisingly simple.</p> <p>An indicator random variable \\(I_A\\) for an event \\(A\\) is defined as:</p> \\[I_A = \\begin{cases}  1 &amp; \\text{if event } A \\text{ occurs} \\\\ 0 &amp; \\text{if event } A \\text{ does not occur} \\end{cases}\\] <p>Key properties:</p> <ul> <li> <p>Binary values: Only takes values 0 or 1</p> </li> <li> <p>Event representation: Directly represents whether an event occurs</p> </li> <li> <p>Probability connection: \\(E[I_A] = P(A)\\)</p> </li> </ul> <p>Bridge Between Probability and Expectation</p> <p>The fundamental connection: \\(E[I_A] = P(A)\\)</p> <p>Proof:</p> \\[E[I_A] = 1 \\cdot P(I_A = 1) + 0 \\cdot P(I_A = 0) = 1 \\cdot P(A) + 0 \\cdot P(A^c) = P(A)\\] <p>This simple result allows us to convert probability problems into expectation problems, which are often easier to solve.</p> <p>Linearity of Expectation</p> <p>Since indicators only take values 0 and 1, they work beautifully with linearity of expectation:</p> \\[E[I_{A_1} + I_{A_2} + \\cdots + I_{A_n}] = E[I_{A_1}] + E[I_{A_2}] + \\cdots + E[I_{A_n}] = P(A_1) + P(A_2) + \\cdots + P(A_n)\\] <p>Example: Birthday Problem</p> <p>Problem: In a group of \\(n\\) people, what's the expected number of people with a birthday on January 1st?</p> <p>Solution using indicators:</p> <ul> <li> <p>Let \\(I_i\\) be the indicator that person \\(i\\) has a birthday on January 1st</p> </li> <li> <p>\\(E[I_i] = P(\\text{person } i \\text{ born on Jan 1}) = \\frac{1}{365}\\)</p> </li> <li> <p>Total expected: \\(E[\\sum_{i=1}^n I_i] = \\sum_{i=1}^n E[I_i] = n \\cdot \\frac{1}{365}\\)</p> </li> </ul> <p>Result: We expect \\(\\frac{n}{365}\\) people to have a birthday on January 1st.</p> <p>Without using indicators, we can solve this directly using the definition of expectation:</p> <p>Let \\(X\\) be the number of people with a birthday on January 1st.</p> <p>\\(X\\) follows a binomial distribution: \\(X \\sim \\text{Binomial}(n, \\frac{1}{365})\\)</p> <p>For \\(X \\sim \\text{Binomial}(n, p)\\), we know \\(E[X] = np\\).</p> <p>Therefore:</p> \\[E[X] = n \\cdot \\frac{1}{365} = \\frac{n}{365}\\] <p>Result: Same answer, different method! The indicator method breaks down the problem into individual components, while the direct method recognizes the overall distribution. Both are valid approaches that lead to the same mathematical result.</p> <p>Example: A permutation of numbers 1 to \\(n\\) has a local maximum at the \\(j\\)-th position if the number at the \\(j\\)-th position is bigger than both its neighbors. For the first and last positions, a local maximum exists if that number is bigger than its only neighbor. Given that all \\(n!\\) permutations are equally likely, calculate the expected number of local maxima.</p> <p>Solution using Indicator Random Variables</p> <p>Let \\(I_j\\) be the indicator that position \\(j\\) has a local maximum.</p> <p>Case 1: Interior positions (\\(2 \\leq j \\leq n-1\\))</p> <p>For position \\(j\\) to be a local maximum:</p> <ul> <li> <p>The number at position \\(j\\) must be larger than the number at position \\(j-1\\)</p> </li> <li> <p>The number at position \\(j\\) must be larger than the number at position \\(j+1\\)</p> </li> </ul> <p>Probability calculation:</p> <ul> <li> <p>We need to choose 3 distinct numbers from \\(\\{1, 2, \\ldots, n\\}\\)</p> </li> <li> <p>The middle number must be the largest of the three</p> </li> <li> <p>\\(P(I_j = 1) = \\frac{1}{3}\\) (by symmetry, any of the three numbers is equally likely to be largest)</p> </li> </ul> <p>Case 2: Boundary positions (\\(j = 1\\) or \\(j = n\\))</p> <p>For position 1 to be a local maximum:</p> <ul> <li>The number at position 1 must be larger than the number at position 2</li> </ul> <p>Probability calculation:</p> <ul> <li> <p>We need to choose 2 distinct numbers from \\(\\{1, 2, \\ldots, n\\}\\)</p> </li> <li> <p>The first number must be larger than the second</p> </li> <li> <p>\\(P(I_1 = 1) = \\frac{1}{2}\\) (by symmetry, either number is equally likely to be larger)</p> </li> </ul> <p>Similarly, \\(P(I_n = 1) = \\frac{1}{2}\\).</p> <p>Using the bridge between Probability and Expectation for IRV:</p> <ul> <li> <p>Interior positions: \\(E[I_j] = \\frac{1}{3}\\) for \\(j = 2, 3, \\ldots, n-1\\)</p> </li> <li> <p>Boundary positions: \\(E[I_1] = E[I_n] = \\frac{1}{2}\\)</p> </li> </ul> <p>Use linearity of expectation</p> \\[E[\\text{Total local maxima}] = E\\left[\\sum_{j=1}^n I_j\\right] = \\sum_{j=1}^n E[I_j]\\] \\[= E[I_1] + \\sum_{j=2}^{n-1} E[I_j] + E[I_n]\\] \\[= \\frac{1}{2} + (n-2) \\cdot \\frac{1}{3} + \\frac{1}{2}\\] \\[= 1 + (n-2) \\cdot \\frac{1}{3} = 1 + \\frac{n-2}{3} = \\frac{3 + n - 2}{3} = \\frac{n+1}{3}\\] <p>The expected number of local maxima in a random permutation of \\(\\{1, 2, \\ldots, n\\}\\) is:</p> \\[E[\\text{Local maxima}] = \\frac{n+1}{3}\\] <p>This problem demonstrates the power of indicator random variables in complex combinatorial problems where direct counting would be extremely difficult.</p>"},{"location":"math/probability/jensens_inequality/","title":"Jensen's inequality","text":"<p>Jensen's inequality is one of the most fundamental and widely-used inequalities in probability theory and analysis. It provides a powerful tool for relating the expectation of a convex (or concave) function to the function of the expectation.</p>"},{"location":"math/probability/jensens_inequality/#convex-and-concave-functions","title":"Convex and Concave functions","text":""},{"location":"math/probability/jensens_inequality/#convex-functions","title":"Convex functions","text":"<p>Before stating Jensen's inequality, we need to understand what convex and concave functions are.</p> <p>A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is convex if for any two points \\(x_1, x_2\\) in its domain and any \\(\\lambda \\in [0,1]\\):</p> \\[f(\\lambda x_1 + (1-\\lambda) x_2) \\leq \\lambda f(x_1) + (1-\\lambda) f(x_2)\\] <p>Geometric interpretation: A function is convex if the line segment connecting any two points on its graph lies above or on the graph itself. In other words, the graph \"curves upward\" or is \"bowl-shaped.\"</p> <p>Let's see how the mathematical definition translates to the geometric property. Consider two points \\((x_1, f(x_1))\\) and \\((x_2, f(x_2))\\) on the graph of a convex function \\(f\\).</p> <p>The line segment connecting these points can be parameterized as:</p> \\[L(\\lambda) = (1-\\lambda)(x_1, f(x_1)) + \\lambda(x_2, f(x_2))\\] <p>This gives us:</p> <ul> <li> <p>x-coordinate: \\((1-\\lambda)x_1 + \\lambda x_2 = x_1 + \\lambda(x_2 - x_1)\\)</p> </li> <li> <p>y-coordinate: \\((1-\\lambda)f(x_1) + \\lambda f(x_2)\\)</p> </li> </ul> <p>The convexity condition states that for any point on this line segment (i.e., for any \\(\\lambda \\in [0,1]\\)), the y-coordinate of the line is greater than or equal to the function value at the corresponding x-coordinate:</p> \\[(1-\\lambda)f(x_1) + \\lambda f(x_2) \\geq f((1-\\lambda)x_1 + \\lambda x_2)\\] <p>Visual interpretation: This means that if you draw a straight line between any two points on the graph of a convex function, the entire line segment lies above or on the graph. The function \"holds water\" - it forms a bowl shape.</p>"},{"location":"math/probability/jensens_inequality/#concave-functions","title":"Concave functions","text":"<p>A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is concave if for any two points \\(x_1, x_2\\) in its domain and any \\(\\lambda \\in [0,1]\\):</p> \\[f(\\lambda x_1 + (1-\\lambda) x_2) \\geq \\lambda f(x_1) + (1-\\lambda) f(x_2)\\] <p>Geometric interpretation: A function is concave if the line segment connecting any two points on its graph lies below or on the graph itself. In other words, the graph \"curves downward\" or is \"cave-shaped.\"</p> <p>Think of convex functions as \"smiling\" curves (like a bowl) and concave functions as \"frowning\" curves (like a cave). </p> <p>For twice-differentiable functions, we have a convenient test:</p> <ul> <li> <p>\\(f\\) is convex if and only if \\(f''(x) \\geq 0\\) for all \\(x\\) in the domain</p> </li> <li> <p>\\(f\\) is concave if and only if \\(f''(x) \\leq 0\\) for all \\(x\\) in the domain</p> </li> </ul> <p>Examples:</p> <ul> <li> <p>\\(f(x) = x^2\\): \\(f''(x) = 2 &gt; 0\\) \u2192 convex</p> </li> <li> <p>\\(f(x) = \\log(x)\\): \\(f''(x) = -\\frac{1}{x^2} &lt; 0\\) \u2192 concave</p> </li> <li> <p>\\(f(x) = e^x\\): \\(f''(x) = e^x &gt; 0\\) \u2192 convex</p> </li> </ul>"},{"location":"math/probability/jensens_inequality/#theorem-jensens-inequality","title":"Theorem (Jensen's Inequality)","text":"<p>Let \\(X\\) be a random variable and let \\(\\phi\\) be a convex function. Then:</p> \\[\\phi(E[X]) \\leq E[\\phi(X)]\\] <p>If \\(\\phi\\) is strictly convex, then equality holds if and only if \\(X\\) is constant (i.e., \\(X = E[X]\\) with probability 1).</p> <p>For concave functions: If \\(\\phi\\) is concave, then the inequality is reversed:</p> \\[\\phi(E[X]) \\geq E[\\phi(X)]\\] <p>Jensen's inequality captures a fundamental geometric insight: the function of the average is less than or equal to the average of the function (for convex functions).</p> <p>Think of it this way: if you have a convex function (like \\(f(x) = x^2\\)), the graph \"curves upward.\" If you take two points on this curve and draw a line between them, the line lies above the curve. This means that the average of the function values at two points is greater than the function value at the average of those points.</p> <p>Example 1: Quadratic function Let \\(X\\) be any random variable with finite variance, and let \\(\\phi(x) = x^2\\). Since \\(x^2\\) is convex:</p> \\[(E[X])^2 \\leq E[X^2]\\] <p>This immediately gives us the relationship between mean and variance:</p> \\[\\text{Var}(X) = E[X^2] - (E[X])^2 \\geq 0\\] <p>Example 2: Logarithm function Let \\(X\\) be a positive random variable, and let \\(\\phi(x) = \\log(x)\\). Since \\(\\log(x)\\) is concave:</p> \\[\\log(E[X]) \\geq E[\\log(X)]\\] <p>Example 3: Exponential function Let \\(X\\) be any random variable, and let \\(\\phi(x) = e^x\\). Since \\(e^x\\) is convex:</p> \\[e^{E[X]} \\leq E[e^X]\\] <p>This inequality is crucial in proving concentration inequalities like Hoeffding's inequality.</p>"},{"location":"math/probability/jensens_inequality/#multivariate-version","title":"Multivariate version","text":"<p>Jensen's inequality also extends to multivariate functions. If \\(\\phi: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex and \\(\\mathbf{X}\\) is a random vector, then:</p> \\[\\phi(E[\\mathbf{X}]) \\leq E[\\phi(\\mathbf{X})]\\] <p>This multivariate version is particularly useful in machine learning and optimization contexts where we deal with vector-valued random variables.</p>"},{"location":"math/probability/joint_distributions/","title":"Joint Distributions","text":"<p>A joint distribution describes the probability distribution of two or more random variables simultaneously. It captures not only the individual behavior of each random variable but also how they relate to each other.</p> <p>For two discrete random variables \\(X\\) and \\(Y\\), the joint probability mass function (joint PMF) is defined as:</p> \\[p_{X,Y}(x,y) = P(X = x, Y = y)\\] <p>For continuous random variables, the joint probability density function (joint PDF) satisfies:</p> \\[P((X,Y) \\in A) = \\iint_A f_{X,Y}(x,y) \\, dx \\, dy\\] <p>The joint cumulative distribution function (joint CDF) is defined as:</p> \\[F_{X,Y}(x,y) = P(X \\leq x, Y \\leq y)\\] <p>For discrete random variables, this becomes:</p> \\[F_{X,Y}(x,y) = \\sum_{i \\leq x} \\sum_{j \\leq y} p_{X,Y}(i,j)\\] <p>Example: Let's consider two Bernoulli random variables \\(X\\) and \\(Y\\) with parameters \\(p\\) and \\(q\\) respectively:</p> <ul> <li> <p>\\(X \\sim \\text{Bernoulli}(p)\\) where \\(P(X = 1) = p\\) and \\(P(X = 0) = 1-p\\)</p> </li> <li> <p>\\(Y \\sim \\text{Bernoulli}(q)\\) where \\(P(Y = 1) = q\\) and \\(P(Y = 0) = 1-q\\)</p> </li> </ul> <p>The joint PMF \\(p_{X,Y}(x,y)\\) gives us the probability of each possible combination:</p> \\(X \\backslash Y\\) \\(0\\) \\(1\\) \\(0\\) \\(p_{X,Y}(0,0)\\) \\(p_{X,Y}(0,1)\\) \\(1\\) \\(p_{X,Y}(1,0)\\) \\(p_{X,Y}(1,1)\\) <p>If \\(X\\) and \\(Y\\) are independent, then:</p> \\[p_{X,Y}(x,y) = p_X(x) \\cdot p_Y(y)\\] <p>This means:</p> <ul> <li> <p>\\(p_{X,Y}(0,0) = (1-p)(1-q)\\)</p> </li> <li> <p>\\(p_{X,Y}(0,1) = (1-p)q\\)</p> </li> <li> <p>\\(p_{X,Y}(1,0) = p(1-q)\\)</p> </li> <li> <p>\\(p_{X,Y}(1,1) = pq\\)</p> </li> </ul> <p>If \\(X\\) and \\(Y\\) are dependent, the joint PMF cannot be factored this way, and we need additional information to specify the relationship between them.</p> <p>From the joint distribution, we can recover the individual (marginal) distributions:</p> \\[p_X(x) = \\sum_y p_{X,Y}(x,y)\\] \\[p_Y(y) = \\sum_x p_{X,Y}(x,y)\\] <p>For our Bernoulli example:</p> <ul> <li> <p>\\(p_X(0) = p_{X,Y}(0,0) + p_{X,Y}(0,1) = 1-p\\)</p> </li> <li> <p>\\(p_X(1) = p_{X,Y}(1,0) + p_{X,Y}(1,1) = p\\)</p> </li> <li> <p>\\(p_Y(0) = p_{X,Y}(0,0) + p_{X,Y}(1,0) = 1-q\\)</p> </li> <li> <p>\\(p_Y(1) = p_{X,Y}(0,1) + p_{X,Y}(1,1) = q\\)</p> </li> </ul> <p>For continuous random variables, the marginal PDFs are obtained by integrating the joint PDF:</p> \\[f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy\\] \\[f_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx\\] <p>Example: Consider two continuous random variables \\(X\\) and \\(Y\\) with joint PDF:</p> \\[f_{X,Y}(x,y) = \\frac{1}{2\\pi\\sigma_X\\sigma_Y\\sqrt{1-\\rho^2}} \\exp\\left(-\\frac{1}{2(1-\\rho^2)}\\left[\\frac{(x-\\mu_X)^2}{\\sigma_X^2} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y}\\right]\\right)\\] <p>To find the marginal PDF of \\(X\\), we integrate over \\(y\\):</p> \\[f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy\\] <p>After some algebra, this gives us:</p> \\[f_X(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_X} \\exp\\left(-\\frac{(x-\\mu_X)^2}{2\\sigma_X^2}\\right)\\] <p>This shows that the marginal distribution of \\(X\\) is \\(N(\\mu_X, \\sigma_X^2)\\).</p> <p>Example: Consider a uniform distribution over the disc \\(x^2 + y^2 \\leq c\\). The joint PDF is:</p> \\[f_{X,Y}(x,y) = \\begin{cases} \\frac{1}{\\pi c} &amp; \\text{if } x^2 + y^2 \\leq c \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>To find the marginal PDF of \\(X\\), we integrate over \\(y\\):</p> \\[f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy\\] <p>For a given \\(x\\) with \\(|x| \\leq \\sqrt{c}\\), the range of \\(y\\) is from \\(-\\sqrt{c-x^2}\\) to \\(\\sqrt{c-x^2}\\):</p> \\[f_X(x) = \\int_{-\\sqrt{c-x^2}}^{\\sqrt{c-x^2}} \\frac{1}{\\pi c} \\, dy = \\frac{2\\sqrt{c-x^2}}{\\pi c}\\] <p>For \\(|x| &gt; \\sqrt{c}\\), \\(f_X(x) = 0\\).</p> <p>Similarly, the marginal PDF of \\(Y\\) is:</p> \\[f_Y(y) = \\frac{2\\sqrt{c-y^2}}{\\pi c}\\] <p>This shows that the marginal distributions are not uniform - they follow a semi-circular distribution, even though the joint distribution is uniform over the disc.</p> <p>To show that \\(X\\) and \\(Y\\) are dependent, we need to verify that:</p> \\[f_{X,Y}(x,y) \\neq f_X(x) \\cdot f_Y(y)\\] <p>Let's check this for a point inside the disc, say \\((x,y) = (0,0)\\):</p> <ul> <li> <p>Joint PDF: \\(f_{X,Y}(0,0) = \\frac{1}{\\pi c}\\)</p> </li> <li> <p>Marginal PDFs: \\(f_X(0) = \\frac{2\\sqrt{c}}{\\pi c}\\) and \\(f_Y(0) = \\frac{2\\sqrt{c}}{\\pi c}\\)</p> </li> <li> <p>Product of marginals: \\(f_X(0) \\cdot f_Y(0) = \\frac{4c}{\\pi^2 c^2} = \\frac{4}{\\pi^2 c}\\)</p> </li> </ul> <p>Since \\(\\frac{1}{\\pi c} \\neq \\frac{4}{\\pi^2 c}\\), we have:</p> \\[f_{X,Y}(0,0) \\neq f_X(0) \\cdot f_Y(0)\\] <p>This proves that \\(X\\) and \\(Y\\) are dependent random variables. The dependence arises from the geometric constraint \\(x^2 + y^2 \\leq c\\) - knowing the value of \\(X\\) constrains the possible values of \\(Y\\) and vice versa.</p>"},{"location":"math/probability/markov_chains/","title":"Markov Chains","text":""},{"location":"math/probability/markov_chains/#introduction","title":"Introduction","text":"<p>To see where the Markov chain comes from, start by considering an i.i.d. sequence of random variables \\(X_0, X_1, \\ldots, X_n, \\ldots\\) where we think of \\(n\\) as time. An i.i.d. sequence has no dependence between any of the random variables- each \\(X_n\\) is independent of all previous values. A Markov chain is a sequence of r.v.s that exhibits one-step dependence.</p>"},{"location":"math/probability/markov_chains/#state-space-and-time","title":"State Space and Time","text":"<p>Markov chains \"live\" in both space and time.</p> <ul> <li> <p>State Space: The set of all possible values that the random variables \\(X_n\\) can take</p> </li> <li> <p>Time: The index \\(n\\) represents the evolution of some process over time</p> </li> </ul> <p>1. State Space Type:</p> <ul> <li> <p>Discrete State Space: States take values from a countable set (finite or infinite)</p> </li> <li> <p>Continuous State Space: States take values from a continuous set (e.g., real numbers)</p> </li> </ul> <p>2. Time Type:</p> <ul> <li> <p>Discrete Time: Process evolves at discrete time steps (\\(n = 0, 1, 2, \\ldots\\))</p> </li> <li> <p>Continuous Time: Process evolves continuously over time (\\(t \\geq 0\\))</p> </li> </ul>"},{"location":"math/probability/markov_chains/#definition-markov-chain","title":"Definition (Markov chain)","text":"<p>A sequence of random variables \\(X_0, X_1, X_2, \\ldots\\) taking values in the state space \\(\\{1, 2, \\ldots, M\\}\\) is called a Markov chain if for all \\(n \\geq 0\\),</p> \\[P(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1}, \\ldots, X_0 = i_0) = P(X_{n+1} = j | X_n = i)\\] <p>The quantity \\(P(X_{n+1} = j | X_n = i)\\) is called the transition probability from state \\(i\\) to state \\(j\\).</p> <p>If we think of time \\(n\\) as the present, times before \\(n\\) as the past, and times after \\(n\\) as the future, the Markov property says that given the present, the past and future are conditionally independent. The Markov property greatly simplifies computations of conditional probability: instead of having to condition on the entire past, we only need to condition on the most recent value.</p>"},{"location":"math/probability/markov_chains/#transition-matrix","title":"Transition matrix","text":"<p>Definition (Transition matrix): Let \\(X_0, X_1, X_2, \\ldots\\) be a Markov chain with state space \\(\\{1, 2, \\ldots, M\\}\\), and let \\(q_{ij} = P(X_{n+1} = j | X_n = i)\\) be the transition probability from state \\(i\\) to state \\(j\\). The \\(M \\times M\\) matrix \\(Q = (q_{ij})\\) is called the transition matrix of the chain.</p> <p>Note that \\(Q\\) is a nonnegative matrix in which each row sums to 1. This is because, starting from any state \\(i\\), the events \"move to 1\", \"move to 2\", \\(\\ldots\\), \"move to \\(M\\)\" are disjoint, and their union has probability 1 because the chain has to go somewhere.</p> <p>Example: Suppose that on any given day, the weather can either be rainy or sunny. If today is rainy, then tomorrow will be rainy with probability \\(1/3\\) and sunny with probability \\(2/3\\). If today is sunny, then tomorrow will be rainy with probability \\(1/2\\) and sunny with probability \\(1/2\\). Letting \\(X_n\\) be the weather on day \\(n\\), \\(X_0, X_1, X_2, \\ldots\\) is a Markov chain on the state space \\(\\{R, S\\}\\), where \\(R\\) stands for rainy and \\(S\\) for sunny. We know that the Markov property is satisfied because, from the description of the process, only today's weather matters for predicting tomorrow's.</p> <p>The transition matrix of the chain is</p> \\[\\begin{array}{c|cc}  &amp; R &amp; S \\\\ \\hline R &amp; 1/3 &amp; 2/3 \\\\ S &amp; 1/2 &amp; 1/2 \\\\ \\end{array}\\] <p>The transition probabilities of a Markov chain can also be represented with a diagram. Each state is represented by a circle, and the arrows indicate the possible one-step transitions; we can imagine a particle wandering around from state to state, randomly choosing which arrow to follow. Next to the arrows we write the corresponding transition probabilities.</p> <p></p> <p>Let's trace through a specific realization of the rainy-sunny Markov chain. Suppose we start with \\(X_0 = R\\) (rainy on day 0) and simulate the next 5 days:</p> <p>Day-by-day evolution:</p> <ul> <li> <p>\\(X_0 = R\\) (start rainy)</p> </li> <li> <p>\\(X_1 = S\\) (transition: R\u2192S with probability 2/3)</p> </li> <li> <p>\\(X_2 = R\\) (transition: S\u2192R with probability 1/2)  </p> </li> <li> <p>\\(X_3 = S\\) (transition: R\u2192S with probability 2/3)</p> </li> <li> <p>\\(X_4 = S\\) (transition: S\u2192S with probability 1/2)</p> </li> <li> <p>\\(X_5 = R\\) (transition: S\u2192R with probability 1/2)</p> </li> </ul> <p>Key observations:</p> <ul> <li> <p>Each transition depends only on the current state (Markov property)</p> </li> <li> <p>This is just one possible realization - different runs would produce different sequences</p> </li> <li> <p>The probabilities at each step are determined by the transition matrix</p> </li> </ul>"},{"location":"math/probability/markov_chains/#n-step-transition-probabilities","title":"n-Step Transition Probabilities","text":"<p>Once we have the transition matrix \\(Q\\) of a Markov chain, we can work out the transition probabilities for longer timescales.</p> <p>Definition (n-step transition probability): The n-step transition probability from \\(i\\) to \\(j\\) is the probability of being at \\(j\\) exactly \\(n\\) steps after being at \\(i\\). We denote this by \\(q^{(n)}_{ij}\\):</p> \\[q^{(n)}_{ij} = P(X_n = j | X_0 = i)\\] <p>Note that</p> \\[q^{(2)}_{ij} = \\sum_k q_{ik} q_{kj}\\] <p>since to get from \\(i\\) to \\(j\\) in two steps, the chain must go from \\(i\\) to some intermediary state \\(k\\), and then from \\(k\\) to \\(j\\); these transitions are independent because of the Markov property. Since the right-hand side is the \\((i, j)\\) entry of \\(Q^2\\) by definition of matrix multiplication, we conclude that the matrix \\(Q^2\\) gives the two-step transition probabilities.</p> <p>Example: For our rainy-sunny Markov chain with transition matrix:</p> \\[Q = \\begin{pmatrix} 1/3 &amp; 2/3 \\\\ 1/2 &amp; 1/2 \\\\ \\end{pmatrix}\\] <p>The two-step transition matrix is:</p> \\[Q^2 = \\begin{pmatrix} 1/3 &amp; 2/3 \\\\ 1/2 &amp; 1/2 \\\\ \\end{pmatrix} \\begin{pmatrix} 1/3 &amp; 2/3 \\\\ 1/2 &amp; 1/2 \\\\ \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\cdot \\frac{1}{3} + \\frac{2}{3} \\cdot \\frac{1}{2} &amp; \\frac{1}{3} \\cdot \\frac{2}{3} + \\frac{2}{3} \\cdot \\frac{1}{2} \\\\ \\frac{1}{2} \\cdot \\frac{1}{3} + \\frac{1}{2} \\cdot \\frac{1}{2} &amp; \\frac{1}{2} \\cdot \\frac{2}{3} + \\frac{1}{2} \\cdot \\frac{1}{2} \\\\ \\end{pmatrix}\\] \\[= \\begin{pmatrix} \\frac{1}{9} + \\frac{1}{3} &amp; \\frac{2}{9} + \\frac{1}{3} \\\\ \\frac{1}{6} + \\frac{1}{4} &amp; \\frac{1}{3} + \\frac{1}{4} \\\\ \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{9} + \\frac{3}{9} &amp; \\frac{2}{9} + \\frac{3}{9} \\\\ \\frac{2}{12} + \\frac{3}{12} &amp; \\frac{4}{12} + \\frac{3}{12} \\\\ \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{9} &amp; \\frac{5}{9} \\\\ \\frac{5}{12} &amp; \\frac{7}{12} \\\\ \\end{pmatrix}\\] <p>Let's compute \\(q^{(2)}_{RS}\\) (starting rainy, ending sunny after 2 steps) using the formula \\(q^{(2)}_{ij} = \\sum_k q_{ik} q_{kj}\\):</p> \\[q^{(2)}_{RS} = \\sum_{k} q_{Rk} q_{kS} = q_{RR} \\cdot q_{RS} + q_{RS} \\cdot q_{SS}\\] <p>Substituting the values from our transition matrix:</p> \\[q^{(2)}_{RS} = \\frac{1}{3} \\cdot \\frac{2}{3} + \\frac{2}{3} \\cdot \\frac{1}{2} = \\frac{2}{9} + \\frac{1}{3} = \\frac{2}{9} + \\frac{3}{9} = \\frac{5}{9}\\] <p>The total probability is \\(\\frac{5}{9}\\).</p> <p>Verification: The \\((R,S)\\) entry is indeed \\(\\frac{5}{9}\\), which matches our calculation!</p> <p>By induction, the \\(n\\)th power of the transition matrix gives the \\(n\\)-step transition probabilities:</p> <p>\\(q^{(n)}_{ij}\\) is the \\((i, j)\\) entry of \\(Q^n\\).</p>"},{"location":"math/probability/markov_chains/#conditional-distributions-encoded-in-transition-matrices","title":"Conditional Distributions encoded in Transition Matrices","text":"<p>The transition matrix \\(Q\\) encodes the conditional distribution of \\(X_1\\) given the initial state of the chain. Specifically, the \\(i\\)th row of \\(Q\\) is the conditional PMF of \\(X_1\\) given \\(X_0 = i\\), displayed as a row vector. Similarly, the \\(i\\)th row of \\(Q^n\\) is the conditional PMF of \\(X_n\\) given \\(X_0 = i\\).</p> <p>Example: For our rainy-sunny Markov chain with transition matrix:</p> \\[Q = \\begin{pmatrix} 1/3 &amp; 2/3 \\\\ 1/2 &amp; 1/2 \\\\ \\end{pmatrix}\\] <p>One-step conditional distributions:</p> <ul> <li>Given \\(X_0 = R\\) (rainy): The first row \\((1/3, 2/3)\\) gives the conditional PMF of \\(X_1\\):</li> <li>\\(P(X_1 = R | X_0 = R) = 1/3\\)</li> <li> <p>\\(P(X_1 = S | X_0 = R) = 2/3\\)</p> </li> <li> <p>Given \\(X_0 = S\\) (sunny): The second row \\((1/2, 1/2)\\) gives the conditional PMF of \\(X_1\\):</p> </li> <li>\\(P(X_1 = R | X_0 = S) = 1/2\\)</li> <li>\\(P(X_1 = S | X_0 = S) = 1/2\\)</li> </ul> <p>Two-step conditional distributions:</p> <p>From our calculated \\(Q^2 = \\begin{pmatrix} 4/9 &amp; 5/9 \\\\ 5/12 &amp; 7/12 \\end{pmatrix}\\):</p> <ul> <li>Given \\(X_0 = R\\): The first row \\((4/9, 5/9)\\) gives the conditional PMF of \\(X_2\\):</li> <li>\\(P(X_2 = R | X_0 = R) = 4/9\\)</li> <li> <p>\\(P(X_2 = S | X_0 = R) = 5/9\\)</p> </li> <li> <p>Given \\(X_0 = S\\): The second row \\((5/12, 7/12)\\) gives the conditional PMF of \\(X_2\\):</p> </li> <li>\\(P(X_2 = R | X_0 = S) = 5/12\\)</li> <li>\\(P(X_2 = S | X_0 = S) = 7/12\\)</li> </ul> <p>Key insight: Each row of \\(Q^n\\) sums to 1, representing a valid probability distribution over the state space, conditioned on the initial state.</p>"},{"location":"math/probability/markov_chains/#marginal-distributions","title":"Marginal Distributions","text":"<p>To get the marginal distributions of \\(X_0, X_1, \\ldots\\), we need to specify not only the transition matrix, but also the initial conditions of the chain. The initial state \\(X_0\\) can be specified deterministically, or randomly according to some distribution. Let \\((t_1, t_2, \\ldots, t_M)\\) be the PMF of \\(X_0\\) displayed as a vector, that is, \\(t_i = P(X_0 = i)\\). Then the marginal distribution of the chain at any time can be computed from the transition matrix, averaging over all the states using LOTP.</p> <p>Important note: The initial distribution vector \\(t\\) is completely independent of the transition matrix \\(Q\\). The vector \\(t\\) specifies how the chain starts (the probabilities of being in each state at time 0), while \\(Q\\) specifies how the chain evolves from one time step to the next. We can choose any initial distribution \\(t\\) we want - it doesn't need to be related to \\(Q\\) in any way. For example, we could start deterministically in state 1 with \\(t = (1, 0, 0, 0)\\), or with any other probability distribution over the four states.</p> <p>Proposition (Marginal distribution of \\(X_n\\)): Define \\(t = (t_1, t_2, \\ldots, t_M)\\) by \\(t_i = P(X_0 = i)\\), and view \\(t\\) as a row vector. Then the marginal distribution of \\(X_n\\) is given by the vector \\(tQ^n\\). That is, the \\(j\\)th component of \\(tQ^n\\) is \\(P(X_n = j)\\).</p> <p>Proof: By the law of total probability, conditioning on \\(X_0\\), the probability that the chain is in state \\(j\\) after \\(n\\) steps is</p> \\[P(X_n = j) = \\sum_{i=1}^{M} P(X_0 = i)P(X_n = j | X_0 = i) = \\sum_{i=1}^{M} t_i q^{(n)}_{ij}\\] <p>which is the \\(j\\)th component of \\(tQ^n\\) by definition of matrix multiplication.</p> <p>Example (Marginal distributions of 4-state Markov chain): Consider the 4-state Markov chain shown in the figure below. </p> <p></p> <p>Suppose that the initial conditions are \\(t = (1/4, 1/4, 1/4, 1/4)\\), meaning that the chain has equal probability of starting in each of the four states. When no probabilities are written over the arrows, as in this case, it means all arrows originating from a given state are equally likely. For example, there are 3 arrows originating from state 1, so the transitions 1-&gt;3, 1-&gt;2, and 1-&gt;1 all have probability 1/3.Let \\(X_n\\) be the position of the chain at time \\(n\\). Then the marginal distribution of \\(X_1\\) is</p> \\[tQ = \\begin{pmatrix} 1/4 &amp; 1/4 &amp; 1/4 &amp; 1/4 \\\\ \\end{pmatrix} \\begin{pmatrix} 1/3 &amp; 1/3 &amp; 1/3 &amp; 0 \\\\ 0 &amp; 0 &amp; 1/2 &amp; 1/2 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1/2 &amp; 0 &amp; 0 &amp; 1/2 \\\\ \\end{pmatrix} = \\begin{pmatrix} 5/24 &amp; 1/3 &amp; 5/24 &amp; 1/4 \\\\ \\end{pmatrix}\\] <p>The marginal distribution of \\(X_5\\) is</p> \\[tQ^5 = \\begin{pmatrix} 1/4 &amp; 1/4 &amp; 1/4 &amp; 1/4 \\\\ \\end{pmatrix} \\begin{pmatrix} 853/3888 &amp; 509/1944 &amp; 52/243 &amp; 395/1296 \\\\ 173/864 &amp; 85/432 &amp; 31/108 &amp; 91/288 \\\\ 37/144 &amp; 29/72 &amp; 1/9 &amp; 11/48 \\\\ 499/2592 &amp; 395/1296 &amp; 71/324 &amp; 245/864 \\\\ \\end{pmatrix} = \\begin{pmatrix} 3379/15552 &amp; 2267/7776 &amp; 101/486 &amp; 1469/5184 \\\\ \\end{pmatrix}\\]"},{"location":"math/probability/multinomial_distribution/","title":"Multinomial Distribution","text":"<p>The multinomial distribution is a generalization of the binomial distribution to multiple categories or outcomes. It describes the probability distribution of counts across several mutually exclusive and exhaustive categories.</p> <p>A random vector \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_k)\\) follows a multinomial distribution with parameters \\(n\\) and \\(\\mathbf{p} = (p_1, p_2, \\ldots, p_k)\\), denoted as \\(\\mathbf{X} \\sim \\text{Multinomial}(n, \\mathbf{p})\\), if its joint probability mass function is:</p> \\[P(X_1 = x_1, X_2 = x_2, \\ldots, X_k = x_k) = \\frac{n!}{x_1! x_2! \\cdots x_k!} p_1^{x_1} p_2^{x_2} \\cdots p_k^{x_k}\\] <p>where:</p> <ul> <li> <p>\\(n\\) is the total number of trials</p> </li> <li> <p>\\(x_i\\) is the number of outcomes in category \\(i\\) (with \\(x_i \\geq 0\\))</p> </li> <li> <p>\\(p_i\\) is the probability of category \\(i\\) (with \\(0 \\leq p_i \\leq 1\\))</p> </li> <li> <p>The constraint \\(\\sum_{i=1}^k x_i = n\\) must hold</p> </li> <li> <p>The constraint \\(\\sum_{i=1}^k p_i = 1\\) must hold</p> </li> </ul> <p>The multinomial distribution models:</p> <ul> <li> <p>\\(n\\) independent trials where each trial results in exactly one of \\(k\\) possible outcomes</p> </li> <li> <p>Fixed probabilities \\(p_1, p_2, \\ldots, p_k\\) for each outcome</p> </li> <li> <p>Counts \\(X_1, X_2, \\ldots, X_k\\) representing how many times each outcome occurred</p> </li> </ul> <p>When \\(k = 2\\), the multinomial distribution reduces to the binomial distribution:</p> <ul> <li> <p>\\(X_1 \\sim \\text{Binomial}(n, p_1)\\)</p> </li> <li> <p>\\(X_2 = n - X_1\\) (since \\(X_1 + X_2 = n\\))</p> </li> <li> <p>\\(p_2 = 1 - p_1\\)</p> </li> </ul>"},{"location":"math/probability/multinomial_distribution/#marginal-distribution","title":"Marginal Distribution","text":"<p>Let's take a look at the marginal distribution of \\(X_i\\), which is the \\(i\\)th component of \\(\\mathbf{X}\\). Were we to blindly apply the definition, we would have to sum the joint PMF over all components of \\(\\mathbf{X}\\) other than \\(X_i\\). The prospect of \\(k-1\\) summations is an unpleasant one, to say the least.</p> <p>Fortunately, we can avoid tedious calculations if we instead use the story of the Multinomial distribution: \\(X_i\\) is the number of objects in category \\(i\\), where each of the \\(n\\) objects independently belongs to category \\(i\\) with probability \\(p_i\\). Define success as landing in category \\(i\\). Then we just have \\(n\\) independent Bernoulli trials, so the marginal distribution of \\(X_i\\) is \\(\\text{Bin}(n, p_i)\\).</p> <p>The marginals of a Multinomial are Binomial. Specifically, if \\(\\mathbf{X} \\sim \\text{Mult}_k(n, \\mathbf{p})\\), then \\(X_i \\sim \\text{Bin}(n, p_i)\\).</p>"},{"location":"math/probability/multinomial_distribution/#merging-categories","title":"Merging Categories","text":"<p>More generally, whenever we merge multiple categories together in a Multinomial random vector, we get another Multinomial random vector. For example, suppose we randomly sample \\(n\\) people in a country with 5 political parties (if the sampling is done without replacement, the \\(n\\) trials are not independent, but independence is a good approximation as long as the population is large relative to the sample). Let \\(\\mathbf{X} = (X_1, \\ldots, X_5) \\sim \\text{Mult}_5(n, (p_1, \\ldots, p_5))\\) represent the political party affiliations of the sample, i.e., \\(X_j\\) is the number of people in the sample who support party \\(j\\).</p> <p>Suppose that parties 1 and 2 are the dominant parties, while parties 3 through 5 are minor third parties. If we decide that instead of keeping track of all 5 parties, we only want to count the number of people in party 1, party 2, or \"other\", then we can define a new random vector that lumps all the third parties into one category:</p> \\[\\mathbf{Y} = (X_1, X_2, X_3 + X_4 + X_5)\\] <p>By the story of the Multinomial,</p> \\[\\mathbf{Y} \\sim \\text{Mult}_3(n, (p_1, p_2, p_3 + p_4 + p_5))\\] <p>Of course, this idea applies to merging categories in any Multinomial, not just in the context of political parties.</p> <p>General Rule: If \\(\\mathbf{X} \\sim \\text{Mult}_k(n, \\mathbf{p})\\), then for any distinct \\(i\\) and \\(j\\), \\(X_i + X_j \\sim \\text{Bin}(n, p_i + p_j)\\). The random vector of counts obtained from merging categories \\(i\\) and \\(j\\) is still Multinomial. For example, merging categories 1 and 2 gives:</p> \\[(X_1 + X_2, X_3, \\ldots, X_k) \\sim \\text{Mult}_{k-1}(n, (p_1 + p_2, p_3, \\ldots, p_k))\\]"},{"location":"math/probability/multinomial_distribution/#conditional-distribution","title":"Conditional Distribution","text":"<p>Suppose we get to observe \\(X_1\\), the number of objects in category 1, and we wish to update our distribution for the other categories \\((X_2, \\ldots, X_k)\\). One way to do this is with the definition of conditional PMF:</p> \\[P(X_2 = n_2, \\ldots, X_k = n_k|X_1 = n_1) = \\frac{P(X_1 = n_1, X_2 = n_2, \\ldots, X_k = n_k)}{P(X_1 = n_1)}\\] <p>The numerator is the joint PMF of the Multinomial, and the denominator is the marginal PMF of \\(X_1\\), both of which we have already derived. However, we prefer to use the Multinomial story to deduce the conditional distribution of \\((X_2, \\ldots, X_k)\\) without algebra.</p> <p>Given that there are \\(n_1\\) objects in category 1, the remaining \\(n - n_1\\) objects fall into categories 2 through \\(k\\), independently of one another. By Bayes' rule (Bayes' rule: \\(P(\\text{in category } j|\\text{not in category } 1) = \\frac{P(\\text{in category } j \\text{ AND not in category } 1)}{P(\\text{not in category } 1)}\\)), the conditional probability of falling into category \\(j\\) is:</p> \\[P(\\text{in category } j|\\text{not in category } 1) = \\frac{P(\\text{in category } j)}{P(\\text{not in category } 1)} = \\frac{p_j}{p_2 + \\cdots + p_k}\\] <p>for \\(j = 2, \\ldots, k\\). This makes intuitive sense: the updated probabilities are proportional to the original probabilities \\((p_2, \\ldots, p_k)\\), but these must be renormalized to yield a valid probability vector.</p> <p>Putting it all together, we have the following result:</p> <p>If \\(\\mathbf{X} \\sim \\text{Mult}_k(n, \\mathbf{p})\\), then</p> \\[(X_2, \\ldots, X_k)|X_1 = n_1 \\sim \\text{Mult}_{k-1}(n - n_1, (p'_2, \\ldots, p'_k))\\] <p>where \\(p'_j = \\frac{p_j}{p_2 + \\cdots + p_k}\\).</p> <p>Finally, we know that components within a Multinomial random vector are dependent since they are constrained by \\(X_1 + \\cdots + X_k = n\\).</p>"},{"location":"math/probability/multivariate_normal_distribution/","title":"Multivariate Normal Distribution","text":"<p>The multivariate normal distribution is a generalization of the univariate normal distribution to multiple dimensions. It's one of the most important distributions in statistics and machine learning, serving as the foundation for many statistical methods and models.</p>"},{"location":"math/probability/multivariate_normal_distribution/#definition","title":"Definition","text":"<p>A random vector \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_d)^T\\) follows a multivariate normal distribution with mean vector \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^d\\) and covariance matrix \\(\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}\\) (denoted as \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)) if its probability density function is:</p> \\[f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right)\\] <p>where:</p> <ul> <li> <p>\\(\\mathbf{x} = (x_1, x_2, \\ldots, x_d)^T\\) is a \\(d\\)-dimensional vector</p> </li> <li> <p>\\(\\boldsymbol{\\mu} = (\\mu_1, \\mu_2, \\ldots, \\mu_d)^T\\) is the mean vector</p> </li> <li> <p>\\(\\boldsymbol{\\Sigma}\\) is the covariance matrix</p> </li> <li> <p>\\(|\\boldsymbol{\\Sigma}|\\) denotes the determinant of \\(\\boldsymbol{\\Sigma}\\)</p> </li> </ul> <p>Important: This PDF \\(f(\\mathbf{x})\\) is the joint probability density function of the random variables \\(X_1, X_2, \\ldots, X_d\\) in the vector. That is, \\(f(x_1, x_2, \\ldots, x_d)\\) gives the joint density of all \\(d\\) random variables simultaneously.</p> <p>Alternative Definition</p> <p>A random vector \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_d)^T\\) follows a multivariate normal distribution if and only if every linear combination of its components is univariate normal. That is, for any constants \\(t_1, t_2, \\ldots, t_d \\in \\mathbb{R}\\) (not all zero), the random variable:</p> \\[Y = t_1X_1 + t_2X_2 + \\cdots + t_dX_d\\] <p>follows a univariate normal distribution.</p> <p>Mean Vector \\(\\boldsymbol{\\mu}\\):</p> <p>The mean vector contains the expected values of each component:</p> \\[\\boldsymbol{\\mu} = E[\\mathbf{X}] = (E[X_1], E[X_2], \\ldots, E[X_d])^T\\] <p>Covariance Matrix \\(\\boldsymbol{\\Sigma}\\):</p> <p>The covariance matrix captures the relationships between variables:</p> \\[\\boldsymbol{\\Sigma}_{ij} = \\text{Cov}(X_i, X_j) = E[(X_i - \\mu_i)(X_j - \\mu_j)]\\] <p>Properties of \\(\\boldsymbol{\\Sigma}\\):</p> <ul> <li> <p>Symmetric: \\(\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}^T\\)</p> </li> <li> <p>Positive definite: \\(\\mathbf{v}^T \\boldsymbol{\\Sigma} \\mathbf{v} &gt; 0\\) for all non-zero vectors \\(\\mathbf{v}\\)</p> </li> <li> <p>Diagonal elements: \\(\\boldsymbol{\\Sigma}_{ii} = \\text{Var}(X_i) = \\sigma_i^2\\)</p> </li> </ul>"},{"location":"math/probability/multivariate_normal_distribution/#properties","title":"Properties","text":"<p>1. Linear Transformations</p> <p>If \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) and \\(\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}\\), then:</p> \\[\\mathbf{Y} \\sim \\mathcal{N}(\\mathbf{A}\\boldsymbol{\\mu} + \\mathbf{b}, \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^T)\\] <p>2. Marginal Distributions</p> <p>Any subset of components follows a multivariate normal distribution. For example, if \\(\\mathbf{X} = (X_1, X_2, X_3)^T \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), then:</p> \\[(X_1, X_2)^T \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2 \\end{pmatrix}\\right)\\]"},{"location":"math/probability/multivariate_normal_distribution/#case-study-bivariate-normal-distribution-d-2","title":"Case study: Bivariate Normal Distribution (\\(d = 2\\))","text":"<p>For two variables \\(X_1\\) and \\(X_2\\):</p> \\[\\mathbf{X} = \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2 \\end{pmatrix}\\right)\\] <p>The joint density becomes:</p> \\[f(x_1, x_2) = \\frac{1}{2\\pi\\sqrt{\\sigma_1^2\\sigma_2^2 - \\sigma_{12}^2}} \\exp\\left(-\\frac{1}{2(\\sigma_1^2\\sigma_2^2 - \\sigma_{12}^2)}\\left[\\sigma_2^2(x_1-\\mu_1)^2 - 2\\sigma_{12}(x_1-\\mu_1)(x_2-\\mu_2) + \\sigma_1^2(x_2-\\mu_2)^2\\right]\\right)\\] <p>Note: The determinant of the covariance matrix is \\(|\\boldsymbol{\\Sigma}| = \\sigma_1^2\\sigma_2^2 - \\sigma_{12}^2\\), and the inverse covariance matrix is:</p> \\[\\boldsymbol{\\Sigma}^{-1} = \\frac{1}{\\sigma_1^2\\sigma_2^2 - \\sigma_{12}^2}\\begin{pmatrix} \\sigma_2^2 &amp; -\\sigma_{12} \\\\ -\\sigma_{12} &amp; \\sigma_1^2 \\end{pmatrix}\\] <p></p>"},{"location":"math/probability/normal_distribution/","title":"Normal Distribution","text":"<p>The Normal distribution (also called the Gaussian distribution) is one of the most important continuous distributions in probability and statistics. It appears naturally in many contexts due to the Central Limit Theorem and provides a foundation for many statistical methods.</p>"},{"location":"math/probability/normal_distribution/#standard-normal-distribution","title":"Standard Normal Distribution","text":"<p>A continuous random variable \\(Z\\) is said to have the standard Normal distribution if its PDF \\(f_Z\\) is given by:</p> \\[f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}, \\quad -\\infty &lt; z &lt; \\infty\\] <p>We write this as \\(Z \\sim N(0, 1)\\) since, as we will show, \\(Z\\) has mean 0 and variance 1.</p> <p>The constant \\(\\frac{1}{\\sqrt{2\\pi}}\\) in front of the PDF may look surprising (why is something with \\(\\pi\\) needed in front of something with \\(e\\), when there are no circles in sight?), but it's exactly what is needed to make the PDF integrate to 1. Such constants are called normalizing constants because they normalize the total area under the PDF to 1.</p> <p>By symmetry, the mean of the standard normal distribution is 0. Here's why:</p> <p>The standard normal PDF is symmetric about 0:</p> \\[f_Z(z) = f_Z(-z) \\quad \\text{for all } z\\] <p>This means the distribution looks the same on both sides of 0. The mean is defined as:</p> \\[E[Z] = \\int_{-\\infty}^{\\infty} z \\cdot f_Z(z) \\, dz\\] <p>Let's split this integral into two parts:</p> \\[E[Z] = \\int_{-\\infty}^0 z \\cdot f_Z(z) \\, dz + \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz\\] <p>First integral (\\(-\\infty\\) to 0): Let \\(u = -z\\), so \\(z = -u\\) and \\(dz = -du\\)</p> \\[\\int_{-\\infty}^0 z \\cdot f_Z(z) \\, dz = \\int_{\\infty}^0 (-u) \\cdot f_Z(-u) \\cdot (-du) = \\int_0^{\\infty} u \\cdot f_Z(u) \\, du\\] <p>Second integral (0 to \\(\\infty\\)): This is already in the right form</p> \\[\\int_0^{\\infty} z \\cdot f_Z(z) \\, dz\\] \\[E[Z] = \\int_0^{\\infty} u \\cdot f_Z(u) \\, du + \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz\\] <p>Since \\(u\\) and \\(z\\) are just dummy variables, we can write this as:</p> \\[E[Z] = \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz + \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz = 2 \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz\\] <p>The integrand \\(z \\cdot f_Z(z)\\) is odd because:</p> <ul> <li> <p>\\(f_Z(z) = f_Z(-z)\\) (even function)</p> </li> <li> <p>\\(z\\) is odd</p> </li> <li> <p>Product of even and odd functions is odd</p> </li> </ul> <p>For odd functions, the integral from \\(-\\infty\\) to \\(\\infty\\) equals 0.</p> <p>Therefore, \\(E[Z] = 0\\).</p> <p>Now let's show that the variance of the standard normal distribution is 1. The variance is defined as:</p> \\[\\text{Var}(Z) = E[(Z - E[Z])^2] = E[Z^2]\\] <p>Since we already showed that \\(E[Z] = 0\\), we have \\(\\text{Var}(Z) = E[Z^2]\\).</p> <p>Calculating \\(E[Z^2]\\):</p> \\[E[Z^2] = \\int_{-\\infty}^{\\infty} z^2 \\cdot f_Z(z) \\, dz = \\int_{-\\infty}^{\\infty} z^2 \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz\\] <p>Let's use integration by parts with:</p> <ul> <li> <p>\\(u = z\\) and \\(dv = z \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz\\)</p> </li> <li> <p>\\(du = dz\\) and \\(v = -\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\)</p> </li> </ul> <p>Integration by parts formula: \\(\\int u \\, dv = uv - \\int v \\, du\\)</p> \\[E[Z^2] = \\left[z \\cdot \\left(-\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\right)\\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} \\left(-\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\right) \\, dz\\] <p>The boundary term evaluates to 0:</p> <ul> <li> <p>At \\(z = \\infty\\): \\(z \\cdot e^{-z^2/2} \\to 0\\) (exponential decay dominates)</p> </li> <li> <p>At \\(z = -\\infty\\): \\(z \\cdot e^{-z^2/2} \\to 0\\) (exponential decay dominates)</p> </li> </ul> <p>Therefore:</p> \\[E[Z^2] = 0 - \\int_{-\\infty}^{\\infty} \\left(-\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\right) \\, dz = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz\\] <p>The remaining integral is exactly the integral of the PDF from \\(-\\infty\\) to \\(\\infty\\), which equals 1:</p> \\[E[Z^2] = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz = 1\\] <p>Therefore, \\(\\text{Var}(Z) = E[Z^2] = 1\\).</p> <p>The standard Normal CDF \\(\\Phi\\) is the accumulated area under the PDF:</p> \\[\\Phi(z) = \\int_{-\\infty}^z f_Z(t) \\, dt = \\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}} e^{-t^2/2} \\, dt\\] <p>Some people, upon seeing the function \\(\\Phi\\) for the first time, express dismay that it is left in terms of an integral. Unfortunately, we have little choice in the matter: it turns out to be mathematically impossible to find a closed-form expression for the antiderivative of \\(f_Z\\), meaning that we cannot express \\(\\Phi\\) as a finite sum of more familiar functions like polynomials or exponentials. But closed-form or no, it's still a well-defined function: if we give \\(\\Phi\\) an input \\(z\\), it returns the accumulated area under the PDF from \\(-\\infty\\) up to \\(z\\).</p>"},{"location":"math/probability/normal_distribution/#general-normal-distribution","title":"General Normal Distribution","text":"<p>The general Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) (or variance \\(\\sigma^2\\)) is obtained by applying a linear transformation to the standard normal distribution. If \\(Z \\sim N(0, 1)\\) is a standard normal random variable, then:</p> \\[X = \\mu + \\sigma Z\\] <p>has a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). We write this as \\(X \\sim N(\\mu, \\sigma^2)\\).</p> <p>Mean and Variance: </p> <ul> <li> <p>\\(E[X] = E[\\mu + \\sigma Z] = \\mu + \\sigma E[Z] = \\mu + \\sigma \\cdot 0 = \\mu\\)</p> </li> <li> <p>\\(\\text{Var}(X) = \\text{Var}(\\mu + \\sigma Z) = \\sigma^2 \\text{Var}(Z) = \\sigma^2 \\cdot 1 = \\sigma^2\\)</p> </li> </ul> <p>CDF: The CDF of \\(X \\sim N(\\mu, \\sigma^2)\\) is:</p> \\[F_X(x) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\\] <p>where \\(\\Phi\\) is the standard normal CDF.</p> <p>Derivation of the CDF: Since \\(X = \\mu + \\sigma Z\\), we can find the CDF by:</p> \\[F_X(x) = P(X \\leq x) = P(\\mu + \\sigma Z \\leq x) = P\\left(Z \\leq \\frac{x - \\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\\] <p>PDF: The PDF of \\(X \\sim N(\\mu, \\sigma^2)\\) is:</p> \\[f_X(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\quad -\\infty &lt; x &lt; \\infty\\] <p>Derivation of the PDF from the CDF: We can derive the PDF by taking the derivative of the CDF with respect to \\(x\\):</p> \\[f_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx} \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\\] <p>Using the chain rule and the fact that \\(\\frac{d}{dz} \\Phi(z) = \\phi(z)\\) (where \\(\\phi(z)\\) is the standard normal PDF):</p> \\[f_X(x) = \\phi\\left(\\frac{x - \\mu}{\\sigma}\\right) \\cdot \\frac{d}{dx}\\left(\\frac{x - \\mu}{\\sigma}\\right)\\] <p>Since \\(\\frac{d}{dx}\\left(\\frac{x - \\mu}{\\sigma}\\right) = \\frac{1}{\\sigma}\\), we have:</p> \\[f_X(x) = \\phi\\left(\\frac{x - \\mu}{\\sigma}\\right) \\cdot \\frac{1}{\\sigma}\\] <p>Substituting the standard normal PDF \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\):</p> \\[f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\cdot \\frac{1}{\\sigma} = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] <p>Standardization: Any normal random variable \\(X \\sim N(\\mu, \\sigma^2)\\) can be standardized to a standard normal by:</p> \\[Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\] <p>This is the inverse of the transformation \\(X = \\mu + \\sigma Z\\).</p>"},{"location":"math/probability/poisson_distribution/","title":"Poisson Distribution","text":"<p>The Poisson distribution is one of the most important discrete probability distributions, modeling the number of events occurring in a fixed interval of time or space when these events happen independently at a constant average rate.</p> <p>This is arguably the most important discrete distribution in Statistics.</p> <p>A random variable \\(X\\) follows a Poisson distribution with parameter \\(\\lambda &gt; 0\\) (denoted \\(X \\sim \\text{Poisson}(\\lambda)\\)) if its probability mass function is:</p> \\[P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\quad \\text{for } k = 0, 1, 2, 3, \\ldots\\] <p>Parameters:</p> <ul> <li> <p>\\(\\lambda\\): Average number of events in the interval (also called the rate parameter)</p> </li> <li> <p>Support: \\(X\\) takes values in \\(\\{0, 1, 2, 3, \\ldots\\}\\) (non-negative integers)</p> </li> </ul> <p>What does the Poisson distribution model?</p> <ul> <li> <p>Rare events: Events that occur infrequently but consistently</p> </li> <li> <p>Independent occurrences: Each event is independent of others</p> </li> <li> <p>Constant rate: Events happen at a steady average rate</p> </li> <li> <p>Fixed interval: We count events in a specific time period or region</p> </li> </ul> <p>Examples:</p> <ul> <li> <p>Number of phone calls received in an hour</p> </li> <li> <p>Number of defects in a square meter of fabric</p> </li> <li> <p>Number of accidents at an intersection per day</p> </li> <li> <p>Number of customers arriving at a store in 10 minutes</p> </li> </ul> <p>The Poisson distribution can be derived as a limiting case of the binomial distribution.</p> <p>Setup: Consider \\(n\\) independent Bernoulli trials, each with success probability \\(p = \\frac{\\lambda}{n}\\)</p> <p>Binomial PMF: \\(P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k}\\)</p> <p>Take the limit: As \\(n \\to \\infty\\) while keeping \\(np = \\lambda\\) constant</p> <p>Result: The binomial PMF converges to the Poisson PMF</p> <p>Mathematical details:</p> \\[\\lim_{n \\to \\infty} \\binom{n}{k} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1-\\frac{\\lambda}{n}\\right)^{n-k} = \\frac{e^{-\\lambda} \\lambda^k}{k!}\\] <p>Proof: Let's prove this step by step.</p> <p>Start with the binomial PMF:</p> \\[P(X = k) = \\binom{n}{k} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\] <p>Expand the binomial coefficient:</p> \\[\\binom{n}{k} = \\frac{n!}{k!(n-k)!} = \\frac{n(n-1)(n-2)\\cdots(n-k+1)}{k!}\\] <p>Substitute and rearrange:</p> \\[P(X = k) = \\frac{n(n-1)(n-2)\\cdots(n-k+1)}{k!} \\cdot \\left(\\frac{\\lambda}{n}\\right)^k \\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\] \\[= \\frac{\\lambda^k}{k!} \\cdot \\frac{n(n-1)(n-2)\\cdots(n-k+1)}{n^k} \\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\] <p>Analyze this factor as \\(n \\to \\infty\\):</p> \\[\\frac{n(n-1)(n-2)\\cdots(n-k+1)}{n^k} = \\frac{n}{n} \\cdot \\frac{n-1}{n} \\cdot \\frac{n-2}{n} \\cdots \\frac{n-k+1}{n}\\] \\[= 1 \\cdot \\left(1-\\frac{1}{n}\\right) \\cdot \\left(1-\\frac{2}{n}\\right) \\cdots \\left(1-\\frac{k-1}{n}\\right)\\] <p>As \\(n \\to \\infty\\), each term \\(\\left(1-\\frac{j}{n}\\right) \\to 1\\) for \\(j = 1, 2, \\ldots, k-1\\).</p> <p>Therefore:</p> \\[\\lim_{n \\to \\infty} \\frac{n(n-1)(n-2)\\cdots(n-k+1)}{n^k} = 1\\] <p>Analyze the last factor as \\(n \\to \\infty\\):</p> \\[\\left(1-\\frac{\\lambda}{n}\\right)^{n-k} = \\left(1-\\frac{\\lambda}{n}\\right)^n \\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}\\] <p>As \\(n \\to \\infty\\):</p> <ul> <li> <p>\\(\\left(1-\\frac{\\lambda}{n}\\right)^n \\to e^{-\\lambda}\\) (this is the definition of \\(e\\))</p> </li> <li> <p>\\(\\left(1-\\frac{\\lambda}{n}\\right)^{-k} \\to 1^{-k} = 1\\)</p> </li> </ul> <p>Combine all limits:</p> \\[\\lim_{n \\to \\infty} P(X = k) = \\frac{\\lambda^k}{k!} \\cdot 1 \\cdot e^{-\\lambda} = \\frac{e^{-\\lambda} \\lambda^k}{k!}\\] <p>Intuition: When we have many rare events (large \\(n\\), small \\(p\\)), the binomial distribution becomes approximately Poisson.</p> <p>Example: A student types a 1000-word essay and makes an average of 2 typos per 1000 words. What's the probability of making exactly 3 typos?</p> <p>Solution:</p> <ul> <li> <p>Rate: \\(\\lambda = 2\\) typos per 1000 words</p> </li> <li> <p>Random variable: \\(X \\sim \\text{Poisson}(2)\\)</p> </li> <li> <p>Probability: \\(P(X = 3) = \\frac{e^{-2} \\cdot 2^3}{3!} = \\frac{e^{-2} \\cdot 8}{6} \\approx 0.180\\)</p> </li> </ul> <p>Example: A coffee shop serves an average of 5 customers every 15 minutes. What's the probability of serving at least 2 customers in the next 15 minutes?</p> <p>Solution:</p> <ul> <li> <p>Rate: \\(\\lambda = 5\\) customers per 15 minutes</p> </li> <li> <p>Random variable: \\(X \\sim \\text{Poisson}(5)\\)</p> </li> <li> <p>Probability: \\(P(X \\geq 2) = 1 - P(X = 0) - P(X = 1)\\)</p> </li> </ul> <p>First, calculate individual probabilities:</p> <ul> <li> <p>\\(P(X = 0) = \\frac{e^{-5} \\cdot 5^0}{0!} = e^{-5} \\approx 0.0067\\)</p> </li> <li> <p>\\(P(X = 1) = \\frac{e^{-5} \\cdot 5^1}{1!} = 5e^{-5} \\approx 0.0337\\)</p> </li> </ul> <p>Therefore:</p> <ul> <li>\\(P(X \\geq 2) = 1 - 0.0067 - 0.0337 = 0.9596\\)</li> </ul> <p>Note: </p> <ul> <li> <p>\\(\\lambda\\) is the average rate of events occurring</p> </li> <li> <p>Expectation represents the long-run average of the random variable</p> </li> <li> <p>In a Poisson process, we expect \\(\\lambda\\) events per unit time or space on average</p> </li> </ul> <p>Let's see if  \\(E[X] = \\lambda\\) is the case.</p> <p>Start with the definition of expectation for a discrete random variable:</p> \\[E[X] = \\sum_{k=0}^{\\infty} k \\cdot P(X = k) = \\sum_{k=0}^{\\infty} k \\cdot \\frac{e^{-\\lambda} \\lambda^k}{k!}\\] <p>Notice that the first term (\\(k = 0\\)) is 0, so we can start from \\(k = 1\\):</p> \\[E[X] = \\sum_{k=1}^{\\infty} k \\cdot \\frac{e^{-\\lambda} \\lambda^k}{k!}\\] <p>Cancel the \\(k\\):</p> \\[E[X] = \\sum_{k=1}^{\\infty} \\frac{e^{-\\lambda} \\lambda^k}{(k-1)!}\\] <p>Factor out \\(e^{-\\lambda}\\) and \\(\\lambda\\):</p> \\[E[X] = e^{-\\lambda} \\lambda \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}\\] <p>Make a change of variable: let \\(j = k-1\\). Then \\(k = j+1\\) and when \\(k = 1\\), \\(j = 0\\):</p> \\[E[X] = e^{-\\lambda} \\lambda \\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!}\\] <p>Recognize the series as the Taylor series for \\(e^{\\lambda}\\):</p> \\[\\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!} = e^{\\lambda}\\] <p>Substitute and simplify:</p> \\[E[X] = e^{-\\lambda} \\lambda \\cdot e^{\\lambda} = \\lambda\\] <p>Final result: \\(E[X] = \\lambda\\)</p> <p>Let's verify that the Poisson PMF satisfies all required properties:</p> <p>Property 1: Non-negativity</p> <p>\\(P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\geq 0\\) for all \\(k \\geq 0\\) since:</p> <ul> <li> <p>\\(e^{-\\lambda} &gt; 0\\)</p> </li> <li> <p>\\(\\lambda^k \\geq 0\\) for \\(\\lambda &gt; 0\\)</p> </li> <li> <p>\\(k! &gt; 0\\) for all \\(k \\geq 0\\)</p> </li> </ul> <p>Property 2: Sum to 1</p> \\[\\sum_{k=0}^{\\infty} P(X = k) = \\sum_{k=0}^{\\infty} \\frac{e^{-\\lambda} \\lambda^k}{k!} = e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} = e^{-\\lambda} \\cdot e^{\\lambda} = 1\\] <p>The key insight is that \\(\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} = e^{\\lambda}\\) (the Taylor series for \\(e^{\\lambda}\\)).</p> <p>Property 3: Probability bounds</p> <p>\\(0 \\leq P(X = k) \\leq 1\\) for each \\(k\\), which follows from Properties 1 and 2.</p>"},{"location":"math/probability/probability_and_counting/","title":"Probability and counting","text":""},{"location":"math/probability/probability_and_counting/#sample-spaces","title":"Sample spaces","text":"<p>The mathematical framework for probability is built around sets. Imagine that an experiment is performed, resulting in one out of a set of possible outcomes. Before the experiment is performed, it is unknown which outcome will be the result; after, the result \"crystallizes\" into the actual outcome. The sample space S of an experiment is the set of all possible outcomes of the experiment. An event A is a subset of the sample space S, and we say that A occurred if the actual outcome is in A.  When the sample space is finite, we can visualize it as Pebble World (figure above). Each pebble represents an outcome, and an event is a set of pebbles. Performing the experiment amounts to randomly selecting one pebble. If all the pebbles are of the same mass, all the pebbles are equally likely to be chosen.</p> <p>Set theory is very useful in probability, since it provides a rich language for expressing and working with events. Set operations, especially unions, intersections, and complements, make it easy to build new events in terms of already defined events.</p> <p>Example:  A coin is flipped 10 times. Writing Heads as H and Tails as T, a possible outcome is: HHHTHHTTHT. The sample space is the set of all possible strings of length 10 consisting of H's and T's. We can (and will) encode H as <code>1</code> and T as <code>0</code>, so that an outcome is a sequence: $$ (s_1, s_2, \\dots, s_{10}) \\quad \\text{with} \\quad s_j \\in {0, 1} $$ The sample space is the set of all such sequences.</p> <p>Some events:</p> <ol> <li>Event A_1: the first flip is Heads. As a set: $$ A_1 =  (1, s_2, \\dots, s_{10}) \\; \\mid \\; s_j \\in {0,1} \\; \\text{and } 2 \\leq j \\leq 10  $$ This is a subset of the sample space, so it is indeed an event. Saying that A_1 occurs is equivalent to saying that the first flip is Heads. Similarly, let A_j be the event that the j-th flip is Heads, for: $$ j = 2, 3, \\dots, 10 $$</li> <li>Event B: at least one flip was Heads. As a set: $$ B = \\bigcup_{j=1}^{10} A_j $$</li> <li>Event C: all the flips were Heads. As a set: $$ C = \\bigcap_{j=1}^{10} A_j $$</li> <li>Event D: there were at least two consecutive Heads. As a set: $$ D = \\bigcup_{j=1}^{9} \\left( A_j \\cap A_{j+1} \\right) $$ </li> </ol>"},{"location":"math/probability/probability_and_counting/#naive-definition-of-probability","title":"Naive definition of probability","text":"<p>(Naive definition of probability). Let \\( A \\) be an event for an experiment with a finite sample space \\( S \\). The naive probability of \\( A \\) is</p> \\[ P_{\\text{naive}}(A) = \\frac{|A|}{|S|} = \\frac{\\text{number of outcomes favorable to } A}{\\text{total number of outcomes in } S} \\] <p>The naive definition is very restrictive in that it requires S to be finite, with equal mass for each pebble (all outcomes equally likely). It has often been misapplied by people who assume equally likely outcomes without justification and make arguments to the e\ufb00ect of \u201ceither it will happen or it won\u2019t, and we don\u2019t know which, so it\u2019s 50-50\u201d.</p>"},{"location":"math/probability/probability_and_counting/#counting","title":"Counting","text":"<p>The multiplication rule is a fundamental principle in counting that allows us to count the number of possible outcomes of a sequence of experiments.</p> <p>Multiplication Rule: If Experiment 1 has \\(n_1\\) possible outcomes, Experiment 2 has \\(n_2\\) possible outcomes, ..., and Experiment \\(r\\) has \\(n_r\\) possible outcomes, then the sequence of all \\(r\\) experiments has \\(n_1 \\times n_2 \\times \\cdots \\times n_r\\) possible outcomes.</p> <p>Example: A license plate consists of 3 letters followed by 3 digits. How many different license plates are possible?</p> <ul> <li>Experiment 1: Choose the first letter (26 outcomes)</li> <li>Experiment 2: Choose the second letter (26 outcomes) </li> <li>Experiment 3: Choose the third letter (26 outcomes)</li> <li>Experiment 4: Choose the first digit (10 outcomes)</li> <li>Experiment 5: Choose the second digit (10 outcomes)</li> <li>Experiment 6: Choose the third digit (10 outcomes)</li> </ul> <p>By the multiplication rule, the total number of possible license plates is: \\(26 \\times 26 \\times 26 \\times 10 \\times 10 \\times 10 = 26^3 \\times 10^3 = 17,576,000\\)</p> <p>Key insight: The multiplication rule applies when the experiments are independent- the number of possible outcomes of each experiment doesn't depend on the outcomes of previous experiments.</p> <p>Example: Probability of a Full House in Poker</p> <p>A full house in poker consists of three cards of one rank and two cards of another rank (e.g., three Kings and two 7s). Let's calculate the probability of being dealt a full house in a 5-card hand from a standard 52-card deck.</p> <p>Step 1: Count the total number of possible 5-card hands - This is the number of ways to choose 5 cards from 52 cards - Total hands = \\(\\binom{52}{5} = \\frac{52!}{5!(52-5)!} = \\frac{52!}{5! \\cdot 47!} = 2,598,960\\)</p> <p>Step 2: Count the number of full house hands We can break this down using the multiplication rule:</p> <ol> <li>Choose the rank for the three-of-a-kind: 13 choices (A, 2, 3, ..., 10, J, Q, K)</li> <li>Choose 3 cards of that rank: \\(\\binom{4}{3} = 4\\) ways (since there are 4 cards of each rank)</li> <li>Choose a different rank for the pair: 12 choices (must be different from the three-of-a-kind rank)</li> <li>Choose 2 cards of that rank: \\(\\binom{4}{2} = 6\\) ways</li> </ol> <p>By the multiplication rule, the number of full house hands is: \\(13 \\times 4 \\times 12 \\times 6 = 3,744\\)</p> <p>Step 3: Calculate the probability Using the naive definition of probability:</p> \\[P(\\text{Full House}) = \\frac{\\text{Number of full house hands}}{\\text{Total number of hands}} = \\frac{3,744}{2,598,960} = \\frac{6}{4,165} \\approx 0.00144\\] <p>So the probability of being dealt a full house is approximately 0.144%, or about 1 in 694 hands.</p>"},{"location":"math/probability/probability_and_counting/#sampling-choosing-k-items-from-n-items","title":"Sampling: Choosing \\(k\\) Items from \\(n\\) Items","text":"<p>When we want to choose \\(k\\) items from a set of \\(n\\) items, there are four fundamental cases depending on whether:</p> <ol> <li> <p>Replacement: Can we choose the same item multiple times?</p> </li> <li> <p>Order: Does the order of selection matter?</p> </li> </ol> <p>This gives us four distinct sampling scenarios:</p> <p>Case 1: Sampling with Replacement, Order Matters</p> <ul> <li> <p>Description: Choose \\(k\\) items from \\(n\\) items, allowing repeats, where order matters</p> </li> <li> <p>Example: Rolling a die 3 times (can get same number multiple times, order matters)</p> </li> <li> <p>Formula: \\(n^k\\)</p> </li> <li> <p>Derivation: By the multiplication rule, each of the \\(k\\) choices has \\(n\\) possible outcomes, so total outcomes = \\(n \\times n \\times \\cdots \\times n = n^k\\)</p> </li> </ul> <p>Case 2: Sampling without Replacement, Order Matters</p> <ul> <li> <p>Description: Choose \\(k\\) items from \\(n\\) items, no repeats, where order matters</p> </li> <li> <p>Example: Choosing 3 people from 10 to be president, vice-president, and treasurer</p> </li> <li> <p>Formula: \\(P(n,k) = \\frac{n!}{(n-k)!} = n \\times (n-1) \\times (n-2) \\times \\cdots \\times (n-k+1)\\)</p> </li> <li> <p>Derivation: First choice has \\(n\\) options, second choice has \\((n-1)\\) options (can't repeat), third choice has \\((n-2)\\) options, etc. By multiplication rule: \\(n \\times (n-1) \\times (n-2) \\times \\cdots \\times (n-k+1) = \\frac{n!}{(n-k)!}\\)</p> </li> </ul> <p>Case 3: Sampling without Replacement, Order Doesn't Matter</p> <ul> <li> <p>Description: Choose \\(k\\) items from \\(n\\) items, no repeats, where order doesn't matter</p> </li> <li> <p>Example: Choosing 3 people from 10 to be on a committee</p> </li> <li> <p>Formula: \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)</p> </li> <li> <p>Derivation: This is the number of combinations. We start with Case 2- (\\(P(n,k)\\)) and divide by \\(k!\\) because each group of \\(k\\) items can be arranged in \\(k!\\) different orders, but we only want to count each group once.</p> </li> </ul> <p>Case 4: Sampling with Replacement, Order Doesn't Matter</p> <ul> <li> <p>Description: Choose \\(k\\) items from \\(n\\) items, allowing repeats, where order doesn't matter</p> </li> <li> <p>Example: Choosing 3 scoops of ice cream from 4 flavors, where you can get multiple scoops of the same flavor (e.g., 2 chocolate and 1 vanilla)</p> </li> <li> <p>Formula: \\(\\binom{n+k-1}{k} = \\binom{n+k-1}{n-1}\\)</p> </li> <li> <p>Derivation: Let's derive the formula using a visual approach. Imagine we have 3 scoops (represented by x's) and 4 flavors (separated by dividers |). For example, xx|x|| means 2 scoops of flavor 1, 1 scoop of flavor 2, and 0 scoops of flavors 3 and 4. We need to arrange 3 x's and 3 dividers (to create 4 sections) in a total of 6 positions. Now, in these 6 positions, we place 3 scoops. Once the 3 scoops (x) are placed, the dividers (|) are automatically determined. A few examples to illustrate this:</p> </li> <li> <p>xxx||| means 3 scoops of flavor 1, 0 scoops of flavors 2, 3, and 4</p> </li> <li>x|x|x| means 1 scoop of each flavor and 0 scoops of flavor 4</li> <li>||xxx| means 0 scoops of flavors 1 and 2, 3 scoops of flavor 3, 0 scoops of flavor 4</li> </ul> <p>The key insight is that we're choosing 3 positions out of 6 total positions to place our scoops (x's). The number of ways to do this is \\(\\binom{6}{3} = 20\\). </p> <p>In general, for \\(k\\) scoops and \\(n\\) flavors, we have \\((k + n - 1)\\) total positions and we need to choose \\(k\\) positions for the scoops. This gives us \\(\\binom{k + n - 1}{k} = \\binom{n + k - 1}{k}\\).</p> <p>Summary Table:</p> Replacement Order Matters Formula Name With Yes \\(n^k\\) Permutations with replacement Without Yes \\(\\frac{n!}{(n-k)!}\\) Permutations without replacement Without No \\(\\frac{n!}{k!(n-k)!}\\) Combinations With No \\(\\binom{n+k-1}{k}\\) Combinations with replacement <p>Example: Splitting 10 People into teams</p> <p>Case 1: Distinguishable teams (Team A and Team B)</p> <p>Suppose we want to split 10 people into two teams: Team A with 6 people and Team B with 4 people.</p> <p>Question: How many ways can we do this?</p> <p>Solution: We need to choose 6 people out of 10 for Team A. The remaining 4 people automatically go to Team B. The number of ways is:</p> \\[\\binom{10}{6} = \\frac{10!}{6! \\cdot 4!} = \\frac{10 \\times 9 \\times 8 \\times 7}{4 \\times 3 \\times 2 \\times 1} = 210\\] <p>Case 2: Indistinguishable teams</p> <p>Now suppose we want to split 10 people into two teams of 5, but the teams are indistinguishable (no Team A vs Team B).</p> <p>Question: How many ways can we do this?</p> <p>Solution: Since the teams are indistinguishable, we've double-counted each arrangement. For example, the arrangement where people {1,2,3,4,5} are on one team and {6,7,8,9,10} are on the other is the same as the arrangement where {6,7,8,9,10} are on one team and {1,2,3,4,5} are on the other.</p> <p>The number of ways is:</p> \\[\\frac{\\binom{10}{6}}{2} = \\frac{210}{2} = 105\\] <p>For unequal-sized groups (like 6 and 4), the two sides of the split are inherently distinguishable due to their sizes.</p>"},{"location":"math/probability/random_variables/","title":"Random Variables","text":""},{"location":"math/probability/random_variables/#definition-of-a-random-variable","title":"Definition of a Random Variable","text":"<p>A random variable is a function that maps outcomes from a sample space to real numbers. Formally, if \\(S\\) is a sample space, then a random variable \\(X\\) is a function:</p> \\[X: S \\rightarrow \\mathbb{R}\\] <p>Example 1: Coin toss</p> <ul> <li> <p>Sample space: \\(S = \\{\\text{Heads}, \\text{Tails}\\}\\)</p> </li> <li> <p>Random variable: \\(X(\\text{Heads}) = 1\\), \\(X(\\text{Tails}) = 0\\)</p> </li> <li> <p>Interpretation: \\(X\\) counts the number of heads</p> </li> </ul> <p>Example 2: Rolling a die</p> <ul> <li> <p>Sample space: \\(S = \\{1, 2, 3, 4, 5, 6\\}\\)</p> </li> <li> <p>Random variable: \\(X(\\omega) = \\omega\\) (identity function)</p> </li> <li> <p>Interpretation: \\(X\\) gives the face value of the die</p> </li> </ul> <p>Example 3: Multiple coin tosses</p> <ul> <li> <p>Sample space: \\(S = \\{\\text{HH}, \\text{HT}, \\text{TH}, \\text{TT}\\}\\)</p> </li> <li> <p>Random variable: \\(X(\\text{HH}) = 2\\), \\(X(\\text{HT}) = 1\\), \\(X(\\text{TH}) = 1\\), \\(X(\\text{TT}) = 0\\)</p> </li> <li> <p>Interpretation: \\(X\\) counts the total number of heads</p> </li> </ul>"},{"location":"math/probability/random_variables/#probability-mass-function-pmf","title":"Probability Mass Function (PMF)","text":"<p>Before discussing distributions, let's understand what a Probability Mass Function (PMF) is.</p> <p>A Probability Mass Function is a function that gives the probability that a discrete random variable takes on a specific value. For a discrete random variable \\(X\\), the PMF is defined as:</p> \\[p_X(x) = P(X = x)\\] <p>Key Properties of PMF:</p> <ol> <li> <p>Non-negativity: \\(p_X(x) \\geq 0\\) for all possible values \\(x\\)</p> </li> <li> <p>Sum to 1: \\(\\sum_x p_X(x) = 1\\) (sum over all possible values)</p> </li> <li> <p>Probability interpretation: \\(0 \\leq p_X(x) \\leq 1\\) for each \\(x\\)</p> </li> </ol>"},{"location":"math/probability/random_variables/#cumulative-distribution-function-cdf","title":"Cumulative Distribution Function (CDF)","text":"<p>Another important function for describing random variables is the Cumulative Distribution Function (CDF).</p> <p>A Cumulative Distribution Function is a function that gives the probability that a random variable takes on a value less than or equal to a given number. For a random variable \\(X\\), the CDF is defined as:</p> \\[F_X(x) = P(X \\leq x)\\] <p>Key Properties of CDF:</p> <ol> <li> <p>Non-decreasing: \\(F_X(x) \\leq F_X(y)\\) whenever \\(x \\leq y\\) (monotonicity)</p> </li> <li> <p>Bounded: \\(0 \\leq F_X(x) \\leq 1\\) for all \\(x\\)</p> </li> <li> <p>Limits: \\(\\lim_{x \\to -\\infty} F_X(x) = 0\\) and \\(\\lim_{x \\to \\infty} F_X(x) = 1\\)</p> </li> </ol>"},{"location":"math/probability/random_variables/#distribution-of-a-random-variable","title":"Distribution of a Random Variable","text":"<p>The distribution of a random variable \\(X\\) describes how the probability mass (or density) is distributed across all possible values that \\(X\\) can take. It tells us the complete probabilistic behavior of \\(X\\). It can be represented in various ways, including a PMF, a cumulative distribution function (CDF), or other methods.</p> <p>The distribution answers the question: \"What is the probability that \\(X\\) takes on a particular value or falls within a particular range?\"</p> <p>Consider the die-rolling random variable \\(X\\):</p> <ul> <li> <p>Sample space: \\(S = \\{1, 2, 3, 4, 5, 6\\}\\)</p> </li> <li> <p>Random variable: \\(X(\\omega) = \\omega\\)</p> </li> <li> <p>Distribution: \\(P(X = k) = \\frac{1}{6}\\) for \\(k = 1, 2, 3, 4, 5, 6\\)</p> </li> </ul> <p>The distribution tells us that:</p> <ul> <li> <p>Each face is equally likely</p> </li> <li> <p>The probability of any specific value is \\(\\frac{1}{6}\\)</p> </li> <li> <p>The probability of rolling an even number is \\(P(X \\in \\{2, 4, 6\\}) = \\frac{1}{2}\\)</p> </li> </ul> <p>While a PMF is a key concept for describing discrete distributions, it's not the only way to define a distribution (e.g., CDF, table, etc.). </p>"},{"location":"math/probability/random_variables/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>The Bernoulli distribution is the simplest discrete probability distribution, modeling a random experiment with exactly two possible outcomes: success or failure.</p> <p>A random variable \\(X\\) follows a Bernoulli distribution with parameter \\(p\\) (denoted \\(X \\sim \\text{Bernoulli}(p)\\)) if:</p> <ul> <li> <p>Possible values: \\(X\\) takes only two values: \\(0\\) (failure) and \\(1\\) (success)</p> </li> <li> <p>Parameter: \\(p \\in [0, 1]\\) represents the probability of success</p> </li> <li> <p>Probability mass function:</p> </li> </ul> \\[P(X = 1) = p \\quad \\text{and} \\quad P(X = 0) = 1 - p\\] \\[P(X = k) = p^k(1-p)^{1-k} \\quad \\text{for } k \\in \\{0, 1\\}\\] <p>This compact formula gives:</p> <ul> <li> <p>\\(P(X = 1) = p^1(1-p)^0 = p\\)</p> </li> <li> <p>\\(P(X = 0) = p^0(1-p)^1 = 1-p\\)</p> </li> </ul> <p>Example: coin toss</p> <ul> <li> <p>Success: Heads (with probability \\(p = \\frac{1}{2}\\))</p> </li> <li> <p>Failure: Tails (with probability \\(1-p = \\frac{1}{2}\\))</p> </li> <li> <p>Random variable: \\(X(\\text{Heads}) = 1\\), \\(X(\\text{Tails}) = 0\\)</p> </li> </ul>"},{"location":"math/probability/random_variables/#binomial-distribution","title":"Binomial Distribution","text":"<p>The binomial distribution models the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success.</p> <p>A random variable \\(X\\) follows a binomial distribution with parameters \\(n\\) and \\(p\\) (denoted \\(X \\sim \\text{Binomial}(n, p)\\)) if:</p> <ul> <li> <p>Possible values: \\(X\\) takes values in \\(\\{0, 1, 2, \\ldots, n\\}\\)</p> </li> <li> <p>Parameters: </p> </li> <li> <p>\\(n \\in \\mathbb{N}\\) (number of trials)</p> </li> <li> <p>\\(p \\in [0, 1]\\) (probability of success in each trial)</p> </li> <li> <p>Probability mass function:</p> </li> </ul> \\[P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k} \\quad \\text{for } k = 0, 1, 2, \\ldots, n\\] <p>The PMF formula \\(P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k}\\) has three components:</p> <ol> <li> <p>\\(\\binom{n}{k}\\): Number of ways to choose \\(k\\) successes from \\(n\\) trials</p> </li> <li> <p>\\(p^k\\): Probability of \\(k\\) successes</p> </li> <li> <p>\\((1-p)^{n-k}\\): Probability of \\(n-k\\) failures</p> </li> </ol> <p>Example: Quality Control</p> <ul> <li> <p>Experiment: Test 100 products from a production line</p> </li> <li> <p>Random variable: \\(X\\) = number of defective products</p> </li> <li> <p>Distribution: \\(X \\sim \\text{Binomial}(100, 0.02)\\) (assuming 2% defect rate)</p> </li> <li> <p>Probability of at most 3 defects: \\(P(X \\leq 3) = \\sum_{k=0}^3 \\binom{100}{k} (0.02)^k (0.98)^{100-k}\\)</p> </li> </ul> <p>Let's derive the binomial PMF formula \\(P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k}\\) using combinatorial reasoning.</p> <p>We want to find the probability of getting exactly \\(k\\) successes in \\(n\\) independent Bernoulli trials, where each trial has success probability \\(p\\).</p> <p>Since the trials are independent, the probability of any specific sequence of outcomes is the product of individual probabilities.</p> <p>Example: For \\(n = 3\\) trials with \\(k = 2\\) successes, one possible sequence is:</p> <ul> <li> <p>Trial 1: Success (probability \\(p\\))</p> </li> <li> <p>Trial 2: Success (probability \\(p\\))</p> </li> <li> <p>Trial 3: Failure (probability \\(1-p\\))</p> </li> </ul> <p>Probability of this specific sequence: \\(p \\cdot p \\cdot (1-p) = p^2(1-p)^1 = p^k(1-p)^{n-k}\\)</p> <p>The key insight is that there are multiple sequences that result in exactly \\(k\\) successes and \\(n-k\\) failures.</p> <p>Question: How many different ways can we arrange \\(k\\) successes and \\(n-k\\) failures in \\(n\\) positions?</p> <p>Answer: \\(\\binom{n}{k}\\) , that is, the number of ways to choose \\(k\\) positions out of \\(n\\) for the successes</p> <p>Probability of any specific sequence with \\(k\\) successes and \\(n-k\\) failures:</p> \\[P(\\text{specific sequence}) = p^k(1-p)^{n-k}\\] <p>Number of different sequences with exactly \\(k\\) successes:</p> \\[\\text{Number of sequences} = \\binom{n}{k}\\] <p>Total probability using the addition rule (sums probabilities of mutually exclusive sequences):</p> \\[P(X = k) = \\binom{n}{k} \\cdot p^k(1-p)^{n-k}\\] <p>Let's verify that the binomial PMF \\(p_X(k) = \\binom{n}{k} p^k(1-p)^{n-k}\\) satisfies all the required properties of a PMF.</p> <p>Property 1: Non-negativity</p> <p>We need to show that \\(p_X(k) \\geq 0\\) for all \\(k \\in \\{0, 1, 2, \\ldots, n\\}\\).</p> <p>Proof: </p> <ul> <li> <p>\\(\\binom{n}{k} \\geq 0\\) (combinatorial coefficient is always non-negative)</p> </li> <li> <p>\\(p^k \\geq 0\\) (since \\(p \\in [0, 1]\\) and \\(k \\geq 0\\))</p> </li> <li> <p>\\((1-p)^{n-k} \\geq 0\\) (since \\(1-p \\in [0, 1]\\) and \\(n-k \\geq 0\\))</p> </li> </ul> <p>Since all three factors are non-negative, their product \\(p_X(k) = \\binom{n}{k} p^k(1-p)^{n-k} \\geq 0\\) \u2713</p> <p>Property 2: Sum to 1</p> <p>We need to show that \\(\\sum_{k=0}^n p_X(k) = 1\\).</p> <p>Proof: </p> \\[\\sum_{k=0}^n p_X(k) = \\sum_{k=0}^n \\binom{n}{k} p^k(1-p)^{n-k}\\] <p>This is exactly the binomial expansion of \\((p + (1-p))^n\\):</p> \\[(p + (1-p))^n = \\sum_{k=0}^n \\binom{n}{k} p^k(1-p)^{n-k}\\] <p>But \\(p + (1-p) = 1\\), so:</p> \\[(p + (1-p))^n = 1^n = 1\\] <p>Therefore, \\(\\sum_{k=0}^n p_X(k) = 1\\) \u2713</p> <p>Property 3: Probability Interpretation</p> <p>We need to show that \\(0 \\leq p_X(k) \\leq 1\\) for each \\(k\\).</p> <p>Proof: </p> <ul> <li> <p>We already showed \\(p_X(k) \\geq 0\\) (Property 1)</p> </li> <li> <p>Since \\(\\sum_{k=0}^n p_X(k) = 1\\) (Property 2) and all terms are non-negative, no individual term can exceed 1</p> </li> <li> <p>Therefore, \\(0 \\leq p_X(k) \\leq 1\\) for each \\(k\\) \u2713</p> </li> </ul> <p>One of the most important properties of the binomial distribution is the addition property, which states that the sum of two independent binomial random variables is also binomial under certain conditions.</p> <p>Theorem: If \\(X \\sim \\text{Binomial}(n_1, p)\\) and \\(Y \\sim \\text{Binomial}(n_2, p)\\) are independent random variables with the same success probability \\(p\\), then:</p> \\[X + Y \\sim \\text{Binomial}(n_1 + n_2, p)\\] <p>This property makes intuitive sense:</p> <ul> <li> <p>\\(X\\): Random variable outputting number of successes in \\(n_1\\) trials with success probability \\(p\\)</p> </li> <li> <p>\\(Y\\): Random variable outputting number of successes in \\(n_2\\) trials with success probability \\(p\\)</p> </li> <li> <p>\\(X + Y\\): Random variable outputting number total number of successes in \\(n_1 + n_2\\) trials with success probability \\(p\\)</p> </li> </ul>"},{"location":"math/probability/random_variables/#hypergeometric-distribution","title":"Hypergeometric Distribution","text":"<p>The hypergeometric distribution models situations where we sample without replacement from a finite population, making it fundamentally different from the binomial distribution.</p> <p>Imagine you're on a treasure hunt with a group of friends. You've discovered an ancient chest containing 20 precious gems:</p> <ul> <li> <p>8 diamonds (the rare, valuable ones)</p> </li> <li> <p>12 emeralds (still beautiful, but less valuable)</p> </li> </ul> <p>The chest is sealed with a magical lock that only opens if you can correctly identify the composition of gems inside. Here's the catch: you can only draw 5 gems to examine them, and once you draw a gem, you cannot put it back (no replacement).</p> <p>Your mission: What's the probability that exactly 3 of your 5 draws are diamonds?</p> <p>Understanding the Problem:</p> <p>Population: 20 total gems</p> <ul> <li> <p>Success items: 8 diamonds (what we're interested in)</p> </li> <li> <p>Failure items: 12 emeralds</p> </li> <li> <p>Sample size: 5 gems drawn</p> </li> <li> <p>Target: Exactly 3 diamonds</p> </li> </ul> <p>Key difference from binomial: In the binomial case, you'd put each gem back after examining it, so the probability of drawing a diamond would stay constant at \\(\\frac{8}{20} = 0.4\\). But here, each draw affects the remaining population!</p> <p>For a hypergeometric random variable \\(X\\) with parameters:</p> <ul> <li> <p>\\(N\\) = total population size</p> </li> <li> <p>\\(K\\) = number of success items in population</p> </li> <li> <p>\\(n\\) = sample size</p> </li> </ul> <p>The PMF is:</p> \\[P(X = k) = \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\] <p>Where \\(k\\) is the number of success items in your sample.</p> <p>Numerator: \\(\\binom{K}{k} \\cdot \\binom{N-K}{n-k}\\)</p> <ul> <li> <p>\\(\\binom{K}{k}\\): Ways to choose \\(k\\) diamonds from \\(K\\) diamonds</p> </li> <li> <p>\\(\\binom{N-K}{n-k}\\): Ways to choose \\((n-k)\\) emeralds from \\((N-K)\\) emeralds</p> </li> <li> <p>Product: Total ways to get exactly \\(k\\) diamonds and \\((n-k)\\) emeralds</p> </li> </ul> <p>Denominator: \\(\\binom{N}{n}\\)</p> <ul> <li>Total ways to choose any \\(n\\) gems from \\(N\\) gems</li> </ul> <p>Result: Probability of getting exactly \\(k\\) diamonds</p> <p>Solution:</p> <ul> <li> <p>\\(N = 20\\) (total gems)</p> </li> <li> <p>\\(K = 8\\) (diamonds)</p> </li> <li> <p>\\(n = 5\\) (sample size)</p> </li> <li> <p>\\(k = 3\\) (target diamonds)</p> </li> </ul> <p>Calculation:</p> \\[P(X = 3) = \\frac{\\binom{8}{3} \\cdot \\binom{12}{2}}{\\binom{20}{5}}\\] <p>Computing the combinations:</p> <ul> <li> <p>\\(\\binom{8}{3} = \\frac{8!}{3! \\cdot 5!} = 56\\) (ways to choose 3 diamonds)</p> </li> <li> <p>\\(\\binom{12}{2} = \\frac{12!}{2! \\cdot 10!} = 66\\) (ways to choose 2 emeralds)</p> </li> <li> <p>\\(\\binom{20}{5} = \\frac{20!}{5! \\cdot 15!} = 15,504\\) (total ways to choose 5 gems)</p> </li> </ul> <p>Final probability:</p> \\[P(X = 3) = \\frac{56 \\cdot 66}{15,504} = \\frac{3,696}{15,504} \\approx 0.238\\] <p>So there's about a 23.8% chance of drawing exactly 3 diamonds!</p> <p>Key insight: The hypergeometric distribution captures the dependence between draws that occurs in finite populations, unlike the binomial distribution which assumes independence.</p> Aspect Binomial Hypergeometric Replacement With replacement Without replacement Probability Constant \\(p\\) Changes with each draw Independence Independent trials Dependent trials Population Infinite/very large Finite Formula \\(\\binom{n}{k} p^k(1-p)^{n-k}\\) \\(\\frac{\\binom{K}{k} \\binom{N-K}{n-k}}{\\binom{N}{n}}\\) <p>Let's verify that the hypergeometric PMF \\(P(X = k) = \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\) satisfies all the required properties of a valid PMF.</p> <p>Property 1: Non-negativity</p> <p>We need to show that \\(P(X = k) \\geq 0\\) for all valid values of \\(k\\).</p> <p>Proof: </p> <ul> <li> <p>\\(\\binom{K}{k} \\geq 0\\) (combinatorial coefficient is always non-negative)</p> </li> <li> <p>\\(\\binom{N-K}{n-k} \\geq 0\\) (combinatorial coefficient is always non-negative)</p> </li> <li> <p>\\(\\binom{N}{n} &gt; 0\\) (denominator is positive since \\(n \\leq N\\))</p> </li> </ul> <p>Since all factors are non-negative and the denominator is positive, the ratio \\(P(X = k) \\geq 0\\) \u2713</p> <p>Note: \\(k\\) must satisfy \\(0 \\leq k \\leq \\min(K, n)\\) and \\(n-k \\leq N-K\\) (i.e., \\(k \\geq \\max(0, n-(N-K))\\)).</p> <p>Property 2: Sum to 1</p> <p>We need to show that \\(\\sum_{k} P(X = k) = 1\\) over all valid values of \\(k\\).</p> <p>Proof: </p> \\[\\sum_{k} P(X = k) = \\sum_{k} \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\] <p>The valid range for \\(k\\) is \\(\\max(0, n-(N-K)) \\leq k \\leq \\min(K, n)\\). Let's call this range \\(k_{\\min}\\) to \\(k_{\\max}\\).</p> \\[\\sum_{k=k_{\\min}}^{k_{\\max}} P(X = k) = \\frac{1}{\\binom{N}{n}} \\sum_{k=k_{\\min}}^{k_{\\max}} \\binom{K}{k} \\cdot \\binom{N-K}{n-k}\\] <p>Key insight: This sum equals \\(\\binom{N}{n}\\) by the Vandermonde identity!</p> <p>The Vandermonde identity states:</p> \\[\\sum_{k=0}^{\\min(K,n)} \\binom{K}{k} \\binom{N-K}{n-k} = \\binom{N}{n}\\] <p>Therefore, \\(\\sum_{k=k_{\\min}}^{k_{\\max}} P(X = k) = \\frac{1}{\\binom{N}{n}} \\cdot \\binom{N}{n} = 1\\) \u2713</p> <p>Property 3: Probability Interpretation</p> <p>We need to show that \\(0 \\leq P(X = k) \\leq 1\\) for each valid \\(k\\).</p> <p>Proof: </p> <ul> <li> <p>We already showed \\(P(X = k) \\geq 0\\) (Property 1)</p> </li> <li> <p>Since \\(\\sum_{k} P(X = k) = 1\\) (Property 2) and all terms are non-negative, no individual term can exceed 1</p> </li> <li> <p>Therefore, \\(0 \\leq P(X = k) \\leq 1\\) for each valid \\(k\\) \u2713</p> </li> </ul> <p>Let's verify these properties for our treasure hunt example:</p> <ul> <li> <p>\\(N = 20\\) (total gems)</p> </li> <li> <p>\\(K = 8\\) (diamonds)</p> </li> <li> <p>\\(n = 5\\) (sample size)</p> </li> </ul> <p>Valid range for \\(k\\): \\(\\max(0, 5-(20-8)) = \\max(0, -7) = 0\\) to \\(\\min(8, 5) = 5\\)</p> <p>So \\(k\\) can be 0, 1, 2, 3, 4, or 5.</p> <p>PMF values:</p> <ul> <li> <p>\\(P(X = 0) = \\frac{\\binom{8}{0} \\cdot \\binom{12}{5}}{\\binom{20}{5}} = \\frac{1 \\cdot 792}{15,504} \\approx 0.051\\)</p> </li> <li> <p>\\(P(X = 1) = \\frac{\\binom{8}{1} \\cdot \\binom{12}{4}}{\\binom{20}{5}} = \\frac{8 \\cdot 495}{15,504} \\approx 0.255\\)</p> </li> <li> <p>\\(P(X = 2) = \\frac{\\binom{8}{2} \\cdot \\binom{12}{3}}{\\binom{20}{5}} = \\frac{28 \\cdot 220}{15,504} \\approx 0.397\\)</p> </li> <li> <p>\\(P(X = 3) = \\frac{\\binom{8}{3} \\cdot \\binom{12}{2}}{\\binom{20}{5}} = \\frac{56 \\cdot 66}{15,504} \\approx 0.238\\)</p> </li> <li> <p>\\(P(X = 4) = \\frac{\\binom{8}{4} \\cdot \\binom{12}{1}}{\\binom{20}{5}} = \\frac{70 \\cdot 12}{15,504} \\approx 0.054\\)</p> </li> <li> <p>\\(P(X = 5) = \\frac{\\binom{8}{5} \\cdot \\binom{12}{0}}{\\binom{20}{5}} = \\frac{56 \\cdot 1}{15,504} \\approx 0.004\\)</p> </li> <li> <p>Non-negativity: All values are positive \u2713</p> </li> <li> <p>Sum to 1: \\(0.051 + 0.255 + 0.397 + 0.238 + 0.054 + 0.004 = 0.999 \\approx 1\\) (small rounding error) \u2713</p> </li> <li> <p>Probability bounds: All values are between 0 and 1 \u2713</p> </li> </ul>"},{"location":"math/probability/random_variables/#independence-of-random-variables","title":"Independence of Random Variables","text":"<p>Independence is one of the most important concepts in probability theory, as it allows us to simplify complex calculations and understand the structure of random phenomena.</p> <p>Two discrete random variables \\(X\\) and \\(Y\\) are independent if and only if their joint probability mass function (PMF) factors into the product of their individual PMFs:</p> \\[P(X = x, Y = y) = P(X = x) \\cdot P(Y = y) \\quad \\text{for all } x, y\\] <p>Knowing the value of one random variable gives you no information about the value of the other.</p>"},{"location":"math/probability/some_famous_problems/","title":"Some famous problems","text":""},{"location":"math/probability/some_famous_problems/#the-monty-hall-problem","title":"The Monty Hall Problem","text":"<p>The Monty Hall Problem is a famous probability puzzle named after the host of the television game show \"Let's Make a Deal.\" It demonstrates how our intuition about probability can be misleading and how conditional probability can lead to counterintuitive results.</p> <p>You are a contestant on a game show. There are three doors:</p> <ul> <li> <p>Door 1: Behind one door is a car (the prize you want)</p> </li> <li> <p>Door 2: Behind another door is a goat</p> </li> <li> <p>Door 3: Behind the third door is another goat</p> </li> </ul> <p>You pick a door (say Door 1). The host, who knows what's behind each door, opens another door (say Door 3) to reveal a goat. The host then asks: \"Do you want to switch to the remaining unopened door (Door 2)?\"</p> <p>Question: Should you switch doors to maximize your probability of winning the car?</p> <p>Many people think that after one door is revealed, there are only two doors left, so the probability of winning the car is \\(\\frac{1}{2}\\) regardless of whether you switch or stay. This reasoning is incorrect.</p> <p>Correct Solution: always switch!</p> <p>The optimal strategy is to always switch. When you switch, your probability of winning the car is \\(\\frac{2}{3}\\), while if you stay with your original choice, your probability is only \\(\\frac{1}{3}\\).</p> <p></p> <p>The above tree diagram shows all the paths the game is played with the probabilities. The two diagrams below show the probabilities for winning and losing if we always switch.</p> <p></p> <p></p> <p>Let's look at some logical reasoning to gain intuition. If we picked a goat first (diagram below), and decided to always switch, we are guaranteed to win! So the probability of winning is the probability of picking a goat first. Picking a goat first has the probability \\(\\frac{2}{3}\\). And if we pick a car first, and decided to always switch, we are guaranteed to lose! So the probability of losing is the probability of picking a car first. Picking a car first has the probability \\(\\frac{1}{3}\\).</p> <p></p> <p>Let's use the Law of Total Probability to formally calculate the probability of success when our strategy is to always switch. Let's assume we initially pick door 1. The soultion is symmetric if we pick any other door initially.</p> <ul> <li> <p>Let \\(S\\) be the event \"We win by switching\"</p> </li> <li> <p>Let \\(D_j\\) be the event \"The car is behind door \\(j\\)\" for \\(j = 1, 2, 3\\)</p> </li> </ul> <p>The law of total probability states that:</p> \\[P(S) = P(S|D_1)P(D_1) + P(S|D_2)P(D_2) + P(S|D_3)P(D_3)\\] <p>1. \\(P(S|D_1)\\) - Probability of winning by switching given car is behind Door 1</p> <ul> <li> <p>If the car is behind Door 1, we initially picked the correct door</p> </li> <li> <p>When we switch, we must switch to a door with a goat</p> </li> <li> <p>Therefore, \\(P(S|D_1) = 0\\)</p> </li> </ul> <p>2. \\(P(S|D_2)\\) - Probability of winning by switching given car is behind Door 2</p> <ul> <li> <p>If the car is behind Door 2, we initially picked Door 1 (incorrect)</p> </li> <li> <p>Monty opens Door 3 (revealing a goat)</p> </li> <li> <p>When we switch, we switch to Door 2 (which has the car)</p> </li> <li> <p>Therefore, \\(P(S|D_2) = 1\\)</p> </li> </ul> <p>3. \\(P(S|D_3)\\) - Probability of winning by switching given car is behind Door 3</p> <ul> <li> <p>If the car is behind Door 3, we initially picked Door 1 (incorrect)</p> </li> <li> <p>Monty opens Door 2 (revealing a goat)</p> </li> <li> <p>When we switch, we switch to Door 3 (which has the car)</p> </li> <li> <p>Therefore, \\(P(S|D_3) = 1\\)</p> </li> </ul> <p>4. Prior Probabilities</p> <ul> <li> <p>\\(P(D_1) = \\frac{1}{3}\\) (car equally likely to be behind any door initially)</p> </li> <li> <p>\\(P(D_2) = \\frac{1}{3}\\)</p> </li> <li> <p>\\(P(D_3) = \\frac{1}{3}\\)</p> </li> </ul> <p>Substituting into the law of total probability:</p> \\[P(S) = P(S|D_1)P(D_1) + P(S|D_2)P(D_2) + P(S|D_3)P(D_3)\\] \\[P(S) = 0 \\cdot \\frac{1}{3} + 1 \\cdot \\frac{1}{3} + 1 \\cdot \\frac{1}{3}\\] \\[P(S) = 0 + \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}\\] <p>The law of total probability provides a rigorous mathematical foundation for why the switching strategy is optimal.</p> <p>To build stronger intuition for why switching is optimal, consider this thought experiment:</p> <p>Scenario: There are 1,000,000 doors. Behind one door is a car, and behind the other 999,999 doors are goats.</p> <ol> <li>You pick one door (say Door 1)</li> <li>The host opens 999,998 doors, revealing goats behind each one</li> <li>Two doors remain: Your original choice (Door 1) and one other door (say Door 535,780)</li> <li>The host asks: \"Do you want to switch to the remaining unopened door?\"</li> </ol> <p>Intuition: In this case, it seems very intuitive to always switch because the probability that your initial pick is the car is literally 1 in a million!</p> <p>The same logic applies to the original 3-door problem, just on a smaller scale. The million doors experiment helps us see that the 3-door problem is not a special case - it's a general principle that applies regardless of the number of doors.</p>"},{"location":"math/probability/some_famous_problems/#simpsons-paradox","title":"Simpson's Paradox","text":"<p>Simpson's Paradox is a statistical phenomenon where a trend appears in different groups of data but disappears or reverses when these groups are combined. This paradox demonstrates how aggregating data can sometimes hide important underlying relationships and why it's crucial to examine data at multiple levels.</p> <p>Simpson's Paradox occurs when Group A shows a trend in one direction, Group B shows the same trend in the same direction but the combined data shows the trend in the opposite direction.</p> <p>This happens because of confounding variables that affect the relationship between the variables of interest.</p> <p>Example:</p> Gender Department A Department B Overall Admit Rate Admit Rate Admit Rate Men 300/400 (75%) 50/100 (50%) 350/500 (70%) Women 100/100 (100%) 25/100 (25%) 125/200 (62.5%) <p>Analysis by Department: Department A: Women (100%) &gt; Men (75%) Department B: Women (25%) &lt; Men (50%)</p> <p>Overall Analysis: Combined: Men (70%) &gt; Women (62.5%)</p> <p>This is Simpson's Paradox! Women have higher admission rates in both departments individually, but men have a higher overall admission rate.</p> <p>Let's denote:</p> <ul> <li> <p>\\(p_{ij}\\) = admission rate for gender \\(i\\) in department \\(j\\)</p> </li> <li> <p>\\(n_{ij}\\) = number of applicants for gender \\(i\\) in department \\(j\\)</p> </li> </ul> <p>Overall admission rate for gender \\(i\\):</p> \\[P_i = \\frac{\\sum_j n_{ij} p_{ij}}{\\sum_j n_{ij}}\\] <p>The paradox occurs when:</p> <ul> <li> <p>\\(p_{1A} &gt; p_{2A}\\) and \\(p_{1B} &gt; p_{2B}\\) (women win in each department)</p> </li> <li> <p>But \\(P_1 &lt; P_2\\) (men win overall)</p> </li> </ul> <p>This happens because the \\(n_{ij}\\) values (group sizes) create different weights in the overall calculation.</p> <p>Simpson's Paradox teaches us that aggregated data can be misleading. The relationship between variables can change dramatically when we combine different groups, especially when those groups have different sizes or characteristics. This is why it's essential to examine data both individually and collectively to understand the true underlying relationships.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/","title":"Story Proofs and Axioms of Probability","text":""},{"location":"math/probability/story_proofs_and_axioms_of_probability/#story-proofs","title":"Story Proofs","text":"<p>Story proofs are a powerful technique in combinatorics where we prove identities by interpreting both sides of an equation as counting the same thing in different ways. This is also known as proof by interpretation or bijective proof.</p> <p>Example: \\(\\binom{n}{k} = \\binom{n}{n-k}\\)</p> <p>Identity: \\(\\binom{n}{k} = \\binom{n}{n-k}\\)</p> <p>Story Proof: Think of choosing \\(k\\) people from a group of \\(n\\) people to be on a committee. The left side \\(\\binom{n}{k}\\) counts the number of ways to choose \\(k\\) people for the committee. The right side \\(\\binom{n}{n-k}\\) counts the number of ways to choose \\(n-k\\) people to be left out of the committee. But choosing \\(k\\) people for the committee is exactly the same as choosing \\(n-k\\) people to leave out! Therefore, both sides count the same thing.</p> <p>Key insight: Every choice of \\(k\\) people corresponds uniquely to a choice of \\(n-k\\) people (the complement), and vice versa.</p> <p>Example: \\(n \\cdot \\binom{n-1}{k-1} = k \\cdot \\binom{n}{k}\\)</p> <p>Identity: \\(n \\cdot \\binom{n-1}{k-1} = k \\cdot \\binom{n}{k}\\)</p> <p>Story Proof: Think of choosing \\(k\\) people from \\(n\\) people, with one of them designated as President.</p> <p>Left side: \\(n \\cdot \\binom{n-1}{k-1}\\) First, choose who will be President (\\(n\\) choices). Then, from the remaining \\(n-1\\) people, choose \\(k-1\\) more people to complete the committee. Total: \\(n \\cdot \\binom{n-1}{k-1}\\)</p> <p>Right side: \\(k \\cdot \\binom{n}{k}\\) First, choose any \\(k\\) people from \\(n\\) people (\\(\\binom{n}{k}\\) ways). Then, from those \\(k\\) people, choose one to be President (\\(k\\) choices). Total: \\(k \\cdot \\binom{n}{k}\\)</p> <p>Example: Vandermonde Identity</p> <p>Identity: \\(\\sum_{k=0}^n \\binom{m}{k} \\binom{p}{n-k} = \\binom{m+p}{n}\\)</p> <p>Story Proof: Think of choosing \\(n\\) people from a group of \\(m\\) men and \\(p\\) women to form a committee.</p> <p>Left side: \\(\\sum_{k=0}^n \\binom{m}{k} \\binom{p}{n-k}\\) For each \\(k\\) from \\(0\\) to \\(n\\), choose \\(k\\) men from \\(m\\) men (\\(\\binom{m}{k}\\) ways). Then, choose \\(n-k\\) women from \\(p\\) women (\\(\\binom{p}{n-k}\\) ways). Total for this \\(k\\): \\(\\binom{m}{k} \\binom{p}{n-k}\\). Sum over all possible values of \\(k\\): \\(\\sum_{k=0}^n \\binom{m}{k} \\binom{p}{n-k}\\)</p> <p>Right side: \\(\\binom{m+p}{n}\\) Simply choose \\(n\\) people from the total group of \\(m+p\\) people</p> <p>Key insight: The left side partitions the counting by gender composition, while the right side ignores gender entirely. Both approaches must give the same result.</p> <p>Story proofs are powerful because they:</p> <ol> <li> <p>Provide intuition - You understand why the identity is true</p> </li> <li> <p>Are memorable - The story helps you remember the result</p> </li> <li> <p>Avoid algebra - No need for complex manipulations</p> </li> <li> <p>Generalize well - The same story often works for related problems</p> </li> </ol> <p>Key principle: If two expressions count the same thing, they must be equal.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/#formal-definition-of-probability","title":"Formal Definition of Probability","text":"<p>Let \\(S\\) be a sample space (the set of all possible outcomes of an experiment). An event \\(A\\) is a subset of \\(S\\) (i.e., \\(A \\subseteq S\\)).</p> <p>A probability function \\(P\\) is a function that takes an event \\(A\\) as input and returns a real number \\(P(A)\\) as output, where \\(P(A) \\in [0, 1]\\) for any event \\(A \\subseteq S\\).</p> <p>Philosophically, there are different interpretations of Probability, arguments even. But mathematically speaking, the formal definition of Probability along with the axioms is well defined without any ambiguity. And this definition coupled with the axioms constitute a foundation for this field where every theorem or result can be derived from.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/#axioms-of-probability","title":"Axioms of Probability","text":"<p>The probability function \\(P\\) must satisfy the following axioms:</p> <p>Axiom 1 (Non-negativity): For any event \\(A \\subseteq S\\),</p> \\[P(A) \\geq 0\\] <p>Axiom 2 (Normalization): For the entire sample space \\(S\\),</p> \\[P(S) = 1\\] <p>Axiom 3 (Additivity): For any collection of mutually exclusive events \\(A_1, A_2, A_3, \\ldots\\) (i.e., \\(A_i \\cap A_j = \\emptyset\\) for \\(i \\neq j\\)),</p> \\[P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i)\\] <p>From these axioms, we can derive several important properties:</p> <ol> <li>Probability of the empty set: \\(P(\\emptyset) = 0\\)</li> <li>Complement rule: \\(P(A^c) = 1 - P(A)\\)</li> <li>Monotonicity: If \\(A \\subseteq B\\), then \\(P(A) \\leq P(B)\\)</li> <li>Union rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)</li> </ol> <p>Inclusion-Exclusion Principle (3 events): </p> \\[P(A \\cup B \\cup C) = P(A) + P(B) + P(C) - P(A \\cap B) - P(A \\cap C) - P(B \\cap C) + P(A \\cap B \\cap C)\\] <p>Inclusion-Exclusion Principle (n events): For events \\(A_1, A_2, \\ldots, A_n\\),</p> \\[P\\left(\\bigcup_{i=1}^n A_i\\right) = \\sum_{i=1}^n P(A_i) - \\sum_{1 \\leq i &lt; j \\leq n} P(A_i \\cap A_j) + \\sum_{1 \\leq i &lt; j &lt; k \\leq n} P(A_i \\cap A_j \\cap A_k) - \\cdots + (-1)^{n+1} P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n)\\] <p>These axioms provide the mathematical foundation for probability theory and ensure that probability behaves in an intuitive and consistent way.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/#the-birthday-problem","title":"The Birthday Problem","text":"<p>The Birthday Problem is a classic probability puzzle that asks: What is the probability that in a group of \\(n\\) people, at least two people share the same birthday?</p> <p>This seemingly simple question leads to a surprising result that challenges our intuition about probability.</p> <p>Given: A group of \\(n\\) people chosen randomly from a population where birthdays are uniformly distributed across 365 days (ignoring leap years).</p> <p>Find: The probability that at least two people in the group share the same birthday.</p> <p>The result is counterintuitive: In a group of just 23 people, there is approximately a 50% chance that at least two people share the same birthday!</p> <p>This seems impossible at first glance- with 365 possible birthdays and only 23 people, how can there be a 50% chance of a match?</p> <p>We solve this using the complement rule: Instead of calculating the probability of at least one match directly, we calculate the probability of no matches and subtract from 1.</p> <p>Let \\(A\\) be the event \"at least two people share a birthday\". \\(P(A) = 1 - P(A^c)\\), where \\(A^c\\) is \"no two people share a birthday\". First person: Can have any birthday (365/365 = 1). Second person: Must have a different birthday (364/365). Third person: Must have a different birthday from the first two (363/365). And so on...</p> <p>General formula:</p> \\[P(A^c) = \\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\cdots \\times \\frac{365-n+1}{365}\\] <p>This can be written more compactly as:</p> \\[P(A^c) = \\frac{365!}{(365-n)! \\times 365^n}\\] \\[P(A) = 1 - \\frac{365!}{(365-n)! \\times 365^n}\\] <p>Let's calculate some key values:</p> Number of People (\\(n\\)) Probability of at least one match 10 11.7% 15 25.3% 20 41.1% 23 50.7% 30 70.6% 40 89.1% 50 97.0% 60 99.4% <p>The result feels wrong because we're thinking about individual comparisons rather than all possible pairs.</p> <p>Number of pairs: In a group of \\(n\\) people, there are \\(\\binom{n}{2} = \\frac{n(n-1)}{2}\\) possible pairs. With 23 people: \\(\\binom{23}{2} = 253\\) pairs. With 30 people: \\(\\binom{30}{2} = 435\\) pairs. With 50 people: \\(\\binom{50}{2} = 1,225\\) pairs.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/#de-montmorts-problem","title":"De Montmort's Problem","text":"<p>De Montmort's Problem is a classic probability puzzle that asks: What is the probability that when \\(n\\) cards are dealt from a deck, at least one card appears in a position that matches its value?</p> <p>This problem is also known as the matching problem or the coincidence problem and was first studied by Pierre Raymond de Montmort in the early 18th century.</p> <p>Given: A deck of \\(n\\) cards numbered from 1 to \\(n\\). The cards are shuffled and dealt face up in a row.</p> <p>Find: The probability that at least one card appears in the \\(k\\)-th position where the card's value is \\(k\\).</p> <p>Example: For \\(n = 4\\), we have cards [1, 2, 3, 4]. A deal of [2, 1, 4, 3] has no matches, but [1, 4, 2, 3] has a match (card 1 in position 1).</p> <p>We solve this using the inclusion-exclusion principle. Let \\(A_i\\) be the event that card \\(i\\) appears in position \\(i\\).</p> <p>Key insight: We want \\(P(A_1 \\cup A_2 \\cup \\cdots \\cup A_n)\\), which we can calculate using inclusion-exclusion.</p> <p>Step-by-Step Solution</p> <ol> <li>Individual probabilities: \\(P(A_i) = \\frac{1}{n}\\) for each card</li> <li>Pairwise intersections: \\(P(A_i \\cap A_j) = \\frac{1}{n(n-1)}\\) for \\(i \\neq j\\)</li> <li>Triple intersections: \\(P(A_i \\cap A_j \\cap A_k) = \\frac{1}{n(n-1)(n-2)}\\) for distinct \\(i, j, k\\)</li> <li>and so on...</li> </ol> <p>Using inclusion-exclusion:</p> \\[P(A_1 \\cup A_2 \\cup \\cdots \\cup A_n) = \\sum_{i=1}^n P(A_i) - \\sum_{1 \\leq i &lt; j \\leq n} P(A_i \\cap A_j) + \\sum_{1 \\leq i &lt; j &lt; k \\leq n} P(A_i \\cap A_j \\cap A_k) - \\cdots\\] <p>Calculating the terms: First term: \\(\\sum_{i=1}^n P(A_i) = n \\cdot \\frac{1}{n} = 1\\). Second term: \\(\\sum_{1 \\leq i &lt; j \\leq n} P(A_i \\cap A_j) = \\binom{n}{2} \\cdot \\frac{1}{n(n-1)} = \\frac{n(n-1)}{2} \\cdot \\frac{1}{n(n-1)} = \\frac{1}{2}\\). Third term: \\(\\sum_{1 \\leq i &lt; j &lt; k \\leq n} P(A_i \\cap A_j \\cap A_k) = \\binom{n}{3} \\cdot \\frac{1}{n(n-1)(n-2)} = \\frac{1}{3!} = \\frac{1}{6}\\). And so on...</p> <p>General pattern: The \\(k\\)-th term is \\(\\frac{1}{k!}\\)</p> <p>Final result:</p> \\[P(\\text{at least one match}) = 1 - \\frac{1}{2!} + \\frac{1}{3!} - \\frac{1}{4!} + \\cdots + (-1)^{n+1} \\frac{1}{n!}\\] <p>As \\(n\\) approaches infinity, the probability approaches:</p> \\[\\lim_{n \\to \\infty} P(\\text{at least one match}) = 1 - \\frac{1}{e} \\approx 0.632\\] <p>This means that even with infinitely many cards, there's still only about a 63.2% chance that at least one card appears in its \"correct\" position!</p>"},{"location":"math/probability/transformations_of_random_variables/","title":"Transformations of Random Variables","text":"<p>Let \\(X\\) be a continuous random variable with PDF \\(f_X\\), and let \\(Y = g(X)\\), where \\(g\\) is differentiable and strictly increasing (or strictly decreasing). Then the PDF of \\(Y\\) is given by:</p> \\[f_Y(y) = f_X(x) \\left|\\frac{dx}{dy}\\right|\\] <p>where \\(x = g^{-1}(y)\\).</p> <p>Proof. Let \\(g\\) be strictly increasing. The CDF of \\(Y\\) is:</p> \\[F_Y(y) = P(Y \\leq y) = P(g(X) \\leq y) = P(X \\leq g^{-1}(y)) = F_X(g^{-1}(y)) = F_X(x)\\] <p>So by the chain rule, the PDF of \\(Y\\) is:</p> \\[f_Y(y) = f_X(x) \\frac{dx}{dy}\\] <p>The proof for \\(g\\) strictly decreasing is analogous. In that case the PDF ends up as \\(-f_X(x) \\frac{dx}{dy}\\), which is nonnegative since \\(\\frac{dx}{dy} &lt; 0\\) if \\(g\\) is strictly decreasing. Using \\(\\left|\\frac{dx}{dy}\\right|\\), as in the statement of the theorem, covers both cases.</p> <p>Key points:</p> <ol> <li>Differentiability: The function \\(g\\) must be differentiable</li> <li>Monotonicity: \\(g\\) must be strictly increasing or strictly decreasing</li> <li>Absolute value: The absolute value ensures the PDF is non-negative</li> </ol> <p>Note: When applying the change of variables formula, we can choose whether to compute \\(\\frac{dx}{dy}\\), or compute \\(\\frac{dy}{dx}\\) and take the reciprocal. By the chain rule, these give the same result, so we can do whichever is easier.</p> <p>Example: Let \\(X \\sim N(0, 1)\\), \\(Y = e^X\\). We name the distribution of \\(Y\\) the Log-Normal. Now we can use the change of variables formula to find the PDF of \\(Y\\), since \\(g(x) = e^x\\) is strictly increasing.</p> <p>Let \\(y = e^x\\), so \\(x = \\log y\\) and \\(\\frac{dy}{dx} = e^x\\). Then:</p> \\[f_Y(y) = f_X(x) \\left|\\frac{dx}{dy}\\right| = \\phi(x) \\frac{1}{e^x} = \\phi(\\log y) \\frac{1}{y}, \\quad y &gt; 0\\] <p>Note that after applying the change of variables formula, we write everything on the right-hand side in terms of \\(y\\), and we specify the support of the distribution. To determine the support, we just observe that as \\(x\\) ranges from \\(-\\infty\\) to \\(\\infty\\), \\(e^x\\) ranges from \\(0\\) to \\(\\infty\\).</p> <p>We can get the same result by working from the definition of the CDF, translating the event \\(Y \\leq y\\) into an equivalent event involving \\(X\\). For \\(y &gt; 0\\):</p> \\[F_Y(y) = P(Y \\leq y) = P(e^X \\leq y) = P(X \\leq \\log y) = \\Phi(\\log y)\\] <p>So the PDF is again:</p> \\[f_Y(y) = \\frac{d}{dy} \\Phi(\\log y) = \\phi(\\log y) \\frac{1}{y}, \\quad y &gt; 0\\]"},{"location":"math/probability/transformations_of_random_variables/#change-of-variables-in-multiple-dimensions","title":"Change of Variables in multiple Dimensions","text":"<p>The change of variables formula generalizes to \\(n\\) dimensions, where it tells us how to use the joint PDF of a random vector \\(\\mathbf{X}\\) to get the joint PDF of the transformed random vector \\(\\mathbf{Y} = g(\\mathbf{X})\\). The formula is analogous to the one-dimensional version, but it involves a multivariate generalization of the derivative called a Jacobian matrix.</p> <p>Let \\(\\mathbf{X} = (X_1, \\ldots, X_n)\\) be a continuous random vector with joint PDF \\(f_{\\mathbf{X}}\\). Let \\(g : A_0 \\to B_0\\) be an invertible function, where \\(A_0\\) and \\(B_0\\) are open subsets of \\(\\mathbb{R}^n\\), \\(A_0\\) contains the support of \\(\\mathbf{X}\\), and \\(B_0\\) is the range of \\(g\\).</p> <p>Note: A set \\(C \\subseteq \\mathbb{R}^n\\) is open if for each \\(\\mathbf{x} \\in C\\), there exists \\(\\epsilon &gt; 0\\) such that all points with distance less than \\(\\epsilon\\) from \\(\\mathbf{x}\\) are contained in \\(C\\). Sometimes we take \\(A_0 = B_0 = \\mathbb{R}^n\\), but often we would like more flexibility for the domain and range of \\(g\\). For example, if \\(n = 2\\), and \\(X_1\\) and \\(X_2\\) have support \\((0,\\infty)\\), we may want to work with the open set \\(A_0 = (0,\\infty) \\times (0,\\infty)\\) rather than all of \\(\\mathbb{R}^2\\). When we say \"\\(A_0\\) contains the support,\" we mean that \\(A_0\\) must be a superset of the support of \\(\\mathbf{X}\\). In the example above, if the support is \\((0,\\infty) \\times (0,\\infty)\\), then \\(A_0 = (0,\\infty) \\times (0,\\infty)\\) does contain the support (in fact, it equals the support). We could also take \\(A_0 = \\mathbb{R}^2\\), which would definitely contain the support, but choosing \\(A_0\\) to be the minimal open set containing the support is often more convenient and natural.</p> <p>Let \\(\\mathbf{Y} = g(\\mathbf{X})\\), and mirror this by letting \\(\\mathbf{y} = g(\\mathbf{x})\\). Since \\(g\\) is invertible, we also have \\(\\mathbf{X} = g^{-1}(\\mathbf{Y})\\) and \\(\\mathbf{x} = g^{-1}(\\mathbf{y})\\).</p> <p>Suppose that all the partial derivatives \\(\\frac{\\partial x_i}{\\partial y_j}\\) exist and are continuous, so we can form the Jacobian matrix:</p> \\[\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{y}} = \\begin{pmatrix} \\frac{\\partial x_1}{\\partial y_1} &amp; \\frac{\\partial x_1}{\\partial y_2} &amp; \\cdots &amp; \\frac{\\partial x_1}{\\partial y_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial x_n}{\\partial y_1} &amp; \\frac{\\partial x_n}{\\partial y_2} &amp; \\cdots &amp; \\frac{\\partial x_n}{\\partial y_n} \\end{pmatrix}\\] <p>Also assume that the determinant of this Jacobian matrix is never 0. Then the joint PDF of \\(\\mathbf{Y}\\) is:</p> \\[f_{\\mathbf{Y}}(\\mathbf{y}) = f_{\\mathbf{X}}(g^{-1}(\\mathbf{y})) \\left|\\det\\left(\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{y}}\\right)\\right|\\] <p>for \\(\\mathbf{y} \\in B_0\\), and 0 otherwise.</p> <p>That is, to convert \\(f_{\\mathbf{X}}(\\mathbf{x})\\) to \\(f_{\\mathbf{Y}}(\\mathbf{y})\\) we express the \\(\\mathbf{x}\\) in \\(f_{\\mathbf{X}}(\\mathbf{x})\\) in terms of \\(\\mathbf{y}\\) and then multiply by the absolute value of the determinant of the Jacobian \\(\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{y}}\\).</p> <p>As in the 1D case, \\(\\left|\\det\\left(\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{y}}\\right)\\right| = \\left|\\det\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)\\right|^{-1}\\), so we can compute whichever of the two Jacobians is easier, and then at the end express the joint PDF of \\(\\mathbf{Y}\\) as a function of \\(\\mathbf{y}\\).</p> <p>The idea is to apply the change of variables formula from multivariable calculus and the fact that if \\(A\\) is a region in \\(A_0\\) and \\(B = \\{g(\\mathbf{x}) : \\mathbf{x} \\in A\\}\\) is the corresponding region in \\(B_0\\), then \\(\\mathbf{X} \\in A\\) is equivalent to \\(\\mathbf{Y} \\in B\\)\u2014they are the same event. So \\(P(\\mathbf{X} \\in A) = P(\\mathbf{Y} \\in B)\\), which shows that:</p> \\[\\int_A f_{\\mathbf{X}}(\\mathbf{x}) d\\mathbf{x} = \\int_B f_{\\mathbf{Y}}(\\mathbf{y}) d\\mathbf{y}\\]"},{"location":"productivity/how_to_build_your_career_in_ai/make_every_day_count/","title":"Make Every Day Count","text":"<p>We could expect to live a total of 27,649 days. How small this number is! Print it in a large font and paste it somewhere as a daily reminder. That\u2019s all the days we have to spend with loved ones, learn, build for the future, and help others. Whatever you\u2019re doing today, is it worth 1/30,000 of your life?</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_1_learning/","title":"Phase 1: Learning","text":"<p>More research papers have been published on AI than anyone can read in a lifetime. So, when learning, it's critical to prioritize topic selection.</p> <p>Foundational Machine Learning Skills For example, it\u2019s important to understand models such as linear regression, logistic regression, decision trees, clustering, and anomaly detection. Beyond specific models, it\u2019s even more important to understand the core concepts behind how and why machine learning works, such as bias/variance, cost functions, regularization, optimization algorithms, and error analysis.</p> <p>Deep learning This has become such a large fraction of machine learning that it\u2019s hard to excel in the field without some understanding of it! It\u2019s valuable to know the basics of neural networks, practical skills for making them work (such as hyperparameter tuning), convolutional networks, sequence models, and transformers.</p> <p>Math relevant to machine learning Key areas include linear algebra (vectors, matrices, and various manipulations of them) as well as probability and statistics (including discrete and continuous probability, standard probability distributions, basic rules such as independence and Bayes\u2019 rule, and hypothesis testing). In addition, exploratory data analysis (EDA)\u2014 using visualizations and other methods to systematically explore a dataset\u2014 is an underrated skill. Finally, a basic intuitive understanding of calculus will also help. The math needed to do machine learning well has been changing. For instance, although some tasks require calculus, improved automatic differentiation software makes it possible to invent and implement new neural network architectures without doing any calculus.</p> <p>Software development  While you can get a job and make huge contributions with only machine learning modeling skills, your job opportunities will increase if you can also write good software to implement complex AI systems.</p> <p>A good course\u2014 in which a body of material has been organized into a coherent and logical form\u2014 is often the most time-efficient way to master a meaningful body of knowledge. When you\u2019ve absorbed the knowledge available in courses, you can switch over to research papers and other resources.</p> <p>Given how quickly our field is changing, there\u2019s little choice but to keep learning if you want to keep up. If you can cultivate the habit of learning a little bit every week, you can make significant progress with what feels like less effort.</p> <p>The best way to build a new habit is to start small and succeed, rather than start too big and fail.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/","title":"Phase 2: Projects","text":""},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#scoping-successful-ai-projects","title":"Scoping Successful AI Projects","text":"<p>One of the most important skills of an AI architect is the ability to identify ideas that are worth working on.</p> <p>Here are five steps to help you scope projects.</p> <p>Step 1: Identify a business problem (not an AI problem).</p> <p>Find a domain expert and ask, \"What are the top three things that you wish worked better? Why aren't they working yet?\"</p> <p>For example, if you want to apply AI to climate change, you might discover that power-grid operators can't accurately predict how much power intermittent sources like wind and solar might generate in the future.</p> <p>Step 2: Brainstorm AI solutions.</p> <p>Don't execute on the first idea you get excited about. Sometimes this works out okay, but sometimes you end up missing an even better idea that might not have taken any more effort to build.</p> <p>Once you understand a problem, you can brainstorm potential solutions more efficiently. For instance, to predict power generation from intermittent sources, we might consider:</p> <ul> <li> <p>Using satellite imagery to map the locations of wind turbines more accurately</p> </li> <li> <p>Using satellite imagery to estimate the height and generation capacity of wind turbines  </p> </li> <li> <p>Using weather data to better predict cloud cover and thus solar irradiance</p> </li> </ul> <p>Sometimes there isn't a good AI solution, and that's okay too.</p> <p>Step 3: Assess the feasibility and value of potential solutions.</p> <p>You can determine whether an approach is technically feasible by looking at:</p> <ul> <li> <p>Published work</p> </li> <li> <p>What competitors have done</p> </li> <li> <p>Building a quick proof of concept implementation</p> </li> </ul> <p>You can determine its value by consulting with domain experts (say, power-grid operators, who can advise on the utility of the potential solutions mentioned above).</p> <p>Step 4: Determine milestones.</p> <p>Once you've deemed a project sufficiently valuable, the next step is to determine the metrics to aim for. This includes both Machine learning metrics (such as accuracy) and Business metrics (such as revenue).</p> <p>Unfortunately, not every business problem can be reduced to optimizing test set accuracy! If you aren't able to determine reasonable milestones, it may be a sign that you need to learn more about the problem. A quick proof of concept can help supply the missing perspective.</p> <p>Step 5: Budget for resources.</p> <p>Think through everything you'll need to get the project done including:</p> <ul> <li> <p>Data: Raw data, labeled data, data cleaning tools</p> </li> <li> <p>Personnel: Team members, skills required, roles and responsibilities</p> </li> <li> <p>Time: Project timeline, milestones, deadlines</p> </li> <li> <p>Integrations: APIs, third-party services, system connections</p> </li> <li> <p>Support: Other teams, external vendors, domain experts</p> </li> </ul> <p>For example, if you need funds to purchase satellite imagery, make sure that's in the budget.</p> <p>Note: Working on projects is an iterative process. If, at any step, you find that the current direction is infeasible, return to an earlier step and proceed with your new understanding.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#finding-projects-that-complement-your-career-goals","title":"Finding Projects that Complement Your Career Goals","text":"<p>A fruitful career will include many projects, hopefully growing in scope, complexity, and impact over time. Thus, it is fine to start small. Use early projects to learn and gradually step up to bigger projects as your skills grow.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#what-if-you-dont-have-any-project-ideas","title":"What if you don't have any project ideas?","text":"<p>Here are a few ways to generate them:</p> <p>Join existing projects. If you find someone else with an idea, ask to join their project.</p> <p>Keep reading and talking to people. You can come up with new ideas whenever you spend a lot of time reading, taking courses, or talking with domain experts.</p> <p>Focus on an application area. Many researchers are trying to advance basic AI technology \u2014 say, by inventing the next generation of transformers or further scaling up language models \u2014 so, while this is an exciting direction, it is also very hard. But the variety of applications to which machine learning has not yet been applied is vast!</p> <p>Develop a side hustle. Even if you have a full-time job, a fun project that may or may not develop into something bigger can stir the creative juices and strengthen bonds with collaborators. Silicon Valley abounds with stories of startups that started as side projects.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#given-a-few-project-ideas-which-one-should-you-jump-into","title":"Given a few project ideas, which one should you jump into?","text":"<p>Here's a quick checklist of factors to consider:</p> <p>Will the project help you grow technically? Ideally, it should be challenging enough to stretch your skills but not so hard that you have little chance of success. This will put you on a path toward mastering ever-greater technical complexity.</p> <p>Do you have good teammates to work with? If not, are there people you can discuss things with? We learn a lot from the people around us, and good collaborators will have a huge impact on your growth.</p> <p>Can it be a stepping stone? If the project is successful, will its technical complexity and/or business impact make it a meaningful stepping stone to larger projects? If the project is bigger than those you've worked on before, there's a good chance it could be such a stepping stone.</p> <p>Building models is an iterative process. For many applications, the cost of training and conducting error analysis is not prohibitive. Furthermore, it is very difficult to carry out a study that will shed light on the appropriate model, data, and hyperparameters. So it makes sense to build an end-to-end system quickly and revise it until it works well.</p> <p>But when committing to a direction means making a costly investment or entering a one-way door (meaning a decision that's hard to reverse), it's often worth spending more time in advance to make sure it really is a good idea.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#building-a-portfolio-of-projects-that-shows-skill-progression","title":"Building a Portfolio of Projects that Shows Skill Progression","text":"<p>Over the course of a career, you\u2019re likely to work on projects in succession, each growing in scope and complexity. Each project is only one step on a longer journey, hopefully one that has a positive impact. Don\u2019t worry about starting too small. Communication is key. You need to be able to explain your thinking if you want others to see the value in your work and trust you with resources that you can invest in larger projects. Leadership isn\u2019t just for managers. When you reach the point of working on larger AI projects that require teamwork, your ability to lead projects will become more important, whether or not you are in a formal position of leadership.</p> <p>Building a portfolio of projects, especially one that shows progress over time from simple to complex undertakings, will be a big help when it comes to looking for a job.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_3_job/","title":"Phase 3: Job","text":""},{"location":"productivity/how_to_build_your_career_in_ai/phase_3_job/#a-simple-framework-for-starting-your-ai-job-search","title":"A Simple Framework for Starting Your AI Job Search","text":"<p>Are you switching roles? For example, if you\u2019re a software engineer, university student, or physicist who\u2019s looking to become a machine learning engineer, that\u2019s a role switch. Are you switching industries? For example, if you work for a healthcare company, financial services company, or a government agency and want to work for a software company, that\u2019s a switch in industries. If you\u2019re looking for your first job in AI, you\u2019ll probably find switching either roles or industries easier than doing both at the same time.</p> <p>If you\u2019re considering a role switch, a startup can be an easier place to do it than a big company. While there are exceptions, startups usually don\u2019t have enough people to do all the desired work. If you\u2019re able to help with AI tasks\u2014 even if it\u2019s not your official job \u2014 your work is likely to be appreciated. This lays the groundwork for a possible role switch without needing to leave the company. In contrast, in a big company, a rigid reward system is more likely to reward you for doing your job well (and your manager for supporting you in doing the job for which you were hired), but it\u2019s not as likely to reward contributions outside your job\u2019s scope.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_3_job/#using-informational-interviews-to-find-the-right-job","title":"Using Informational Interviews to Find the Right Job","text":"<p>An informational interview involves finding someone in a company or role you\u2019d like to know more about and informally interviewing them about their work. Such conversations are separate from searching for a job. In fact, it\u2019s helpful to interview people who hold positions that align with your interests well before you\u2019re ready to kick off a job search.</p> <p>Finding someone to interview isn\u2019t always easy, but many people who are in senior positions today received help when they were new from those who had entered the field ahead of them, and many are eager to pay it forward.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_3_job/#overcoming-imposter-syndrome","title":"Overcoming Imposter Syndrome","text":"<p>AI is technically complex, and it has its fair share of smart and highly capable people. But it is easy to forget that to become good at anything, the first step is to suck at it. If you\u2019ve succeeded at sucking at AI\u2014 congratulations, you\u2019re on your way! It is guaranteed that everyone who has published a seminal AI paper struggled with simple technical challenges at some point.</p> <p>No one is an expert at everything! Recognize what you do well.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/","title":"Three Steps to Career Growth","text":"<p>Three key steps of career growth are learning foundational skills, working on projects (to deepen your skills, build a portfolio, and create impact), and finding a job. These steps stack on top of each other.</p> <p></p> <p>Initially, you focus on learning foundational skills. After having gained foundational technical skills, you will begin working on projects. During this period, you\u2019ll also keep learning. Later, you will work on finding a job. Throughout this process, you\u2019ll continue to learn and work on meaningful projects.</p>"}]}