{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my wiki!","text":""},{"location":"ai/deep_generative_models/autoregressive_models/","title":"Autoregressive models","text":"<p>We assume we are given access to a dataset: $$ \\mathcal{D} = { \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_m } $$ where each datapoint is n-dimensional. For simplicity, we assume the datapoints are binary. $$ x_i \\in {0,1}^n $$</p>"},{"location":"ai/deep_generative_models/autoregressive_models/#representation","title":"Representation","text":"<p>If you have n random variables: $$ X_1, X_2, \\dots, X_n $$ then their joint probability can be written as a product of conditional probabilities: $$ P(X_1, X_2, \\dots, X_n) = P(X_1) \\cdot P(X_2 \\mid X_1) \\cdot P(X_3 \\mid X_1, X_2) \\cdot \\dots \\cdot P(X_n \\mid X_1, X_2, \\dots, X_{n-1}) $$ In words:</p> <p>The probability of all n variables taking particular values equals: \u2192 the probability of the first variable, \u2192 times the probability of the second variable given the first, \u2192 times the probability of the third variable given the first two, \u2192 and so on, until the n-th variable.</p> <p>By this chain rule of probability, we can factorize the joint distribution over the n-dimensions as:</p> \\[ p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i \\mid x_1, x_2, \\dots, x_{i-1}) = \\prod_{i=1}^n p(x_i \\mid x_{&lt;i}) \\] <p>where</p> \\[ x_{&lt;i} = [x_1, x_2, \\dots, x_{i-1}] \\] <p>denotes the vector of random variables with index less than i.</p> <p>The chain rule factorization can be expressed graphically as a Bayesian network.</p> <p></p> <p>Such a Bayesian network that makes no conditional independence assumptions is said to obey the autoregressive property. The term autoregressive originates from the literature on time-series models where observations from the previous time-steps are used to predict the value at the current time step. Here, we fix an ordering of the variables x1, x2, \u2026, xn and the distribution for the i-th random variable depends on the values of all the preceding random variables in the chosen ordering x1, x2, \u2026, xi\u22121.</p> <p>If we allow for every conditional p(xi|x&lt;i) to be specified in a tabular form, then such a representation is fully general and can represent any possible distribution over n random variables. However, the space complexity for such a representation grows exponentially with n.</p> <p>To see why, let us consider the conditional for the last dimension, given by p(xn|x&lt;n). In order to fully specify this conditional, we need to specify a probability for 2^(n\u22121) configurations of the variables x1, x2, \u2026, xn\u22121. Since the probabilities should sum to 1, the total number of parameters for specifying this conditional is given by 2^(n\u22121)\u22121. Hence, a tabular representation for the conditionals is impractical for learning the joint distribution factorized via chain rule.</p> <p>In an autoregressive generative model, the conditionals are specified as parameterized functions with a fixed number of parameters. Specifically, we assume that each conditional distribution corresponds to a Bernoulli random variable. We then learn a function that maps the preceding random variables to the parameter (mean) of this Bernoulli distribution. Hence, we have:</p> \\[ p_{\\theta_i}(x_i \\mid x_{&lt;i}) = \\text{Bern} \\left( f_i(x_1, x_2, \\dots, x_{i-1}) \\right) \\] <p>where the function is defined as:</p> \\[ f_i : \\{0,1\\}^{i-1} \\to [0,1] \\] <p>and theta_i denotes the set of parameters used to specify this function. This function takes in a vector of size (i-1) where each element is a 0 or a 1, and outputs a scalar bit.</p> <p>The total number of parameters in an autoregressive generative model is given by:</p> \\[ \\sum_{i=1}^n \\left| \\theta_i \\right| \\] <p>In the simplest case, we can specify the function as a linear combination of the input elements followed by a sigmoid non-linearity (to restrict the output to lie between 0 and 1). This gives us the formulation of a fully-visible sigmoid belief network (FVSBN).</p> \\[ f_i(x_1, x_2, \\dots, x_{i-1}) = \\sigma(\\alpha^{(i)}_0 + \\alpha^{(i)}_1 x_1 + \\dots + \\alpha^{(i)}_{i-1} x_{i-1}) \\] <p>where \\(\\sigma\\) denotes the sigmoid function and \\(\\theta_i = \\{\\alpha^{(i)}_0, \\alpha^{(i)}_1, \\dots, \\alpha^{(i)}_{i-1}\\}\\) denote the parameters of the mean function. The conditional for variable \\(i\\) requires \\(i\\) parameters, and hence the total number of parameters in the model is given by \\(\\sum_{i=1}^n i = O(n^2)\\). Note that the number of parameters are much fewer than the exponential complexity of the tabular case.</p> <p>A natural way to increase the expressiveness of an autoregressive generative model is to use more flexible parameterizations for the mean function e.g., multi-layer perceptrons (MLP). For example, consider the case of a neural network with 1 hidden layer. The mean function for variable \\(i\\) can be expressed as</p> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{A}_i \\mathbf{x}_{&lt;i} + \\mathbf{c}_i) \\\\ f_i(x_1, x_2, \\dots, x_{i-1}) &amp;= \\sigma(\\boldsymbol{\\alpha}^{(i)} \\mathbf{h}_i + b_i) \\end{align} \\] <p>where \\(\\mathbf{h}_i \\in \\mathbb{R}^d\\) denotes the hidden layer activations for the MLP and \\(\\theta_i = \\{\\mathbf{A}_i \\in \\mathbb{R}^{d \\times (i-1)}, \\mathbf{c}_i \\in \\mathbb{R}^d, \\boldsymbol{\\alpha}^{(i)} \\in \\mathbb{R}^d, b_i \\in \\mathbb{R}\\}\\) are the set of parameters for the mean function \\(\\mu_i(\\cdot)\\). The total number of parameters in this model is dominated by the matrices \\(\\mathbf{A}_i\\) and given by \\(O(n^2d)\\).</p> <p>Note: The term \"mean function\" here refers to the function that determines the mean (expected value) of the Bernoulli distribution for each variable. Since we're modeling binary variables, the mean of the Bernoulli distribution is the probability of the variable being 1. The sigmoid function \\(\\sigma\\) ensures that this probability lies between 0 and 1.</p> <p>For a Bernoulli random variable \\(X\\) with parameter \\(p\\), the expectation (mean) is given by:</p> \\[ \\mathbb{E}[X] = 1 \\cdot p + 0 \\cdot (1-p) = p \\] <p>This is because: - \\(X\\) takes value 1 with probability \\(p\\) - \\(X\\) takes value 0 with probability \\((1-p)\\) - The expectation is the weighted sum of all possible values, where the weights are their respective probabilities</p> <p>Therefore, when we say the mean function determines the mean of the Bernoulli distribution, we're saying it determines the probability \\(p\\) of the variable being 1.</p> <p>The Neural Autoregressive Density Estimator (NADE) provides an alternate MLP-based parameterization that is more statistically and computationally efficient than the vanilla approach. In NADE, parameters are shared across the functions used for evaluating the conditionals. In particular, the hidden layer activations are specified as</p> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{W}_{.,&lt;i} \\mathbf{x}_{&lt;i} + \\mathbf{c}) \\\\ f_i(x_1, x_2, \\dots, x_{i-1}) &amp;= \\sigma(\\boldsymbol{\\alpha}^{(i)} \\mathbf{h}_i + b_i) \\end{align} \\] <p>where \\(\\theta = \\{\\mathbf{W} \\in \\mathbb{R}^{d \\times n}, \\mathbf{c} \\in \\mathbb{R}^d, \\{\\boldsymbol{\\alpha}^{(i)} \\in \\mathbb{R}^d\\}_{i=1}^n, \\{b_i \\in \\mathbb{R}\\}_{i=1}^n\\}\\) is the full set of parameters for the mean functions \\(f_1(\\cdot), f_2(\\cdot), \\dots, f_n(\\cdot)\\). The weight matrix \\(\\mathbf{W}\\) and the bias vector \\(\\mathbf{c}\\) are shared across the conditionals. Sharing parameters offers two benefits:</p> <ol> <li> <p>The total number of parameters gets reduced from \\(O(n^2d)\\) to \\(O(nd)\\).</p> </li> <li> <p>The hidden unit activations can be evaluated in \\(O(nd)\\) time via the following recursive strategy:</p> </li> </ol> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{a}_i) \\\\ \\mathbf{a}_{i+1} &amp;= \\mathbf{a}_i + \\mathbf{W}_{[.,i]} x_i \\end{align} \\] <p>with the base case given by \\(\\mathbf{a}_1 = \\mathbf{c}\\).</p> <p>The RNADE algorithm extends NADE to learn generative models over real-valued data. Here, the conditionals are modeled via a continuous distribution such as a equi-weighted mixture of \\(K\\) Gaussians. Instead of learning a mean function, we now learn the means \\(\\mu_{i,1}, \\mu_{i,2}, \\dots, \\mu_{i,K}\\) and variances \\(\\Sigma_{i,1}, \\Sigma_{i,2}, \\dots, \\Sigma_{i,K}\\) of the \\(K\\) Gaussians for every conditional. For statistical and computational efficiency, a single function \\(g_i: \\mathbb{R}^{i-1} \\to \\mathbb{R}^{2K}\\) outputs all the means and variances of the \\(K\\) Gaussians for the \\(i\\)-th conditional distribution.</p> <p>The conditional distribution \\(p_{\\theta_i}(x_i \\mid \\mathbf{x}_{&lt;i})\\) in RNADE is given by:</p> \\[ p_{\\theta_i}(x_i \\mid \\mathbf{x}_{&lt;i}) = \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(x_i; \\mu_{i,k}, \\Sigma_{i,k}) \\] <p>where \\(\\mathcal{N}(x; \\mu, \\Sigma)\\) denotes the probability density of a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\Sigma\\) evaluated at \\(x\\). The parameters \\(\\{\\mu_{i,k}, \\Sigma_{i,k}\\}_{k=1}^K\\) are the outputs of the function \\(g_i(\\mathbf{x}_{&lt;i})\\).</p> <p>This is how RNADE is autoregressive. Example sequence showing autoregressive dependencies:</p> <p>\\(x_1\\):    - Input to \\(g_1\\): \\(\\mathbf{x}_{&lt;1} = []\\) (empty)   - Output: \\(\\{\\mu_{1,k}, \\Sigma_{1,k}\\}_{k=1}^K\\) for \\(p(x_1)\\)</p> <p>\\(x_2\\):    - Input to \\(g_2\\): \\(\\mathbf{x}_{&lt;2} = [x_1]\\)   - Output: \\(\\{\\mu_{2,k}, \\Sigma_{2,k}\\}_{k=1}^K\\) for \\(p(x_2 \\mid x_1)\\)</p> <p>\\(x_3\\):    - Input to \\(g_3\\): \\(\\mathbf{x}_{&lt;3} = [x_1, x_2]\\)   - Output: \\(\\{\\mu_{3,k}, \\Sigma_{3,k}\\}_{k=1}^K\\) for \\(p(x_3 \\mid x_1, x_2)\\)</p> <p>\\(x_4\\):    - Input to \\(g_4\\): \\(\\mathbf{x}_{&lt;4} = [x_1, x_2, x_3]\\)   - Output: \\(\\{\\mu_{4,k}, \\Sigma_{4,k}\\}_{k=1}^K\\) for \\(p(x_4 \\mid x_1, x_2, x_3)\\)</p> <p>This sequential, conditional generation process is what makes RNADE an autoregressive model. The mixture of Gaussians is just the form of the conditional distribution, but the autoregressive property comes from how these distributions are parameterized based on previous variables.</p>"},{"location":"ai/deep_generative_models/autoregressive_models/#learning-and-inference","title":"Learning and inference","text":"<p>Recall that learning a generative model involves optimizing the closeness between the data and model distributions. One commonly used notion of closeness is the KL divergence between the data and the model distributions:</p> \\[ \\min_{\\theta \\in \\Theta} d_{KL}(p_{data}, p_{\\theta}) = \\min_{\\theta \\in \\Theta} \\mathbb{E}_{x \\sim p_{data}}[\\log p_{data}(x) - \\log p_{\\theta}(x)] \\] <p>where: - \\(p_{data}\\) is the true data distribution - \\(p_{\\theta}\\) is our model distribution parameterized by \\(\\theta\\) - \\(\\Theta\\) is the set of all possible parameter values - \\(d_{KL}\\) is the Kullback-Leibler divergence</p> <p>Let's break down how this minimization works:</p> <ol> <li>For a fixed value of \\(\\theta\\), we compute:</li> <li>The expectation over all possible data points \\(x\\) from \\(p_{data}\\)</li> <li>For each \\(x\\), we compute \\(\\log p_{data}(x) - \\log p_{\\theta}(x)\\)</li> <li> <p>This gives us a single scalar value for this particular \\(\\theta\\)</p> </li> <li> <p>The minimization operator \\(\\min_{\\theta \\in \\Theta}\\) then:</p> </li> <li>Tries different values of \\(\\theta\\) in the parameter space \\(\\Theta\\)</li> <li> <p>Finds the \\(\\theta\\) that gives the smallest expected value</p> </li> <li> <p>Since \\(p_{data}\\) is constant with respect to \\(\\theta\\), minimizing the KL divergence is equivalent to maximizing the expected log-likelihood of the data under our model:</p> </li> </ol> \\[ \\max_{\\theta \\in \\Theta} \\mathbb{E}_{x \\sim p_{data}}[\\log p_{\\theta}(x)] \\] <p>This is because \\(\\log p_{data}(x)\\) doesn't depend on \\(\\theta\\), so it can be treated as a constant. Minimizing \\(-\\log p_{\\theta}(x)\\) is the same as maximizing \\(\\log p_{\\theta}(x)\\)</p> <p>To approximate the expectation over the unknown \\(p_{data}\\), we make an assumption: points in the dataset \\(\\mathcal{D}\\) are sampled i.i.d. from \\(p_{data}\\). This allows us to obtain an unbiased Monte Carlo estimate of the objective as:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\log p_{\\theta}(x) = \\mathcal{L}(\\theta | \\mathcal{D}) \\] <p>The maximum likelihood estimation (MLE) objective has an intuitive interpretation: pick the model parameters \\(\\theta \\in \\Theta\\) that maximize the log-probability of the observed datapoints in \\(\\mathcal{D}\\).</p> <p>In practice, we optimize the MLE objective using mini-batch gradient ascent. The algorithm operates in iterations. At every iteration \\(t\\), we sample a mini-batch \\(\\mathcal{B}_t\\) of datapoints sampled randomly from the dataset (\\(|\\mathcal{B}_t| &lt; |\\mathcal{D}|\\)) and compute gradients of the objective evaluated for the mini-batch. These parameters at iteration \\(t+1\\) are then given via the following update rule:</p> \\[ \\theta^{(t+1)} = \\theta^{(t)} + r_t \\nabla_{\\theta} \\mathcal{L}(\\theta^{(t)} | \\mathcal{B}_t) \\] <p>where \\(\\theta^{(t+1)}\\) and \\(\\theta^{(t)}\\) are the parameters at iterations \\(t+1\\) and \\(t\\) respectively, and \\(r_t\\) is the learning rate at iteration \\(t\\). Typically, we only specify the initial learning rate \\(r_1\\) and update the rate based on a schedule.</p> <p>Now that we have a well-defined objective and optimization procedure, the only remaining task is to evaluate the objective in the context of an autoregressive generative model. To this end, we first write the MLE objective in terms of the joint probability:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\log p_{\\theta}(x) \\] <p>Then, we substitute the factorized joint distribution of an autoregressive model. Since \\(p_{\\theta}(x) = \\prod_{i=1}^n p_{\\theta_i}(x_i | x_{&lt;i})\\), we have:</p> \\[ \\log p_{\\theta}(x) = \\log \\prod_{i=1}^n p_{\\theta_i}(x_i | x_{&lt;i}) = \\sum_{i=1}^n \\log p_{\\theta_i}(x_i | x_{&lt;i}) \\] <p>Substituting this into the MLE objective, we get:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\sum_{i=1}^n \\log p_{\\theta_i}(x_i | x_{&lt;i}) \\] <p>where \\(\\theta = \\{\\theta_1, \\theta_2, \\dots, \\theta_n\\}\\) now denotes the collective set of parameters for the conditionals.</p> <p>Inference in an autoregressive model is straightforward. For density estimation of an arbitrary point \\(x\\), we simply evaluate the log-conditionals \\(\\log p_{\\theta_i}(x_i | x_{&lt;i})\\) for each \\(i\\) and add these up to obtain the log-likelihood assigned by the model to \\(x\\). Since we have the complete vector \\(x = [x_1, x_2, \\dots, x_n]\\), we know all the values needed for each conditional \\(x_{&lt;i}\\), so each of the conditionals can be evaluated in parallel. Hence, density estimation is efficient on modern hardware.</p> <p>For example, given a 4-dimensional vector \\(x = [x_1, x_2, x_3, x_4]\\), we can compute all conditionals in parallel:</p> <ul> <li>\\(\\log p_{\\theta_1}(x_1)\\) (no conditioning needed)</li> <li>\\(\\log p_{\\theta_2}(x_2 | x_1)\\) (using known \\(x_1\\))</li> <li>\\(\\log p_{\\theta_3}(x_3 | x_1, x_2)\\) (using known \\(x_1, x_2\\))</li> <li>\\(\\log p_{\\theta_4}(x_4 | x_1, x_2, x_3)\\) (using known \\(x_1, x_2, x_3\\))</li> </ul> <p>Then sum them to get the total log-likelihood: \\(\\log p_{\\theta}(x) = \\sum_{i=1}^4 \\log p_{\\theta_i}(x_i | x_{&lt;i})\\)</p> <p>Sampling from an autoregressive model is a sequential procedure. Here, we first sample \\(x_1\\), then we sample \\(x_2\\) conditioned on the sampled \\(x_1\\), followed by \\(x_3\\) conditioned on both \\(x_1\\) and \\(x_2\\) and so on until we sample \\(x_n\\) conditioned on the previously sampled \\(x_{&lt;n}\\). For applications requiring real-time generation of high-dimensional data such as audio synthesis, the sequential sampling can be an expensive process.</p> <p>Finally, an autoregressive model does not directly learn unsupervised representations of the data. This is because:</p> <ol> <li>The model directly models the data distribution \\(p(x)\\) through a sequence of conditional distributions \\(p(x_i | x_{&lt;i})\\)</li> <li>There is no explicit latent space or bottleneck that forces the model to learn a compressed representation</li> <li>Each variable is modeled based on previous variables, but there's no mechanism to learn a global, compressed representation of the entire data point</li> <li>The model's parameters \\(\\theta_i\\) are specific to each conditional distribution and don't encode a meaningful representation of the data</li> </ol> <p>In contrast, latent variable models like variational autoencoders explicitly learn a compressed representation by: 1. Introducing a latent space \\(z\\) that captures the essential features of the data 2. Learning an encoder that maps data to this latent space 3. Learning a decoder that reconstructs data from the latent space 4. Using a bottleneck that forces the model to learn meaningful representations</p>"},{"location":"ai/deep_generative_models/energy_based_models/","title":"Energy-Based Models","text":""},{"location":"ai/deep_generative_models/energy_based_models/#parameterizing-probability-distributions","title":"Parameterizing Probability Distributions","text":"<p>Probability distributions \\(p(x)\\) are a key building block in generative modeling. Building a neural network that ensures \\(p(x) \\geq 0\\) is not hard. However, the real challenge lies in ensuring that the distribution satisfies the normalization constraint: for discrete variables, the sum over all possible values of \\(x\\) must equal 1, while for continuous variables, the integral over the entire domain must equal 1.</p> <p>Problem: \\(g_\\theta(x) \\geq 0\\) is easy. But \\(\\sum_x g_\\theta(x) = Z(\\theta) \\neq 1\\) in general, so \\(g_\\theta(x)\\) is not a valid probability mass function. For continuous variables, \\(\\int g_\\theta(x) dx = Z(\\theta) \\neq 1\\) in general, so \\(g_\\theta(x)\\) is not a valid probability density function.</p> <p>Solution:</p> \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} g_\\theta(x) = \\frac{1}{\\int g_\\theta(x) dx} g_\\theta(x) = \\frac{1}{\\text{Volume}(g_\\theta)} g_\\theta(x)\\] <p>Then by definition,</p> \\[\\int p_\\theta(x) dx = \\int \\frac{1}{Z(\\theta)} g_\\theta(x) dx = \\frac{Z(\\theta)}{Z(\\theta)} = 1\\] <p>Here, \\(g_\\theta(x)\\) is the output of the neural network with parameters \\(\\theta\\) at input \\(x\\). The volume of \\(g_\\theta\\), denoted as \\(\\text{Volume}(g_\\theta)\\), is defined as the integral of \\(g_\\theta(x)\\) over the entire domain: \\(\\text{Volume}(g_\\theta) = \\int g_\\theta(x) dx = Z(\\theta)\\). It is a normalizing constant (w.r.t. \\(x\\)) but changes for different \\(\\theta\\). For example, we choose \\(g_\\theta(x)\\) so that we know the volume analytically as a function of \\(\\theta\\).</p> <p>The partition function \\(Z(\\theta)\\) is the normalization constant that ensures a probability distribution integrates (or sums) to 1. It's called a \"partition function\" because it partitions the unnormalized function \\(g_\\theta(x)\\) into a proper probability distribution.</p> <p>Example: \\(g_{(\\mu, \\sigma)}(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)</p> \\[\\text{Volume}(g_{(\\mu, \\sigma)}) = \\int_{-\\infty}^{\\infty} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx = \\sqrt{2\\pi\\sigma^2}\\] <p>Therefore, the normalized probability density function is:</p> \\[p_{(\\mu, \\sigma)}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] <p>This is the standard normal (Gaussian) distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Functional forms \\(g_\\theta(x)\\) need to allow analytical integration. Despite being restrictive, they are very useful as building blocks for more complex distributions.</p> <p>Note: What we've been doing with autoregressive models, flow models, and VAEs are essentially tricks for composing simple functions that are normalized to build more complex probabilistic models that are by construction guaranteed to be normalized. These approaches avoid the intractability of computing the partition function for complex distributions by designing architectures where normalization is preserved through the composition of simple, analytically tractable components.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#energy-based-models_1","title":"Energy Based Models","text":"<p>We are going to formalize EBMs the following way:</p> \\[p_\\theta(x) = \\frac{1}{\\int e^{f_\\theta(x)} dx} \\cdot e^{f_\\theta(x)}\\] \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} \\cdot e^{f_\\theta(x)}\\] <p>Why \\(e^{f_\\theta(x)}\\) and not \\(f_\\theta(x)^2\\)?</p> <p>Both \\(e^{f_\\theta(x)}\\) and \\(f_\\theta(x)^2\\) produce non-negative outputs, but we choose the exponential form for several important reasons:</p> <ol> <li> <p>Additive Energy: The exponential form allows us to work with additive energy functions. If we have \\(f_\\theta(x) = f_1(x) + f_2(x)\\), then \\(e^{f_\\theta(x)} = e^{f_1(x)} \\cdot e^{f_2(x)}\\), which is a natural way to combine energy terms.</p> </li> <li> <p>Log-Probability Interpretation: Taking the logarithm gives us \\(\\log p_\\theta(x) = f_\\theta(x) - \\log Z(\\theta)\\). This means \\(f_\\theta(x)\\) directly represents the unnormalized log-probability, making it easier to work with in practice.</p> </li> <li> <p>Gradient Properties: The exponential function has the property that \\(\\frac{d}{dx}e^{f(x)} = e^{f(x)} \\cdot f'(x)\\). This makes gradient-based learning more stable and interpretable.</p> </li> <li> <p>Numerical Stability: The exponential function grows more smoothly than quadratic functions, which can lead to better numerical stability during training.</p> </li> <li> <p>Dynamic Range: The exponential function can capture much larger variations in probability compared to quadratic functions. While \\(f_\\theta(x)^2\\) is bounded by the square of the function's range, \\(e^{f_\\theta(x)}\\) can represent probabilities that vary by many orders of magnitude.</p> </li> <li> <p>Statistical Mechanics Connection: The exponential form follows from the Boltzmann distribution in statistical mechanics, where \\(p(x) \\propto e^{-E(x)/kT}\\), where \\(-E(x)\\) is the energy of state \\(x\\). Hence the name.</p> </li> </ol> <p>Pros: Very flexible, can use any \\(f_\\theta(x)\\)</p> <p>Cons: \\(Z(\\theta)\\) is intractable, so no access to likelihood. Thus, evaluating and optimizing likelihood \\(p_\\theta(x)\\) is hard (learning is hard). Also, sampling from \\(p_\\theta(x)\\) is hard. Another con is there is no feature learning (but can add latent variables). EBMs also suffer from the curse of dimensionality - as the dimension of \\(x\\) increases, the volume of the space grows exponentially, making it increasingly difficult to learn meaningful energy functions and sample efficiently.</p> <p>Given two points \\(x_1\\) and \\(x_2\\), evaluating \\(p_\\theta(x_1)\\) or \\(p_\\theta(x_2)\\) requires calculating \\(Z(\\theta)\\). However, their ratio does not involve calculating \\(Z(\\theta)\\).</p> \\[\\frac{p_\\theta(x_1)}{p_\\theta(x_2)} = \\frac{\\frac{1}{Z(\\theta)} \\cdot e^{f_\\theta(x_1)}}{\\frac{1}{Z(\\theta)} \\cdot e^{f_\\theta(x_2)}} = \\frac{e^{f_\\theta(x_1)}}{e^{f_\\theta(x_2)}} = e^{f_\\theta(x_1) - f_\\theta(x_2)}\\] <p>The partition function \\(Z(\\theta)\\) cancels out in the ratio, so we only need to evaluate the energy function at the two points and take their difference. This means we can determine which of \\(x_1\\) or \\(x_2\\) is more likely under our model without needing to compute the intractable partition function.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#training-ebms-with-contrastive-divergence","title":"Training EBMs with Contrastive Divergence","text":"<p>Let's assume we want to maximize \\(\\frac{\\exp(f_\\theta(x_{train}))}{Z(\\theta)}\\). \\(x_{train}\\) is the 'correct answer'- we want to increase the probability of this under the model. Let's also assume we have a 'wrong answer'. The objective is to not just maximize \\(\\exp(f_\\theta(x_{train}))\\) but also minimize \\(Z(\\theta)\\) because that's going to result in the 'wrong' answer being pushed down.</p> <p>Instead of evaluating \\(Z(\\theta)\\) exactly, we use a Monte Carlo estimate.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#contrastive-divergence-algorithm","title":"Contrastive Divergence Algorithm","text":""},{"location":"ai/deep_generative_models/energy_based_models/#high-level-idea","title":"High-Level Idea","text":"<p>The contrastive divergence algorithm works as follows:</p> <p>Algorithm:</p> <ol> <li> <p>Assuming we can sample from the model, sample \\(x_{sample} \\sim p_\\theta\\)</p> </li> <li> <p>Take a step on the gradient: \\(\\nabla_\\theta(f_\\theta(x_{train}) - f_\\theta(x_{sample}))\\)</p> </li> <li> <p>Keep repeating this to make the training data more likely than typical samples from the model</p> </li> </ol>"},{"location":"ai/deep_generative_models/energy_based_models/#why-does-this-work","title":"Why Does This Work?","text":"<p>We want to maximize the log-likelihood: \\(\\max_\\theta(f_\\theta(x_{train}) - \\log Z(\\theta))\\)</p> <p>Mathematical Derivation:</p> <p>The gradient of the log-likelihood is:</p> \\[\\nabla_\\theta \\log p_\\theta(x_{train}) = \\nabla_\\theta(f_\\theta(x_{train}) - \\log Z(\\theta))\\] <p>Let's split the terms and take the derivative:</p> \\[\\nabla_\\theta \\log p_\\theta(x_{train}) = \\nabla_\\theta f_\\theta(x_{train}) - \\nabla_\\theta \\log Z(\\theta)\\] <p>Now we need to compute \\(\\nabla_\\theta \\log Z(\\theta)\\). Let's expand this:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\nabla_\\theta \\log \\int e^{f_\\theta(x)} dx\\] <p>Using the chain rule and the fact that \\(\\nabla \\log f(x) = \\frac{\\nabla f(x)}{f(x)}\\):</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\nabla_\\theta \\int e^{f_\\theta(x)} dx\\] <p>Since the integral and derivative can be exchanged:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\int \\nabla_\\theta e^{f_\\theta(x)} dx\\] <p>Using the chain rule again:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\int e^{f_\\theta(x)} \\nabla_\\theta f_\\theta(x) dx\\] <p>Notice that \\(\\frac{e^{f_\\theta(x)}}{Z(\\theta)} = p_\\theta(x)\\), so:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\int p_\\theta(x) \\nabla_\\theta f_\\theta(x) dx = \\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)]\\] <p>Final Result:</p> <p>Putting it all together:</p> \\[\\nabla_\\theta \\log p_\\theta(x_{train}) = \\nabla_\\theta f_\\theta(x_{train}) - \\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)]\\] <p>The Key Insight:</p> <p>The second term \\(\\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)]\\) is an expectation over the model distribution. We approximate (Monte Carlo approximation) this expectation using samples from the model:</p> \\[\\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)] \\approx \\nabla_\\theta f_\\theta(x_{sample})\\] <p>where \\(x_{sample} \\sim p_\\theta\\) is a sample from our model.</p> <p>Important note on sampling:</p> <p>Unlike autoregressive models or normalizing flow models, Energy-Based Models do not provide a direct way to sample from \\(p_\\theta(x)\\). In autoregressive models, we can sample sequentially by conditioning on previous values. In flow models, we can sample from a simple base distribution and transform it through invertible functions. However, in EBMs, we need to use approximate sampling methods like:</p> <ul> <li>Langevin Dynamics: Gradient-based sampling with noise</li> <li>Gibbs Sampling: For discrete variables, updating one variable at a time</li> <li>Metropolis-Hastings: Markov chain Monte Carlo methods</li> <li>Hamiltonian Monte Carlo: More sophisticated MCMC methods</li> </ul> <p>This sampling challenge is one of the main difficulties in training EBMs, as we need to run these sampling procedures every time we want to estimate the gradient.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#sampling-from-ebms-with-markov-monte-carlo-methods","title":"Sampling from EBMs with Markov Monte Carlo Methods","text":""},{"location":"ai/deep_generative_models/energy_based_models/#metropolis-hastings-algorithm","title":"Metropolis-Hastings Algorithm","text":"<p>Metropolis-Hastings (MH) is a general-purpose Markov Chain Monte Carlo (MCMC) method for sampling from complex probability distributions. It's particularly useful for Energy-Based Models where direct sampling is not possible.</p> <p>The Algorithm</p> <p>Step 1: Initialize Start with an initial sample \\(x^{(0)}\\) (could be random or from training data)</p> <p>Step 2: Propose a New Sample For each iteration \\(t\\):</p> <ul> <li> <p>Generate a proposal \\(x^*\\) from a proposal distribution \\(q(x^* | x^{(t)})\\)</p> </li> <li> <p>The proposal distribution should be easy to sample from (e.g., Gaussian centered at current point)</p> </li> </ul> <p>Step 3: Accept or Reject</p> <p>Compute the acceptance probability:</p> \\[\\alpha = \\min\\left(1, \\frac{e^{f_\\theta(x^*)} \\cdot q(x^{(t)} | x^*)}{e^{f_\\theta(x^{(t)})} \\cdot q(x^* | x^{(t)})}\\right)\\] <p>The <code>min(1, ...)</code> ensures the acceptance probability is between 0 and 1. When the ratio is &gt; 1, we always accept (probability = 1). When the ratio is \u2264 1, we accept with probability equal to the ratio.</p> <p>Step 4: Update With probability \\(\\alpha\\), accept the proposal: \\(x^{(t+1)} = x^*\\). With probability \\(1-\\alpha\\), reject and keep current: \\(x^{(t+1)} = x^{(t)}\\)</p> <p>Step 5: Repeat Continue for many iterations until convergence</p> <p>This algorithm provides a robust foundation for sampling from Energy-Based Models, though it may require careful tuning and monitoring for optimal performance.</p>"},{"location":"ai/deep_generative_models/energy_based_models/#unadjusted-langevin-mcmc","title":"Unadjusted Langevin MCMC","text":"<p>Unadjusted Langevin MCMC (ULMCMC) is another popular method for sampling from Energy-Based Models. Unlike Metropolis-Hastings, it doesn't use an accept/reject step, making it computationally more efficient.</p> <p>The Algorithm</p> <p>Step 1: Initialize Start with an initial sample \\(x^{(0)}\\) (could be random or from training data)</p> <p>Step 2: Langevin Dynamics Update For each iteration \\(t\\):</p> \\[x^{(t+1)} = x^{(t)} + \\epsilon \\nabla_x f_\\theta(x^{(t)}) + \\sqrt{2\\epsilon} \\eta_t\\] <p>where:</p> <ul> <li> <p>\\(\\epsilon\\) is the step size (learning rate)</p> </li> <li> <p>\\(\\nabla_x f_\\theta(x^{(t)})\\) is the gradient of the energy function</p> </li> <li> <p>\\(\\eta_t \\sim \\mathcal{N}(0, I)\\) is Gaussian noise</p> </li> </ul> <p>Step 3: Repeat Continue for many iterations until convergence</p> <p>Intuition</p> <p>The update rule can be understood as:</p> <ol> <li> <p>Gradient Ascent: \\(\\epsilon \\nabla_x f_\\theta(x^{(t)})\\) moves the sample toward higher energy regions</p> </li> <li> <p>Noise Injection: \\(\\sqrt{2\\epsilon} \\eta_t\\) adds randomness to prevent getting stuck in local optima</p> </li> <li> <p>Balance: The step size \\(\\epsilon\\) controls the trade-off between exploration and exploitation</p> </li> </ol> <p>High-Dimensional Expense</p> <p>In high dimensions, gradient computation becomes expensive, and the noise term \\(\\sqrt{2\\epsilon} \\eta_t\\) scales with dimension, making each step computationally costly. This computational burden is particularly problematic when training Energy-Based Models using Contrastive Divergence.</p> <p>The Training Bottleneck:</p> <p>Each training step in Contrastive Divergence requires sampling from the model distribution \\(p_\\theta(x)\\). This sampling process itself is computationally expensive:</p> <ol> <li>Single Sampling Step: Each Langevin step requires computing gradients and adding noise, both of which scale with dimension</li> <li>Multiple Sampling Steps: To get a good sample, we typically need hundreds or thousands of Langevin steps</li> <li>Per Training Step: Each gradient update of the model parameters requires multiple samples</li> </ol> <p>Computational Complexity:</p> <ul> <li>Gradient Computation: \\(O(d)\\) where \\(d\\) is the dimension</li> <li>Noise Generation: \\(O(d)\\) for generating \\(\\eta_t \\sim \\mathcal{N}(0, I)\\)</li> <li>Per Langevin Step: \\(O(d)\\) total cost</li> <li>Sampling Process: \\(O(k \\cdot d)\\) where \\(k\\) is the number of Langevin steps (typically 100-1000)</li> <li>Per Training Step: \\(O(n \\cdot k \\cdot d)\\) where \\(n\\) is the number of samples needed</li> </ul> <p>Practical Impact:</p> <p>This means that training an EBM using Contrastive Divergence with Langevin sampling can be extremely slow, especially for high-dimensional data like images. The sampling process becomes the computational bottleneck, making it difficult to scale EBMs to large datasets or high-dimensional problems.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/","title":"Evaluating Generative Models","text":"<p>In any research field, evaluation drives progress. How do we evaluate generative models? The evaluation of discriminative models (classification, regression, etc.) is well understood because:</p> <p>Clear ground truth: For discriminative tasks, we have access to labeled data that serves as ground truth. We can directly compare the model's predictions with the true labels.</p> <p>Simple Metrics: Evaluation metrics are straightforward and interpretable:</p> <ul> <li> <p>Classification: Accuracy, precision, recall, F1-score, ROC-AUC</p> </li> <li> <p>Regression: Mean squared error (MSE), mean absolute error (MAE), R\u00b2</p> </li> <li> <p>Ranking: NDCG, MAP, MRR</p> </li> </ul> <p>Domain-Agnostic: These metrics work across different domains (computer vision, NLP, etc.) with minimal adaptation.</p> <p>Example: For a binary classifier, we can compute accuracy as \\(\\frac{\\text{correct predictions}}{\\text{total predictions}}\\) and immediately understand how well the model performs.</p> <p>Evaluating generative models is highly non-trivial.  Key question: What is the task you care about? Density estimation- do you care about evaluating probabilities of images? Compression? Pure sampling/generation? Representation learning from unlabelled data? More than one task?</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#density-estimation-or-compression","title":"Density Estimation or Compression","text":"<p>Likelihood as a metric is pretty good for Density Estimation.</p> <ul> <li> <p>Split data into train, validation and test sets.</p> </li> <li> <p>Learn model \\(p_{\\theta}\\) using the train set.</p> </li> <li> <p>Tune hyperparameters on the validation set.</p> </li> <li> <p>Evaluate generalization with likelihood on test set: \\(\\mathbb{E}_{p_{data}}[\\log p_\\theta]\\)</p> </li> </ul> <p>Note: This is the same as compression because, by Shannon's source coding theorem, the optimal code length for encoding data from distribution \\(p_{data}\\) using model \\(p_\\theta\\) is \\(-\\log p_\\theta(x)\\). The average number of bits needed to encode data from \\(p_{data}\\) using model \\(p_\\theta\\) is:</p> \\[\\text{Average Code Length} = \\mathbb{E}_{p_{data}}[-\\log p_\\theta(x)] = -\\mathbb{E}_{p_{data}}[\\log p_\\theta(x)]\\] <p>Therefore, maximizing \\(\\mathbb{E}_{p_{data}}[\\log p_\\theta]\\) is equivalent to minimizing the expected code length, which is the goal of compression. The intuition is that we assign short codes to frequent data points.</p> <p>Perplexity: Another common metric for evaluating generative models is perplexity, defined as:</p> \\[\\text{Perplexity} = 2^{-\\frac{1}{D}\\mathbb{E}_{p_{data}}[\\log p_\\theta(x)]}\\] <p>where \\(D\\) is the dimension of the data. This normalizes the log-likelihood by the data dimension, making perplexity comparable across different dimensionalities.</p> <p>Perplexity measures how \"surprised\" the model is by the data. Lower perplexity indicates better performance. For language models, perplexity represents the average number of choices the model has at each step when predicting the next token.</p> <p>Not all generative models have tractable likelihoods. For models where exact likelihood computation is intractable, we need alternative evaluation approaches:</p> <p>VAEs: We can compare models using the Evidence Lower BOund (ELBO):</p> \\[\\text{ELBO} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p(z))\\] <p>While ELBO is a lower bound on the true likelihood, it provides a reasonable proxy for model comparison within the VAE framework.</p> <p>GANs: GANs pose a unique challenge because they don't provide explicit likelihood estimates.</p> <p>In general, unbiased estimation of probability density functions from samples is impossible.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#sample-quality","title":"Sample Quality","text":"<p>Human evaluations are the gold standard.</p> <p>HYPE_time: A metric that measures the minimum time it takes for a human to distinguish between real and generated samples. Higher HYPE_time indicates better sample quality, as it takes humans longer to detect that samples are fake.</p> <p>HYPE_infinity: The percentage of samples that deceive people under unlimited time. The larger the better.</p> <p>Key Insight: HYPE metrics provide a human-centric evaluation of generative models, measuring how convincingly the model can fool human evaluators. This is particularly relevant for applications where human perception is the ultimate judge of quality.</p> <p>Human evaluations are expensive, biased and hard to reproduce.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#inception-score","title":"Inception Score","text":"<p>The Inception Score measures the quality and diversity of generated samples using a pre-trained classifier (typically Inception-v3 for images). It is based on two key principles:</p> <ol> <li>Sharpness: Generated samples should be easily classifiable (high confidence predictions)</li> <li>Diversity: The model should generate samples from different classes</li> </ol>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#frechet-inception-distance-fid","title":"Fr\u00e9chet Inception Distance (FID)","text":"<p>The Fr\u00e9chet Inception Distance (FID) measures similarities in the feature representations for datapoints sampled from \\(p_{\\theta}\\) and the test dataset.</p> <p>How FID is Computed:</p> <p>Feature Extraction: Use a pre-trained Inception network (typically Inception-v3) to extract features from both real and generated samples. Let \\(f_r(x)\\) and \\(f_g(x)\\) be the feature extractors for real and generated samples respectively.</p> <p>Distribution Modeling: Model the feature distributions as multivariate Gaussians.</p> <ul> <li> <p>For real data: \\(\\mathcal{N}(\\mu_r, \\Sigma_r)\\) where:</p> <ul> <li> <p>\\(\\mu_r = \\mathbb{E}_{x \\sim p_{data}}[f_r(x)]\\) (mean of real features)</p> </li> <li> <p>\\(\\Sigma_r = \\mathbb{E}_{x \\sim p_{data}}[(f_r(x) - \\mu_r)(f_r(x) - \\mu_r)^T]\\) (covariance of real features)</p> </li> </ul> </li> <li> <p>For generated data: \\(\\mathcal{N}(\\mu_g, \\Sigma_g)\\) where:</p> <ul> <li> <p>\\(\\mu_g = \\mathbb{E}_{x \\sim p_\\theta}[f_g(x)]\\) (mean of generated features)</p> </li> <li> <p>\\(\\Sigma_g = \\mathbb{E}_{x \\sim p_\\theta}[(f_g(x) - \\mu_g)(f_g(x) - \\mu_g)^T]\\) (covariance of generated features)</p> </li> </ul> </li> </ul> <p>Fr\u00e9chet Distance Calculation: Compute the Fr\u00e9chet distance between the two Gaussian distributions:</p> \\[\\text{FID} = \\|\\mu_r - \\mu_g\\|^2 + \\text{tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})\\] <p>where:</p> <ul> <li> <p>\\(\\|\\mu_r - \\mu_g\\|^2\\) is the squared Euclidean distance between means</p> </li> <li> <p>\\(\\text{tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})\\) is the trace of the covariance difference term</p> </li> <li> <p>The matrix square root \\((\\Sigma_r \\Sigma_g)^{1/2}\\) is computed using eigendecomposition</p> </li> </ul> <p>Note: Check this resource on evaluating Text-To-Image Models: HEIM</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#evaluating-latent-representations-and-prompting","title":"Evaluating Latent Representations and Prompting","text":""},{"location":"ai/deep_generative_models/evaluating_generative_models/#clustering","title":"Clustering","text":"<p>Clustering is a powerful method for evaluating the quality and structure of latent representations learned by generative models. It provides insights into how well the model organizes and separates different concepts in its latent space. Clusters can be obtained by applying k-means or any other algorithm in the latent space of the generative model.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#lossy-compression-or-reconstruction","title":"Lossy Compression or Reconstruction","text":"<p>Latent representations can be evaluated based on the maximum compression they can achieve without significant loss in reconstruction accuracy. This involves measuring the trade-off between the dimensionality of the latent space and the quality of reconstructed samples. A good latent representation should maintain high reconstruction fidelity while using a compact, low-dimensional encoding that captures the essential features of the data. There are many quantitative evaluation metrics for this.</p>"},{"location":"ai/deep_generative_models/evaluating_generative_models/#disentanglement","title":"Disentanglement","text":"<p>Intuitively, we want representations that disentangle independent and interpretable attributes of the observed data. Disentanglement means that different dimensions of the latent space should correspond to distinct, meaningful factors of variation in the data. For example, in face generation, one latent dimension might control facial expression while another controls hair color, allowing for independent manipulation of these attributes. There are many quantitative evaluation metrics for this.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/","title":"Generative Adversarial Networks","text":""},{"location":"ai/deep_generative_models/generative_adversarial_networks/#introduction-gans-as-a-paradigm-shift","title":"Introduction: GANs as a Paradigm Shift","text":"<p>GANs are unique from all the other model families that we have seen so far, such as autoregressive models, VAEs, and normalizing flow models, because we do not train them using maximum likelihood.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#the-traditional-likelihood-based-paradigm","title":"The Traditional Likelihood-Based Paradigm","text":"<p>All the generative models we've explored so far follow a similar training paradigm:</p> <ol> <li>Autoregressive Models: Maximize \\(\\log p_\\theta(x) = \\sum_{i=1}^N \\log p_\\theta(x_i|x_{&lt;i})\\)</li> <li>Variational Autoencoders: Maximize the ELBO \\(\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z))\\)</li> <li>Normalizing Flow Models: Maximize \\(\\log p_\\theta(x) = \\log p_z(f^{-1}_\\theta(x)) + \\log |\\det(\\frac{\\partial f^{-1}_\\theta(x)}{\\partial x})|\\)</li> </ol> <p>Common Theme: All these models are trained by maximizing some form of likelihood or likelihood approximation.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#gans-a-different-approach","title":"GANs: A Different Approach","text":"<p>GANs break away from this paradigm entirely. Instead of maximizing likelihood, GANs use adversarial training - a fundamentally different approach to generative modeling. We'll get to what a Generator and a Discriminator are in a bit but here is a quick table showing how GAN is different.</p> <p>Key Differences:</p> Aspect Likelihood-Based Models GANs Training Objective Maximize likelihood/ELBO Minimax game between generator and discriminator Loss Function \\(\\mathcal{L} = -\\log p_\\theta(x)\\) \\(\\mathcal{L}_G = -\\log D(G(z))\\), \\(\\mathcal{L}_D = -\\log D(x) - \\log(1-D(G(z)))\\) Model Evaluation Direct likelihood computation No explicit likelihood computation Training Stability Generally stable Can be unstable, requires careful tuning Sample Quality May produce blurry samples Often produces sharp, realistic samples"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#likelihood-free-learning","title":"Likelihood-Free Learning","text":"<p>Why not use maximum likelihood? In fact, it is not so clear that better likelihood numbers necessarily correspond to higher sample quality. We know that the optimal generative model will give us the best sample quality and highest test log-likelihood. However, models with high test log-likelihoods can still yield poor samples, and vice versa.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#the-likelihood-vs-sample-quality-disconnect","title":"The Likelihood vs. Sample Quality Disconnect","text":"<p>To see why, consider pathological cases in which our model is comprised almost entirely of noise, or our model simply memorizes the training set:</p> <ol> <li> <p>Noise Model: A model that outputs pure noise might assign some probability to real data points, leading to a non-zero (though poor) likelihood, but produces completely useless samples.</p> </li> <li> <p>Memorization Model: A model that perfectly memorizes the training set will have very high likelihood on training data but will only reproduce exact training examples, lacking generalization and diversity.</p> </li> </ol> <p>Therefore, we turn to likelihood-free training with the hope that optimizing a different objective will allow us to disentangle our desiderata of obtaining high likelihoods as well as high-quality samples.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#the-two-sample-test-framework","title":"The Two-Sample Test Framework","text":"<p>Recall that maximum likelihood required us to evaluate the likelihood of the data under our model \\(p_\\theta\\). A natural way to set up a likelihood-free objective is to consider the two-sample test, a statistical test that determines whether or not a finite set of samples from two distributions are from the same distribution using only samples from \\(P\\) and \\(Q\\).</p> <p>Concretely, given \\(S_1 = \\{x \\sim P\\}\\) and \\(S_2 = \\{x \\sim Q\\}\\), we compute a test statistic \\(T\\) according to the difference in \\(S_1\\) and \\(S_2\\) that, when less than a threshold \\(\\alpha\\), accepts the null hypothesis that \\(P = Q\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#application-to-generative-modeling","title":"Application to Generative Modeling","text":"<p>Analogously, we have in our generative modeling setup access to our training set \\(S_1 = \\{x \\sim p_{data}\\}\\) and \\(S_2 = \\{x \\sim p_\\theta\\}\\). The key idea is to train the model to minimize a two-sample test objective between \\(S_1\\) and \\(S_2\\).</p> <p>However, this objective becomes extremely difficult to work with in high dimensions, so we choose to optimize a surrogate objective that instead maximizes some distance between \\(S_1\\) and \\(S_2\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#why-this-approach-makes-sense","title":"Why this approach makes sense","text":"<p>1. Avoiding pathological cases: - The two-sample test framework naturally avoids the noise and memorization problems - It forces the model to learn the true underlying distribution structure</p> <p>3. Flexibility: - We can choose different distance metrics or test statistics - This allows us to focus on different aspects of sample quality</p> <p>Key Insight: GANs implement likelihood-free learning by using a neural network (the discriminator) to learn an optimal test statistic for distinguishing between real and generated data, and then training the generator to minimize this learned distance.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#gan-objective","title":"GAN Objective","text":"<p>We thus arrive at the generative adversarial network formulation. There are two components in a GAN: (1) a generator and (2) a discriminator. The generator \\(G_\\theta\\) is a directed latent variable model that deterministically generates samples \\(x\\) from \\(z\\), and the discriminator \\(D_\\phi\\) is a function whose job is to distinguish samples from the real dataset and the generator.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#components","title":"Components","text":"<ul> <li>Generator \\(G_\\theta\\): A neural network that transforms noise \\(z \\sim p(z)\\) to samples \\(G_\\theta(z)\\)</li> <li>Discriminator \\(D_\\phi\\): A neural network that outputs a probability \\(D_\\phi(x) \\in [0,1]\\) indicating whether \\(x\\) is real or generated</li> </ul>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#the-minimax-game","title":"The Minimax Game","text":"<p>The generator and discriminator both play a two player minimax game, where: - Generator: Minimizes a two-sample test objective (\\(p_{data} = p_\\theta\\)) - Discriminator: Maximizes the objective (\\(p_{data} \\neq p_\\theta\\))</p> <p>Intuitively, the generator tries to fool the discriminator to the best of its ability by generating samples that look indistinguishable from \\(p_{data}\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#formal-objective","title":"Formal Objective","text":"<p>The GAN objective can be written as:</p> \\[\\min_\\theta \\max_\\phi V(G_\\theta, D_\\phi) = \\mathbb{E}_{x \\sim p_{data}}[\\log D_\\phi(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1-D_\\phi(G_\\theta(z)))]\\]"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#understanding-the-objective","title":"Understanding the Objective","text":"<p>Let's unpack this expression:</p> <p>For the Discriminator (maximizing with respect to \\(\\phi\\)): - Given a fixed generator \\(G_\\theta\\), the discriminator performs binary classification - It tries to assign probability 1 to data points from the training set \\(x \\sim p_{data}\\) - It tries to assign probability 0 to generated samples \\(x \\sim p_G\\)</p> <p>For the Generator (minimizing with respect to \\(\\theta\\)): - Given a fixed discriminator \\(D_\\phi\\), the generator tries to maximize \\(D_\\phi(G_\\theta(z))\\) - This is equivalent to minimizing \\(\\log(1-D_\\phi(G_\\theta(z)))\\)</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#optimal-discriminator","title":"Optimal Discriminator","text":"<p>In this setup, the optimal discriminator is:</p> \\[D^*_G(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_G(x)}\\] <p>Derivation: The discriminator's objective is to maximize:</p> \\[\\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{x \\sim p_G}[\\log(1-D(x))]\\] <p>This is maximized when:</p> \\[D(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_G(x)}\\] <p>On the other hand, the generator minimizes this objective for a fixed discriminator \\(D_\\phi\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#connection-to-jensen-shannon-divergence","title":"Connection to Jensen-Shannon Divergence","text":"<p>After performing some algebra, plugging in the optimal discriminator \\(D^*_G(\\cdot)\\) into the overall objective \\(V(G_\\theta, D^*_G(x))\\) gives us:</p> \\[2D_{JSD}[p_{data}, p_G] - \\log 4\\] <p>The \\(D_{JSD}\\) term is the Jensen-Shannon Divergence, which is also known as the symmetric form of the KL divergence:</p> \\[D_{JSD}[p,q] = \\frac{1}{2}\\left(D_{KL}\\left[p, \\frac{p+q}{2}\\right] + D_{KL}\\left[q, \\frac{p+q}{2}\\right]\\right)\\]"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#properties-of-jsd","title":"Properties of JSD","text":"<p>The JSD satisfies all properties of the KL divergence, and has the additional perk that \\(D_{JSD}[p,q] = D_{JSD}[q,p]\\) (symmetry).</p> <p>Key Properties:</p> <ol> <li> <p>Non-negative: \\(D_{JSD}[p,q] \\geq 0\\)</p> </li> <li> <p>Symmetric: \\(D_{JSD}[p,q] = D_{JSD}[q,p]\\)</p> </li> <li> <p>Zero iff equal: \\(D_{JSD}[p,q] = 0\\) if and only if \\(p = q\\)</p> </li> <li> <p>Bounded: \\(D_{JSD}[p,q] \\leq \\log 2\\)</p> </li> </ol>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#optimal-solution","title":"Optimal Solution","text":"<p>With this distance metric, the optimal generator for the GAN objective becomes \\(p_G = p_{data}\\), and the optimal objective value that we can achieve with optimal generators and discriminators \\(G^*(\\cdot)\\) and \\(D^*_{G^*}(x)\\) is \\(-\\log 4\\).</p> <p>Why \\(-\\log 4\\)? - When \\(p_G = p_{data}\\), we have \\(D_{JSD}[p_{data}, p_G] = 0\\) - Therefore, \\(V(G^*, D^*) = 2 \\cdot 0 - \\log 4 = -\\log 4\\)</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#gan-training-algorithm","title":"GAN Training Algorithm","text":"<p>Thus, the way in which we train a GAN is as follows:</p> <p>For epochs \\(1, \\ldots, N\\) do:</p> <ol> <li>Sample minibatch of size \\(m\\) from data: \\(x^{(1)}, \\ldots, x^{(m)} \\sim p_{data}\\)</li> <li>Sample minibatch of size \\(m\\) of noise: \\(z^{(1)}, \\ldots, z^{(m)} \\sim p_z\\)</li> <li>Take a gradient descent step on the generator parameters \\(\\theta\\):</li> </ol> \\[\\nabla_\\theta V(G_\\theta, D_\\phi) = \\frac{1}{m}\\nabla_\\theta \\sum_{i=1}^m \\log(1-D_\\phi(G_\\theta(z^{(i)})))\\] <ol> <li>Take a gradient ascent step on the discriminator parameters \\(\\phi\\):</li> </ol> \\[\\nabla_\\phi V(G_\\theta, D_\\phi) = \\frac{1}{m}\\nabla_\\phi \\sum_{i=1}^m [\\log D_\\phi(x^{(i)}) + \\log(1-D_\\phi(G_\\theta(z^{(i)})))]\\] <p>Key Points:</p> <ol> <li>Alternating Updates: We update the generator and discriminator in alternating steps</li> <li>Minibatch Training: We use minibatches of both real data and noise samples</li> <li>Generator Update: Minimizes the probability that the discriminator correctly identifies generated samples</li> <li>Discriminator Update: Maximizes the probability of correctly classifying real vs. generated samples</li> </ol> <p>Practical Considerations:</p> <ul> <li>Learning Rate Balance: The learning rates for generator and discriminator must be carefully balanced</li> <li>Update Frequency: Often the discriminator is updated multiple times per generator update</li> <li>Convergence Monitoring: Training progress is monitored through discriminator accuracy and sample quality</li> <li>Early Stopping: Training may be stopped when the discriminator can no longer distinguish real from fake</li> </ul> <p>This formulation shows that GANs are essentially implementing an adaptive two-sample test, where the discriminator learns the optimal way to distinguish between real and generated data, and the generator learns to minimize this learned distance.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#challenges","title":"Challenges","text":"<p>Although GANs have been successfully applied to several domains and tasks, working with them in practice is challenging because of their: (1) unstable optimization procedure, (2) potential for mode collapse, (3) difficulty in evaluation.</p> <p>1. Unstable Optimization Procedure</p> <p>During optimization, the generator and discriminator loss often continue to oscillate without converging to a clear stopping point. Due to the lack of a robust stopping criteria, it is difficult to know when exactly the GAN has finished training.</p> <p>Causes of Instability: - Minimax Nature: The adversarial game creates competing objectives - Gradient Issues: Vanishing/exploding gradients can occur - Learning Rate Sensitivity: Small changes in learning rates can cause divergence - Network Capacity Imbalance: If one network becomes too powerful, training collapses</p> <p>Symptoms: - Oscillating loss curves - No clear convergence pattern - Sudden collapse of training - Generator or discriminator loss going to zero/infinity</p> <p>2. Mode Collapse</p> <p>The generator of a GAN can often get stuck producing one of a few types of samples over and over again (mode collapse). This occurs when the generator finds a few \"safe\" modes that consistently fool the discriminator and stops exploring the full data distribution.</p> <p>What is Mode Collapse: - Definition: Generator only produces samples from a subset of the true distribution modes - Example: In image generation, only producing images of one type (e.g., only front-facing faces) - Problem: Lack of diversity in generated samples</p> <p>Causes: - Discriminator Overfitting: Discriminator becomes too good at detecting certain types of fake samples - Generator Optimization: Generator finds local optima that work well against current discriminator - Training Imbalance: One network becomes too powerful relative to the other</p> <p>3. Difficulty in Evaluation</p> <p>Unlike likelihood-based models, GANs don't provide explicit likelihood values, making evaluation challenging.</p> <p>Evaluation Challenges: - No Likelihood: Can't use traditional metrics like log-likelihood - Subjective Quality: Sample quality is often subjective and domain-specific - Diversity vs. Quality Trade-off: Hard to balance sample quality with diversity - Mode Coverage: Difficult to measure if all modes of the data distribution are captured</p> <p>Addressing the Challenges:</p> <p>Most fixes to these challenges are empirically driven, and there has been a significant amount of work put into developing new architectures, regularization schemes, and noise perturbations in an attempt to circumvent these issues.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#selected-gans","title":"Selected GANs","text":"<p>Next, we focus our attention to a few select types of GAN architectures and explore them in more detail.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#f-gan","title":"f-GAN","text":"<p>The f-GAN optimizes the variant of the two-sample test objective that we have discussed so far, but using a very general notion of distance: the f-divergence. Given two densities \\(p\\) and \\(q\\), the f-divergence can be written as:</p> \\[D_f(p,q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>where \\(f\\) is any convex, lower-semicontinuous function with \\(f(1) = 0\\). Several of the distance \"metrics\" that we have seen so far fall under the class of f-divergences, such as KL and Jensen-Shannon.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#understanding-the-requirements","title":"Understanding the Requirements","text":"<p>What is a convex function? A function \\(f\\) is convex if for any two points \\(x, y\\) and any \\(\\lambda \\in [0,1]\\), we have:</p> \\[f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\\] <p>This means that the line segment between any two points on the function lies above or on the function itself. </p> <p>Understanding the Line Segment Property:</p> <p>Let's break down what this means geometrically:</p> <p>Two Points: Consider any two points \\((x, f(x))\\) and \\((y, f(y))\\) on the graph of the function \\(f\\)</p> <p>Line Segment: The line segment connecting these points consists of all points of the form:</p> \\[(\\lambda x + (1-\\lambda)y, \\lambda f(x) + (1-\\lambda)f(y))\\] <p>where \\(\\lambda \\in [0,1]\\)</p> <p>Function Value: At the same \\(x\\)-coordinate \\(\\lambda x + (1-\\lambda)y\\), the function value is:</p> \\[f(\\lambda x + (1-\\lambda)y)\\] <p>Convexity Condition: The inequality \\(f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\\) means that the function value at any point on the line segment is less than or equal to the corresponding point on the straight line connecting \\((x, f(x))\\) and \\((y, f(y))\\)</p> <p>Visual Interpretation: If you draw a straight line between any two points on a convex function's graph. The entire function between those points must lie on or below that straight line.</p> <p>Convex functions have important properties:</p> <ul> <li> <p>Single minimum: If a minimum exists, it's global</p> </li> <li> <p>Well-behaved gradients: Useful for optimization</p> </li> <li> <p>Jensen's inequality: \\(\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X])\\) for convex \\(f\\)</p> </li> </ul> <p>What is a lower-semicontinuous function? A function \\(f\\) is lower-semicontinuous at a point \\(x_0\\) if:</p> \\[\\liminf_{x \\to x_0} f(x) \\geq f(x_0)\\] <p>Understanding \\(\\liminf\\) and the Infimum:</p> <p>The notation \\(\\liminf_{x \\to x_0} f(x)\\) involves two concepts:</p> <ol> <li> <p>Infimum (inf): The infimum of a set is the greatest lower bound. For a set \\(S\\), \\(\\inf S\\) is the largest number that is less than or equal to all elements in \\(S\\).</p> </li> <li> <p>Limit Inferior: \\(\\liminf_{x \\to x_0} f(x)\\) is the infimum of all limit points of \\(f(x)\\) as \\(x\\) approaches \\(x_0\\).</p> </li> </ol> <p>How \\(\\liminf\\) works:</p> <p>Consider all sequences \\(\\{x_n\\}\\) that converge to \\(x_0\\). For each sequence, we look at the limit of \\(f(x_n)\\) (if it exists). The \\(\\liminf\\) is the infimum of all these possible limit values.</p> <p>Mathematical Definition:</p> \\[\\liminf_{x \\to x_0} f(x) = \\inf \\left\\{ \\lim_{n \\to \\infty} f(x_n) : x_n \\to x_0 \\text{ and } \\lim_{n \\to \\infty} f(x_n) \\text{ exists} \\right\\}\\] <p>Why consider Sequences even for Continuous Functions?</p> <p>You might wonder: \"If \\(f\\) is continuous, why do we need sequences? Can't we just use the regular limit?\"</p> <p>Key Insight: Lower-semicontinuity is a weaker condition than continuity. A function can be lower-semicontinuous without being continuous.</p> <p>The Relationship:</p> <ol> <li>Continuous functions are always lower-semicontinuous</li> <li>Lower-semicontinuous functions may have discontinuities (but only \"jumps up\")</li> </ol> <p>Example of Lower-Semicontinuous but NOT Continuous: Consider \\(f(x) = \\begin{cases} 0 &amp; \\text{if } x &lt; 0 \\\\ 1 &amp; \\text{if } x \\geq 0 \\end{cases}\\). This function is lower-semicontinuous at \\(x = 0\\) (no \"jump down\"). But it's NOT continuous at \\(x = 0\\) (there's a \"jump up\")</p> <p>Example of NOT Lower-Semicontinuous: Consider \\(f(x) = \\begin{cases} 0 &amp; \\text{if } x &lt; 0 \\\\ 1 &amp; \\text{if } x = 0 \\\\ 0 &amp; \\text{if } x &gt; 0 \\end{cases}\\) at \\(x_0 = 0\\):</p> <p>\\(\\liminf_{x \\to 0^-} f(x) = 0\\) (approaching from left)</p> <p>\\(\\liminf_{x \\to 0^+} f(x) = 0\\) (approaching from right)</p> <p>\\(\\liminf_{x \\to 0} f(x) = 0\\) (overall limit inferior)</p> <p>Since \\(f(0) = 1\\) and \\(0 \\not\\geq 1\\), this function is NOT lower-semicontinuous at \\(x = 0\\)</p> <p>Why the Sequence definition is uiversal:</p> <p>The sequence-based definition works for ALL functions, whether they're: - Continuous everywhere - Lower-semicontinuous but not continuous - Neither continuous nor lower-semicontinuous</p> <p>For Continuous Functions: If \\(f\\) is continuous at \\(x_0\\), then:</p> \\[\\liminf_{x \\to x_0} f(x) = \\lim_{x \\to x_0} f(x) = f(x_0)\\] <p>So the sequence definition \"reduces\" to the regular limit, but it's still the same mathematical concept.</p> <p>Why this matters for f-Divergences: The f-divergence framework needs to work with functions that might not be continuous everywhere, so we need the more general sequence-based definition.</p> <p>Why do we need \\(f(1) = 0\\)? This requirement ensures that the f-divergence has the correct properties for a distance measure:</p> <p>Zero when distributions are equal: When \\(p = q\\), we have \\(\\frac{p(x)}{q(x)} = 1\\) everywhere, so:</p> \\[D_f(p,p) = \\mathbb{E}_{x \\sim p}[f(1)] = \\mathbb{E}_{x \\sim p}[0] = 0\\] <p>Distance-like behavior: This property ensures that the f-divergence behaves like a proper distance measure, being zero only when the distributions are identical</p> <p>Example: For KL divergence, \\(f(u) = u \\log u\\) satisfies \\(f(1) = 1 \\cdot \\log 1 = 0\\), making it a valid choice for an f-divergence.</p> <p>Important Clarification: KL Divergence and Convexity</p> <p>You might be wondering: \"But \\(\\log u\\) is concave, so how can KL divergence be an f-divergence?\" This is a great observation! The key is that the \\(f\\) function for KL divergence is \\(f(u) = u \\log u\\), not just \\(\\log u\\).</p> <p>The KL Divergence Formula: The KL divergence between distributions \\(p\\) and \\(q\\) is:</p> \\[D_{KL}(p||q) = \\mathbb{E}_{x \\sim p}\\left[\\log\\frac{p(x)}{q(x)}\\right] = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)}\\log\\frac{p(x)}{q(x)}\\right]\\] <p>Notice that the second form matches the f-divergence formula with \\(f(u) = u \\log u\\).</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#f-divergence-examples","title":"f-Divergence Examples","text":"<p>Common f-divergences:</p> <ol> <li>KL Divergence: \\(f(u) = u \\log u\\)</li> <li>Reverse KL: \\(f(u) = -\\log u\\)</li> <li>Jensen-Shannon: \\(f(u) = u \\log u - (u+1) \\log \\frac{u+1}{2}\\)</li> <li>Total Variation: \\(f(u) = \\frac{1}{2}|u-1|\\)</li> <li>Pearson \u03c7\u00b2: \\(f(u) = (u-1)^2\\)</li> </ol>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#setting-up-the-f-gan-objective","title":"Setting up the f-GAN Objective","text":"<p>To set up the f-GAN objective, we borrow two commonly used tools from convex optimization: the Fenchel conjugate and duality. Specifically, we obtain a lower bound to any f-divergence via its Fenchel conjugate:</p> \\[D_f(p,q) \\geq \\sup_{T \\in \\mathcal{T}} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>Where \\(f^*\\) is the Fenchel conjugate of \\(f\\):</p> \\[f^*(t) = \\sup_{u \\in \\text{dom}(f)} (tu - f(u))\\] <p>What is the Fenchel Conjugate?</p> <p>The Fenchel conjugate \\(f^*\\) of a function \\(f\\) is defined as:</p> \\[f^*(t) = \\sup_{u \\in \\text{dom}(f)} (tu - f(u))\\] <p>Economic Intuition for the Fenchel Conjugate:</p> <p>One of the most intuitive ways to understand the convex conjugate function \\(f^*(t)\\) is through an economic lens. Imagine \\(f(u)\\) as the cost function representing the total expense incurred to produce a quantity \\(u\\) of a certain product. The variable \\(y\\) corresponds to the market price per unit of that product.</p> <p>In this context, the product \\(xy\\) represents the revenue generated by selling \\(x\\) units at price \\(t\\). The term \\(f(u)\\), as mentioned, is the cost of producing those units. Therefore, the expression \\(tu - f(u)\\) represents the profit earned by producing and selling \\(u\\) units at price \\(t\\).</p> <p>The convex conjugate \\(f^*(t)\\) is defined as the supremum (or maximum) of this profit over all possible production quantities \\(u\\):</p> \\[f^*(t) = \\sup_u (tu - f(u))\\] <p>Thus, \\(f^*(t)\\) gives the optimal profit achievable at the market price \\(t\\), assuming the producer chooses the best production quantity \\(u\\) to maximize profit.</p> <p>Geometric Interpretation in Economics:</p> <p>Now, consider the graph of the cost function \\(f(u)\\). Assume \\(f\\) is convex, continuous, and differentiable, which is a reasonable assumption for many cost functions in economics.</p> <p>The slope of the cost curve at any point \\(u\\) is given by the derivative \\(f'(u)\\). This derivative represents the marginal cost \u2014 the additional cost to produce one more unit at quantity \\(u\\).</p> <p>The condition for optimal production quantity \\(u\\) at price \\(t\\) arises from maximizing profit:</p> \\[\\max_u \\{tu - f(u)\\}\\] <p>Taking the derivative with respect to \\(u\\) and setting it to zero for an optimum:</p> \\[t - f'(u) = 0 \\implies t = f'(u)\\] <p>This means the optimal production quantity \\(u\\) is found where the price \\(t\\) equals the marginal cost \\(f'(u)\\).</p> <p>Geometrically, this corresponds to finding a tangent line to the graph of \\(f(u)\\) that has slope \\(t\\). Using a ruler, you can \"slide\" the line around until it just touches the cost curve without crossing it. The point of tangency corresponds to the optimal \\(u\\).</p> <p>Importantly, the vertical intercept of this tangent line relates directly to the optimal profit. The tangent line can be expressed as:</p> \\[\\ell(u) = f(u_0) + f'(u_0)(u - u_0)\\] <p>At \\(u = 0\\), the intercept is:</p> \\[\\ell(0) = f(u_0) - u_0 f'(u_0)\\] <p>Notice that:</p> \\[-(u_0 t - f(u_0)) = f(u_0) - u_0 t\\] <p>Since \\(t = f'(u_0)\\), the intercept equals the negative of the optimal profit. Therefore, the intercept of the tangent line with slope \\(t\\) gives \\(-f^*(t)\\).</p> <p></p> <p>In the above diagram, for a given \\(y\\), we are trying to maximize the difference between the line \\(xy\\) and \\(f(x)\\). For that given \\(y\\), it turns out that the maximum value that \\(xy - f(x)\\) occurs when we draw a tangent to \\(f(x)\\) with slope \\(y\\). The point at which the tangent occurs is the optimum \\(x\\). It also turns out that the vertical intercept is \\(-f^*(y)\\) = maximum value that \\(xy - f(x)\\) occurs for the given \\(y\\). For a different \\(y\\), there will be a different \\(x\\) where a line parallel to the line \\(xy\\) becomes tangent. The graph of \\(f^*(y)\\) is essentially how the negative of the vertical intercept varies over the domain \\(y\\). The graphs of \\(f(x)\\) and \\(f^*(y)\\) live in different dual spaces. \\(f(x)\\) is a function of \"quantities\", while \\(f^*(y)\\) is a function of \"prices\" or \"slopes\". </p> <p>Key Properties: 1. Convexity: If \\(f\\) is convex, then \\(f^*\\) is also convex 2. Duality: \\((f^*)^* = f\\) (the conjugate of the conjugate is the original function) 3. Domain: The domain of \\(f^*\\) depends on the behavior of \\(f\\)</p> <p>Examples of Fenchel Conjugates:</p> <p>Example 1: KL Divergence</p> <p>For \\(f(u) = u \\log u\\):</p> \\[f^*(t) = \\sup_{u &gt; 0} (tu - u \\log u)\\] <p>To find this, we set the derivative to zero:</p> \\[\\frac{d}{du}(tu - u \\log u) = t - \\log u - 1 = 0\\] \\[\\log u = t - 1\\] \\[u = e^{t-1}\\] <p>Substituting back:</p> \\[f^*(t) = te^{t-1} - e^{t-1}(t-1) = e^{t-1}\\] <p>Example 2: Reverse KL</p> <p>For \\(f(u) = -\\log u\\):</p> \\[f^*(t) = \\sup_{u &gt; 0} (tu + \\log u)\\] <p>Setting derivative to zero:</p> \\[\\frac{d}{du}(tu + \\log u) = t + \\frac{1}{u} = 0\\] \\[u = -\\frac{1}{t}\\] <p>Substituting back:</p> \\[f^*(t) = t(-\\frac{1}{t}) + \\log(-\\frac{1}{t}) = -1 + \\log(-\\frac{1}{t}) = -1 - \\log(-t)\\] <p>Example 3: Total Variation</p> <p>For \\(f(u) = \\frac{1}{2}|u-1|\\):</p> \\[f^*(t) = \\sup_{u} (tu - \\frac{1}{2}|u-1|)\\] <p>This gives:</p> \\[f^*(t) = \\begin{cases} t &amp; \\text{if } |t| \\leq \\frac{1}{2} \\\\ +\\infty &amp; \\text{otherwise} \\end{cases}\\] <p>The Duality Principle:</p> <p>The Fenchel conjugate provides a way to transform optimization problems. The key insight is that:</p> <p>Primal Problem: \\(\\inf_{u} f(u)\\)</p> <p>Dual Problem: \\(\\sup_{t} -f^*(t)\\)</p> <p>The Primal-Dual Relationship:</p> <p>The f-divergence can be expressed in both forms:</p> \\[D_f(p,q) = \\mathbb{E}_{x \\sim q}\\left[f\\left(\\frac{p(x)}{q(x)}\\right)\\right] = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)]- \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>Where:</p> <ul> <li> <p>Primal form: \\(\\mathbb{E}_{x \\sim q}\\left[f\\left(\\frac{p(x)}{q(x)}\\right)\\right]\\) (direct computation)</p> </li> <li> <p>Dual form: \\(\\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\) (optimization problem)</p> </li> </ul> <p>Derivation: From Primal to Dual Form</p> <p>Let's walk through the step-by-step derivation of how we transform the primal form into the dual form:</p> <p>Step 1: Start with the Primal Form</p> <p>The f-divergence is defined as:</p> \\[D_f(p,q) = \\mathbb{E}_{x \\sim q}\\left[f\\left(\\frac{p(x)}{q(x)}\\right)\\right]\\] <p>This is the \"primal form\" - it directly computes the divergence by evaluating the function \\(f\\) at the ratio \\(\\frac{p(x)}{q(x)}\\).</p> <p>Step 2: Apply the Fenchel Conjugate Identity</p> <p>The key insight comes from the Fenchel conjugate identity. For any convex function \\(f\\) and any point \\(u\\), we have:</p> \\[f(u) = \\sup_{t} (tu - f^*(t))\\] <p>This is a fundamental result in convex analysis known as the Fenchel-Moreau theorem. It states that a convex function can be recovered from its conjugate.</p> <p>Step 3: Substitute the Identity</p> <p>We substitute this identity into our primal form:</p> \\[D_f(p,q) = \\mathbb{E}_{x \\sim q}\\left[\\sup_{t} \\left(t \\cdot \\frac{p(x)}{q(x)} - f^*(t)\\right)\\right]\\] <p>Step 4: Exchange Supremum and Expectation</p> <p>This is the crucial step. We can exchange the supremum and expectation under certain conditions (satisfied for convex \\(f\\)):</p> \\[D_f(p,q) = \\sup_{T} \\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)} - f^*(T(x))\\right]\\] <p>Here, we've replaced the variable \\(t\\) with a function \\(T(x)\\) that can depend on \\(x\\).</p> <p>Step 5: Simplify the Expression</p> <p>We can rewrite the expectation:</p> \\[\\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)} - f^*(T(x))\\right] = \\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)}\\right] - \\mathbb{E}_{x \\sim q}[f^*(T(x))]\\] <p>The first term can be simplified using the definition of expectation:</p> \\[\\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)}\\right] = \\int T(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x) dx = \\int T(x) \\cdot p(x) dx = \\mathbb{E}_{x \\sim p}[T(x)]\\] <p>Step 6: Arrive at the Dual Form</p> <p>Putting it all together:</p> \\[D_f(p,q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x \\sim q}[f^*(T(x))]\\right)\\] <p>In other words (to distinguish the two different inputs to the Discriminator):</p> \\[D_f(p,q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>This is the dual form of the f-divergence.</p> <p>Why This Derivation Works:</p> <ol> <li>Convexity: The convexity of \\(f\\) ensures that the Fenchel conjugate identity holds</li> <li>Exchange of Supremum and Expectation: This is valid because we're optimizing over a convex set of functions</li> <li>Duality Gap: Under certain conditions, there is no duality gap, meaning the primal and dual forms give the same value</li> </ol> <p>Key Insights from the Derivation:</p> <ol> <li> <p>From Direct Computation to Optimization: The primal form requires direct computation of \\(f(\\frac{p(x)}{q(x)})\\), while the dual form transforms this into an optimization problem over functions \\(T\\).</p> </li> <li> <p>Role of the Fenchel Conjugate: The conjugate \\(f^*\\) appears naturally in the dual form.</p> </li> <li> <p>Connection to GANs: The dual form is perfect for GANs because we can parameterize \\(T\\) as a neural network (the discriminator). The optimization becomes a minimax game. We can use gradient-based optimization.</p> </li> </ol> <p>Example: KL Divergence Derivation</p> <p>Let's see this in action for KL divergence where \\(f(u) = u \\log u\\):</p> <p>Primal Form:</p> \\[D_{KL}(p||q) = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)} \\log \\frac{p(x)}{q(x)}\\right]\\] <p>Step 1: Use the Fenchel conjugate identity for \\(f(u) = u \\log u\\)</p> <p>We know that \\(f^*(t) = e^{t-1}\\) (from our earlier examples)</p> <p>Step 2: Substitute:</p> \\[D_{KL}(p||q) = \\mathbb{E}_{x \\sim q}\\left[\\sup_{t} \\left(t \\cdot \\frac{p(x)}{q(x)} - e^{t-1}\\right)\\right]\\] <p>Step 3: Exchange supremum and expectation:</p> \\[D_{KL}(p||q) = \\sup_{T} \\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)} - e^{T(x)-1}\\right]\\] <p>Step 4: Simplify:</p> \\[D_{KL}(p||q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x \\sim q}[e^{T(x)-1}]\\right)\\] <p>This gives us the dual form for KL divergence, which can be used in f-GAN training.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#steps-in-f-gan","title":"Steps in f-GAN","text":"<p>Step 1: Choose an f-divergence Select a convex function \\(f\\) with \\(f(1) = 0\\) (e.g., KL divergence, Jensen-Shannon, etc.)</p> <p>Step 2: Compute the Fenchel conjugate Find \\(f^*\\) analytically or numerically</p> <p>Step 3: Parameterize the dual variable Replace \\(T\\) with a neural network \\(T_\\phi\\) parameterized by \\(\\phi\\)</p> <p>Step 4: Set up the minimax game</p> \\[\\min_\\theta \\max_\\phi F(\\theta,\\phi) = \\mathbb{E}_{x \\sim p_{data}}[T_\\phi(x)] - \\mathbb{E}_{x_{fake} \\sim p_{G_\\theta}}[f^*(T_\\phi(x_{fake}))]\\] <p>Understanding the Roles:</p> <ul> <li>Generator (\\(G_\\theta\\)): Tries to minimize the divergence estimate</li> <li>Discriminator (\\(T_\\phi\\)): Tries to tighten the lower bound by maximizing the dual objective</li> </ul> <p>Key Insight: The discriminator \\(T_\\phi\\) is not a binary classifier like in standard GANs, but rather a function.</p> <p>Important Distinction: Vanilla GAN vs f-GAN Generator</p> <p>There is a fundamental difference between how generators work in vanilla GANs versus f-GANs:</p> <p>Vanilla GAN Generator:</p> <ul> <li> <p>Explicit generator network: \\(G_\\theta(z)\\) where \\(z \\sim p(z)\\) (noise)</p> </li> <li> <p>Direct transformation: Noise \\(z\\) \u2192 Generated sample \\(G_\\theta(z)\\)</p> </li> <li> <p>Objective: \\(\\min_\\theta \\max_\\phi V(G_\\theta, D_\\phi) = \\mathbb{E}_{x \\sim p_{data}}[\\log D_\\phi(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1-D_\\phi(G_\\theta(z)))]\\)</p> </li> </ul> <p>f-GAN Generator:</p> <ul> <li> <p>No explicit generator network: We work with \\(p_{G_\\theta}\\) as a distribution directly</p> </li> <li> <p>Implicit generator: The \"generator\" is whatever mechanism produces samples from \\(p_{G_\\theta}\\)</p> </li> <li> <p>Objective: \\(\\min_\\theta \\max_\\phi F(\\theta,\\phi) = \\mathbb{E}_{x \\sim p_{data}}[T_\\phi(x)] - \\mathbb{E}_{x_{fake} \\sim p_{G_\\theta}}[f^*(T_\\phi(x_{fake}))]\\)</p> </li> </ul> <p>The Key Insight:</p> <p>In f-GAN, the \"generator\" is implicit - it's whatever mechanism produces samples from the distribution \\(p_{G_\\theta}\\). This could be:</p> <ol> <li>A neural network \\(G_\\theta\\) that transforms noise (like in vanilla GANs)</li> <li>A flow-based model that transforms a base distribution</li> <li>A VAE decoder that generates from a latent space</li> <li>Any other generative model that produces samples from \\(p_{G_\\theta}\\)</li> </ol> <p>Why This Matters:</p> <p>The f-GAN framework is more general than vanilla GANs because:</p> <ul> <li>Vanilla GANs: Require a specific generator architecture \\(G_\\theta(z)\\)</li> <li>f-GANs: Can work with any generative model that produces samples from \\(p_{G_\\theta}\\)</li> </ul> <p>Practical Implementation:</p> <p>In practice, when implementing f-GAN, you would typically:</p> <ol> <li>Choose a generative model (e.g., a neural network \\(G_\\theta\\))</li> <li>Use it to generate samples from \\(p_{G_\\theta}\\)</li> <li>Apply the f-GAN objective to train both the generator and discriminator</li> </ol> <p>So in the f-GAN formulation, there's no explicit \\(G_\\theta\\) network like in vanilla GANs. The \"generator\" is the abstract distribution \\(p_{G_\\theta}\\), and the actual implementation depends on what generative model you choose to use.</p>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#advantages-of-f-gan","title":"Advantages of f-GAN","text":"<ol> <li>Unified Framework: One formulation covers many different GAN variants</li> <li>Theoretical Rigor: Based on well-established convex optimization theory</li> <li>Flexibility: Can adapt the divergence measure to the specific problem</li> <li>Stability: Some f-divergences may lead to more stable training</li> </ol>"},{"location":"ai/deep_generative_models/generative_adversarial_networks/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>Choice of f: Different f-divergences have different properties</li> <li>Fenchel Conjugate: Must be computable for the chosen f-divergence</li> <li>Training: Similar alternating optimization as standard GANs</li> <li>Evaluation: Same challenges as other GAN variants</li> </ul>"},{"location":"ai/deep_generative_models/introduction/","title":"Introduction","text":"<p>Natural agents excel at discovering patterns, extracting knowledge, and performing complex reasoning based on the data they observe. How can we build artificial learning systems to do the same?</p> <p>Generative models view the world under the lens of probability. In such a worldview, we can think of any kind of observed data, say , as a finite set of samples from an underlying distribution, say  pdata. At its very core, the goal of any generative model is then to approximate this data distribution given access to the dataset . The hope is that if we are able to  learn  a good generative model, we can use the learned model for downstream  inference.</p>"},{"location":"ai/deep_generative_models/introduction/#learning","title":"Learning","text":"<p>We will be primarily interested in parametric approximations (parametric models assume a specific data distribution (like a normal distribution) and estimate parameters (like the mean and standard deviation) of that distribution, while non-parametric models make no assumptions about the underlying distribution) to the data distribution, which summarize all the information about the dataset  in a finite set of parameters. In contrast with non-parametric models, parametric models scale more efficiently with large datasets but are limited in the family of distributions they can represent.</p> <p>In the parametric setting, we can think of the task of learning a generative model as picking the parameters within a family of model distributions that minimizes some notion of distance between the model distribution and the data distribution.</p> <p> </p> <p>For instance, we might be given access to a dataset of dog images  and our goal is to learn the parameters of a generative model \u03b8 within a model family M such that the model distribution p\u03b8 is close to the data distribution over dogs  pdata. Mathematically, we can specify our goal as the following optimization problem:</p> <p>min d(pdata,p\u03b8) where \u03b8\u2208M</p> <p>where pdata is accessed via the dataset  and  d(\u22c5) is a notion of distance between probability distributions.</p> <p>It is interesting to take note of the difficulty of the problem at hand. A typical image from a modern phone camera has a resolution of approximately  700\u00d71400700\u00d71400 pixels. Each pixel has three channels: R(ed), G(reen) and B(lue) and each channel can take a value between 0 to 255. Hence, the number of possible images is given by 256700\u00d71400\u00d73\u224810800000256700\u00d71400\u00d73\u224810800000. In contrast, Imagenet, one of the largest publicly available datasets, consists of only about 15 million images. Hence, learning a generative model with such a limited dataset is a highly underdetermined problem.</p> <p>Fortunately, the real world is highly structured and automatically discovering the underlying structure is key to learning generative models. For example, we can hope to learn some basic artifacts about dogs even with just a few images: two eyes, two ears, fur etc. Instead of incorporating this prior knowledge explicitly, we will hope the model learns the underlying structure directly from data.  We will be primarily interested in the following questions:</p> <ul> <li>What is the representation for the model family M?</li> <li>What is the objective function  d(\u22c5)?</li> <li>What is the optimization procedure for minimizing  d(\u22c5)?</li> </ul>"},{"location":"ai/deep_generative_models/introduction/#inference","title":"Inference","text":"<p>Discriminative models, also referred to as  conditional models, are a class of models frequently used for  classification. They are typically used to solve  binary classification  problems, i.e. assign labels, such as pass/fail, win/lose, alive/dead or healthy/sick, to existing datapoints.</p> <p>Types of discriminative models include  logistic regression  (LR),  conditional random fields  (CRFs),  decision trees  among many others.  Generative model  approaches which uses a joint probability distribution instead, include  naive Bayes classifiers,  Gaussian mixture models,  variational autoencoders,  generative adversarial networks  and others.</p> <p>Unlike generative modelling, which studies the joint probability  P(x,y), discriminative modeling studies the P(y|x) or maps the given unobserved variable (target) x to a class label y dependent on the observed variables (training samples). For example, in object recognition, x is likely to be a vector of raw pixels (or features extracted from the raw pixels of the image). Within a probabilistic framework, this is done by modeling the conditional probability distribution  P(y|x), which can be used for predicting y from x.</p> <p>For a discriminative model such as logistic regression, the fundamental inference task is to predict a label for any given datapoint. Generative models, on the other hand, learn a joint distribution over the entire data. While the range of applications to which generative models have been used continue to grow, we can identify three fundamental inference queries for evaluating a generative model.:</p> <ol> <li> <p>Density estimation:  Given a datapoint  x, what is the probability assigned by the model, i.e.,  p\u03b8(x)?</p> </li> <li> <p>Sampling:  How can we  generate  novel data from the model distribution, i.e.,  xnew\u223cp\u03b8(x)?</p> </li> <li> <p>Unsupervised representation learning:  How can we learn meaningful feature representations for a datapoint  x?</p> </li> </ol> <p>Going back to our example of learning a generative model over dog images, we can intuitively expect a good generative model to work as follows. For density estimation, we expect  p\u03b8(x) to be high for dog images and low otherwise. Alluding to the name  generative model, sampling involves generating novel images of dogs beyond the ones we observe in our dataset. Finally, representation learning can help discover high-level structure in the data such as the breed of dogs.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/","title":"Normalizing Flow Models","text":"<p>So far we have learned two types of likelihood based generative models:</p> <p>Autoregressive Models: \\(p_\\theta(x) = \\prod_{i=1}^N p_\\theta(x_i|x_{&lt;i})\\)</p> <p>Variational autoencoders: \\(p_\\theta(x) = \\int p_\\theta(x,z)dz\\)</p> <p>The two methods have relative strengths and weaknesses. Autoregressive models provide tractable likelihoods but no direct mechanism for learning features, whereas variational autoencoders can learn feature representations but have intractable marginal likelihoods.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#change-of-variables-formula","title":"Change of Variables Formula","text":"<p>In normalizing flows, we wish to map simple distributions (easy to sample and evaluate densities) to complex ones (learned via data). The change of variables formula describes how to evaluate densities of a random variable that is a deterministic transformation from another variable.</p> <p>Let's start with the univariate case and then generalize to multivariate random variables.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#univariate-case","title":"Univariate Case","text":"<p>Consider two random variables \\(Z\\) and \\(X\\) related by a strictly monotonic function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) such that \\(X = f(Z)\\). We want to find the probability density function of \\(X\\) in terms of the density of \\(Z\\).</p> <p>The key insight comes from the fact that probabilities must be preserved under the transformation. For any interval \\([a, b]\\) in the \\(X\\) space:</p> \\[P(a \\leq X \\leq b) = P(f^{-1}(a) \\leq Z \\leq f^{-1}(b))\\] <p>This can be written as:</p> \\[\\int_a^b p_X(x) dx = \\int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz\\] <p>To perform the substitution \\(z = f^{-1}(x)\\), we need to express \\(dz\\) in terms of \\(dx\\). Since \\(z = f^{-1}(x)\\), we can use the chain rule to find:</p> \\[\\frac{dz}{dx} = \\frac{d}{dx}f^{-1}(x) = \\frac{1}{f'(f^{-1}(x))}\\] <p>This follows from the inverse function theorem: if \\(y = f(x)\\), then \\(\\frac{dx}{dy} = \\frac{1}{f'(x)}\\).</p> <p>Therefore, \\(dz = \\frac{1}{f'(f^{-1}(x))} dx\\). However, we need to take the absolute value because probability densities must be non-negative. If \\(f'(f^{-1}(x)) &lt; 0\\) (meaning \\(f\\) is decreasing), then \\(\\frac{1}{f'(f^{-1}(x))} &lt; 0\\), which would make the density negative. Therefore, we use:</p> \\[dz = \\frac{1}{|f'(f^{-1}(x))|} dx\\] <p>Substituting this into our integral:</p> \\[\\int_a^b p_X(x) dx = \\int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz = \\int_a^b p_Z(f^{-1}(x)) \\cdot \\frac{1}{|f'(f^{-1}(x))|} dx\\] <p>Since this equality must hold for all intervals \\([a, b]\\), the integrands must be equal:</p> \\[p_X(x) = p_Z(f^{-1}(x)) \\cdot \\frac{1}{|f'(f^{-1}(x))|}\\] <p>This is the univariate change of variables formula. The factor \\(\\frac{1}{|f'(f^{-1}(x))|}\\) accounts for how the transformation stretches or compresses the probability mass.</p> <p>Why should \\(f\\) be monotonic? The monotonicity requirement ensures that \\(f\\) is invertible (one-to-one), which is crucial for the change of variables formula to work correctly. If \\(f\\) were not monotonic, there could be multiple values of \\(z\\) that map to the same value of \\(x\\), making the inverse function \\(f^{-1}\\) ill-defined. This would violate the fundamental assumption that we can uniquely determine the original variable \\(z\\) from the transformed variable \\(x\\).</p> <p>For example, if \\(f(z) = z^2\\) (which is not monotonic on \\(\\mathbb{R}\\)), then both \\(z = 2\\) and \\(z = -2\\) map to \\(x = 4\\). This creates ambiguity in the inverse mapping and would require special handling to account for multiple pre-images.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#multivariate-case","title":"Multivariate Case","text":"<p>For the multivariate case, we have random variables \\(\\mathbf{Z}\\) and \\(\\mathbf{X}\\) related by a bijective function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(\\mathbf{X} = f(\\mathbf{Z})\\).</p> <p>The key insight is that the probability mass in any region must be preserved under the transformation. For any region \\(A\\) in the \\(\\mathbf{X}\\) space:</p> \\[P(\\mathbf{X} \\in A) = P(\\mathbf{Z} \\in f^{-1}(A))\\] <p>This can be written as:</p> \\[\\int_A p_X(\\mathbf{x}) d\\mathbf{x} = \\int_{f^{-1}(A)} p_Z(\\mathbf{z}) d\\mathbf{z}\\] <p>To perform the multivariate substitution \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\), we need to understand how the volume element \\(d\\mathbf{z}\\) transforms. The Jacobian matrix \\(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\) is an \\(n \\times n\\) matrix where:</p> \\[\\left[\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right]_{ij} = \\frac{\\partial f^{-1}_i(\\mathbf{x})}{\\partial x_j}\\] <p>This matrix describes how small changes in \\(\\mathbf{x}\\) correspond to changes in \\(\\mathbf{z}\\). In multivariate calculus, when we perform a change of variables \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\), the volume element transforms as:</p> \\[d\\mathbf{z} = \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right| d\\mathbf{x}\\] <p>This is the multivariate generalization of the univariate substitution \\(dz = \\frac{1}{|f'(f^{-1}(x))|} dx\\). The determinant of the Jacobian matrix measures how the transformation affects the volume of a small region: - If \\(|\\det(J)| &gt; 1\\), the transformation expands volume - If \\(|\\det(J)| &lt; 1\\), the transformation contracts volume - If \\(|\\det(J)| = 1\\), the transformation preserves volume</p> <p>Substituting this into our integral:</p> \\[\\int_A p_X(\\mathbf{x}) d\\mathbf{x} = \\int_{f^{-1}(A)} p_Z(\\mathbf{z}) d\\mathbf{z} = \\int_A p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right| d\\mathbf{x}\\] <p>Since this equality must hold for all regions \\(A\\), the integrands must be equal:</p> \\[p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>This is the multivariate change of variables formula. The determinant of the Jacobian matrix accounts for how the transformation affects the volume of probability mass.</p> <p>Alternative Form Using Forward Mapping: Using the property that \\(\\det(A^{-1}) = \\det(A)^{-1}\\) for any invertible matrix \\(A\\), we can rewrite this as:</p> \\[p_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right|^{-1}\\] <p>This form is often more convenient in practice because it uses the forward mapping \\(f\\) rather than the inverse mapping \\(f^{-1}\\).</p> <p>Final result: Let \\(Z\\) and \\(X\\) be random variables which are related by a mapping \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(X = f(Z)\\) and \\(Z = f^{-1}(X)\\). Then</p> \\[p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>There are several things to note here:</p> <ul> <li>\\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) need to be continuous and have the same dimension.</li> <li>\\(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\) is a matrix of dimension \\(n \\times n\\), where each entry at location \\((i,j)\\) is defined as \\(\\frac{\\partial f^{-1}(\\mathbf{x})_i}{\\partial x_j}\\). This matrix is also known as the Jacobian matrix.</li> <li>\\(\\det(A)\\) denotes the determinant of a square matrix \\(A\\).</li> </ul> <p>For any invertible matrix \\(A\\), \\(\\det(A^{-1}) = \\det(A)^{-1}\\), so for \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\) we have</p> \\[p_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right|^{-1}\\] <p>If \\(\\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right| = 1\\), then the mapping is volume preserving, which means that the transformed distribution \\(p_X\\) will have the same \"volume\" compared to the original one \\(p_Z\\).</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#normalizing-flow-models-deep-dive","title":"Normalizing Flow Models deep dive","text":"<p>Let us consider a directed, latent-variable model over observed variables \\(X\\) and latent variables \\(Z\\). In a normalizing flow model, the mapping between \\(Z\\) and \\(X\\), given by \\(f_\\theta: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\), is deterministic and invertible such that \\(X = f_\\theta(Z)\\) and \\(Z = f^{-1}_\\theta(X)\\).</p> <p>Using change of variables, the marginal likelihood \\(p(x)\\) is given by</p> \\[p_X(\\mathbf{x}; \\theta) = p_Z(f^{-1}_\\theta(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}_\\theta(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>The name \"normalizing flow\" can be interpreted as the following:</p> <ul> <li> <p>\"Normalizing\" means that the change of variables gives a normalized density after applying an invertible transformation. When we transform a random variable through an invertible function, the resulting density automatically integrates to 1 (is normalized) because the change of variables formula preserves the total probability mass. This is different from other methods where we might need to explicitly normalize or approximate the density.</p> </li> <li> <p>\"Flow\" means that the invertible transformations can be composed with each other to create more complex invertible transformations. If we have two invertible functions \\(f_1\\) and \\(f_2\\), then their composition \\(f_2 \\circ f_1\\) is also invertible. This allows us to build complex transformations by chaining simpler ones, creating a \"flow\" of transformations.</p> </li> </ul> <p>Different from autoregressive models and variational autoencoders, deep normalizing flow models require specific architectural structures:</p> <ol> <li> <p>The input and output dimensions must be the same - This is necessary for the transformation to be invertible. If the dimensions don't match, we can't uniquely map back and forth between the spaces.</p> </li> <li> <p>The transformation must be invertible - This is fundamental to the change of variables formula and allows us to compute both the forward transformation (for sampling) and the inverse transformation (for density evaluation).</p> </li> <li> <p>Computing the determinant of the Jacobian needs to be efficient (and differentiable) - The change of variables formula requires computing the determinant of the Jacobian matrix. For high-dimensional spaces, this can be computationally expensive, so we need architectures that make this computation tractable.</p> </li> </ol> <p>Next, we introduce several popular forms of flow models that satisfy these properties.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#planar-flow","title":"Planar Flow","text":"<p>The Planar Flow introduces the following invertible transformation:</p> \\[\\mathbf{x} = f_\\theta(\\mathbf{z}) = \\mathbf{z} + \\mathbf{u}h(\\mathbf{w}^\\top\\mathbf{z} + b)\\] <p>where \\(\\mathbf{u}, \\mathbf{w}, b\\) are parameters.</p> <p>The absolute value of the determinant of the Jacobian is given by:</p> \\[\\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right| = |1 + h'(\\mathbf{w}^\\top\\mathbf{z} + b)\\mathbf{u}^\\top\\mathbf{w}|\\] <p>However, \\(\\mathbf{u}, \\mathbf{w}, b, h(\\cdot)\\) need to be restricted in order to be invertible. For example, \\(h = \\tanh\\) and \\(h'(\\mathbf{w}^\\top\\mathbf{z} + b)\\mathbf{u}^\\top\\mathbf{w} \\geq -1\\). Note that while \\(f_\\theta(\\mathbf{z})\\) is invertible, computing \\(f^{-1}_\\theta(\\mathbf{z})\\) could be difficult analytically. The following models address this problem, where both \\(f_\\theta\\) and \\(f^{-1}_\\theta\\) have simple analytical forms.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#nice-and-realnvp","title":"NICE and RealNVP","text":"<p>The Nonlinear Independent Components Estimation (NICE) model and Real Non-Volume Preserving (RealNVP) model compose two kinds of invertible transformations: additive coupling layers and rescaling layers. The coupling layer in NICE partitions a variable \\(\\mathbf{z}\\) into two disjoint subsets, say \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\). Then it applies the following transformation:</p> <p>Forward mapping \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\):</p> <ul> <li>\\(\\mathbf{x}_1 = \\mathbf{z}_1\\), which is an identity mapping.</li> <li>\\(\\mathbf{x}_2 = \\mathbf{z}_2 + m_\\theta(\\mathbf{z}_1)\\), where \\(m_\\theta\\) is a neural network.</li> </ul> <p>Inverse mapping \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\):</p> <ul> <li>\\(\\mathbf{z}_1 = \\mathbf{x}_1\\), which is an identity mapping.</li> <li>\\(\\mathbf{z}_2 = \\mathbf{x}_2 - m_\\theta(\\mathbf{x}_1)\\), which is the inverse of the forward transformation.</li> </ul> <p>Therefore, the Jacobian of the forward mapping is lower triangular, whose determinant is simply the product of the elements on the diagonal, which is 1. Therefore, this defines a volume preserving transformation. RealNVP adds scaling factors to the transformation:</p> \\[\\mathbf{x}_2 = \\exp(s_\\theta(\\mathbf{z}_1)) \\odot \\mathbf{z}_2 + m_\\theta(\\mathbf{z}_1)\\] <p>where \\(\\odot\\) denotes elementwise product. This results in a non-volume preserving transformation.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#autoregressive-flow-models","title":"Autoregressive Flow Models","text":"<p>Some autoregressive models can also be interpreted as flow models. For a Gaussian autoregressive model, one receives some Gaussian noise for each dimension of \\(\\mathbf{x}\\), which can be treated as the latent variables \\(\\mathbf{z}\\). Such transformations are also invertible, meaning that given \\(\\mathbf{x}\\) and the model parameters, we can obtain \\(\\mathbf{z}\\) exactly.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#masked-autoregressive-flow-maf","title":"Masked Autoregressive Flow (MAF)","text":"<p>Masked Autoregressive Flow (MAF) uses this interpretation, where the forward mapping is an autoregressive model. However, sampling is sequential and slow, in \\(O(n)\\) time where \\(n\\) is the dimension of the samples.</p> <p>MAF Architecture and Mathematical Formulation:</p> <p>The MAF is comprised of Masked Autoencoder for Distribution Estimation (MADE) blocks, which has a special masking scheme at each layer such that the autoregressive property is preserved. In particular, we consider a Gaussian autoregressive model:</p> \\[p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i | \\mathbf{x}_{&lt;i})\\] <p>such that the conditional Gaussians \\(p(x_i | \\mathbf{x}_{&lt;i}) = \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\) are parameterized by neural networks \\(\\mu_i = f_{\\mu_i}(\\mathbf{x}_{&lt;i})\\) and \\(\\alpha_i = f_{\\alpha_i}(\\mathbf{x}_{&lt;i})\\). Note that \\(\\alpha_i\\) denotes the log standard deviation of the Gaussian \\(p(x_i | \\mathbf{x}_{&lt;i})\\).</p> <p>As seen in the change of variables formula, a normalizing flow uses a series of deterministic and invertible mappings \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(\\mathbf{x} = f(\\mathbf{z})\\) and \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\) to transform a simple prior distribution \\(p_z\\) (e.g. isotropic Gaussian) into a more expressive one. In particular, a normalizing flow which composes \\(k\\) invertible transformations \\(\\{f_j\\}_{j=1}^k\\) such that \\(\\mathbf{x} = f_k \\circ f_{k-1} \\circ \\cdots \\circ f_1(\\mathbf{z}_0)\\) takes advantage of the change-of-variables property:</p> \\[\\log p(\\mathbf{x}) = \\log p_z(f^{-1}(\\mathbf{x})) + \\sum_{j=1}^k \\log \\left|\\det\\left(\\frac{\\partial f_j^{-1}(\\mathbf{x}_j)}{\\partial \\mathbf{x}_j}\\right)\\right|\\] <p>In MAF, the forward mapping is: \\(x_i = \\mu_i + z_i \\cdot \\exp(\\alpha_i)\\), and the inverse mapping is: \\(z_i = (x_i - \\mu_i)/\\exp(\\alpha_i)\\). The log of the absolute value of the determinant of the Jacobian is:</p> \\[\\log \\left|\\det\\left(\\frac{\\partial f^{-1}}{\\partial \\mathbf{x}}\\right)\\right| = -\\sum_{i=1}^n \\alpha_i\\] <p>where \\(\\mu_i\\) and \\(\\alpha_i\\) are as defined above.</p> <p>Connection between \\(p(\\mathbf{x})\\) and \\(\\log p(\\mathbf{x})\\) formulations:</p> <p>The two formulations are equivalent but serve different purposes:</p> <ol> <li>\\(p(\\mathbf{x})\\) formulation (autoregressive view):</li> </ol> \\[p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i | \\mathbf{x}_{&lt;i}) = \\prod_{i=1}^n \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\] <ol> <li>\\(\\log p(\\mathbf{x})\\) formulation (flow view):</li> </ol> \\[\\log p(\\mathbf{x}) = \\log p_z(f^{-1}(\\mathbf{x})) + \\sum_{j=1}^k \\log \\left|\\det\\left(\\frac{\\partial f_j^{-1}(\\mathbf{x}_j)}{\\partial \\mathbf{x}_j}\\right)\\right|\\] <p>How they relate:</p> <p>Taking the logarithm of the autoregressive formulation:</p> \\[\\log p(\\mathbf{x}) = \\sum_{i=1}^n \\log p(x_i | \\mathbf{x}_{&lt;i}) = \\sum_{i=1}^n \\log \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\] <p>For a Gaussian distribution \\(\\mathcal{N}(x | \\mu, \\sigma^2)\\), we have:</p> \\[\\log \\mathcal{N}(x | \\mu, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\] <p>Substituting \\(\\sigma = \\exp(\\alpha_i)\\) and using the inverse mapping \\(z_i = (x_i - \\mu_i)/\\exp(\\alpha_i)\\):</p> \\[\\log p(\\mathbf{x}) = \\sum_{i=1}^n \\left[-\\frac{1}{2}\\log(2\\pi) - \\alpha_i - \\frac{z_i^2}{2}\\right] = \\sum_{i=1}^n \\log \\mathcal{N}(z_i | 0, 1) - \\sum_{i=1}^n \\alpha_i\\] <p>This shows that the autoregressive formulation (using conditional Gaussians) is equivalent to the flow formulation (using change of variables with a standard normal prior and the Jacobian determinant term \\(-\\sum_{i=1}^n \\alpha_i\\)).</p> <p>Key insight: The \\(\\alpha_i\\) terms serve dual purposes - they parameterize the conditional standard deviations in the autoregressive view, and they contribute to the Jacobian determinant in the flow view.</p> <p>What are \\(\\mu_1\\) and \\(\\alpha_1\\) in MAF?</p> <p>In MAF, for the first dimension (\\(i=1\\)):</p> <ul> <li> <p>\\(\\mu_1\\): This is the mean of the first conditional distribution \\(p(x_1)\\). Since \\(x_1\\) has no previous dimensions to condition on (\\(\\mathbf{x}_{&lt;1}\\) is empty), \\(\\mu_1\\) is typically a learned constant parameter or computed from a bias term in the neural network.</p> </li> <li> <p>\\(\\alpha_1\\): This is the log standard deviation of the first conditional distribution \\(p(x_1)\\). The actual standard deviation is \\(\\exp(\\alpha_1)\\), and \\(\\alpha_1\\) is also typically a learned constant parameter.</p> </li> </ul> <p>This makes sense because the first dimension has no autoregressive dependencies - it's the starting point of the autoregressive chain.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#made-blocks","title":"MADE Blocks","text":"<p>MADE (Masked Autoencoder for Distribution Estimation) is a key architectural component that enables efficient autoregressive modeling. MADE uses a special masking scheme to ensure that the autoregressive property is preserved while allowing for efficient parallel computation of all conditional parameters.</p> <p>How MADE Works:</p> <ol> <li> <p>Masking Scheme: Each layer in the neural network has a mask that ensures each output unit only depends on a subset of input units, maintaining the autoregressive ordering.</p> </li> <li> <p>Autoregressive Property: For dimension \\(i\\), the network can only access inputs \\(x_j\\) where \\(j &lt; i\\), ensuring that \\(p(x_i | \\mathbf{x}_{&lt;i})\\) only depends on previous dimensions.</p> </li> <li> <p>Parallel Parameter Computation: Despite the autoregressive constraints, MADE can compute all \\(\\mu_i\\) and \\(\\alpha_i\\) parameters in parallel during training, making it much more efficient than sequential autoregressive models.</p> </li> </ol> <p>Mathematical Implementation:</p> <p>The masking is implemented by multiplying the weight matrices with binary masks:</p> \\[W_{masked} = W \\odot M\\] <p>where \\(M\\) is a binary mask matrix that enforces the autoregressive dependencies. The mask ensures that: - Output \\(i\\) can only depend on inputs \\(j &lt; i\\) - This creates a lower triangular dependency structure</p> <p>Connection to MAF: MAF uses MADE blocks as its core building blocks, allowing it to efficiently compute all the conditional parameters \\(\\mu_i\\) and \\(\\alpha_i\\) while maintaining the autoregressive structure required for the flow transformation.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#detailed-maf-implementation-analysis","title":"Detailed MAF Implementation Analysis","text":"<p>Let's analyze a complete MAF implementation that demonstrates the concepts discussed above:</p> <p>Core Components:</p> <ol> <li>MaskedLinear: Implements the masking mechanism for autoregressive dependencies</li> <li>PermuteLayer: Reorders dimensions between flow layers</li> <li>MADE: Single MADE block with forward and inverse transformations</li> <li>MAF: Complete model with multiple MADE blocks</li> </ol> <p>1. MaskedLinear Layer:</p> <pre><code>class MaskedLinear(nn.Linear):\n    def __init__(self, input_size, output_size, mask):\n        super().__init__(input_size, output_size)\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, x):\n        return F.linear(x, self.mask * self.weight, self.bias)\n</code></pre> <p>Key Features: - Masking: The mask is a binary matrix that enforces autoregressive dependencies - Element-wise Multiplication: <code>self.mask * self.weight</code> zeros out forbidden connections - Autoregressive Property: Ensures output \\(i\\) only depends on inputs \\(j &lt; i\\)</p> <p>2. PermuteLayer:</p> <pre><code>class PermuteLayer(nn.Module):\n    def __init__(self, num_inputs):\n        super().__init__()\n        self.perm = np.array(np.arange(0, num_inputs)[::-1])\n\n    def forward(self, inputs):\n        return inputs[:, self.perm], torch.zeros(inputs.size(0), 1, device=inputs.device)\n\n    def inverse(self, inputs):\n        return inputs[:, self.perm], torch.zeros(inputs.size(0), 1, device=inputs.device)\n</code></pre> <p>Purpose: - Dimension Reordering: Reverses the order of dimensions between flow layers - Expressiveness: Allows different autoregressive orderings across layers - Jacobian: Since it's just a permutation, the Jacobian determinant is 1 (log_det = 0)</p> <p>3. MADE Block Implementation:</p> <p>Forward Method (z \u2192 x): <pre><code>def forward(self, z):\n    x = torch.zeros_like(z)\n    log_det = None\n    for i in range(self.input_size):\n        out = self.net(x)  # MADE network with masking\n        mean, alpha = out.chunk(2, dim=1)  # Split into mean and log_std\n        x[:, i] = mean[:, i] + z[:, i] * torch.exp(alpha[:, i])  # Transform\n        if log_det is None:\n            log_det = alpha[:, i].unsqueeze(1)\n        else:\n            log_det = torch.cat((log_det, alpha[:, i].unsqueeze(1)), dim=1)\n    log_det = -torch.sum(log_det, dim=1)  # Negative sum for change of variables\n    return x, log_det\n</code></pre></p> <p>Key Implementation Details: - Sequential Processing: Each dimension is processed one by one - Autoregressive Access: The MADE network can only access previously computed \\(x\\) values - Transformation: \\(x_i = \\mu_i + z_i \\cdot \\exp(\\alpha_i)\\) - Log Determinant: Accumulates \\(\\alpha_i\\) values and takes negative sum</p> <p>Inverse Method (x \u2192 z): <pre><code>def inverse(self, x):\n    out = self.net(x)  # MADE network with masking\n    mean, alpha = out.chunk(2, dim=1)  # Split into mean and log_std\n    z = (x - mean) * torch.exp(-alpha)  # Inverse transform\n    log_det = -torch.sum(alpha, dim=1)  # Negative sum for change of variables\n    return z, log_det\n</code></pre></p> <p>Key Implementation Details: - Parallel Processing: All dimensions can be processed simultaneously - Autoregressive Masking: The masking ensures proper dependencies - Inverse Transformation: \\(z_i = (x_i - \\mu_i) / \\exp(\\alpha_i)\\) - Log Determinant: Same formula as forward, but computed in parallel</p> <p>4. Complete MAF Model:</p> <p>Architecture: <pre><code>def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n    nf_blocks = []\n    for i in range(self.n_flows):\n        nf_blocks.append(MADE(self.input_size, hidden_size, n_hidden))\n        nf_blocks.append(PermuteLayer(self.input_size))\n    self.nf = nn.Sequential(*nf_blocks)\n</code></pre></p> <p>Structure: <pre><code>Input \u2192 MADE\u2081 \u2192 Permute\u2081 \u2192 MADE\u2082 \u2192 Permute\u2082 \u2192 ... \u2192 MADE\u2096 \u2192 Permute\u2096 \u2192 Output\n</code></pre></p> <p>Log Probability Computation: <pre><code>def log_probs(self, x):\n    log_det_list = []\n    for flow in self.nf:\n        x, log_det = flow.inverse(x)  # Transform x \u2192 z\n        log_det_list.append(log_det)\n\n    sum_log_det = torch.stack(log_det_list, dim=1).sum(dim=1)\n    z = x  # Final z after all transformations\n    p_z = self.base_dist.log_prob(z).sum(-1)  # Prior log probability\n    log_prob = (p_z + sum_log_det).mean()  # Change of variables formula\n    return log_prob\n</code></pre></p> <p>Sampling Process: <pre><code>def sample(self, device, n):\n    x_sample = torch.randn(n, self.input_size).to(device)  # Sample from prior\n    for flow in self.nf[::-1]:  # Reverse order for sampling\n        x_sample, log_det = flow.forward(x_sample)  # Transform z \u2192 x\n    return x_sample.cpu().data.numpy()\n</code></pre></p> <p>Understanding the Flow Methods:</p> <ol> <li> <p>During Training (likelihood computation): <pre><code># We have x, want to compute log p(x)\nfor flow in self.nf:  # Forward order\n    x, log_det = flow.inverse(x)  # x \u2192 z (inverse of this flow)\n</code></pre></p> </li> <li> <p>During Sampling: <pre><code># We have z, want to get x\nfor flow in self.nf[::-1]:  # Reverse order\n    x_sample, log_det = flow.forward(x_sample)  # z \u2192 x (forward of this flow)\n</code></pre></p> </li> </ol> <p>In other words:</p> <ul> <li>Each flow's <code>forward()</code> method: Transforms \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\) for that specific flow</li> <li>Each flow's <code>inverse()</code> method: Transforms \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\) for that specific flow</li> <li>During training: We use <code>inverse()</code> to go from data space to latent space</li> <li>During sampling: We use <code>forward()</code> to go from latent space to data space</li> </ul> <p>Mathematical Perspective: Let \\(f_i\\) denote the forward transformation of the \\(i\\)-th flow (from \\(\\mathbf{z}\\) to \\(\\mathbf{x}\\)), and \\(f_i^{-1}\\) denote its inverse transformation (from \\(\\mathbf{x}\\) to \\(\\mathbf{z}\\)).</p> <ul> <li>Training: \\(f_k^{-1} \\circ f_{k-1}^{-1} \\circ \\cdots \\circ f_1^{-1}(\\mathbf{x}) = \\mathbf{z}\\) (using <code>inverse()</code> methods)</li> <li>Sampling: \\(f_1 \\circ f_2 \\circ \\cdots \\circ f_k(\\mathbf{z}) = \\mathbf{x}\\) (using <code>forward()</code> methods)</li> </ul> <p>What is \\(k\\)?</p> <p>The parameter \\(k\\) represents the total number of flow layers in the MAF model. In the implementation, this corresponds to <code>n_flows</code> in the MAF constructor.</p> <p>In the MAF Architecture: <pre><code>def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n    # n_flows = k (total number of flow layers)\n    for i in range(self.n_flows):  # i goes from 0 to k-1\n        nf_blocks.append(MADE(self.input_size, hidden_size, n_hidden))\n        nf_blocks.append(PermuteLayer(self.input_size))\n</code></pre></p> <p>Flow Composition Structure: <pre><code>Input \u2192 MADE\u2081 \u2192 Permute\u2081 \u2192 MADE\u2082 \u2192 Permute\u2082 \u2192 ... \u2192 MADE\u2096 \u2192 Permute\u2096 \u2192 Output\n</code></pre></p> <p>Where: - \\(f_1\\): First MADE block (MADE\u2081) - \\(f_2\\): Second MADE block (MADE\u2082) - ... - \\(f_k\\): Last MADE block (MADE\u2096)</p> <p>Example with \\(k = 3\\): - Training: \\(f_3^{-1} \\circ f_2^{-1} \\circ f_1^{-1}(\\mathbf{x}) = \\mathbf{z}\\) - Sampling: \\(f_1 \\circ f_2 \\circ f_3(\\mathbf{z}) = \\mathbf{x}\\)</p> <p>Key Insight: The <code>forward()</code> method of each flow is designed to be the inverse transformation for the overall model's training direction. This is why we use <code>forward()</code> during sampling in reverse order.</p> <p>This implementation demonstrates how the theoretical concepts of MAF translate into practical code, showing the interplay between autoregressive structure, masking, and flow transformations.</p>"},{"location":"ai/deep_generative_models/normalizing_flow_models/#inverse-autoregressive-flow-iaf","title":"Inverse Autoregressive Flow (IAF)","text":"<p>To address the sampling problem (sequential) in MAF, the Inverse Autoregressive Flow (IAF) simply inverts the generating process. In this case, the sampling (generation), is still parallelized. However, computing the likelihood of new data points is slow.</p> <p>Forward mapping from \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\) (parallel):</p> <ol> <li> <p>Sample \\(z_i \\sim \\mathcal{N}(0,1)\\) for \\(i = 1, \\ldots, n\\)</p> </li> <li> <p>Compute all \\(\\mu_i, \\alpha_i\\) (can be done in parallel)</p> </li> <li> <p>Let \\(x_1 = \\exp(\\alpha_1)z_1 + \\mu_1\\)</p> </li> <li> <p>Let \\(x_2 = \\exp(\\alpha_2)z_2 + \\mu_2\\)</p> </li> <li> <p>\\(\\ldots\\)</p> </li> </ol> <p>Inverse mapping from \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\) (sequential):</p> <ol> <li> <p>Let \\(z_1 = (x_1 - \\mu_1)/\\exp(\\alpha_1)\\)</p> </li> <li> <p>Compute \\(\\mu_2(z_1), \\alpha_2(z_1)\\)</p> </li> <li> <p>Let \\(z_2 = (x_2 - \\mu_2)/\\exp(\\alpha_2)\\)</p> </li> <li> <p>Compute \\(\\mu_3(z_1,z_2), \\alpha_3(z_1,z_2)\\)</p> </li> <li> <p>\\(\\ldots\\)</p> </li> </ol> <p>Key insight: Fast to sample from, slow to evaluate likelihoods of data points (train).</p> <p>Efficient Likelihood for Generated Points: However, for generated points the likelihood can be computed efficiently (since the noise are already obtained). When we generate samples using IAF, we start with known noise values \\(\\mathbf{z}\\) and transform them to get \\(\\mathbf{x}\\). Since we already have the noise values, we don't need to perform the expensive sequential inverse mapping to recover them. We can directly compute the likelihood using the change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) - \\sum_{i=1}^n \\alpha_i\\] <p>where we already know all the \\(\\alpha_i\\) values from the forward pass. This is much faster than the \\(O(n)\\) sequential computation required for arbitrary data points.</p> <p>Derivation of the Change of Variables Formula for IAF:</p> <p>Let's derive how we get this formula. Starting with the general change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) + \\log \\left|\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>For IAF, the forward transformation is:</p> \\[x_i = \\exp(\\alpha_i)z_i + \\mu_i\\] <p>The inverse transformation is:</p> \\[z_i = \\frac{x_i - \\mu_i}{\\exp(\\alpha_i)}\\] <p>The Jacobian matrix \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\) is diagonal because each \\(z_i\\) only depends on \\(x_i\\):</p> \\[\\frac{\\partial z_i}{\\partial x_j} = \\begin{cases}  \\frac{1}{\\exp(\\alpha_i)} &amp; \\text{if } i = j \\\\ 0 &amp; \\text{if } i \\neq j \\end{cases}\\] <p>Therefore, the determinant is the product of the diagonal elements:</p> \\[\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right) = \\prod_{i=1}^n \\frac{1}{\\exp(\\alpha_i)} = \\exp\\left(-\\sum_{i=1}^n \\alpha_i\\right)\\] <p>Taking the absolute value and logarithm:</p> \\[\\log \\left|\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)\\right| = \\log \\exp\\left(-\\sum_{i=1}^n \\alpha_i\\right) = -\\sum_{i=1}^n \\alpha_i\\] <p>Substituting back into the change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) - \\sum_{i=1}^n \\alpha_i\\] <p>This derivation shows why the likelihood computation is efficient for generated samples - we already have all the \\(\\alpha_i\\) values from the forward pass, so we just need to sum them up.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/","title":"Recap at this point","text":""},{"location":"ai/deep_generative_models/recap_at_this_point/#the-generative-modeling-framework","title":"The Generative Modeling Framework","text":"<p>We have some data from an unknown probability distribution, that we denote \\(p_{data}\\). We have a model family \\(\\mathcal{M}\\) which is a set of probability distributions. We denote some kind of notion of similarity between \\(p_{data}\\) and the distributions in \\(\\mathcal{M}\\). We try to find the probability distribution \\(p_\\theta\\) that is the closest to \\(p_{data}\\) in this notion of similarity.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#model-families-weve-explored","title":"Model Families We've Explored","text":"<p>We have seen different ways of constructing probability distributions in \\(\\mathcal{M}\\):</p> <ol> <li>Autoregressive Models: \\(p_\\theta(x) = \\prod_{i=1}^N p_\\theta(x_i|x_{&lt;i})\\)</li> <li>Variational Autoencoders (VAEs): \\(p_\\theta(x) = \\int p_\\theta(x,z)dz\\)</li> <li>Normalizing Flow Models: \\(p_\\theta(x) = p_z(f^{-1}_\\theta(x)) \\left|\\det\\left(\\frac{\\partial f^{-1}_\\theta(x)}{\\partial x}\\right)\\right|\\)</li> </ol>"},{"location":"ai/deep_generative_models/recap_at_this_point/#the-likelihood-based-training-paradigm","title":"The Likelihood-Based Training Paradigm","text":"<p>The key thing is we always try to assign some probability assigned by the model to a data point. All the above families are trained by minimizing KL divergence \\(D_{KL}(p_{data} || p_\\theta)\\), or equivalently maximizing likelihoods (or approximations). In these techniques, the machinery involves setting up models such that we can evaluate likelihoods (or approximations) pretty efficiently.</p> <p>Mathematical Foundation:</p> \\[\\arg\\min_\\theta D_{KL}(p_{data} || p_\\theta) = \\arg\\max_\\theta \\mathbb{E}_{x \\sim p_{data}}[\\log p_\\theta(x)]\\] <p>This equivalence shows that minimizing KL divergence is equivalent to maximizing the expected log-likelihood of the data.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#alternative-similarity-measures","title":"Alternative Similarity Measures","text":"<p>However, the training objective of maximizing likelihoods is not the only way to measure similarity between \\(p_{data}\\) and \\(p_\\theta\\). There are other ways to measure the notion of similarity:</p> <ol> <li>Wasserstein Distance: Measures the minimum \"cost\" of transporting mass from one distribution to another</li> <li>Maximum Mean Discrepancy (MMD): Compares distributions using kernel methods</li> <li>Adversarial Training: Uses a discriminator to distinguish between real and generated samples</li> <li>Energy-Based Models: Learn an energy function that assigns low energy to real data and high energy to fake data</li> </ol>"},{"location":"ai/deep_generative_models/recap_at_this_point/#the-likelihood-vs-sample-quality-dilemma","title":"The Likelihood vs. Sample Quality Dilemma","text":"<p>The Problem: It is possible that models with high likelihood could be bad at sample generation and vice versa. This creates a fundamental tension in generative modeling.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#why-this-disconnect-occurs","title":"Why This Disconnect Occurs","text":"<p>1. Likelihood Measures Average Performance:</p> \\[\\mathbb{E}_{x \\sim p_{data}}[\\log p_\\theta(x)] = \\int p_{data}(x) \\log p_\\theta(x) dx\\] <p>This measures how well the model assigns probability to the average data point, not necessarily how well it captures the fine-grained structure needed for high-quality generation.</p> <p>2. Sample Quality Requires Fine-Grained Structure: High-quality generation requires the model to capture: - Sharp boundaries between different modes - Fine details and textures - Proper spatial relationships - Realistic variations within modes</p> <p>3. Different Optimization Objectives: - Likelihood: Optimizes for probability assignment over the entire distribution - Sample Quality: Requires optimization of perceptual and structural properties</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#implications-for-model-design","title":"Implications for Model Design","text":"<p>The Training Objective Trade-off: Although training objective of maximizing likelihoods could be a good one, it might not be the best one if the objective is to generate the best samples.</p> <p>This suggests that it might be useful to disentangle likelihoods and sample quality.</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#alternative-training-paradigms","title":"Alternative Training Paradigms","text":"<p>1. Adversarial Training (GANs): - Objective: Direct optimization of sample quality through adversarial training - Advantage: Can generate very high-quality samples - Disadvantage: No explicit likelihood computation, training instability</p> <p>2. Hybrid Approaches: - VAE-GAN: Combines likelihood-based training with adversarial training - Flow-GAN: Combines normalizing flows with adversarial training - Objective: Balance between likelihood and sample quality</p> <p>3. Perceptual Losses: - Objective: Use pre-trained networks to measure perceptual similarity - Advantage: Better alignment with human perception - Example: LPIPS (Learned Perceptual Image Patch Similarity)</p> <p>4. Multi-Objective Training: - Objective: Combine multiple loss functions - Example: \\(\\mathcal{L} = \\mathcal{L}_{likelihood} + \\lambda \\mathcal{L}_{perceptual} + \\mu \\mathcal{L}_{adversarial}\\)</p>"},{"location":"ai/deep_generative_models/recap_at_this_point/#conclusion","title":"Conclusion","text":"<p>The tension between likelihood and sample quality is a fundamental challenge in generative modeling. While likelihood-based training provides a principled framework, it may not always lead to the best sample quality. This motivates the exploration of alternative training paradigms and evaluation metrics that better align with the ultimate goal of generating high-quality, diverse samples.</p> <p>The key insight is that generative modeling is not just about fitting a distribution to data, but about creating models that can generate samples that are both high-quality and diverse. This requires careful consideration of both the training objective and the evaluation metrics used to assess model performance.</p>"},{"location":"ai/deep_generative_models/score_based_diffusion_models/","title":"Score Based Diffusion Models","text":""},{"location":"ai/deep_generative_models/score_based_diffusion_models/#quick-recap-score-based-models","title":"Quick Recap: Score Based Models","text":"<p>From our exploration of score-based generative modeling, we learned several key concepts:</p> <p>Score Function: The gradient of the log probability density, \\(\\nabla_x \\log p(x)\\), which points \"uphill\" in the probability landscape toward high-density regions.</p> <p>Score Matching: A training objective that learns the score function by minimizing the Fisher divergence between the learned and true score functions.</p> <p>Score Matching Objective: The original score matching objective is:</p> \\[\\mathcal{L}(\\theta) = \\mathbb{E}_{x \\sim p_{data}(x)} \\left[ \\frac{1}{2} \\| s_\\theta(x) \\|_2^2 + \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] <p>where \\(\\text{tr}(\\nabla_x s_\\theta(x))\\) is the trace of the Jacobian of the score function, which is computationally expensive to evaluate.</p> <p>Denoising Score Matching (DSM): A practical variant that trains the score function to predict the direction from noisy to clean data, avoiding the need to compute the true score function.</p> <p>DSM Objective: The denoising score matching objective is:</p> \\[\\mathcal{L}(\\theta) = \\mathbb{E}_{y \\sim p_{data}(y)} \\mathbb{E}_{x \\sim \\mathcal{N}(x; y, \\sigma^2 I)} \\left[ \\frac{1}{2} \\left\\| s_\\theta(x) - \\frac{y - x}{\\sigma^2} \\right\\|_2^2 \\right]\\] <p>where \\(s_\\theta(x)\\) learns to predict the score function of the noise-perturbed distribution, and \\(\\frac{y - x}{\\sigma^2}\\) is the target score function that points from noisy sample \\(x\\) toward clean data \\(y\\).</p> <p>Langevin Dynamics: A continuous-time stochastic process that uses the score function to guide sampling:</p> \\[dx_t = \\nabla_x \\log p(x_t) dt + \\sqrt{2} dW_t\\] <p>Discretized Form: For practical implementation:</p> \\[x_{t+1} = x_t + \\frac{\\epsilon}{2} \\cdot s_\\theta(x_t) + \\sqrt{2\\epsilon} \\cdot \\eta_t\\] <p>Mode Collapse: Standard Langevin dynamics struggles with multi-modal distributions and low-density regions.</p> <p>Annealed Langevin Dynamics: Addresses this by using multiple noise scales \\(\\sigma_1 &lt; \\sigma_2 &lt; \\ldots &lt; \\sigma_L\\), creating a sequence of increasingly noisy distributions that are easier to sample from.</p> <p>Stochastic Differential Equations (SDEs): General framework for continuous-time stochastic processes:</p> \\[dx = f(x, t)dt + g(t)dw\\] <p>Reverse SDE: Any SDE has a corresponding reverse process for sampling:</p> \\[dx = [f(x, t) - g^2(t)\\nabla_x \\log p_t(x)]dt + g(t)d\\bar{w}\\] <p>Time-Dependent Score Models: Neural networks that learn \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\) for continuous-time processes.</p> <p>Key insights:</p> <ol> <li>Score functions act as denoisers: They point from noisy to clean data</li> <li>Multiple noise scales help: Annealing from high to low noise improves sampling</li> <li>Continuous-time generalizes discrete: SDEs provide a unified framework</li> <li>Reverse processes enable generation: The reverse SDE naturally incorporates the score function for sampling</li> </ol>"},{"location":"ai/deep_generative_models/score_based_diffusion_models/#diffusion-models-as-score-based-models-hierarchical-vaes","title":"Diffusion Models as Score Based Models &amp; Hierarchical VAEs","text":"<p>Iterative Denoising perspective: In annealed Langevin dynamics with multiple noise scales, the sampling process can be viewed as iterative denoising. Starting from high noise levels and gradually reducing noise, each step uses the score function to denoise the sample, progressively refining it from a noisy state toward the clean data distribution.</p> <p>Training perspective: The inverse process involves iteratively adding Gaussian noise to clean data during training. By corrupting data with increasing levels of noise, the model learns to predict the score function at each noise level, enabling it to reverse the corruption process during sampling.</p> <p></p> <p>VAE Perspective: This entire framework can be viewed as a VAE where:</p> <ul> <li> <p>Encoder process: The forward process that converts clean data to noise through iterative corruption</p> </li> <li> <p>Decoder process: The reverse process that generates samples by iteratively denoising from noise</p> </li> </ul> <p>Noise Perturbation process: Each \\(x_t\\) represents a noise-perturbed density that is obtained by adding Gaussian noise to \\(x_{t-1}\\). This creates a Markov chain where each step adds a small amount of noise to the previous state.</p> <p>We can write the forward process as a conditional distribution:</p> \\[q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)\\] <p>where \\(\\beta_t\\) is the noise schedule that determines how much noise is added at each step.</p> <p>The joint distribution of the entire forward process is:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\] <p>This factorization follows from the chain rule of probability and the Markov property of the forward process:</p> <p>Chain Rule: For any joint distribution, we can write:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = q(x_1 | x_0) \\cdot q(x_2 | x_0, x_1) \\cdot q(x_3 | x_0, x_1, x_2) \\cdots q(x_T | x_0, x_1, \\ldots, x_{T-1})\\] <p>Markov Property: In the forward process, each \\(x_t\\) depends only on \\(x_{t-1}\\), not on earlier states:</p> \\[q(x_t | x_0, x_1, \\ldots, x_{t-1}) = q(x_t | x_{t-1})\\] <p>Substituting the Markov property into the chain rule:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = q(x_1 | x_0) \\cdot q(x_2 | x_1) \\cdot q(x_3 | x_2) \\cdots q(x_T | x_{T-1})\\] <p>This can be written compactly as:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\] <p>This represents the probability of the entire noise corruption sequence, where each step depends only on the previous step (Markov property).</p> <p>Comparison with VAEs: In a typical VAE, you would take \\(x_0\\) and map it via a neural network to obtain some mean and standard deviation to parameterize the distribution of the latent variable. Here, we obtain the distribution of the latent variables through the predefined noise corruption procedure we defined above, rather than learning it with a neural network.</p> <p>Multistep transitions: A key advantage of this process is that we can compute transitions between any two time steps efficiently. For example, we can directly compute \\(q(x_t | x_0)\\) without going through all intermediate steps.</p> <p>Starting from \\(x_0\\), we can write:</p> \\[x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}\\] <p>where \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\epsilon_{t-1} \\sim \\mathcal{N}(0, I)\\).</p> <p>Recursively substituting:</p> \\[x_t = \\sqrt{\\alpha_t} (\\sqrt{\\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_{t-1}} \\epsilon_{t-2}) + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}\\] <p>Continuing this recursion, we get:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>where \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\) and \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>Result: The multistep transition is:</p> \\[q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\] <p>This allows us to sample \\(x_t\\) directly from \\(x_0\\) in a single step, making training much more efficient.</p> <p>Diffusion analogy: We can think of this as a diffusion process. This is like a diffuser where given an initial state, we keep adding noise at every step. This is analogous to heat diffusion in a space- just as heat spreads out and becomes more uniform over time, our data distribution becomes increasingly noisy and uniform Gaussian as we add more noise at each step.</p> <p>The process gradually \"diffuses\" the structured information in the data into random noise, creating a smooth transition from the complex data distribution to a simple Gaussian noise distribution.</p> <p></p> <p>The ideal sampling process would be:</p> <ol> <li>Sample \\(x_T\\) from \\(\\pi(x_T)\\). Start with pure noise from the prior distribution</li> <li>Iteratively sample from the true denoising distribution \\(q(x_{t-1} | x_t)\\).</li> </ol> <p>This would generate samples by following the exact reverse of the forward diffusion process, gradually denoising from pure noise back to clean data.</p> <p>The challenge however, is that we don't know the true denoising distributions \\(q(x_{t-1} | x_t)\\). While the forward process \\(q(x_t | x_{t-1})\\) is predefined and tractable, the reverse process is not.</p> <p>However, we can learn an approximation \\(p_\\theta(x_{t-1} | x_t)\\) which is a Gaussian distribution with learned parameters:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>where \\(\\mu_\\theta(x_t, t)\\) is a neural network that learns the mean of the denoising distribution, and \\(\\sigma_t^2 I\\) is the fixed variance schedule.</p> <p>This is similar to a VAE decoder:</p> <p>VAE Decoder:</p> \\[p_\\theta(x | z) = \\mathcal{N}(x; \\mu_\\theta(z), \\sigma_\\theta^2(z) I)\\] <p>Diffusion reverse process:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>The diffusion decoder \\(p_\\theta(x_{t-1} | x_t)\\) is trying to learn to approximate the true denoising distributions \\(q(x_{t-1} | x_t)\\).</p> <p>The joint distribution of the learned reverse process is:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>Let's derive the joint distribution of the learned reverse process step by step.</p> <p>In the general case of \\(n\\) random variables \\(X_1, X_2, \\ldots, X_n\\), the values of an arbitrary subset of variables can be known and one can ask for the joint probability of all other variables. For example, if the values of \\(X_{k+1}, X_{k+2}, \\ldots, X_n\\) are known, the probability for \\(X_1, X_2, \\ldots, X_k\\) given these known values is:</p> \\[P(X_1, X_2, \\ldots, X_k|X_{k+1}, X_{k+2}, \\ldots, X_n) = \\frac{P(X_1, X_2, \\ldots, X_n)}{P(X_{k+1}, X_{k+2}, \\ldots, X_n)}\\] <p>This is the fundamental definition of conditional probability for multiple random variables.</p> <p>For any three events \\(A\\), \\(B\\), and \\(C\\), the joint conditional probability is defined as:</p> \\[P(A, B|C) = \\frac{P(A, B, C)}{P(C)}\\] <p>We can write the joint probability \\(P(A, B, C)\\) using the chain rule:</p> \\[P(A, B, C) = P(A|B, C) \\cdot P(B, C)\\] <p>Substituting this into our definition:</p> \\[P(A, B|C) = \\frac{P(A|B, C) \\cdot P(B, C)}{P(C)}\\] <p>We can write \\(P(B, C)\\) as:</p> \\[P(B, C) = P(B|C) \\cdot P(C)\\] \\[P(A, B|C) = \\frac{P(A|B, C) \\cdot P(B|C) \\cdot P(C)}{P(C)}\\] <p>The \\(P(C)\\) terms cancel out:</p> \\[P(A, B|C) = P(A|B, C) \\cdot P(B|C)\\] <p>The learned reverse process consists of a sequence of conditional distributions:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>where \\(\\mu_\\theta(x_t, t)\\) is a neural network that learns the mean of the denoising distribution.</p> <p>For the joint distribution \\(p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T)\\), we can apply the chain rule:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = p_\\theta(x_0 | x_1, \\ldots, x_T) \\cdot p_\\theta(x_1 | x_2, \\ldots, x_T) \\cdots p_\\theta(x_{T-1} | x_T)\\] <p>In the reverse process, we assume that each \\(x_{t-1}\\) depends only on \\(x_t\\), not on future states. This is the reverse Markov property:</p> \\[p_\\theta(x_{t-1} | x_t, x_{t+1}, \\ldots, x_T) = p_\\theta(x_{t-1} | x_t)\\] <p>Substituting the reverse Markov property into the chain rule:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = p_\\theta(x_0 | x_1) \\cdot p_\\theta(x_1 | x_2) \\cdots p_\\theta(x_{T-1} | x_T)\\] <p>This can be written compactly as:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>To get the complete joint distribution, we need to include the prior distribution over \\(x_T\\):</p> \\[p_\\theta(x_0, x_1, \\ldots, x_T) = p(x_T) \\cdot p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T)\\] \\[p_\\theta(x_0, x_1, \\ldots, x_T) = p(x_T) \\cdot \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>A crucial aspect of the diffusion process is choosing the values of \\(\\bar{\\alpha}_t\\) such that after many steps, we are left with pure noise. This ensures that the forward process converges to a simple, known distribution.</p> <p>Common choices for the noise schedule include:</p> <ol> <li>Linear Schedule: \\(\\beta_t = \\frac{t}{T} \\cdot \\beta_{\\text{max}}\\)</li> <li>Cosine Schedule: \\(\\beta_t = \\cos\\left(\\frac{t}{T} \\cdot \\frac{\\pi}{2}\\right)\\)</li> <li>Quadratic Schedule: \\(\\beta_t = \\left(\\frac{t}{T}\\right)^2 \\cdot \\beta_{\\text{max}}\\)</li> </ol> <p>Example: For a linear schedule with \\(\\beta_{\\text{max}} = 0.02\\) and \\(T = 1000\\), we get \\(\\beta_1 = 0.00002\\), \\(\\beta_{500} = 0.01\\) and \\(\\beta_{1000} = 0.02\\).</p> <p>Once we have trained the diffusion model and learned the reverse process \\(p_\\theta(x_{t-1} | x_t)\\), we can generate new samples by running the reverse process. Here's how sampling works. </p> <p>Sample \\(x_T\\) from the prior distribution \\(x_T \\sim \\mathcal{N}(x_T; 0, I)\\).</p> <p>For \\(t = T, T-1, \\ldots, 1\\), sample from the learned reverse process \\(x_{t-1} \\sim p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\).</p> <p>After \\(T\\) steps, we obtain \\(x_0\\), which is our generated sample.</p> <p>This entire diffusion framework can be viewed as a Variational Autoencoder (VAE) with a crucial difference: the encoder is fixed and predefined, while only the decoder is learned.</p> <p>Standard VAE Structure:</p> <ul> <li> <p>Encoder: \\(q_\\phi(z | x) = \\mathcal{N}(z; \\mu_\\phi(x), \\sigma_\\phi^2(x) I)\\)</p> </li> <li> <p>Decoder: \\(p_\\theta(x | z) = \\mathcal{N}(x; \\mu_\\theta(z), \\sigma_\\theta^2(z) I)\\)</p> </li> <li> <p>Prior: \\(p(z) = \\mathcal{N}(z; 0, I)\\)</p> </li> </ul> <p>Vanilla VAE ELBO (Non-KL form):</p> \\[ELBO_{\\text{VAE}} = \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right]\\] <p>Hierarchical VAE Structure (z\u2082 \u2192 z\u2081 \u2192 x):</p> <ul> <li> <p>Encoder: \\(q_\\phi(z_1, z_2 | x) = q_\\phi(z_1 | x) \\cdot q_\\phi(z_2 | z_1)\\)</p> </li> <li> <p>\\(q_\\phi(z_1 | x) = \\mathcal{N}(z_1; \\mu_\\phi(x), \\sigma_\\phi^2(x) I)\\)</p> </li> <li> <p>\\(q_\\phi(z_2 | z_1) = \\mathcal{N}(z_2; \\mu_\\phi(z_1), \\sigma_\\phi^2(z_1) I)\\)</p> </li> <li> <p>Decoder: \\(p_\\theta(x, z_1 | z_2) = p_\\theta(x | z_1) \\cdot p_\\theta(z_1 | z_2)\\)</p> </li> <li> <p>\\(p_\\theta(x | z_1) = \\mathcal{N}(x; \\mu_\\theta(z_1), \\sigma_\\theta^2(z_1) I)\\)</p> </li> <li> <p>\\(p_\\theta(z_1 | z_2) = \\mathcal{N}(z_1; \\mu_\\theta(z_2), \\sigma_\\theta^2(z_2) I)\\)</p> </li> <li> <p>Prior: \\(p(z_2) = \\mathcal{N}(z_2; 0, I)\\)</p> </li> </ul> <p>Hierarchical VAE ELBO (Non-KL form):</p> \\[ELBO_{\\text{HVAE}} = \\mathbb{E}_{q_\\phi(z_1,z_2|x)} \\left[ \\log \\frac{p_\\theta(x, z_1, z_2)}{q_\\phi(z_1, z_2|x)} \\right]\\] <p>Following the hierarchical VAE formulation, we can write the ELBO for diffusion models. In diffusion models, we have a sequence of latent variables \\(x_1, x_2, \\ldots, x_T\\) where \\(x_T\\) is the most abstract (pure noise) and \\(x_0\\) is the data.</p> <p>Diffusion Model Structure (x_T \u2192 x_{T-1} \u2192 ... \u2192 x_1 \u2192 x_0):</p> <ul> <li> <p>Encoder: \\(q(x_1, x_2, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\) - Fixed noise corruption process</p> </li> <li> <p>Decoder: \\(p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\) - Learned denoising process</p> </li> <li> <p>Prior: \\(p(x_T) = \\mathcal{N}(x_T; 0, I)\\)</p> </li> </ul> <p>Diffusion Model ELBO (Non-KL form):</p> \\[ELBO_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ \\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>The Negative Evidence Lower BOund (NELBO) is the negative of the ELBO, which is what we actually minimize during training:</p> \\[\\mathcal{L}_{\\text{Diff}} = -\\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ \\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>This can be rewritten as:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>The decoder learns to predict the mean function \\(\\mu_\\theta(x_t, t)\\) for the reverse process. Let's derive how this function is parameterized.</p> <p>The true reverse process \\(q(x_{t-1} | x_t, x_0)\\) can be derived using Bayes' theorem. For Gaussian distributions, this gives us:</p> \\[q(x_{t-1} | x_t, x_0) = \\mathcal{N}(x_{t-1}; \\mu_t(x_t, x_0), \\sigma_t^2 I)\\] <p>where it can be shown that:</p> \\[\\mu_t(x_t, x_0) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\right)\\] <p>and:</p> \\[\\sigma_t^2 = \\frac{\\beta_t(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\\] <p>The learned reverse process is:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>Since we want the learned process to approximate the true reverse process, we parameterize \\(\\mu_\\theta(x_t, t)\\) to match the form of \\(\\mu_t(x_t, x_0)\\):</p> \\[\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\] <p>where \\(\\epsilon_\\theta(x_t, t)\\) is a neural network that predicts the noise \\(\\epsilon\\) given \\(x_t\\) and \\(t\\).</p>"},{"location":"ai/deep_generative_models/score_based_diffusion_models/#rewriting-the-elbo-for-diffusion-models","title":"Rewriting the ELBO for Diffusion Models","text":"<p>Let's rewrite the diffusion model ELBO and transform it to resemble denoising score matching.</p> <p>Starting with the diffusion model ELBO:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>We can expand this as:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log p_\\theta(x_0, x_1, \\ldots, x_T) + \\log q(x_1, \\ldots, x_T|x_0) \\right]\\] <p>The learned joint distribution is:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_T) = p(x_T) \\cdot \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>The true joint distribution is:</p> \\[q(x_1, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\] <p>Substituting these:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log p(x_T) - \\sum_{t=1}^T \\log p_\\theta(x_{t-1} | x_t) + \\sum_{t=1}^T \\log q(x_t | x_{t-1}) \\right]\\] <p>For Gaussian distributions, the log-likelihood is:</p> \\[\\log \\mathcal{N}(x; \\mu, \\sigma^2 I) = -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 + C\\] <p>where \\(C\\) is a constant that doesn't depend on the parameters.</p> <p>For the learned reverse process:</p> \\[\\log p_\\theta(x_{t-1} | x_t) = -\\frac{1}{2\\sigma_t^2} \\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 + C\\] <p>For the true forward process:</p> \\[\\log q(x_t | x_{t-1}) = -\\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2 + C\\] <p>Using the definition of \\(\\mu_\\theta(x_t, t)\\):</p> \\[\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\] <p>The squared error term becomes:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\left\\|x_{t-1} - \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\right\\|^2\\] <p>From the forward process, we know:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>And from the multistep transition:</p> \\[x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\epsilon_{t-1}\\] <p>Substituting these into the squared error:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\left\\|\\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\epsilon_{t-1} - \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\right\\|^2\\] <p>Using the relationship \\(\\bar{\\alpha}_t = \\bar{\\alpha}_{t-1} \\cdot \\alpha_t\\), we can simplify:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\left\\|\\frac{\\beta_t}{\\sqrt{\\alpha_t(1 - \\bar{\\alpha}_t)}} (\\epsilon - \\epsilon_\\theta(x_t, t))\\right\\|^2\\] <p>This simplifies to:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\frac{\\beta_t^2}{\\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\] <p>Substituting back into the ELBO:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log p(x_T) + \\sum_{t=1}^T \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2 + \\sum_{t=1}^T \\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2 \\right]\\] <p>The key term in the ELBO is:</p> \\[\\sum_{t=1}^T \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\] <p>Let's define:</p> \\[\\lambda_t = \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)}\\] <p>From the forward process, we know:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>The expectation \\(\\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)}\\) can be rewritten as:</p> \\[\\mathbb{E}_{x_0 \\sim p_{data}(x_0)} \\mathbb{E}_{\\epsilon_1, \\ldots, \\epsilon_T \\sim \\mathcal{N}(0, I)}\\] <p>Since each \\(x_t\\) is generated as:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t\\] <p>The ELBO can be simplified to:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\lambda_t \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|^2 \\right] + \\text{constant terms}\\] <p>where:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t\\] <p>This can be written more compactly as:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\lambda_t \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|^2 \\right] + \\text{constant terms}\\] <p>where \\(x_0 \\sim p_{data}(x_0)\\) (clean data), \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) (noise), \\(t \\sim \\text{Uniform}(1, T)\\) (timestep)</p> <p>The diffusion model ELBO is equivalent to:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\lambda_t \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|^2 \\right]\\] <p>where \\(\\lambda_t = \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)}\\) is the weighting factor for each timestep.</p> <p>Note In the original ELBO, we had two summation-terms:</p> <ol> <li> <p>\\(\\sum_{t=1}^T \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\) (noise prediction term)</p> </li> <li> <p>\\(\\sum_{t=1}^T \\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2\\) (forward process term)</p> </li> </ol> <p>The second summation-term \\(\\sum_{t=1}^T \\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2\\) represents the log-likelihood of the forward process \\(q(x_t | x_{t-1})\\). This summation-term does not depend on the model parameters \\(\\theta\\) because the forward process is fixed and predefined. It only depends on the noise schedule \\(\\beta_t\\) and the data. When we take the gradient with respect to \\(\\theta\\) to optimize the model, this summation-term vanishes.</p>"},{"location":"ai/deep_generative_models/score_based_diffusion_models/#sampling","title":"Sampling","text":"<p>While the ELBO loss \\(\\mathcal{L}_{\\text{Diff}}\\) and the score-based objective are roughly equivalent in terms of what they learn, the sampling procedures differ between these two approaches.</p> <p>In a Score-Based Model (SBM), sampling is performed using Langevin dynamics. In a Diffusion Model (VAE form), sampling follows the learned reverse process.</p> <p>The connection between the two approaches comes from the relationship between the score function and the noise predictor:</p> <p>Score function: \\(s_\\theta(x_t, t) = \\nabla_x \\log p_t(x_t)\\)</p> <p>Noise predictor: \\(\\epsilon_\\theta(x_t, t)\\) predicts the noise added during the forward process</p> <p>Relationship: For Gaussian noise, the score function is proportional to the negative noise.</p> <p>From the forward process, we have:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>The distribution of \\(x_t\\) given \\(x_0\\) is:</p> \\[q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\] <p>The score function is the gradient of the log probability density:</p> \\[s(x_t, t) = \\nabla_{x_t} \\log q(x_t | x_0)\\] <p>For the Gaussian distribution \\(q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\):</p> \\[\\log q(x_t | x_0) = -\\frac{1}{2(1 - \\bar{\\alpha}_t)} \\|x_t - \\sqrt{\\bar{\\alpha}_t} x_0\\|^2 + C\\] <p>where \\(C\\) is a constant that doesn't depend on \\(x_t\\).</p> <p>Taking the gradient with respect to \\(x_t\\):</p> \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} (x_t - \\sqrt{\\bar{\\alpha}_t} x_0)\\] <p>From the forward process, we can express \\(x_0\\) in terms of \\(x_t\\) and \\(\\epsilon\\):</p> \\[x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon}{\\sqrt{\\bar{\\alpha}_t}}\\] \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} \\left(x_t - \\sqrt{\\bar{\\alpha}_t} \\cdot \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon}{\\sqrt{\\bar{\\alpha}_t}}\\right)\\] <p>Simplifying the expression:</p> \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} \\left(x_t - (x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon)\\right)\\] \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} \\cdot \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{\\epsilon}{\\sqrt{1 - \\bar{\\alpha}_t}}\\] <p>Therefore, the score function is:</p> \\[s(x_t, t) = \\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{\\epsilon}{\\sqrt{1 - \\bar{\\alpha}_t}}\\] <p>In practice, we learn:</p> <ul> <li> <p>Score function: \\(s_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log q(x_t | x_0)\\)</p> </li> <li> <p>Noise predictor: \\(\\epsilon_\\theta(x_t, t) \\approx \\epsilon\\)</p> </li> </ul> <p>Therefore, \\(s_\\theta(x_t, t) \\approx -\\frac{\\epsilon_\\theta(x_t, t)}{\\sqrt{1 - \\bar{\\alpha}_t}}\\).</p> <p>Both sampling methods work because they're learning the same underlying structure:</p> <ol> <li>Score-based: Learns the gradient of the log-density at each noise level</li> <li>Diffusion: Learns the noise that was added during the forward process</li> </ol> <p>Since the score function and noise predictor are mathematically related, both approaches can generate high-quality samples, but they use different sampling algorithms.</p>"},{"location":"ai/deep_generative_models/score_based_generative_modeling/","title":"Score Based Generative Modeling","text":""},{"location":"ai/deep_generative_models/score_based_generative_modeling/#langevin-dynamics-sampling","title":"Langevin Dynamics Sampling","text":"<p>Langevin dynamics is a powerful MCMC method that uses gradient information to efficiently sample from complex probability distributions. For score-based models, it provides a natural way to generate samples by following the learned score field.</p> <p>Mathematical Foundation</p> <p>Langevin dynamics is based on the Langevin equation, a stochastic differential equation that describes the motion of particles in a potential field:</p> \\[dx_t = \\nabla_x \\log p(x_t) dt + \\sqrt{2} dW_t\\] <p>where:</p> <ul> <li> <p>\\(x_t\\) is the particle position at time \\(t\\)</p> </li> <li> <p>\\(\\nabla_x \\log p(x_t)\\) is the score function (gradient of log probability)</p> </li> <li> <p>\\(W_t\\) is a Wiener process (Brownian motion)</p> </li> <li> <p>The first term is the drift term (gradient guidance)</p> </li> <li> <p>The second term is the diffusion term (random exploration)</p> </li> </ul> <p>Discretized Langevin Dynamics</p> <p>For practical implementation, we discretize the continuous-time equation:</p> \\[x_{t+1} = x_t + \\epsilon \\cdot \\nabla_x \\log p(x_t) + \\sqrt{2\\epsilon} \\cdot \\eta_t\\] <p>where:</p> <ul> <li> <p>\\(\\epsilon\\) is the step size (time discretization)</p> </li> <li> <p>\\(\\eta_t \\sim \\mathcal{N}(0, I)\\) is Gaussian noise</p> </li> <li> <p>\\(t\\) indexes the discrete time steps</p> </li> </ul> <p>Score-Based Langevin Sampling</p> <p>For our trained score function \\(s_\\theta(x) \\approx \\nabla_x \\log p_{data}(x)\\), the sampling algorithm becomes:</p> <p>Algorithm: Score-Based Langevin Sampling</p> <ol> <li> <p>Initialize: \\(x_0 \\sim \\mathcal{N}(0, I)\\) (random noise)</p> </li> <li> <p>Iterate: For \\(t = 0, 1, 2, \\ldots, T-1\\):</p> </li> <li> <p>Compute score: \\(s_t = s_\\theta(x_t)\\)</p> </li> <li> <p>Add gradient step: \\(x_{t+1} = x_t + \\frac{\\epsilon}{2} \\cdot s_t + \\sqrt{2\\epsilon} \\cdot \\eta_t\\)</p> </li> <li> <p>Where \\(\\eta_t \\sim \\mathcal{N}(0, I)\\)</p> </li> <li> <p>Return: \\(x_T\\) as the generated sample</p> </li> </ol> <p>Intuition Behind Langevin Dynamics</p> <p>The Drift Term (\\(\\frac{\\epsilon}{2} \\cdot s_\\theta(x_t)\\)):</p> <ul> <li> <p>Pushes the sample toward high-probability regions</p> </li> <li> <p>The score function points \"uphill\" in the probability landscape</p> </li> <li> <p>Larger step sizes \\(\\epsilon\\) lead to more aggressive movement</p> </li> <li> <p>The factor of \\(\\frac{1}{2}\\) comes from proper discretization of the continuous Langevin equation</p> </li> </ul> <p>The Diffusion Term (\\(\\sqrt{2\\epsilon} \\cdot \\eta_t\\)):</p> <ul> <li> <p>Adds random exploration to avoid getting stuck in local modes</p> </li> <li> <p>Balances the deterministic gradient guidance</p> </li> <li> <p>Ensures the chain can escape local optima and explore the full distribution</p> </li> </ul> <p>Balance Between Drift and Diffusion:</p> <ul> <li> <p>Small \\(\\epsilon\\): More exploration, slower convergence, better mixing</p> </li> <li> <p>Large \\(\\epsilon\\): Faster convergence, but may miss modes or become unstable</p> </li> <li> <p>Optimal \\(\\epsilon\\): Depends on the data distribution and model architecture</p> </li> </ul> <p>Convergence Guarantees</p> <p>Under mild conditions on the target distribution and score function, Langevin dynamics provides strong theoretical guarantees:</p> <p>Asymptotic Convergence:</p> <p>If \\(\\epsilon \\to 0\\) and \\(T \\to \\infty\\), we are guaranteed that \\(x_T \\sim p_{data}(x)\\).</p> <p>Mathematical Interpretation:</p> <ul> <li>\\(\\epsilon \\to 0\\): The discretization becomes arbitrarily fine, approaching the continuous Langevin equation</li> <li>\\(T \\to \\infty\\): The Markov chain runs for an infinite number of steps, allowing it to reach the stationary distribution</li> <li>\\(x_T \\sim p_{data}(x)\\): The final sample is distributed according to the target data distribution</li> </ul> <p>Challenge in Low Density Regions:</p> <p>One significant limitation of Langevin dynamics is its poor performance in low density regions of the target distribution:</p> <ul> <li>Weak Score Signals: In regions where \\(p_{data}(x) \\approx 0\\), the score function \\(\\nabla_x \\log p_{data}(x)\\) becomes very small or noisy</li> <li>Mode Collapse Risk: The algorithm may fail to explore all modes (mode is a region where the probability density is high, i.e., data points are concentrated) of a multi-modal distribution</li> <li>Slow convergence: Langevin Dynamics converges very slowly. Might not even converge if we have zero probability somewhere.</li> </ul> <p>This challenge motivates the development of annealed Langevin dynamics and other advanced sampling techniques that can better handle complex, multi-modal distributions.</p>"},{"location":"ai/deep_generative_models/score_based_generative_modeling/#annealed-langevin-dynamics","title":"Annealed Langevin Dynamics","text":"<p>Mathematical Formulation</p> <p>We define a sequence of annealed distributions indexed by noise level \\(\\sigma_t\\):</p> \\[p_t(x) = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_t^2 I) dy\\] <p>where each \\(p_t(x)\\) is a smoothed version of the original data distribution.</p> <p>This equation is derived from the convolution of the data distribution with the noise distribution. Here's the step-by-step reasoning:</p> <p>If \\(Y \\sim p_{data}(y)\\) and \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_i^2 I)\\), then the noisy sample is \\(X = Y + \\epsilon\\).</p> <p>The joint distribution of \\((Y, X)\\) is:</p> \\[p(y, x) = p_{data}(y) \\cdot \\mathcal{N}(x; y, \\sigma_i^2 I)\\] <p>The joint distribution is derived using the chain rule of probability:</p> \\[p(y, x) = p(y) \\cdot p(x | y)\\] <p>where:</p> <ul> <li> <p>\\(p(y) = p_{data}(y)\\) is the marginal distribution of the clean data</p> </li> <li> <p>\\(p(x | y)\\) is the conditional distribution of the noisy sample given the clean data</p> </li> </ul> <p>Since \\(X = Y + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_i^2 I)\\), the conditional distribution is:</p> \\[p(x | y) = \\mathcal{N}(x; y, \\sigma_i^2 I)\\] <p>This is because adding a constant (\\(y\\)) to a Gaussian random variable shifts the mean but preserves the variance. Therefore:</p> \\[p(y, x) = p_{data}(y) \\cdot \\mathcal{N}(x; y, \\sigma_i^2 I)\\] <p>To get the distribution of \\(X\\) alone, we marginalize over \\(Y\\):</p> \\[p_{\\sigma_i}(x) = \\int p(y, x) dy = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_i^2 I) dy\\] <p>We're using the law of total probability (also called marginalization). When we have a joint distribution \\(p(y, x)\\), to find the marginal distribution of \\(x\\) alone, we integrate out the other variable:</p> \\[p_{\\sigma_i}(x) = \\int p(y, x) dy\\] <p>This is because:</p> <ul> <li> <p>The joint distribution \\(p(y, x)\\) gives us the probability of both \\(y\\) AND \\(x\\) occurring together</p> </li> <li> <p>To find the probability of just \\(x\\) (regardless of what \\(y\\) is), we sum over all possible values of \\(y\\)</p> </li> <li> <p>In continuous probability, \"summing\" becomes integration</p> </li> </ul> <p>Intuition: We're asking \"What's the probability of observing a noisy sample \\(x\\)?\" The answer is the sum of probabilities over all possible clean samples \\(y\\) that could have generated this noisy sample.</p> <p>Final Form: The noise-perturbed distribution is:</p> \\[p_{\\sigma_i}(x) = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_i^2 I) dy\\] <p>We use multiple scales of noise perturbations simultaneously. Suppose we always perturb the data with isotropic Gaussian noise, and let there be a total of \\(L\\) increasing standard deviations \\(\\sigma_1 &lt; \\sigma_2 &lt; \\ldots &lt; \\sigma_L\\). We first perturb the data distribution \\(p_{data}(y)\\) with each of the Gaussian noise \\(\\mathcal{N}(0, \\sigma_i^2 I)\\) to obtain a noise-perturbed distribution (the final form we derived above):</p> \\[p_{\\sigma_i}(x) = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_i^2 I) dy\\] <p>Note that we can easily draw samples from \\(p_{\\sigma_i}(x)\\) by sampling \\(y \\sim p_{data}(y)\\) and computing \\(x = y + \\sigma_i \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>We estimate the score function of each noise-perturbed distribution, \\(\\nabla_x \\log p_{\\sigma_i}(x)\\), by training a Denoising Score Matching Model (when parameterized with a neural network) with score matching, such that \\(s_\\theta(x, \\sigma_i) \\approx \\nabla_x \\log p_{\\sigma_i}(x)\\) for all \\(i\\). The training objective for \\(s_\\theta\\) is a weighted sum of Fisher divergences for all noise scales. In particular, we use the objective below:</p> \\[\\mathcal{L}(\\theta) = \\frac{1}{L} \\sum_{i=1}^L \\lambda(\\sigma_i) \\mathbb{E}_{p_{\\sigma_i}(x)} \\left[ \\| s_\\theta(x, \\sigma_i) - \\nabla_x \\log p_{\\sigma_i}(x) \\|_2^2 \\right]\\] <p>where \\(\\lambda(\\sigma_i)\\) is a positive weighting function, often chosen to be \\(\\lambda(\\sigma_i) = \\sigma_i^2\\). The objective \\(\\mathcal{L}(\\theta)\\) can be optimized with score matching, exactly as in optimizing the naive score-based model.</p> <p>Denoising Score Matching Format:</p> <p>We can rewrite the objective in Denoising Score Matching model format:</p> \\[\\mathcal{L}(\\theta) = \\frac{1}{L} \\sum_{i=1}^L \\lambda(\\sigma_i) \\mathbb{E}_{y \\sim p_{data}(y), x \\sim \\mathcal{N}(x; y, \\sigma_i^2 I)} \\left[ \\left\\| s_\\theta(x, \\sigma_i) - \\frac{y - x}{\\sigma_i^2} \\right\\|_2^2 \\right]\\] <p>Note: The noise scales \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_L\\) are typically chosen to be in a geometric progression, meaning \\(\\sigma_{i+1} = \\alpha \\cdot \\sigma_i\\) for some constant \\(\\alpha &lt; 1\\). This ensures that the noise levels decrease exponentially, providing a smooth annealing schedule from high noise to low noise.</p> <p>Perturbing an image with multiple scales of Gaussian noise: </p> <p>After training our score-based model \\(s_\\theta(x, \\sigma_i)\\), we can produce samples from it by running Langevin dynamics for \\(\\sigma_L, \\sigma_{L-1}, \\ldots, \\sigma_1\\) in sequence. This method is called Annealed Langevin dynamics, since the noise scale decreases (anneals) gradually over time.</p> <p>We can start from unstructured noise, modify images according to the scores, and generate nice samples: </p>"},{"location":"ai/deep_generative_models/score_based_generative_modeling/#generative-modeling-via-stochastic-differential-equations-sdes","title":"Generative Modeling via Stochastic Differential Equations (SDEs)","text":"<p>When the number of noise scales approaches infinity, we essentially perturb the data distribution with continuously growing levels of noise. In this case, the noise perturbation procedure is a continuous-time stochastic process, as demonstrated below.</p> <p></p> <p>How can we represent a stochastic process in a concise way? Many stochastic processes are solutions of stochastic differential equations (SDEs). In general, an SDE possesses the following form:</p> \\[dx = f(x, t)dt + g(t)dw\\] <p>where \\(f(x, t)\\) is a vector-valued function called the drift coefficient, \\(g(t)\\) is a real-valued function called the diffusion coefficient, \\(w\\) denotes a standard Brownian motion, and \\(dw\\) can be viewed as infinitesimal white noise. The solution of a stochastic differential equation is a continuous collection of random variables \\(\\{x(t)\\}_{t \\in [0, T]}\\).</p> <p>These random variables trace stochastic trajectories as the time index \\(t\\) grows from the start time \\(0\\) to the end time \\(T\\). Let \\(p_t(x)\\) denote the (marginal) probability density function of \\(x(t)\\). Here \\(p_t(x)\\) is analogous to \\(p_{\\sigma_i}(x)\\) when we had a finite number of noise scales, and \\(t\\) is analogous to \\(\\sigma_i\\). Clearly, \\(p_0(x)\\) is the data distribution since no perturbation is applied to data at \\(t = 0\\). After perturbing \\(p_0(x)\\) with the stochastic process for a sufficiently long time \\(T\\), \\(p_T(x)\\) becomes close to a tractable noise distribution \\(p_T(x) \\approx \\pi(x)\\), called a prior distribution. We note that \\(\\pi(x)\\) is analogous to \\(p_{\\sigma_L}(x)\\) in the case of finite noise scales, which corresponds to applying the largest noise perturbation \\(\\sigma_L\\) to the data.</p> <p>There are numerous ways to add noise perturbations, and the choice of SDEs is not unique. For example, the following SDE</p> \\[dx = e^t dw\\] <p>perturbs data with a Gaussian noise of mean zero and exponentially growing variance. Therefore, the SDE should be viewed as part of the model, much like \\(\\sigma_i\\).</p> <p>Recall that with a finite number of noise scales, we can generate samples by reversing the perturbation process with annealed Langevin dynamics, i.e., sequentially sampling from each noise-perturbed distribution using Langevin dynamics. For infinite noise scales, we can analogously reverse the perturbation process for sample generation by using the reverse SDE.</p> <p>Importantly, any SDE has a corresponding reverse SDE, whose closed form is given by</p> \\[dx = [f(x, t) - g^2(t)\\nabla_x \\log p_t(x)]dt + g(t)d\\bar{w}\\] <p>Here \\(dt\\) represents a negative infinitesimal time step, since the SDE needs to be solved backwards in time (from \\(T\\) to \\(0\\)). In order to compute the reverse SDE, we need to estimate \\(\\nabla_x \\log p_t(x)\\), which is exactly the score function of \\(p_t(x)\\).</p> <p></p> <p>Note: Langevin dynamics is a specific instance of the reverse SDE where \\(f(x, t) = 0\\) (no forward drift) and \\(g(t) = \\sqrt{2}\\) (constant diffusion). This shows how Langevin dynamics naturally emerges as a special case of the reverse SDE when we want to sample from a target distribution.</p>"},{"location":"ai/deep_generative_models/score_based_generative_modeling/#estimating-the-reverse-sde-with-score-based-models-and-score-matching","title":"Estimating the reverse SDE with score-based models and score matching","text":"<p>Solving the reverse SDE requires us to know the terminal distribution \\(p_T(x)\\), and the score function \\(\\nabla_x \\log p_t(x)\\). By design, the former is close to the prior distribution \\(\\pi(x)\\) which is fully tractable. In order to estimate \\(\\nabla_x \\log p_t(x)\\), we train a Time-Dependent Score-Based Model \\(s_\\theta(x, t)\\), such that \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\). This is analogous to the denoising score matching model \\(s_\\theta(x, \\sigma_i)\\) used for finite noise scales, trained such that \\(s_\\theta(x, \\sigma_i) \\approx \\nabla_x \\log p_{\\sigma_i}(x)\\).</p> <p>Our training objective for \\(s_\\theta(x, t)\\) is a continuous weighted combination of Fisher divergences, given by</p> \\[\\mathcal{L}(\\theta) = \\mathbb{E}_{t \\sim \\mathcal{U}[0, T]} \\mathbb{E}_{x \\sim p_t(x)} \\left[ \\lambda(t) \\| s_\\theta(x, t) - \\nabla_x \\log p_t(x) \\|_2^2 \\right]\\] <p>where \\(\\mathcal{U}[0, T]\\) denotes a uniform distribution over the time interval \\([0, T]\\), and \\(\\lambda(t)\\) is a positive weighting function.</p> <p>As before, our weighted combination of Fisher divergences can be efficiently optimized with score matching methods, such as denoising score matching and sliced score matching. Once our score-based model \\(s_\\theta(x, t)\\) is trained to optimality, we can plug it into the expression of the reverse SDE to obtain an estimated reverse SDE.</p> <p>We can start with \\(x(T) \\sim p_T(x)\\), and solve the above reverse SDE to obtain a sample \\(x(0)\\). Let us denote the distribution of \\(x(0)\\) obtained in such way as \\(p_\\theta(x)\\). When the score-based model \\(s_\\theta(x, t)\\) is well-trained, we have \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\), in which case \\(x(0)\\) is an approximate sample from the data distribution \\(p_0(x)\\).</p> <p>By solving the estimated reverse SDE with numerical SDE solvers, we can simulate the reverse stochastic process for sample generation. Perhaps the simplest numerical SDE solver is the Euler-Maruyama method. When applied to our estimated reverse SDE, it discretizes the SDE using finite time steps and small Gaussian noise. Specifically, it chooses a small negative time step \\(\\Delta t\\), initializes \\(x(T) \\sim p_T(x)\\), and iterates the following procedure until \\(t = 0\\):</p> \\[\\Delta x = [f(x, t) - g^2(t)s_\\theta(x, t)]\\Delta t + g(t)\\sqrt{|\\Delta t|}\\eta_t\\] <p>where \\(\\eta_t \\sim \\mathcal{N}(0, I)\\).</p> <p>Then update: \\(x \\leftarrow x + \\Delta x\\) and \\(t \\leftarrow t + \\Delta t\\).</p> <p>Note: The function \\(f(x, t)\\) in the Euler-Maruyama equation is the drift coefficient from the original forward SDE. Common examples include:</p> <ul> <li> <p>\\(f(x, t) = 0\\) (pure diffusion): Used in simple noise perturbation</p> </li> <li> <p>\\(f(x, t) = -\\frac{1}{2}\\beta(t)x\\) (linear drift): Used in variance-preserving diffusion</p> </li> <li> <p>\\(f(x, t) = -x^2\\) (quadratic drift): Creates potential wells</p> </li> <li> <p>\\(f(x, t) = x - x^3\\) (polynomial drift): Creates multiple stable equilibria</p> </li> </ul> <p>Most Common in Practice: For score-based generative modeling, the most commonly used forms are \\(f(x, t) = 0\\) (pure diffusion) and \\(f(x, t) = -\\frac{1}{2}\\beta(t)x\\) (VP diffusion). The choice of \\(f(x, t)\\) determines how the data is perturbed during the forward process.</p> <p>The Euler-Maruyama method is qualitatively similar to Langevin dynamics\u2014 both update \\(x\\) by following score functions perturbed with Gaussian noise.</p>"},{"location":"ai/deep_generative_models/score_based_models/","title":"Score Based Models","text":""},{"location":"ai/deep_generative_models/score_based_models/#score-matching","title":"Score Matching","text":"<p>Energy-Based Model Probability Distribution</p> <p>In Energy-Based Models, the probability distribution is defined as:</p> \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} e^{f_\\theta(x)}\\] <p>where:</p> <ul> <li> <p>\\(f_\\theta(x)\\) is the energy function (neural network)</p> </li> <li> <p>\\(Z(\\theta) = \\int e^{f_\\theta(x)} dx\\) is the partition function (intractable)</p> </li> </ul> <p>Taking the logarithm of the probability distribution:</p> \\[\\log p_\\theta(x) = f_\\theta(x) - \\log Z(\\theta)\\] <p>Notice that the partition function \\(Z(\\theta)\\) appears as a constant term that doesn't depend on \\(x\\).</p> <p>Stein Score Function</p> <p>The Stein score function \\(s_\\theta(x)\\) is defined as the gradient of the log probability with respect to \\(x\\):</p> \\[s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\] <p>For Energy-Based Models, the score function equals the gradient of the energy function:</p> \\[s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = \\nabla_x (f_\\theta(x) - \\log Z(\\theta)) = \\nabla_x f_\\theta(x)\\] <p>The partition function term \\(\\log Z(\\theta)\\) disappears because it doesn't depend on \\(x\\).</p> <p>Score as a Vector Field</p> <p>The score function \\(s_\\theta(x)\\) is a vector field that assigns a vector to each point \\(x\\) in the data space. This vector has both:</p> <ol> <li> <p>Magnitude: How quickly the log probability changes</p> </li> <li> <p>Direction: The direction of steepest increase in log probability</p> </li> </ol> <p>Intuition: The score vector points \"uphill\" in the log probability landscape, indicating the direction where the model assigns higher probability.</p> <p>Example: Gaussian Distribution</p> <p>Consider a Gaussian distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\):</p> \\[p(x) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\right)\\] <p>Log Probability:</p> \\[\\log p(x) = -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu) - \\frac{1}{2}\\log((2\\pi)^d |\\Sigma|)\\] <p>Score Function:</p> \\[s(x) = \\nabla_x \\log p(x) = -\\Sigma^{-1}(x - \\mu)\\] <p>Interpretation:</p> <ul> <li> <p>The score points toward the mean \\(\\mu\\) (direction of higher probability)</p> </li> <li> <p>The magnitude is proportional to the distance from the mean</p> </li> <li> <p>For isotropic Gaussian (\\(\\Sigma = \\sigma^2 I\\)): \\(s(x) = -\\frac{1}{\\sigma^2}(x - \\mu)\\)</p> </li> </ul> <p>This example shows how the score function naturally guides samples toward high-probability regions of the distribution.</p>"},{"location":"ai/deep_generative_models/score_based_models/#score-matching-comparing-distributions-via-vector-fields","title":"Score Matching: Comparing Distributions via Vector Fields","text":"<p>The core idea of score matching is that we want to compare two probability distributions by comparing their respective vector fields of gradients (score functions).</p> <p>The Key Insight:</p> <p>Instead of directly comparing probability densities \\(p_{data}(x)\\) and \\(p_\\theta(x)\\) (which requires computing the intractable partition function), we compare their score functions:</p> <ul> <li>Data Score: \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\)</li> <li>Model Score: \\(s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = \\nabla_x f_\\theta(x)\\)</li> </ul> <p>This measures how different the \"pointing directions\" are at each location \\(x\\).</p> <p>L2 Distance Between Score Functions</p> <p>One way to compare the score functions is to calculate the average L2 distance between the score of \\(p_{data}\\) and \\(p_\\theta\\):</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x) - s_{data}(x)\\|^2 \\right]\\] <p>Note: This loss function is also called the Fisher divergence between \\(p_{data}(x)\\) and \\(p_\\theta(x)\\). The Fisher divergence measures the difference between two probability distributions by comparing their score functions (gradients of log densities) rather than the densities themselves.</p> <p>Understanding the L2 Distance:</p> <p>The L2 norm \\(\\|s_\\theta(x) - s_{data}(x)\\|^2\\) measures the squared Euclidean distance between two vectors:</p> \\[\\|s_\\theta(x) - s_{data}(x)\\|^2 = \\sum_{i=1}^d (s_\\theta(x)_i - s_{data}(x)_i)^2\\] <p>where \\(d\\) is the dimension of the data space.</p> <p>Score matching is a method for training Energy-Based Models by minimizing the Fisher divergence between the data distribution \\(p_{data}(x)\\) and the model distribution \\(p_\\theta(x)\\):</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x) - s_{data}(x)\\|^2 \\right]\\] <p>where \\(s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\) and \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\) are the score functions of the model and data distributions respectively.</p> <p>But how do we figure out \\(\\nabla_x \\log p_{data}(x)\\) given only samples?</p> <p>Score Matching Reformulation (Univariate Case)</p> <p>For the univariate case where \\(x \\in \\mathbb{R}\\), we can rewrite the score matching objective to avoid needing the data score. Let's expand the squared difference:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_\\theta(x) - \\frac{d}{dx} \\log p_{data}(x)\\right)^2 \\right]\\] <p>Expanding the square:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_\\theta(x)\\right)^2 - \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) + \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_{data}(x)\\right)^2 \\right]\\] <p>The key insight is to use integration by parts on the cross term. For any function \\(f(x)\\) and \\(g(x)\\):</p> \\[\\int f(x) \\frac{d}{dx} g(x) dx = f(x)g(x) - \\int \\frac{d}{dx} f(x) \\cdot g(x) dx\\] <p>Setting \\(f(x) = \\frac{d}{dx} \\log p_\\theta(x)\\) and \\(g(x) = p_{data}(x)\\), we get:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\right] = \\int \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\cdot p_{data}(x) dx\\] <p>Using the chain rule: \\(\\frac{d}{dx} \\log p_{data}(x) \\cdot p_{data}(x) = \\frac{d}{dx} p_{data}(x)\\), we get:</p> \\[= \\int \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} p_{data}(x) dx\\] <p>Using integration by parts:</p> \\[= \\left. \\frac{d}{dx} \\log p_\\theta(x) \\cdot p_{data}(x) \\right|_{-\\infty}^{\\infty} - \\int \\frac{d^2}{dx^2} \\log p_\\theta(x) \\cdot p_{data}(x) dx\\] <p>Why does the boundary term vanish?</p> <p>The boundary term \\(\\left. \\frac{d}{dx} \\log p_\\theta(x) \\cdot p_{data}(x) \\right|_{-\\infty}^{\\infty}\\) vanishes under reasonable assumptions:</p> <ol> <li>Data distribution decay: \\(p_{data}(x) \\to 0\\) as \\(|x| \\to \\infty\\) (most real-world distributions have finite support or decay to zero)</li> <li>Model score boundedness: \\(\\frac{d}{dx} \\log p_\\theta(x)\\) grows at most polynomially as \\(|x| \\to \\infty\\)</li> <li>Product decay: The product \\(\\frac{d}{dx} \\log p_\\theta(x) \\cdot p_{data}(x) \\to 0\\) as \\(|x| \\to \\infty\\)</li> </ol> <p>This is a standard assumption in score matching literature and holds for most practical distributions.</p> <p>Assuming the boundary term vanishes (which is reasonable for well-behaved distributions), we get:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\right] = -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d^2}{dx^2} \\log p_\\theta(x) \\right]\\] <p>Substituting back into the original objective:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_\\theta(x)\\right)^2 + \\frac{d^2}{dx^2} \\log p_\\theta(x) \\right] + \\text{constant}\\] <p>where the constant term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}} \\left[ \\left(\\frac{d}{dx} \\log p_{data}(x)\\right)^2 \\right]\\) doesn't depend on \\(\\theta\\) and can be ignored during optimization.</p> <p>Key Insight: This reformulation allows us to train the model using only samples from \\(p_{data}(x)\\) and the derivatives of our model's log probability, without needing access to the data score function.</p> <p>Score Matching Reformulation (Multivariate Case)</p> <p>For the multivariate case where \\(x \\in \\mathbb{R}^d\\), we can extend the univariate derivation. The score matching objective becomes:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x) - \\nabla_x \\log p_{data}(x)\\|^2 \\right]\\] <p>Expanding the squared norm:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x)\\|^2 - \\nabla_x \\log p_\\theta(x)^T \\nabla_x \\log p_{data}(x) + \\frac{1}{2} \\|\\nabla_x \\log p_{data}(x)\\|^2 \\right]\\] <p>The key insight is to use integration by parts on the cross term. For the multivariate case, we need to handle each component separately. Let \\(s_\\theta(x)_i\\) and \\(s_{data}(x)_i\\) denote the \\(i\\)-th component of the respective score functions.</p> <p>For each component \\(i\\), we have:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)_i \\cdot s_{data}(x)_i \\right] = \\int s_\\theta(x)_i \\cdot s_{data}(x)_i \\cdot p_{data}(x) dx\\] <p>Using the chain rule: \\(s_{data}(x)_i \\cdot p_{data}(x) = \\frac{\\partial}{\\partial x_i} p_{data}(x)\\), we get:</p> \\[= \\int s_\\theta(x)_i \\cdot \\frac{\\partial}{\\partial x_i} p_{data}(x) dx\\] <p>Using integration by parts (assuming boundary terms vanish):</p> \\[= -\\int \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\cdot p_{data}(x) dx = -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\right]\\] <p>Why do the boundary terms vanish in the multivariate case?</p> <p>For each component \\(i\\), the boundary term is:</p> \\[\\left. s_\\theta(x)_i \\cdot p_{data}(x) \\right|_{x_i = -\\infty}^{x_i = \\infty}\\] <p>This vanishes under similar assumptions as the univariate case:</p> <ol> <li>Data distribution decay: \\(p_{data}(x) \\to 0\\) as \\(\\|x\\| \\to \\infty\\) in any direction</li> <li>Model score boundedness: Each component \\(s_\\theta(x)_i\\) grows at most polynomially as \\(\\|x\\| \\to \\infty\\)</li> <li>Product decay: The product \\(s_\\theta(x)_i \\cdot p_{data}(x) \\to 0\\) as \\(\\|x\\| \\to \\infty\\) for each component</li> </ol> <p>These assumptions ensure that the boundary terms vanish for all components, allowing us to apply integration by parts component-wise.</p> <p>Summing over all components:</p> \\[\\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)_i \\cdot s_{data}(x)_i \\right] = -\\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\right] = -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] <p>where \\(\\text{tr}(\\nabla_x s_\\theta(x)) = \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i\\) is the trace of the Jacobian matrix of the score function.</p> <p>Substituting back into the original objective:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x)\\|^2 + \\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x)) \\right] + \\text{constant}\\] <p>where the constant term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}} \\left[ \\|\\nabla_x \\log p_{data}(x)\\|^2 \\right]\\) doesn't depend on \\(\\theta\\) and can be ignored during optimization.</p> <p>Key Insight: The multivariate case introduces the trace of the Hessian matrix \\(\\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x))\\).</p>"},{"location":"ai/deep_generative_models/score_based_models/#score-matching-algorithm","title":"Score Matching Algorithm","text":"<p>The score matching algorithm follows these steps:</p> <p>Sample a mini-batch of datapoints: \\(\\{x_1, x_2, \\ldots, x_n\\} \\sim p_{data}(x)\\)</p> <p>Estimate the score matching loss with the empirical mean: </p> \\[\\mathcal{L}_{SM}(\\theta) \\approx \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x_i)\\|^2 + \\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x_i)) \\right]\\] <p>Stochastic gradient descent: Update parameters using gradients of the estimated loss</p> <p>Advantages: * No need to sample from EBM: Unlike other training methods for energy-based models, score matching doesn't require generating samples from the model during training. This avoids the computational expense and potential instability of MCMC sampling. * Direct optimization: The objective directly measures how well the model's score function matches the data distribution's score function. * Theoretically sound: Score matching provides a consistent estimator under mild conditions.</p> <p>Disadvantages: * Computing the Hessian is expensive: The term \\(\\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x))\\) requires computing second derivatives, which scales quadratically with the input dimension and can be computationally prohibitive for large models. * Memory requirements: Storing and computing Hessians for large neural networks requires significant memory. * Numerical instability: Second derivatives can be numerically unstable, especially for deep networks.</p> <p>Computational Complexity: For a model with \\(d\\) input dimensions and \\(m\\) parameters, computing the Hessian trace requires \\(O(d^2 \\cdot m)\\) operations, making it impractical for high-dimensional data like images.</p>"},{"location":"ai/deep_generative_models/score_based_models/#recap-distances-for-training-ebms","title":"Recap: Distances for Training EBMs","text":"<p>When training Energy-Based Models, we need to measure how close our model distribution \\(p_\\theta(x)\\) is to the data distribution \\(p_{data}(x)\\). Here are the main approaches:</p>"},{"location":"ai/deep_generative_models/score_based_models/#contrastive-divergence","title":"Contrastive Divergence","text":"<p>Contrastive divergence measures the difference between the data distribution and the model distribution using KL divergence:</p> \\[\\mathcal{L}_{CD}(\\theta) = D_{KL}(p_{data}(x) \\| p_\\theta(x)) - D_{KL}(p_\\theta(x) \\| p_{data}(x))\\] <p>Key insight: This objective encourages the model to match the data distribution while preventing mode collapse.</p> <p>Challenge: Computing the KL divergence requires sampling from the model distribution \\(p_\\theta(x)\\), which is typically done using MCMC methods like Langevin dynamics or Hamiltonian Monte Carlo.</p>"},{"location":"ai/deep_generative_models/score_based_models/#fisher-divergence-score-matching","title":"Fisher Divergence (Score Matching)","text":"<p>Fisher divergence measures the difference between the score functions (gradients of log densities) of the two distributions:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x) - \\nabla_x \\log p_{data}(x)\\|^2 \\right]\\] <p>Key insight: Instead of comparing probability densities directly, we compare their gradients, which avoids the need to compute the intractable partition function.</p> <p>Advantage: No need to sample from the model during training, making it computationally more efficient than contrastive divergence.</p> <p>Challenge: Requires computing second derivatives (Hessian) of the log probability, which can be expensive for high-dimensional data.</p>"},{"location":"ai/deep_generative_models/score_based_models/#noise-contrastive-estimation","title":"Noise Contrastive Estimation","text":"<p>Learning an EBM by contrasting it with a noise distribution.</p> <p>We have the data distribution \\(p_{data}(x)\\). We have the noise distribution \\(p_n(x)\\) which should be analytically tractable and easy to sample from. We can train a discriminator \\(D(x) \\in [0, 1]\\) to distinguish between data samples and noise samples.</p>"},{"location":"ai/deep_generative_models/score_based_models/#optimal-discriminator","title":"Optimal Discriminator","text":"<p>The optimal discriminator \\(D^*(x)\\) that maximizes this objective is given by:</p> \\[D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\]"},{"location":"ai/deep_generative_models/score_based_models/#parameterizing-the-discriminator-as-an-ebm","title":"Parameterizing the Discriminator as an EBM","text":"<p>Key Insight: Instead of training a separate discriminator, we can parameterize it directly in terms of an Energy-Based Model.</p> <p>Let's define a parameterized version of the discriminator as:</p> \\[D_\\theta(x) = \\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)}\\] <p>where \\(p_\\theta(x) = \\frac{1}{Z(\\theta)} e^{f_\\theta(x)}\\) is our Energy-Based Model.</p> <p>Implicit Learning of the Data Distribution</p> <p>By training the discriminator \\(D_\\theta(x)\\) to distinguish between data samples and noise samples, we are implicitly learning the Energy-Based Model \\(p_\\theta(x)\\) to approximate the true data distribution \\(p_{data}(x)\\).</p> <p>Why This Works:</p> <p>Recall that the optimal discriminator (when trained to perfection) satisfies:</p> \\[D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\] <p>But we've parameterized our discriminator as:</p> \\[D_\\theta(x) = \\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)}\\] <p>The Key Insight: When we train \\(D_\\theta(x)\\) to match the optimal discriminator \\(D^*(x)\\), we're essentially forcing:</p> \\[\\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)} \\approx \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\] <p>This equality holds if and only if \\(p_\\theta(x) \\approx p_{data}(x)\\) (assuming \\(p_n(x) &gt; 0\\) everywhere).</p> <p>Mathematical Justification:</p> <p>If \\(\\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)} = \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\), then:</p> \\[p_\\theta(x) \\cdot (p_{data}(x) + p_n(x)) = p_{data}(x) \\cdot (p_\\theta(x) + p_n(x))\\] \\[p_\\theta(x) \\cdot p_{data}(x) + p_\\theta(x) \\cdot p_n(x) = p_{data}(x) \\cdot p_\\theta(x) + p_{data}(x) \\cdot p_n(x)\\] \\[p_\\theta(x) \\cdot p_n(x) = p_{data}(x) \\cdot p_n(x)\\] <p>Since \\(p_n(x) &gt; 0\\), we can divide both sides to get:</p> \\[p_\\theta(x) = p_{data}(x)\\] <p>Modeling the Partition Function as a Trainable Parameter</p> <p>The EBM Equation:</p> <p>Our Energy-Based Model is defined as:</p> \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} e^{f_\\theta(x)}\\] <p>where \\(f_\\theta(x)\\) is the energy function (neural network) and \\(Z(\\theta) = \\int e^{f_\\theta(x)} dx\\) is the partition function.</p> <p>The Partition Function Constraint Problem:</p> <p>The constraint \\(Z(\\theta) = \\int e^{f_\\theta(x)} dx\\) is computationally intractable to satisfy exactly because:</p> <ol> <li>High-dimensional integration: Computing \\(\\int e^{f_\\theta(x)} dx\\) over high-dimensional spaces is extremely expensive</li> <li>No closed form: For complex energy functions, there's no analytical solution</li> <li>Dynamic updates: The integral changes every time we update the energy function parameters</li> </ol> <p>Solution: Treat Z as a Trainable Parameter</p> <p>Instead of enforcing the constraint, we model \\(Z(\\theta)\\) as an additional trainable parameter \\(Z\\) that is not explicitly constrained to satisfy \\(Z = \\int e^{f_\\theta(x)} dx\\).</p> <p>This gives us the modified EBM:</p> \\[p_{\\theta, Z}(x) = \\frac{e^{f_\\theta(x)}}{Z}\\] <p>Why Z Converges to the Correct Partition Function:</p> <p>As we train \\(p_{\\theta, Z}(x)\\) to approximate \\(p_{data}(x)\\), the parameter \\(Z\\) automatically converges to the correct partition function value.</p> <p>Mathematical Justification:</p> <p>When training converges, we have \\(p_{\\theta, Z}(x) \\approx p_{data}(x)\\). This means:</p> \\[\\frac{e^{f_\\theta(x)}}{Z} \\approx p_{data}(x)\\] <p>A direct argument comes from the fact that \\(p_{\\theta, Z}(x)\\) must approximate \\(p_{data}(x)\\), which must integrate to 1:</p> \\[\\int p_{\\theta, Z}(x) dx = \\int \\frac{e^{f_\\theta(x)}}{Z} dx \\approx 1\\] <p>This immediately gives us:</p> \\[Z \\approx \\int e^{f_\\theta(x)} dx\\] <p>Deriving the Discriminator for the Modified EBM</p> <p>Now let's derive the discriminator \\(D_{\\theta, Z}(x)\\) for our modified EBM \\(p_{\\theta, Z}(x) = \\frac{e^{f_\\theta(x)}}{Z}\\).</p> <p>Starting with the discriminator definition:</p> \\[D_{\\theta, Z}(x) = \\frac{p_{\\theta, Z}(x)}{p_{\\theta, Z}(x) + p_n(x)}\\] <p>Substituting our modified EBM:</p> \\[D_{\\theta, Z}(x) = \\frac{\\frac{e^{f_\\theta(x)}}{Z}}{\\frac{e^{f_\\theta(x)}}{Z} + p_n(x)}\\] \\[D_{\\theta, Z}(x) = \\frac{e^{f_\\theta(x)}}{e^{f_\\theta(x)} + Z \\cdot p_n(x)}\\] <p>Noise Contrastive Estimation Training Objective</p> <p>The NCE objective maximizes the log-likelihood of correctly classifying data vs noise samples:</p> \\[\\mathcal{L}_{NCE}(\\theta, Z) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log D_{\\theta, Z}(x) \\right] + \\mathbb{E}_{x \\sim p_n} \\left[ \\log(1 - D_{\\theta, Z}(x)) \\right]\\] <p>In theory, we could have any noise distribution to make this work. But in pratice, a noise distribution that similar (if we can manage) to the data distribution works very well. At the end of the day you learn an EBM and you learn a partition function. In the limit of infinite data and perfect optimization, the EBM matches the data distribution and Z matches the true partition function of the EBM.</p> <p>There is no evolving Generator like we had in GAN. The generator here is fixed, which is the noise distribution. We are training a special Discriminator.</p> <p>Note: The NCE objective function does not guide us to sample any data. We still need to use something like MCMC (e.g., Langevin dynamics, Hamiltonian Monte Carlo) to generate samples from the trained EBM. NCE only provides a way to train the energy function and partition function without computing the intractable partition function integral.</p> <p>Substituting our discriminator:</p> \\[\\mathcal{L}_{NCE}(\\theta, Z) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log \\frac{e^{f_\\theta(x)}}{e^{f_\\theta(x)} + Z \\cdot p_n(x)} \\right] + \\mathbb{E}_{x \\sim p_n} \\left[ \\log \\frac{Z \\cdot p_n(x)}{e^{f_\\theta(x)} + Z \\cdot p_n(x)} \\right]\\] <p>Using the sigmoid formulation with \\(h_{\\theta, Z}(x) = f_\\theta(x) - \\log p_n(x) - \\log Z\\):</p> \\[\\mathcal{L}_{NCE}(\\theta, Z) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log \\sigma(h_{\\theta, Z}(x)) \\right] + \\mathbb{E}_{x \\sim p_n} \\left[ \\log(1 - \\sigma(h_{\\theta, Z}(x))) \\right]\\] <p>This is exactly the binary cross-entropy loss for a binary classifier that distinguishes between data and noise samples.</p> <p>Training Algorithm:</p> <ol> <li> <p>Sample data batch: \\(\\{x_1, x_2, \\ldots, x_n\\} \\sim p_{data}(x)\\)</p> </li> <li> <p>Sample noise batch: \\(\\{\\tilde{x}_1, \\tilde{x}_2, \\ldots, \\tilde{x}_n\\} \\sim p_n(x)\\)</p> </li> <li> <p>Compute logits: \\(h_{\\theta, Z}(x_i)\\) and \\(h_{\\theta, Z}(\\tilde{x}_i)\\)</p> </li> <li> <p>Compute binary cross-entropy loss</p> </li> <li> <p>Update both \\(\\theta\\) (energy function parameters) and \\(Z\\) (partition function parameter) via gradient descent</p> </li> </ol> <p>Key Advantage: Unlike other EBM training methods (like contrastive divergence), NCE does not require sampling from the EBM during the training process. This eliminates the computational expense and potential instability of MCMC sampling during training, making NCE much more efficient and stable.</p>"},{"location":"ai/deep_generative_models/score_based_models/#comparing-nce-and-gan","title":"Comparing NCE and GAN","text":"<p>Both NCE and GANs use binary classification objectives to train generative models, but they differ significantly in their approach and properties.</p> <p>Similarities</p> <ol> <li>Binary Classification Objective: Both use binary cross-entropy loss to distinguish between real and fake samples</li> <li>No Likelihood Computation: Neither requires computing or maximizing explicit likelihood</li> <li>Stable Training: Both avoid the computational challenges of direct likelihood-based training</li> </ol> <p>Key Differences</p> Aspect NCE GAN Generator Fixed noise distribution \\(p_n(x)\\) Learnable generator network \\(G_\\phi(z)\\) Discriminator Parameterized as EBM: \\(D_{\\theta,Z}(x) = \\frac{e^{f_\\theta(x)}}{e^{f_\\theta(x)} + Z \\cdot p_n(x)}\\) Separate neural network \\(D_\\theta(x)\\) Training Single objective: \\(\\mathcal{L}_{NCE}(\\theta, Z)\\) Min-max game: \\(\\min_G \\max_D \\mathcal{L}_{GAN}(G, D)\\) Sampling Requires MCMC after training Direct sampling via generator Mode Coverage Depends on noise distribution choice Can adapt to cover all data modes Convergence Single optimization problem Requires careful balance between generator and discriminator <p>When to Use Each</p> <p>Use NCE when: - You need interpretable energy functions - Training stability is crucial - You want theoretical guarantees - You can afford MCMC sampling at inference time</p> <p>Use GAN when: - Fast sampling is required - You need high-quality, diverse samples - You have computational resources for adversarial training - You want to avoid MCMC entirely</p>"},{"location":"ai/deep_generative_models/score_based_models/#training-score-based-models","title":"Training Score Based Models","text":"<p>Is Score Matching Limited to EBMs?</p> <p>No, score matching is not limited to Energy-Based Models. We can use score matching for other generative model types as well:</p> <ul> <li>Autoregressive Models: Can be trained using score matching</li> <li>Normalizing Flow Models: Can also be trained using score matching</li> <li>Variational Autoencoders: Score matching can be applied to VAEs</li> </ul> <p>But what's the point since likelihoods are tractable?</p> <p>For models like autoregressive models and normalizing flows, the likelihood is indeed tractable, so we could use maximum likelihood estimation (MLE) instead of score matching. However, in principle, we could still train these models using score matching.</p> <p>Practical Considerations: MLE is often preferred when likelihood is tractable because it's more direct and efficient. Score matching might be useful when the likelihood computation is numerically unstable</p> <p>The core ddea of Score-Based Models</p> <p>The fundamental insight behind score-based models is that instead of modeling the energy function or probability density directly, we model the score function \\(s_\\theta(x)\\).</p> <p>What is the Score Function?</p> <p>The score function is the gradient of the log probability density:</p> \\[s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\] <p>Direct Modeling Approach:</p> <p>Instead of learning an energy function \\(f_\\theta(x)\\) and computing \\(s_\\theta(x) = \\nabla_x f_\\theta(x)\\), we directly model:</p> \\[s_\\theta(x): \\mathbb{R}^d \\rightarrow \\mathbb{R}^d\\] <p>This is a vector-valued function that maps from the data space to the same space, representing the gradient field.</p> <p>Key Properties:</p> <ol> <li>Vector Field: \\(s_\\theta(x)\\) is a vector field that assigns a gradient vector to each point \\(x\\) in the data space</li> <li>No Partition Function: We don't need to compute or approximate the partition function \\(Z(\\theta)\\)</li> <li>Direct Approximation: \\(s_\\theta(x) \\approx \\nabla_x \\log p_{data}(x)\\)</li> </ol> <p></p> <p>Deriving the Score Matching Objective</p> <p>Fisher Divergence objective:</p> <p>We want to minimize the Fisher divergence between the data distribution and the distribution induced by our score function:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x) - s_{data}(x)\\|^2 \\right]\\] <p>This measures how well our learned score function \\(s_\\theta(x)\\) approximates the true score function \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\).</p> <p>The Challenge:</p> <p>We don't have access to \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\) since we only have samples from \\(p_{data}(x)\\), not its analytical form.</p> <p>We can rewrite the Fisher divergence to avoid needing the true score function. Let's expand the squared norm:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x)\\|^2 - s_\\theta(x)^T s_{data}(x) + \\frac{1}{2} \\|s_{data}(x)\\|^2 \\right]\\] <p>The key insight is to handle the cross term \\(s_\\theta(x)^T s_{data}(x)\\) using integration by parts.</p> <p>For the univariate case (\\(x \\in \\mathbb{R}\\)), we have:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x) \\cdot s_{data}(x) \\right] = \\int s_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\cdot p_{data}(x) dx\\] \\[= \\int s_\\theta(x) \\cdot \\frac{d}{dx} p_{data}(x) dx\\] <p>Using integration by parts: \\(\\int u \\cdot \\frac{d}{dx} v \\, dx = u \\cdot v - \\int \\frac{d}{dx} u \\cdot v \\, dx\\)</p> <p>Setting \\(u = s_\\theta(x)\\) and \\(v = p_{data}(x)\\):</p> \\[= \\left. s_\\theta(x) \\cdot p_{data}(x) \\right|_{-\\infty}^{\\infty} - \\int \\frac{d}{dx} s_\\theta(x) \\cdot p_{data}(x) dx\\] <p>Assuming the boundary term vanishes (reasonable for well-behaved distributions):</p> \\[= -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d}{dx} s_\\theta(x) \\right]\\] <p>Multivariate Case:</p> <p>For \\(x \\in \\mathbb{R}^d\\), we apply integration by parts component-wise:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)^T s_{data}(x) \\right] = \\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)_i \\cdot s_{data}(x)_i \\right]\\] \\[= -\\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\right]\\] \\[= -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] <p>where \\(\\text{tr}(\\nabla_x s_\\theta(x)) = \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i\\) is the trace of the Jacobian matrix.</p> <p>Final Score Matching Objective:</p> <p>Substituting back into the Fisher divergence:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x)\\|^2 + \\text{tr}(\\nabla_x s_\\theta(x)) \\right] + \\text{constant}\\] <p>where the constant term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}} \\left[ \\|s_{data}(x)\\|^2 \\right]\\) doesn't depend on \\(\\theta\\) and can be ignored during optimization.</p> <p>Key Insight:</p> <p>This reformulation allows us to train the score function using only samples from \\(p_{data}(x)\\) and the derivatives of our score model, without needing access to the true score function \\(s_{data}(x)\\).</p> <p>The computational cost of the second term makes score matching challenging for high-dimensional data, which motivates alternative approaches like denoising score matching and sliced score matching.</p>"},{"location":"ai/deep_generative_models/score_based_models/#denoising-score-matching","title":"Denoising Score Matching","text":"<p>Denoising score matching addresses the computational challenges of standard score matching by adding noise to the data.</p> <p>The Key Idea:</p> <p>Instead of trying to learn the score function of the original data distribution \\(p_{data}(x)\\), we learn the score function of a noisy version of the data.</p> <p>Noise Distribution:</p> <p>We define a noise distribution \\(q_\\sigma(\\tilde{x} | x)\\) that adds noise to clean data points. A common choice is Gaussian noise:</p> \\[q_\\sigma(\\tilde{x} | x) = \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)\\] <p>This means: \\(\\tilde{x} = x + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\)</p> <p>Noisy Data Distribution:</p> <p>The noisy data distribution is the convolution of the original data distribution with the noise:</p> \\[q_\\sigma(\\tilde{x}) = \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx\\] <p>The denoising score matching objective minimizes the Fisher divergence between the noise perturbed data distribution \\(q_\\sigma(\\tilde{x})\\) and our score model \\(s_\\theta(\\tilde{x})\\):</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\frac{1}{2} \\|s_\\theta(\\tilde{x}) - \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right]\\] \\[\\mathcal{L}_{DSM}(\\theta) = \\int q_\\sigma(\\tilde{x}) \\left[ \\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2 - s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) + \\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right] d\\tilde{x}\\] <p>Focusing on the Cross Term:</p> <p>The cross term \\(-s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\) is the challenging part. First, let's write the cross term as an integral:</p> \\[\\int q_\\sigma(\\tilde{x}) \\left[ -s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) \\right] d\\tilde{x}\\] <p>Using the chain rule: \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) \\cdot q_\\sigma(\\tilde{x}) = \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x})\\), we get:</p> \\[= -\\int s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x}) d\\tilde{x}\\] <p>Substituting the noisy data distribution:</p> <p>Recall that \\(q_\\sigma(\\tilde{x}) = \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx\\). Substituting this into the integral:</p> \\[= -\\int s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\left[ \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx \\right] d\\tilde{x}\\] <p>Since the gradient operator \\(\\nabla_{\\tilde{x}}\\) acts only on \\(\\tilde{x}\\) and not on \\(x\\), we can interchange the gradient and the integral over \\(x\\):</p> \\[= -\\int s_\\theta(\\tilde{x})^T \\left[ \\int \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x} | x) p_{data}(x) dx \\right] d\\tilde{x}\\] <p>We can rearrange this as a double integral:</p> \\[= -\\iint s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x} | x) \\cdot p_{data}(x) \\, dx \\, d\\tilde{x}\\] <p>Now we can use the chain rule in reverse: \\(\\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x} | x) = \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) \\cdot q_\\sigma(\\tilde{x} | x)\\)</p> \\[= -\\iint s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) \\cdot q_\\sigma(\\tilde{x} | x) \\cdot p_{data}(x) \\, dx \\, d\\tilde{x}\\] <p>Final Expression:</p> <p>This can be written as an expectation:</p> \\[= -\\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) \\right]\\] <p>Or equivalently:</p> \\[= -\\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right]\\] <p>Completing the Denoising Score Matching Objective:</p> <p>Now let's bring this back to the complete objective function. Recall that we started with:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\int q_\\sigma(\\tilde{x}) \\left[ \\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2 - s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) + \\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right] d\\tilde{x}\\] <p>We've derived the cross term. Now let's handle all three terms:</p> <p>Term 1: \\(\\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2\\)</p> \\[\\int q_\\sigma(\\tilde{x}) \\cdot \\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2 d\\tilde{x} = \\frac{1}{2} \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right]\\] <p>Substituting \\(q_\\sigma(\\tilde{x}) = \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx\\):</p> \\[= \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right]\\] <p>Term 2: Cross term (already derived)</p> \\[- \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right]\\] <p>Term 3: \\(\\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2\\)</p> \\[\\int q_\\sigma(\\tilde{x}) \\cdot \\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 d\\tilde{x} = \\frac{1}{2} \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right]\\] <p>This term is a constant with respect to \\(\\theta\\) and can be ignored during optimization.</p> <p>Combining all terms:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right] - \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right] + \\text{const}\\] <p>Simplifying to the final form:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right] - \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right] + \\text{const}\\] <p>To see how this becomes the final form, let's expand the squared difference:</p> \\[\\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2 = \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\|^2 - 2 \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) + \\|s_\\theta(\\tilde{x})\\|^2\\] <p>Taking the expectation and multiplying by \\(\\frac{1}{2}\\):</p> \\[\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2 \\right] = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right] - \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right] + \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\|^2 \\right]\\] <p>The last term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\|^2 \\right]\\) is a constant with respect to \\(\\theta\\) and can be absorbed into the constant term.</p> <p>This can be written as:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2 \\right] + \\text{const}\\] <p>Key Advantage: easy computation of the Target Score Function</p> <p>The major advantage of denoising score matching is that \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\) is easy to compute analytically, unlike the true data score function \\(\\nabla_x \\log p_{data}(x)\\).</p> <p>For Gaussian Noise:</p> <p>The most common choice is Gaussian noise: \\(q_\\sigma(\\tilde{x} | x) = \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)\\)</p> <p>The log probability is:</p> \\[\\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{2\\sigma^2} \\|\\tilde{x} - x\\|^2 - \\frac{d}{2} \\log(2\\pi\\sigma^2)\\] <p>Taking the gradient with respect to \\(\\tilde{x}\\):</p> \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\] <p>This gradient is analytically tractable and computationally cheap.</p> <p>Comparison with Standard Score Matching:</p> <p>In standard score matching, we need to compute:</p> <ul> <li> <p>\\(\\nabla_x \\log p_\\theta(x)\\) (our model's score function)</p> </li> <li> <p>\\(\\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x))\\) (Hessian trace - expensive!)</p> </li> </ul> <p>In denoising score matching, we only need:</p> <ul> <li> <p>\\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\) (known analytically)</p> </li> <li> <p>\\(s_\\theta(\\tilde{x})\\) (our model's score function)</p> </li> </ul> <p>Training Algorithm:</p> <ol> <li>Sample clean data: \\(x \\sim p_{data}(x)\\)</li> <li>Add noise: \\(\\tilde{x} = x + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\)</li> <li>Compute target: \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x) = -\\frac{1}{\\sigma^2} \\epsilon\\)</li> <li>Compute prediction: \\(s_\\theta(\\tilde{x})\\)</li> <li>Minimize: \\(\\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2\\)</li> </ol> <p>Monte Carlo Estimation:</p> <p>For a batch of \\(N\\) samples \\(\\{x_1, x_2, \\ldots, x_N\\}\\), the Monte Carlo estimate of the loss is:</p> \\[\\mathcal{L}_{DSM}(\\theta) \\approx \\frac{1}{2N} \\sum_{i=1}^N \\|\\nabla_{\\tilde{x}_i} \\log q_\\sigma(\\tilde{x}_i | x_i) - s_\\theta(\\tilde{x}_i)\\|^2\\] <p>where \\(\\tilde{x}_i = x_i + \\epsilon_i\\) with \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I)\\).</p> <p>Practical Implementation:</p> <pre><code># For a batch of data\ndef denoising_score_matching_loss(model, data_batch, sigma):\n    # Add noise to data\n    noise = torch.randn_like(data_batch) * sigma\n    noisy_data = data_batch + noise\n\n    # Compute target score (gradient of log noise distribution)\n    target_score = -noise / (sigma ** 2)\n\n    # Compute model prediction\n    predicted_score = model(noisy_data)\n\n    # Compute loss\n    loss = 0.5 * torch.mean((target_score - predicted_score) ** 2)\n\n    return loss\n</code></pre> <p>Intuition behind the loss function</p> <p>The denoising score matching objective has a beautiful intuition: we're teaching our model to estimate the score function of noisy data, and when the noise is very small, this approximates the score function of the clean data distribution.</p> <p>The Core Idea:</p> <ol> <li> <p>Learning Noisy Data Structure: Instead of trying to learn the score function of the complex, unknown data distribution \\(p_{data}(x)\\), we learn the score function of a simpler, known noisy distribution \\(q_\\sigma(\\tilde{x})\\).</p> </li> <li> <p>Denoising as Learning: By learning to predict \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\), our model learns to \"point\" from noisy points \\(\\tilde{x}\\) back toward their clean counterparts \\(x\\).</p> </li> </ol> <p>What does \"point\" mean?</p> <p>The score function \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\) is a vector field that assigns a direction vector to each point \\(\\tilde{x}\\) in the data space. This vector \"points\" in the direction of steepest increase in the log probability.</p> <p>For Gaussian noise, this vector is:</p> \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\] <p>Interpretation:</p> <ul> <li> <p>Direction: The vector points from the noisy point \\(\\tilde{x}\\) toward the clean point \\(x\\)</p> </li> <li> <p>Magnitude: The length of the vector is proportional to the distance between \\(\\tilde{x}\\) and \\(x\\), scaled by \\(\\frac{1}{\\sigma^2}\\)</p> </li> <li> <p>Purpose: This vector tells us \"if you want to increase the probability of this noisy point, move in this direction\"</p> </li> </ul> <p>Visual Example: Imagine a 2D space where:</p> <ul> <li> <p>Clean point \\(x = (0, 0)\\)</p> </li> <li> <p>Noisy point \\(\\tilde{x} = (1, 1)\\) </p> </li> <li> <p>Noise level \\(\\sigma = 1\\)</p> </li> </ul> <p>The score vector at \\(\\tilde{x}\\) is:</p> \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -(1, 1)\\] <p>This vector points from \\((1, 1)\\) toward \\((0, 0)\\), indicating the direction to move to increase the probability of the noisy point under the noise distribution.</p> <p>Why this matters: When our model learns to predict this vector, it's learning to identify the direction that leads back to the clean data. This implicitly teaches it about the local structure of the data manifold - where the \"good\" data points are located relative to any given noisy point.</p> <p>Why this is a denoiser:</p> <p>The score function \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\) acts as a denoiser because it provides the exact direction and magnitude needed to remove the noise from a noisy data point.</p> <p>When our model learns to predict this score function, it's learning to:</p> <ul> <li> <p>Identify noise: Recognize what part of the data is noise</p> </li> <li> <p>Compute denoising direction: Determine which direction to move to remove the noise</p> </li> <li> <p>Estimate noise magnitude: Understand how much to move in that direction</p> </li> </ul> <p>This is why denoising score matching is so powerful - by learning to denoise, the model implicitly learns the structure of the clean data distribution, even though it never directly sees the clean data score function.</p> <p>Con: Estimates the score of the noise-perturbed data, not the score of the actual data. We have shifted the goal post basically.</p> <p>Generating Samples with MCMC:</p> <p>Once we've trained our score function \\(s_\\theta(x)\\), we can generate samples using Markov Chain Monte Carlo (MCMC) methods, typically Langevin dynamics.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/","title":"Variational autoencoders","text":""},{"location":"ai/deep_generative_models/variational_autoencoders/#representation","title":"Representation","text":"<p>Consider a directed, latent variable model as shown below.</p> <p></p> <p>In the model above, \\(z\\) and \\(x\\) denote the latent and observed variables respectively. The joint distribution expressed by this model is given as</p> \\[p(x,z) = p(x|z)p(z).\\] <p>From a generative modeling perspective, this model describes a generative process for the observed data \\(x\\) using the following procedure:</p> \\[z \\sim p(z)\\] \\[x \\sim p(x|z)\\] <p>If one adopts the belief that the latent variables \\(z\\) somehow encode semantically meaningful information about \\(x\\), it is natural to view this generative process as first generating the \"high-level\" semantic information about \\(x\\) first before fully generating \\(x\\).</p> <p>We now consider a family of distributions \\(\\mathcal{Z}\\) where \\(p(z) \\in \\mathcal{Z}\\) describes a probability distribution over \\(z\\). Next, consider a family of conditional distributions \\(\\mathcal{X|Z}\\) where \\(p(x|z) \\in \\mathcal{X|Z}\\) describes a conditional probability distribution over \\(x\\) given \\(z\\). Then our hypothesis class of generative models is the set of all possible combinations</p> \\[\\mathcal{X,Z} = \\{p(x,z) \\mid p(z) \\in \\mathcal{Z}, p(x|z) \\in \\mathcal{X|Z}\\}.\\] <p>Given a dataset \\(\\mathcal{D} = \\{x^{(1)}, \\ldots, x^{(n)}\\}\\), we are interested in the following learning and inference tasks:</p> <ol> <li>Selecting \\(p \\in \\mathcal{X,Z}\\) that \"best\" fits \\(\\mathcal{D}\\).</li> <li>Given a sample \\(x\\) and a model \\(p \\in \\mathcal{X,Z}\\), what is the posterior distribution over the latent variables \\(z\\)?</li> </ol> <p>The posterior distribution \\(p(z|x)\\) represents our updated beliefs about the latent variables \\(z\\) after observing the data \\(x\\). In other words, it tells us what values of \\(z\\) are most likely to have generated the observed \\(x\\). This is particularly useful for tasks like feature extraction, where we want to understand what latent factors might have generated our observed data.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#learning-directed-latent-variable-models","title":"Learning Directed Latent Variable Models","text":"<p>One way to measure how closely \\(p(x,z)\\) fits the observed dataset \\(\\mathcal{D}\\) is to measure the Kullback-Leibler (KL) divergence between the data distribution (which we denote as \\(p_{data}(x)\\)) and the model's marginal distribution \\(p(x) = \\int p(x,z)dz\\). The distribution that \"best\" fits the data is thus obtained by minimizing the KL divergence.</p> \\[\\min_{p \\in \\mathcal{X,Z}} D_{KL}(p_{data}(x) \\| p(x)).\\] <p>As we have seen previously, optimizing an empirical estimate of the KL divergence is equivalent to maximizing the marginal log-likelihood \\(\\log p(x)\\) over \\(\\mathcal{D}\\):</p> \\[\\max_{p \\in \\mathcal{X,Z}} \\sum_{x \\in \\mathcal{D}} \\log p(x) = \\max_{p \\in \\mathcal{X,Z}} \\sum_{x \\in \\mathcal{D}} \\log \\int p(x,z)dz.\\] <p>However, it turns out this problem is generally intractable for high-dimensional \\(z\\) as it involves an integration (or sums in the case \\(z\\) is discrete) over all the possible latent sources of variation \\(z\\). This intractability arises from several challenges:</p> <ol> <li> <p>Computational Complexity: The integral \\(\\int p(x,z)dz\\) requires evaluating the joint distribution \\(p(x,z)\\) for all possible values of \\(z\\). In high-dimensional spaces, this becomes computationally prohibitive as the number of points to evaluate grows exponentially with the dimension of \\(z\\).</p> </li> <li> <p>Numerical Integration: Even if we could evaluate the integrand at all points, computing the integral numerically becomes increasingly difficult as the dimension of \\(z\\) grows. Traditional numerical integration methods like quadrature become impractical in high dimensions.</p> </li> <li> <p>Posterior Inference: The intractability of the marginal likelihood also makes it difficult to compute the posterior distribution \\(p(z|x)\\), which is crucial for tasks like feature extraction and data generation.</p> </li> </ol> <p>This intractability motivates the need for approximate inference methods, such as variational inference. One option is to estimate the objective via Monte Carlo. For any given datapoint \\(x\\), we can obtain the following estimate for its marginal log-likelihood:</p> \\[\\log p(x) \\approx \\log \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>This Monte Carlo estimate is derived as follows:</p> <p>First, recall that the marginal likelihood \\(p(x)\\) can be written as an expectation:</p> \\[p(x) = \\int p(x|z)p(z)dz = \\mathbb{E}_{z \\sim p(z)}[p(x|z)]\\] <p>The Monte Carlo method approximates this expectation by drawing \\(k\\) samples from \\(p(z)\\) and computing their average:</p> \\[\\mathbb{E}_{z \\sim p(z)}[p(x|z)] \\approx \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>Taking the logarithm of both sides gives us our final estimate:</p> \\[\\log p(x) \\approx \\log \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>This approximation becomes more accurate as \\(k\\) increases, but at the cost of more computational resources. The key insight is that we're using random sampling to approximate the intractable integral, trading exact computation for statistical estimation.</p> <p>Rather than maximizing the log-likelihood directly, an alternate is to instead construct a lower bound that is more amenable to optimization. To do so, we note that evaluating the marginal likelihood \\(p(x)\\) is at least as difficult as as evaluating the posterior \\(p(z|x)\\) for any latent vector \\(z\\) since by definition \\(p(z|x) = p(x,z)/p(x)\\).</p> <p>Next, we introduce a variational family \\(\\mathcal{Q}\\) of distributions that approximate the true, but intractable posterior \\(p(z|x)\\). Further henceforth, we will assume a parameteric setting where any distribution in the model family \\(\\mathcal{X,Z}\\) is specified via a set of parameters \\(\\theta \\in \\Theta\\) and distributions in the variational family \\(\\mathcal{Q}\\) are specified via a set of parameters \\(\\lambda \\in \\Lambda\\).</p> <p>Given \\(\\mathcal{X,Z}\\) and \\(\\mathcal{Q}\\), we note that the following relationships hold true for any \\(x\\) and all variational distributions \\(q_\\lambda(z) \\in \\mathcal{Q}\\):</p> \\[\\log p_\\theta(x) = \\log \\int p_\\theta(x,z)dz = \\log \\int \\frac{q_\\lambda(z)}{q_\\lambda(z)}p_\\theta(x,z)dz \\geq \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] := \\text{ELBO}(x;\\theta,\\lambda)\\] <p>where we have used Jensen's inequality in the final step. The key insight here is that since the logarithm function is concave, Jensen's inequality tells us that for any random variable \\(X\\) and concave function \\(f\\), we have \\(\\mathbb{E}[f(X)] \\leq f(\\mathbb{E}[X])\\). In our case:</p> <p>We first multiply and divide by \\(q_\\lambda(z)\\) inside the integral to get:</p> \\[\\log \\int \\frac{q_\\lambda(z)}{q_\\lambda(z)}p_\\theta(x,z)dz = \\log \\int q_\\lambda(z)\\frac{p_\\theta(x,z)}{q_\\lambda(z)}dz\\] <p>The integral \\(\\int q_\\lambda(z)\\frac{p_\\theta(x,z)}{q_\\lambda(z)}dz\\) can be seen as an expectation \\(\\mathbb{E}_{q_\\lambda(z)}\\left[\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\)</p> <p>Since \\(\\log\\) is a concave function, Jensen's inequality gives us:</p> \\[\\log \\mathbb{E}_{q_\\lambda(z)}\\left[\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] \\geq \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\] <p>This inequality is what allows us to obtain a lower bound on the log-likelihood, which we call the Evidence Lower BOund (ELBO). The ELBO admits a tractable unbiased Monte Carlo estimator</p> \\[\\frac{1}{k}\\sum_{i=1}^k \\log\\frac{p_\\theta(x,z^{(i)})}{q_\\lambda(z^{(i)})}, \\text{ where } z^{(i)} \\sim q_\\lambda(z),\\] <p>so long as it is easy to sample from and evaluate densities for \\(q_\\lambda(z)\\).</p> <p>In summary, we can learn a latent variable model by maximizing the ELBO with respect to both the model parameters \\(\\theta\\) and the variational parameters \\(\\lambda\\) for any given datapoint \\(x\\):</p> \\[\\max_\\theta \\sum_{x \\in \\mathcal{D}} \\max_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right].\\] <p>This optimization objective can be broken down into two parts:</p> <ol> <li>Inner Optimization: For each datapoint \\(x\\), we find the best variational parameters \\(\\lambda\\) that make \\(q_\\lambda(z)\\) as close as possible to the true posterior \\(p(z|x)\\). This is done by maximizing the ELBO with respect to \\(\\lambda\\). </li> </ol> <p>Why do we need \\(q_\\lambda(z)\\) to approximate \\(p(z|x)\\)? Since \\(p(x) = p(x,z)/p(z|x)\\), as \\(q_\\lambda(z)\\) tends to \\(p(z|x)\\), the ratio \\(p(x,z)/q_\\lambda(z)\\) tends to \\(p(x)\\). This means that by making our variational approximation closer to the true posterior, we get a better estimate of the marginal likelihood \\(p(x)\\).</p> <ol> <li>Outer Optimization: Across all datapoints in the dataset \\(\\mathcal{D}\\), we find the best model parameters \\(\\theta\\) that maximize the average ELBO. This improves the generative model's ability to explain the data.</li> </ol> <p>The outer sum \\(\\sum_{x \\in \\mathcal{D}}\\) is necessary because we want to learn a model that works well for all datapoints in our dataset, not just a single example. This is equivalent to maximizing the average ELBO across all datapoints.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#black-box-variational-inference","title":"Black-Box Variational Inference","text":"<p>We shall focus on first-order stochastic gradient methods for optimizing the ELBO. This inspires Black-Box Variational Inference (BBVI), a general-purpose Expectation-Maximization-like algorithm for variational learning of latent variable models, where, for each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), the following two steps are performed.</p> <p>Step 1</p> <p>We first do per-sample optimization of \\(q\\) by iteratively applying the update</p> \\[\\lambda^{(i)} \\leftarrow \\lambda^{(i)} + \\tilde{\\nabla}_\\lambda \\text{ELBO}(x^{(i)}; \\theta, \\lambda^{(i)}),\\] <p>where \\(\\text{ELBO}(x; \\theta, \\lambda) = \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\), and \\(\\tilde{\\nabla}_\\lambda\\) denotes an unbiased estimate of the ELBO gradient. This step seeks to approximate the log-likelihood \\(\\log p_\\theta(x^{(i)})\\).</p> <p>Step 2</p> <p>We then perform a single update step based on the mini-batch</p> \\[\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_i \\text{ELBO}(x^{(i)}; \\theta, \\lambda^{(i)}),\\] <p>which corresponds to the step that hopefully moves \\(p_\\theta\\) closer to \\(p_{data}\\).</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#gradient-estimation","title":"Gradient Estimation","text":"<p>The gradients \\(\\nabla_\\lambda \\text{ELBO}\\) and \\(\\nabla_\\theta \\text{ELBO}\\) can be estimated via Monte Carlo sampling. While it is straightforward to construct an unbiased estimate of \\(\\nabla_\\theta \\text{ELBO}\\) by simply pushing \\(\\nabla_\\theta\\) through the expectation operator, the same cannot be said for \\(\\nabla_\\lambda\\). Instead, we see that</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\mathbb{E}_{q_\\lambda(z)}\\left[\\left(\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z)\\right].\\] <p>This equality follows from the log-derivative trick (also commonly referred to as the REINFORCE trick). To derive this, we start with the gradient of the expectation:</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\nabla_\\lambda \\int q_\\lambda(z) \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>Using the product rule and chain rule:</p> \\[= \\int \\nabla_\\lambda q_\\lambda(z) \\cdot \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} + q_\\lambda(z) \\cdot \\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>The second term vanishes because: \\(\\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} = \\nabla_\\lambda [\\log p_\\theta(x,z) - \\log q_\\lambda(z)]\\). Since \\(p_\\theta(x,z)\\) doesn't depend on \\(\\lambda\\), \\(\\nabla_\\lambda \\log p_\\theta(x,z) = 0\\). Therefore, \\(\\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} = -\\nabla_\\lambda \\log q_\\lambda(z)\\).  When we multiply by \\(q_\\lambda(z)\\) and integrate, we get:</p> \\[\\int q_\\lambda(z) \\cdot (-\\nabla_\\lambda \\log q_\\lambda(z)) dz = -\\int \\nabla_\\lambda q_\\lambda(z) dz = -\\nabla_\\lambda \\int q_\\lambda(z) dz = -\\nabla_\\lambda 1 = 0\\] <p>where we used the fact that \\(\\int q_\\lambda(z) dz = 1\\) for any valid probability distribution.</p> <p>For the first term, we use the identity \\(\\nabla_\\lambda q_\\lambda(z) = q_\\lambda(z) \\nabla_\\lambda \\log q_\\lambda(z)\\):</p> \\[= \\int q_\\lambda(z) \\nabla_\\lambda \\log q_\\lambda(z) \\cdot \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>This can be rewritten as an expectation:</p> \\[= \\mathbb{E}_{q_\\lambda(z)}\\left[\\left(\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z)\\right]\\] <p>The gradient estimator \\(\\tilde{\\nabla}_\\lambda \\text{ELBO}\\) is thus</p> \\[\\frac{1}{k}\\sum_{i=1}^k \\left[\\left(\\log\\frac{p_\\theta(x,z^{(i)})}{q_\\lambda(z^{(i)})}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z^{(i)})\\right], \\text{ where } z^{(i)} \\sim q_\\lambda(z).\\] <p>However, it is often noted that this estimator suffers from high variance. One of the key contributions of the variational autoencoder paper is the reparameterization trick, which introduces a fixed, auxiliary distribution \\(p(\\epsilon)\\) and a differentiable function \\(T(\\epsilon; \\lambda)\\) such that the procedure</p> \\[\\epsilon \\sim p(\\epsilon)\\] \\[z \\leftarrow T(\\epsilon; \\lambda),\\] <p>is equivalent to sampling from \\(q_\\lambda(z)\\). This two-step procedure works as follows:</p> <ol> <li>First, we sample \\(\\epsilon\\) from a fixed distribution \\(p(\\epsilon)\\) that doesn't depend on \\(\\lambda\\) (e.g., standard normal)</li> <li>Then, we transform this sample using a deterministic function \\(T(\\epsilon; \\lambda)\\) that depends on \\(\\lambda\\)</li> </ol> <p>The key insight is that if we choose \\(T\\) appropriately, the distribution of \\(z = T(\\epsilon; \\lambda)\\) will be exactly \\(q_\\lambda(z)\\). For example, if \\(q_\\lambda(z)\\) is a normal distribution with mean \\(\\mu_\\lambda\\) and standard deviation \\(\\sigma_\\lambda\\), we can use:</p> <p>\\(p(\\epsilon) = \\mathcal{N}(0, 1)\\)</p> <p>\\(T(\\epsilon; \\lambda) = \\mu_\\lambda + \\sigma_\\lambda \\cdot \\epsilon\\)</p> <p>By the Law of the Unconscious Statistician, we can see that</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\mathbb{E}_{p(\\epsilon)}\\left[\\nabla_\\lambda \\log\\frac{p_\\theta(x,T(\\epsilon; \\lambda))}{q_\\lambda(T(\\epsilon; \\lambda))}\\right].\\] <p>In contrast to the REINFORCE trick, the reparameterization trick is often noted empirically to have lower variance and thus results in more stable training.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#parameterizing-distributions-via-deep-neural-networks","title":"Parameterizing Distributions via Deep Neural Networks","text":"<p>So far, we have described \\(p_\\theta(x,z)\\) and \\(q_\\lambda(z)\\) in the abstract. To instantiate these objects, we consider choices of parametric distributions for \\(p_\\theta(z)\\), \\(p_\\theta(x|z)\\), and \\(q_\\lambda(z)\\). A popular choice for \\(p_\\theta(z)\\) is the unit Gaussian</p> \\[p_\\theta(z) = \\mathcal{N}(z|0,I),\\] <p>in which case \\(\\theta\\) is simply the empty set since the prior is a fixed distribution.</p> <p>In the case where \\(p_\\theta(x|z)\\) is a Gaussian distribution, we can thus represent it as</p> \\[p_\\theta(x|z) = \\mathcal{N}(x|\\mu_\\theta(z), \\Sigma_\\theta(z)),\\] <p>where \\(\\mu_\\theta(z)\\) and \\(\\Sigma_\\theta(z)\\) are neural networks that specify the mean and covariance matrix for the Gaussian distribution over \\(x\\) when conditioned on \\(z\\).</p> <p>Finally, the variational family for the proposal distribution \\(q_\\lambda(z)\\) needs to be chosen judiciously so that the reparameterization trick is possible. Many continuous distributions in the location-scale family can be reparameterized. In practice, a popular choice is again the Gaussian distribution, where</p> \\[\\begin{align*} \\lambda &amp;= (\\mu, \\Sigma) \\\\ q_\\lambda(z) &amp;= \\mathcal{N}(z|\\mu, \\Sigma) \\\\ p(\\varepsilon) &amp;= \\mathcal{N}(z|0,I) \\\\ T(\\varepsilon; \\lambda) &amp;= \\mu + \\Sigma^{1/2}\\varepsilon, \\end{align*}\\] <p>where \\(\\Sigma^{1/2}\\) is the Cholesky decomposition of \\(\\Sigma\\). For simplicity, practitioners often restrict \\(\\Sigma\\) to be a diagonal matrix (which restricts the distribution family to that of factorized Gaussians).</p> <p>The reparameterization trick consists of four key steps:</p> <ol> <li> <p>Parameter Definition: We define the variational parameters \\(\\lambda\\) as a tuple containing the mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\) of our Gaussian distribution. These parameters will be learned during training.</p> </li> <li> <p>Variational Distribution: We specify that our variational distribution \\(q_\\lambda(z)\\) is a Gaussian distribution parameterized by \\(\\mu\\) and \\(\\Sigma\\). This is the distribution we ideally want to sample from.</p> </li> <li> <p>Auxiliary Distribution: Instead of sampling directly from \\(q_\\lambda(z)\\), we introduce a fixed auxiliary distribution \\(p(\\varepsilon)\\) which is a standard normal distribution (mean 0, identity covariance). This distribution doesn't depend on our parameters \\(\\lambda\\).</p> </li> <li> <p>Transformation Function: We define a deterministic function \\(T(\\varepsilon; \\lambda)\\) that transforms samples from the auxiliary distribution into samples from our variational distribution. The transformation is given by \\(\\mu + \\Sigma^{1/2}\\varepsilon\\), where \\(\\Sigma^{1/2}\\) is the Cholesky decomposition of \\(\\Sigma\\).</p> </li> </ol> <p>The key insight is that instead of sampling directly from \\(q_\\lambda(z)\\), we can: 1. Sample \\(\\varepsilon\\) from the standard normal distribution \\(p(\\varepsilon)\\) 2. Transform it using \\(T(\\varepsilon; \\lambda)\\) to make it seem like we're getting a sample from \\(q_\\lambda(z)\\)</p> <p>This trick is crucial because it allows us to compute gradients with respect to \\(\\lambda\\) through the sampling process. Since the transformation \\(T\\) is differentiable, we can backpropagate through it to update the parameters \\(\\lambda\\) during training. This is why the reparameterization trick often leads to lower variance in gradient estimates compared to the REINFORCE trick.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#amortized-variational-inference","title":"Amortized Variational Inference","text":"<p>A noticeable limitation of black-box variational inference is that Step 1 executes an optimization subroutine that is computationally expensive. Recall that the goal of Step 1 is to find</p> \\[\\lambda^* = \\arg\\max_{\\lambda \\in \\Lambda} \\text{ELBO}(x; \\theta, \\lambda).\\] <p>For a given choice of \\(\\theta\\), there is a well-defined mapping from \\(x \\mapsto \\lambda^*\\). A key realization is that this mapping can be learned. In particular, one can train an encoding function (parameterized by \\(\\phi\\)) \\(f_\\phi: \\mathcal{X} \\to \\Lambda\\) (where \\(\\Lambda\\) is the space of \\(\\lambda\\) parameters) on the following objective</p> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, f_\\phi(x)).\\] <p>It is worth noting at this point that \\(f_\\phi(x)\\) can be interpreted as defining the conditional distribution \\(q_\\phi(z|x)\\). With a slight abuse of notation, we define</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right],\\] <p>and rewrite the optimization problem as</p> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, \\phi).\\] <p>It is also worth noting that optimizing \\(\\phi\\) over the entire dataset as a subroutine every time we sample a new mini-batch is clearly not reasonable. However, if we believe that \\(f_\\phi\\) is capable of quickly adapting to a close-enough approximation of \\(\\lambda^*\\) given the current choice of \\(\\theta\\), then we can interleave the optimization of \\(\\phi\\) and \\(\\theta\\). This yields the following procedure, where for each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), we perform the following two updates jointly:</p> \\[\\begin{align*} \\phi &amp;\\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\\\ \\theta &amp;\\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi), \\end{align*}\\] <p>rather than running BBVI's Step 1 as a subroutine. By leveraging the learnability of \\(x \\mapsto \\lambda^*\\), this optimization procedure amortizes the cost of variational inference. If one further chooses to define \\(f_\\phi\\) as a neural network, the result is the variational autoencoder.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#steps-of-amortized-variational-inference","title":"Steps of Amortized Variational Inference","text":"<p>Let's break down the amortized variational inference procedure in detail:</p> <ol> <li>Initial Setup:</li> <li>We have a dataset \\(\\mathcal{D} = \\{x^{(1)}, \\ldots, x^{(n)}\\}\\)</li> <li>We have a generative model \\(p_\\theta(x,z)\\) with parameters \\(\\theta\\)</li> <li> <p>We want to learn both the model parameters \\(\\theta\\) and the variational parameters \\(\\lambda\\) for each datapoint</p> </li> <li> <p>Traditional BBVI Approach:</p> </li> <li>For each datapoint \\(x\\), we would need to run an optimization to find:</li> </ol> \\[\\lambda^* = \\arg\\max_{\\lambda \\in \\Lambda} \\text{ELBO}(x; \\theta, \\lambda)\\] <ul> <li> <p>This is computationally expensive as it requires running an optimization subroutine for each datapoint</p> </li> <li> <p>Key Insight - Learnable Mapping:</p> </li> <li>Instead of optimizing \\(\\lambda\\) separately for each \\(x\\), we realize that there's a mapping from \\(x\\) to \\(\\lambda^*\\)</li> <li>This mapping can be learned using a function \\(f_\\phi: \\mathcal{X} \\to \\Lambda\\) parameterized by \\(\\phi\\)</li> <li> <p>The function \\(f_\\phi\\) takes a datapoint \\(x\\) and outputs the variational parameters \\(\\lambda\\)</p> </li> <li> <p>Training the Encoder:</p> </li> <li>We train \\(f_\\phi\\) to maximize the ELBO across all datapoints:</li> </ul> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, f_\\phi(x))\\] <ul> <li> <p>This is equivalent to learning a conditional distribution \\(q_\\phi(z|x)\\)</p> </li> <li> <p>Joint Optimization:</p> </li> <li>Instead of running BBVI's Step 1 as a subroutine, we interleave the optimization of \\(\\phi\\) and \\(\\theta\\)</li> <li>For each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), we perform two updates:</li> </ul> \\[\\begin{align*} \\phi &amp;\\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\\\ \\theta &amp;\\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\end{align*}\\] <ol> <li>Practical Implementation:</li> <li>When \\(f_\\phi\\) is implemented as a neural network, we get a variational autoencoder</li> <li>The encoder network \\(f_\\phi\\) maps inputs \\(x\\) to variational parameters</li> <li>The decoder network maps latent variables \\(z\\) to reconstructed inputs</li> <li>Both networks are trained end-to-end using the ELBO objective</li> </ol> <p>In practice, the encoder neural network \\(f_\\phi\\) outputs the parameters of a diagonal Gaussian distribution:</p> \\[q_\\phi(z|x) = \\mathcal{N}(z|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))\\] <p>where \\(\\mu_\\phi(x)\\) and \\(\\sigma^2_\\phi(x)\\) are the mean and variance vectors output by the encoder network. To sample from this distribution during training, we use the reparameterization trick:</p> \\[z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,I)\\] <p>where \\(\\odot\\) denotes element-wise multiplication. This allows us to backpropagate through the sampling process and train the encoder network end-to-end.</p> <p>The key advantage of this approach is that it amortizes the cost of variational inference by learning a single function \\(f_\\phi\\) that can quickly approximate the optimal variational parameters for any input \\(x\\), rather than running a separate optimization for each datapoint.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#decomposition-of-the-negative-elbo","title":"Decomposition of the Negative ELBO","text":"<p>Starting with the definition of the ELBO:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right]\\] <p>We can expand the joint distribution \\(p_\\theta(x,z)\\) using the chain rule of probability:</p> \\[p_\\theta(x,z) = p_\\theta(x|z)p_\\theta(z)\\] <p>Substituting this into the ELBO:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x|z)p_\\theta(z)}{q_\\phi(z|x)}\\right]\\] <p>Using the properties of logarithms, we can split this into three terms:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)]\\] <p>The second and third terms can be combined to form the KL divergence between \\(q_\\phi(z|x)\\) and \\(p_\\theta(z)\\):</p> \\[\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)] = -\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{q_\\phi(z|x)}{p_\\theta(z)}\\right] = -D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\] <p>Therefore, the ELBO can be written as:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\] <p>It is insightful to note that the negative ELBO can be decomposed into two terms:</p> \\[-\\text{ELBO}(x; \\theta, \\phi) = \\underbrace{-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{Reconstruction Loss}} + \\underbrace{D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))}_{\\text{KL Divergence}}\\] <p>This decomposition reveals two key components of the training objective:</p> <ol> <li>Reconstruction Loss: \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\)</li> <li>This term measures how well the model can reconstruct the input \\(x\\) from its latent representation \\(z\\)</li> <li>It encourages the encoder to produce latent codes that preserve the essential information about the input</li> <li> <p>In practice, this is often implemented as the mean squared error or binary cross-entropy between the input and its reconstruction</p> </li> <li> <p>KL Divergence: \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\)</p> </li> <li>This term measures how far the approximate posterior \\(q_\\phi(z|x)\\) is from the prior \\(p_\\theta(z)\\)</li> <li>It encourages the latent space to follow the prior distribution (typically a standard normal distribution)</li> </ol>"},{"location":"ai/deep_generative_models/variational_autoencoders/#practical-implementation-of-elbo-computation","title":"Practical Implementation of ELBO Computation","text":"<p>Let's look at how the ELBO is actually computed in practice. Here's a detailed implementation with explanations:</p> <p>We implement the (rec+kl) decomposed form for practicality and clarity because:</p> <ul> <li>KL has a closed form (for two Gaussians \\(q_\\phi(z|x) \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and \\(p(z) \\sim \\mathcal{N}(0,I)\\), the KL term can be computed analytically). A closed form means we can compute the exact value using a finite number of standard operations (addition, multiplication, logarithms, etc.) without needing numerical integration or approximation. This closed form is derived as follows:</li> </ul> <p>For two multivariate Gaussians \\(q_\\phi(z|x) = \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(z) = \\mathcal{N}(0,I)\\), the KL divergence is:</p> \\[D_{KL}(q_\\phi(z|x) \\| p(z)) = \\frac{1}{2}\\left[\\text{tr}(\\Sigma) + \\mu^T\\mu - d - \\log|\\Sigma|\\right]\\] <p>where \\(\\text{tr}(\\Sigma)\\) is the trace of the covariance matrix \\(\\Sigma\\) (the sum of its diagonal elements), \\(\\mu^T\\mu\\) is the squared L2 norm of the mean vector, \\(d\\) is the dimension of the latent space, and \\(|\\Sigma|\\) is the determinant of \\(\\Sigma\\). For diagonal covariance matrices \\(\\Sigma = \\text{diag}(\\sigma^2)\\), this simplifies to:</p> \\[D_{KL}(q_\\phi(z|x) \\| p(z)) = \\frac{1}{2}\\sum_{i=1}^d (\\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1)\\] <p>This analytical solution is not only computationally efficient but also provides exact gradients, unlike Monte Carlo estimates which would require sampling.</p> <ul> <li> <p>The analytical KL avoids noisy gradients that arise from computing KL via sampling so the decomposition makes training more stable. When using Monte Carlo estimation, the gradients can have high variance due to the randomness in sampling. The analytical form provides deterministic gradients, which leads to more stable optimization. This is particularly important because the KL term acts as a regularizer, and having stable gradients for this term helps prevent the model from either collapsing to a degenerate solution (where the KL term becomes too small) or failing to learn meaningful representations (where the KL term dominates).</p> </li> <li> <p>The decomposed form allows you to monitor reconstruction loss and KL separately which is very helpful in debugging and understanding model behavior</p> </li> </ul> <pre><code>def negative_elbo_bound(self, x):\n    \"\"\"\n    Computes the Evidence Lower Bound, KL and, Reconstruction costs\n\n    Args:\n        x: tensor: (batch, dim): Observations\n\n    Returns:\n        nelbo: tensor: (): Negative evidence lower bound\n        kl: tensor: (): ELBO KL divergence to prior\n        rec: tensor: (): ELBO Reconstruction term\n    \"\"\"\n    # Step 1: Get the parameters of the approximate posterior q_phi(z|x)\n    q_phi_z_given_x_m, q_phi_z_given_x_v = self.enc(x)\n\n    # Step 2: Compute the KL divergence term\n    # This computes D_KL(q_phi(z|x) || p_theta(z))\n    kl = ut.kl_normal(q_phi_z_given_x_m, q_phi_z_given_x_v,\n                      self.z_prior_m, self.z_prior_v)\n\n    # Step 3: Take m samples from the approximate posterior using reparameterization\n    # This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)\n    z_samples = ut.sample_gaussian(\n        q_phi_z_given_x_m.expand(x.shape[0], self.z_dim),\n        q_phi_z_given_x_v.expand(x.shape[0], self.z_dim))\n\n    # Step 4: Get the decoder outputs (logits)\n    # These parameterize the Bernoulli distributions for reconstruction\n    f_theta_of_z = self.dec(z_samples)\n\n    # Step 5: Compute the reconstruction term\n    # This computes -E_q[log p_theta(x|z)] using binary cross-entropy\n    rec = -ut.log_bernoulli_with_logits(x, f_theta_of_z)\n\n    # Step 6: Combine terms to get the negative ELBO\n    nelbo = kl + rec\n\n    # Step 7: Average over the batch\n    nelbo_avg = torch.mean(nelbo)\n    kl_avg = torch.mean(kl)\n    rec_avg = torch.mean(rec)\n\n    return nelbo_avg, kl_avg, rec_avg\n</code></pre> <p>Let's break down each step:</p> <ol> <li>Encoder Output: </li> <li>The encoder network takes input \\(x\\) and outputs the parameters of the approximate posterior \\(q_\\phi(z|x)\\)</li> <li> <p>These parameters are the mean (\\(\\mu_\\phi(x)\\)) and variance (\\(\\sigma^2_\\phi(x)\\)) of a diagonal Gaussian</p> </li> <li> <p>KL Divergence:</p> </li> <li>Computes \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\)</li> <li>For diagonal Gaussians, this has a closed-form solution</li> <li> <p>The prior \\(p_\\theta(z)\\) is typically a standard normal distribution</p> </li> <li> <p>Sampling:</p> </li> <li>Uses the reparameterization trick to sample from \\(q_\\phi(z|x)\\)</li> <li>Implements \\(z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon\\) where \\(\\varepsilon \\sim \\mathcal{N}(0,I)\\)</li> <li> <p>The samples are used to estimate the reconstruction term</p> </li> <li> <p>Decoder Output:</p> </li> <li>The decoder network takes the sampled \\(z\\) and outputs logits</li> <li> <p>These logits parameterize Bernoulli distributions for each element of \\(x\\)</p> </li> <li> <p>Reconstruction Term:</p> </li> <li>Computes \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\)</li> <li>Uses binary cross-entropy loss which takes logits directly</li> <li> <p>The sigmoid function is incorporated into the loss computation</p> </li> <li> <p>Final ELBO:</p> </li> <li>Combines the KL divergence and reconstruction terms</li> <li> <p>The negative ELBO is what we minimize during training</p> </li> <li> <p>Batch Averaging:</p> </li> <li>Averages the losses over the batch</li> <li>This gives us the final training objective</li> </ol> <p>This implementation shows how the theoretical ELBO decomposition we discussed earlier is actually computed in practice, with all the necessary components for training a VAE on binary data.</p> <p>Note on Sampling from \\(q_\\phi(z|x)\\): The sampling step in the implementation is crucial for two reasons:</p> <p>Monte Carlo Estimation: The reconstruction term \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\) involves an expectation over \\(q_\\phi(z|x)\\). We estimate this expectation using Monte Carlo sampling:</p> \\[-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] \\approx -\\frac{1}{K}\\sum_{k=1}^K \\log p_\\theta(x|z^{(k)})\\] <p>where \\(z^{(k)} \\sim q_\\phi(z|x)\\). In practice, we often use \\(K=1\\) (a single sample) as it works well and is computationally efficient.</p> <p>Gradient Estimation: We need to compute gradients of this expectation with respect to both \\(\\phi\\) (encoder parameters) and \\(\\theta\\) (decoder parameters). The reparameterization trick allows us to: - Sample from a fixed distribution \\(p(\\varepsilon)\\) that doesn't depend on \\(\\phi\\) - Transform these samples using a deterministic function that depends on \\(\\phi\\) - Backpropagate through this transformation to compute gradients - This results in lower variance gradient estimates compared to the REINFORCE trick</p> <p>The sampling step is therefore essential for both estimating the ELBO and computing its gradients during training.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#-vae","title":"\u03b2-VAE","text":"<p>A popular variation of the normal VAE is called the \u03b2-VAE. The \u03b2-VAE optimizes the following objective:</p> \\[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\beta D_{KL}(q_\\phi(z|x) || p(z)) \\] <p>Here, \u03b2 is a positive real number. From a training objective, we want to decrease the negative of ELBO, also called NELBO:</p> \\[ \\text{NELBO} = \\text{Reconstruction Loss} + \\beta D_{KL}(q_\\phi(z|x) \\| p(z)) \\] <p>We see that the second term acts as a regularization term. \u03b2 can be thought of as a hyperparameter that adjusts how much we want to regularize. Greater the \u03b2, more is the training optimized to reduce KL divergence, and a higher possibility of overfitting (and also more the \\(q_\\phi(z|x)\\) closely approximates \\(p(z)\\)). Lesser the \u03b2, optimization is geared towards increasing the KL divergence, leading to a more general model. When \u03b2 is set to 1 however, we get the standard VAE.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#importance-weighted-autoencoder-iwae","title":"Importance Weighted Autoencoder (IWAE)","text":"<p>While the ELBO serves as a lower bound to the true marginal log-likelihood, it may be loose if the variational posterior \\(q_\\phi(z|x)\\) is a poor approximation to the true posterior \\(p_\\theta(z|x)\\). The key idea behind IWAE is to use \\(m &gt; 1\\) samples from the approximate posterior \\(q_\\phi(z|x)\\) to obtain the following IWAE bound:</p> \\[ \\mathcal{L}_m(x; \\theta,\\phi) = \\mathbb{E}_{z^{(1)},...,z^{(m)} \\text{ i.i.d.} \\sim q_\\phi(z|x)} \\log \\frac{1}{m}\\sum_{i=1}^m \\frac{p_\\theta(x,z^{(i)})}{q_\\phi(z^{(i)}|x)} \\] <p>Notice that for the special case of \\(m=1\\), the IWAE objective \\(\\mathcal{L}_m\\) reduces to the standard ELBO \\(\\mathcal{L}_1 = \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\).</p> <p>As a pseudocode, the main modification to the standard VAE would be:</p> <pre><code># Step 3: Take m samples from the approximate posterior using reparameterization\n# This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)\nz_samples = ut.sample_gaussian(\n    q_phi_z_given_x_m.expand(x.shape[0], self.z_dim),\n    q_phi_z_given_x_v.expand(x.shape[0], self.z_dim))\n</code></pre>"},{"location":"ai/deep_generative_models/variational_autoencoders/#gaussian-mixture-vae-gmvae","title":"Gaussian Mixture VAE (GMVAE)","text":"<p>The VAE's prior distribution was a parameter-free isotropic Gaussian \\(p_\\theta(z) = \\mathcal{N}(z|0,I)\\). While this original setup works well, there are settings in which we desire more expressivity to better model our data. Let's look at GMVAE, which has a mixture of Gaussians as the prior distribution.</p> \\[p_\\theta(z) = \\sum_{i=1}^k \\frac{1}{k}\\mathcal{N}(z|\\mu_i, \\text{diag}(\\sigma^2_i))\\] <p>where \\(i \\in \\{1, ..., k\\}\\) denotes the \\(i\\)th cluster index. For notational simplicity, we shall subsume our mixture of Gaussian parameters \\(\\{\\mu_i, \\sigma_i\\}_{i=1}^k\\) into our generative model parameters \\(\\theta\\). For simplicity, we have also assumed fixed uniform weights \\(1/k\\) over the possible different clusters.</p> <p>Apart from the prior, the GMVAE shares an identical setup as the VAE:</p> \\[q_\\phi(z|x) = \\mathcal{N}(z|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))\\] \\[p_\\theta(x|z) = \\text{Bern}(x|f_\\theta(z))\\] <p>Although the ELBO for the GMVAE: \\(\\mathbb{E}_{q_\\phi(z)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\) is identical to that of the VAE, we note that the KL term \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\) cannot be computed analytically between a Gaussian distribution \\(q_\\phi(z|x)\\) and a mixture of Gaussians \\(p_\\theta(z)\\). However, we can obtain its unbiased estimator via Monte Carlo sampling:</p> \\[D_{KL}(q_\\phi(z|x) \\| p_\\theta(z)) \\approx \\log q_\\phi(z^{(1)}|x) - \\log p_\\theta(z^{(1)})\\] \\[= \\underbrace{\\log\\mathcal{N}(z^{(1)}|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))}_{\\text{log normal}} - \\underbrace{\\log\\sum_{i=1}^k \\frac{1}{k}\\mathcal{N}(z^{(1)}|\\mu_i, \\text{diag}(\\sigma^2_i))}_{\\text{log normal mixture}}\\] <p>where \\(z^{(1)} \\sim q_\\phi(z|x)\\) denotes a single sample.</p>"},{"location":"ai/deep_generative_models/variational_autoencoders/#the-semi-supervised-vae-ssvae","title":"The Semi-Supervised VAE (SSVAE)","text":"<p>The Semi-Supervised VAE (SSVAE) extends the standard VAE to handle both labeled and unlabeled data. In a semi-supervised setting, we have a dataset \\(\\mathcal{D}\\) that consists of: - Labeled data: \\(\\mathcal{D}_l = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N_l}\\) - Unlabeled data: \\(\\mathcal{D}_u = \\{x^{(i)}\\}_{i=1}^{N_u}\\)</p> <p>where \\(y^{(i)}\\) represents the class label for the \\(i\\)-th labeled example. The SSVAE introduces an additional latent variable \\(y\\) to model the class labels, and the joint distribution is factorized as:</p> \\[ p_\\theta(x, y, z) = p_\\theta(x|y,z)p_\\theta(y|z)p_\\theta(z) \\] <p>This factorization is derived from the chain rule of probability. We first factorize \\(p_\\theta(x, y, z)\\) as \\(p_\\theta(x|y,z)p_\\theta(y,z)\\), and then further factorize \\(p_\\theta(y,z)\\) as \\(p_\\theta(y|z)p_\\theta(z)\\). This reflects the generative process where: 1. First, we sample \\(z\\) from the prior \\(p_\\theta(z)\\) 2. Then, we sample \\(y\\) conditioned on \\(z\\) from \\(p_\\theta(y|z)\\) 3. Finally, we generate \\(x\\) conditioned on both \\(y\\) and \\(z\\) from \\(p_\\theta(x|y,z)\\)</p> <p>The approximate posterior for labeled data is:</p> \\[ q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y|x) \\] <p>This factorization is derived from the chain rule of probability for the approximate posterior. The chain rule states that for any random variables \\(A\\), \\(B\\), and \\(C\\), we can write:</p> \\[p(A,B|C) = p(A|B,C)p(B|C)\\] <p>This equation is derived from the definition of conditional probability. Let's break it down step by step:</p> <ol> <li>First, recall that conditional probability is defined as:</li> </ol> \\[p(A|B) = \\frac{p(A,B)}{p(B)}\\] <ol> <li>For our case with three variables, we can write:</li> </ol> \\[p(A,B|C) = \\frac{p(A,B,C)}{p(C)}\\] <ol> <li>We can also write:</li> </ol> \\[p(A|B,C) = \\frac{p(A,B,C)}{p(B,C)}\\] <p>and</p> \\[p(B|C) = \\frac{p(B,C)}{p(C)}\\] <ol> <li>Multiplying these last two equations:</li> </ol> \\[p(A|B,C)p(B|C) = \\frac{p(A,B,C)}{p(B,C)} \\cdot \\frac{p(B,C)}{p(C)} = \\frac{p(A,B,C)}{p(C)} = p(A,B|C)\\] <p>Therefore, we have proven that:</p> \\[p(A,B|C) = p(A|B,C)p(B|C)\\] <p>In our case, we can identify: - \\(A\\) as \\(z\\) (the latent code) - \\(B\\) as \\(y\\) (the label) - \\(C\\) as \\(x\\) (the observed data)</p> <p>Therefore, applying the chain rule:</p> \\[q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y|x)\\] <p>This means: 1. First, we predict the label \\(y\\) from \\(x\\) using \\(q_\\phi(y|x)\\) 2. Then, we infer the latent code \\(z\\) using both \\(x\\) and the predicted \\(y\\) through \\(q_\\phi(z|x,y)\\)</p> <p>and for unlabeled data:</p> \\[ q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y) \\] <p>For unlabeled data, since we don't know the true label \\(y\\), we use a prior distribution \\(q_\\phi(y)\\) (typically a uniform distribution over classes) instead of \\(q_\\phi(y|x)\\). The factorization reflects that: 1. We sample a label \\(y\\) from the prior \\(q_\\phi(y)\\) 2. Then, we infer the latent code \\(z\\) using both \\(x\\) and the sampled \\(y\\) through \\(q_\\phi(z|x,y)\\)</p> <p>The training objective for SSVAE combines: 1. The ELBO for labeled data 2. The ELBO for unlabeled data 3. A classification loss for labeled data</p> <p>This allows the model to learn both the data distribution and the class labels in a semi-supervised manner.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/","title":"Backpropagation","text":""},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#introduction","title":"Introduction","text":"<p>We are given some function \\(f(x)\\) where \\(x\\) is a vector of inputs and we are interested in computing the gradient of \\(f\\) at \\(x\\) (i.e. \\(\\nabla f(x)\\)).</p> <p>The primary reason we are interested in this problem is that in the specific case of neural networks, \\(f\\) will correspond to the loss function (\\(L\\)) and the inputs \\(x\\) will consist of the training data and the neural network weights. For example, the loss could be the SVM loss function and the inputs are both the training data \\((x_i, y_i), i=1\\ldots N\\) and the weights and biases \\(W, b\\). Note that (as is usually the case in Machine Learning) we think of the training data as given and fixed, and of the weights as variables we have control over.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#simple-expressions-and-interpretation-of-the-gradient","title":"Simple expressions and interpretation of the gradient","text":"<p>Consider a simple multiplication function of two numbers \\(f(x,y) = xy\\). It is a matter of simple calculus to derive the partial derivative for either input:</p> \\[f(x,y) = xy \\rightarrow \\frac{\\partial f}{\\partial x} = y \\quad \\frac{\\partial f}{\\partial y} = x\\] <p>A derivative at a point represents the instantaneous rate of change of a function at the given point, equivalent to the slope of the tangent line to the function's graph at that point. :</p> \\[\\frac{df(x)}{dx} = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}\\] <p>A technical note is that the division sign on the left-hand side is, unlike the division sign on the right-hand side, not a division. Instead, this notation indicates that the operator \\(\\frac{d}{dx}\\) is being applied to the function \\(f\\), and returns a different function (the derivative). A nice way to think about the expression above is that when \\(h\\) is very small, then the function is well-approximated by a straight line, and the derivative is its slope. In other words, the derivative on each variable tells you the sensitivity of the whole expression on its value. </p> <p>For example, if \\(x = 4, y = -3\\) then \\(f(x,y) = -12\\) and the derivative on \\(x\\) \\(\\frac{\\partial f}{\\partial x} = -3\\). This tells us that if we were to increase the value of this variable by a tiny amount (\\(h\\)), the effect on the whole expression would be to decrease it (due to the negative sign), and by three times that amount (\\(3h\\)). This can be seen by rearranging the above equation (\\(f(x+h) = f(x) + h\\frac{df(x)}{dx}\\)). Analogously, since \\(\\frac{\\partial f}{\\partial y} = 4\\), we expect that increasing the value of \\(y\\) by some very small amount \\(h\\) would also increase the output of the function (due to the positive sign), and by \\(4h\\).</p> <p>As mentioned, the gradient \\(\\nabla f\\) is the vector of partial derivatives, so we have that \\(\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}] = [y, x]\\). Even though the gradient is technically a vector, we will often use terms such as \"the gradient on \\(x\\)\" instead of the technically correct phrase \"the partial derivative on \\(x\\)\" for simplicity.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#compound-expressions-with-chain-rule","title":"Compound expressions with chain rule","text":"<p>Let's now start to consider more complicated expressions that involve multiple composed functions, such as \\(f(x,y,z) = (x+y)z\\). This expression is still simple enough to differentiate directly, but we'll take a particular approach to it that will be helpful with understanding the intuition behind backpropagation. In particular, note that this expression can be broken down into two expressions: \\(q = x + y\\) and \\(f = qz\\). Moreover, we know how to compute the derivatives of both expressions separately. \\(f\\) is just multiplication of \\(q\\) and \\(z\\), so \\(\\frac{\\partial f}{\\partial q} = z, \\frac{\\partial f}{\\partial z} = q\\), and \\(q\\) is addition of \\(x\\) and \\(y\\) so \\(\\frac{\\partial q}{\\partial x} = 1, \\frac{\\partial q}{\\partial y} = 1\\). However, we don't necessarily care about the gradient on the intermediate value \\(q\\) - the value of \\(\\frac{\\partial f}{\\partial q}\\) is not useful. Instead, we are ultimately interested in the gradient of \\(f\\) with respect to its inputs \\(x, y, z\\). The chain rule tells us that the correct way to \"chain\" these gradient expressions together is through multiplication. For example, \\(\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}\\). In practice this is simply a multiplication of the two numbers that hold the two gradients.</p> <p>Let's see this in action with a concrete example:</p> <pre><code># set some inputs\nx = -2; y = 5; z = -4\n\n# perform the forward pass\nq = x + y # q becomes 3\nf = q * z # f becomes -12\n\n# perform the backward pass (backpropagation) in reverse order:\n# first backprop through f = q * z\ndfdz = q # df/dz = q, so gradient on z becomes 3\ndfdq = z # df/dq = z, so gradient on q becomes -4\ndqdx = 1.0\ndqdy = 1.0\n# now backprop through q = x + y\ndfdx = dfdq * dqdx  # The multiplication here is the chain rule!\ndfdy = dfdq * dqdy  \n</code></pre> <p>Forward pass: We compute the function values step by step, storing intermediate results.</p> <p>Backward pass: We compute gradients in reverse order, using the chain rule to propagate gradients backward through the computation graph.</p> <p>We are left with the gradient in the variables <code>[dfdx, dfdy, dfdz]</code>, which tell us the sensitivity of the variables \\(x, y, z\\) on \\(f\\)! This is the simplest example of backpropagation. Going forward, we will use a more concise notation that omits the <code>df</code> prefix. For example, we will simply write <code>dq</code> instead of <code>dfdq</code>, and always assume that the gradient is computed on the final output.</p> <p>This computation can also be nicely visualized with a circuit diagram.</p> <p></p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#intuitive-understanding-of-backpropagation","title":"Intuitive understanding of backpropagation","text":"<p>Notice that backpropagation is a beautifully local process. Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the local gradient of its output with respect to its inputs. Notice that the gates can do this completely independently without being aware of any of the details of the full circuit that they are embedded in.</p> <p>Let's get an intuition for how this works by referring again to the example. The add gate received inputs \\([-2, 5]\\) and computed output \\(3\\). Since the gate is computing the addition operation, its local gradient for both of its inputs is \\(+1\\). The rest of the circuit computed the final value, which is \\(-12\\). During the backward pass in which the chain rule is applied recursively backwards through the circuit, the add gate (which is an input to the multiply gate) learns that the gradient for its output was \\(-4\\). If we anthropomorphize the circuit as wanting to output a higher value (which can help with intuition), then we can think of the circuit as \"wanting\" the output of the add gate to be lower (due to negative sign), and with a force of \\(4\\). To continue the recurrence and to chain the gradient, the add gate takes that gradient and multiplies it to all of the local gradients for its inputs (making the gradient on both \\(x\\) and \\(y\\) \\(1 \\times -4 = -4\\)). Notice that this has the desired effect: If \\(x, y\\) were to decrease (responding to their negative gradient) then the add gate's output would decrease, which in turn makes the multiply gate's output increase.</p> <p>Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#modularity-sigmoid-example","title":"Modularity: Sigmoid example","text":"<p>The gates we introduced above are relatively arbitrary. Any kind of differentiable function can act as a gate, and we can group multiple gates into a single gate, or decompose a function into multiple gates whenever it is convenient. Let's look at another expression that illustrates this point:</p> \\[f(w,x) = \\frac{1}{1 + e^{-(w_0x_0 + w_1x_1 + w_2)}}\\] <p>This expression describes a 2-dimensional neuron (with inputs \\(x\\) and weights \\(w\\)) that uses the sigmoid activation function. But for now let's think of this very simply as just a function from inputs \\(w, x\\) to a single number. The function is made up of multiple gates. In addition to the ones described already above (add, mul, max), there are four more:</p> \\[f(x) = \\frac{1}{x} \\rightarrow \\frac{df}{dx} = -\\frac{1}{x^2}\\] \\[f_c(x) = c + x \\rightarrow \\frac{df}{dx} = 1\\] \\[f(x) = e^x \\rightarrow \\frac{df}{dx} = e^x\\] \\[f_a(x) = ax \\rightarrow \\frac{df}{dx} = a\\] <p>where the functions \\(f_c, f_a\\) translate the input by a constant of \\(c\\) and scale the input by a constant of \\(a\\), respectively. These are technically special cases of addition and multiplication, but we introduce them as (new) unary gates here since we do not need the gradients for the constants \\(c, a\\). The full circuit then looks as follows.</p> <p></p> <p>In the example above, we see a long chain of function applications that operates on the result of the dot product between \\(w, x\\). The function that these operations implement is called the sigmoid function \\(\\sigma(x)\\). It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator):</p> \\[\\sigma(x) = \\frac{1}{1 + e^{-x}} \\rightarrow \\frac{d\\sigma(x)}{dx} = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\left(\\frac{1 + e^{-x} - 1}{1 + e^{-x}}\\right)\\left(\\frac{1}{1 + e^{-x}}\\right) = (1 - \\sigma(x))\\sigma(x)\\] <p>As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be \\((1 - 0.73) \\times 0.73 \\approx 0.2\\), as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Let's see the backprop for this neuron in code.</p> <pre><code>w = [2,-3,-3] # assume some random weights and data\nx = [-1, -2]\n\n# forward pass\ndot = w[0]*x[0] + w[1]*x[1] + w[2]\nf = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n\n# backward pass through the neuron (backpropagation)\nddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\ndx = [w[0] * ddot, w[1] * ddot] # backprop into x\ndw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w\n# we're done! we have the gradients on the inputs to the circuit\n</code></pre> <p>Forward pass: We compute the dot product and apply the sigmoid function.</p> <p>Backward pass: We use the simplified sigmoid gradient formula and propagate gradients back to the inputs using the chain rule.</p> <p>As shown in the code above, in practice it is always helpful to break down the forward pass into stages that are easily backpropped through. For example here we created an intermediate variable <code>dot</code> which holds the output of the dot product between <code>w</code> and <code>x</code>. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. <code>ddot</code>, and ultimately <code>dw</code>, <code>dx</code>) that hold the gradients of those variables.</p> <p>The point of this section is that the details of how the backpropagation is performed, and which parts of the forward function we think of as gates, is a matter of convenience. It helps to be aware of which parts of the expression have easy local gradients, so that they can be chained together with the least amount of code and effort.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#backprop-in-practice-staged-computation","title":"Backprop in practice: Staged computation","text":"<p>Let's see this with another example. Suppose that we have a function of the form:</p> \\[f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}\\] <p>Here is how we would structure the forward pass of such expression.</p> <pre><code>x = 3 # example values\ny = -4\n\n# forward pass\nsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)\nnum = x + sigy # numerator                               #(2)\nsigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)\nxpy = x + y                                              #(4)\nxpysqr = xpy**2                                          #(5)\nden = sigx + xpysqr # denominator                        #(6)\ninvden = 1.0 / den                                       #(7)\nf = num * invden # done!                                 #(8)\n</code></pre> <p>Notice that we have structured the code in such way that it contains multiple intermediate variables, each of which are only simple expressions for which we already know the local gradients. Therefore, computing the backprop pass is easy: We'll go backwards and for every variable along the way in the forward pass (<code>sigy</code>, <code>num</code>, <code>sigx</code>, <code>xpy</code>, <code>xpysqr</code>, <code>den</code>, <code>invden</code>) we will have the same variable, but one that begins with a <code>d</code>, which will hold the gradient of the output of the circuit with respect to that variable. Additionally, note that every single piece in our backprop will involve computing the local gradient of that expression, and chaining it with the gradient on that expression with a multiplication. For each row, we also highlight which part of the forward pass it refers to the following.</p> <pre><code># backprop f = num * invden\ndnum = invden # gradient on numerator                             #(8)\ndinvden = num                                                     #(8)\n# backprop invden = 1.0 / den \ndden = (-1.0 / (den**2)) * dinvden                                #(7)\n# backprop den = sigx + xpysqr\ndsigx = (1) * dden                                                #(6)\ndxpysqr = (1) * dden                                              #(6)\n# backprop xpysqr = xpy**2\ndxpy = (2 * xpy) * dxpysqr                                        #(5)\n# backprop xpy = x + y\ndx = (1) * dxpy                                                   #(4)\ndy = (1) * dxpy                                                   #(4)\n# backprop sigx = 1.0 / (1 + math.exp(-x))\ndx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)\n# backprop num = x + sigy\ndx += (1) * dnum                                                  #(2)\ndsigy = (1) * dnum                                                #(2)\n# backprop sigy = 1.0 / (1 + math.exp(-y))\ndy += ((1 - sigy) * sigy) * dsigy                                 #(1)\n# done! phew\n</code></pre> <p>Note: Gradients add up at forks. The forward expression involves the variables \\(x,y\\) multiple times, so when we perform backpropagation we must be careful to use <code>+=</code> instead of <code>=</code> to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add.</p> <p>Simple example: Consider \\(f(x) = x^2 + x^3\\). Here, \\(x\\) branches out to two different operations. The gradient is:</p> \\[\\frac{df}{dx} = \\frac{d}{dx}(x^2) + \\frac{d}{dx}(x^3) = 2x + 3x^2\\] <p>In backpropagation terms: if we have intermediate variables \\(a = x^2\\) and \\(b = x^3\\), then:</p> <ul> <li> <p>\\(da = 2x\\) (gradient from \\(x^2\\) path)</p> </li> <li> <p>\\(db = 3x^2\\) (gradient from \\(x^3\\) path)  </p> </li> <li> <p>\\(dx = da + db = 2x + 3x^2\\) (gradients add up!)</p> </li> </ul> <p>This is why we use <code>+=</code> instead of <code>=</code> when a variable appears in multiple places.</p> <p>Another example: Consider \\(f(x) = x^2/x\\) (which simplifies to \\(f(x) = x\\)). Here, \\(x\\) appears in both the numerator and denominator. The gradient is:</p> \\[\\frac{df}{dx} = \\frac{d}{dx}\\left(\\frac{x^2}{x}\\right) = \\frac{d}{dx}(x) = 1\\] <p>But if we think of it as \\(f(x) = x^2 \\cdot x^{-1}\\), then using the product rule:</p> \\[\\frac{df}{dx} = \\frac{d}{dx}(x^2) \\cdot x^{-1} + x^2 \\cdot \\frac{d}{dx}(x^{-1}) = 2x \\cdot \\frac{1}{x} + x^2 \\cdot \\left(-\\frac{1}{x^2}\\right) = 2 - 1 = 1\\] <p>In backpropagation terms: if we have intermediate variables \\(a = x^2\\) and \\(b = x^{-1}\\), then:</p> <ul> <li> <p>\\(da = 2x\\) (gradient from numerator path)</p> </li> <li> <p>\\(db = -x^{-2}\\) (gradient from denominator path)</p> </li> <li> <p>\\(dx = da \\cdot b + a \\cdot db = 2x \\cdot \\frac{1}{x} + x^2 \\cdot \\left(-\\frac{1}{x^2}\\right) = 1\\)</p> </li> </ul> <p>This shows how gradients from different paths combine through the chain rule.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#patterns-in-backward-flow","title":"Patterns in backward flow","text":"<p>It is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (add, mul, max), all have very simple interpretations in terms of how they act during backpropagation. Consider this example circuit.</p> <p></p> <p>Looking at the diagram above as an example, we can see that:</p> <p>The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0. In the example circuit above, note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged.</p> <p>The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values. In the example circuit above, the max operation routed the gradient of 2.00 to the z variable, which had a higher value than w, and the gradient on w remains zero.</p> <p>The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. In the example above, the gradient on x is -8.00, which is -4.00 x 2.00.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#unintuitive-effects-and-their-consequences","title":"Unintuitive effects and their consequences","text":"<p>Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted \\(w^T x_i\\) (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples \\(x_i\\) by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you'd have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#gradients-for-vectorized-operations","title":"Gradients for vectorized operations","text":"<p>One must pay closer attention to dimensions and transpose operations. Possibly the most tricky operation is the matrix-matrix multiplication (which generalizes all matrix-vector and vector-vector) multiply operations.</p> <pre><code># forward pass\nW = np.random.randn(5, 10)\nX = np.random.randn(10, 3)\nD = W.dot(X)\n\n# now suppose we had the gradient on D already calculated\ndD = np.random.randn(*D.shape) # same shape as D\ndW = dD.dot(X.T) #.T gives the transpose of the matrix\ndX = W.T.dot(dD)\n</code></pre> <p>Tip: use dimension analysis! Note that you do not need to remember the expressions for <code>dW</code> and <code>dX</code> because they are easy to re-derive based on dimensions. For instance, we know that the gradient on the weights <code>dW</code> must be of the same size as <code>W</code> after it is computed, and that it must depend on matrix multiplication of <code>X</code> and <code>dD</code> (as is the case when both <code>X,W</code> are single numbers and not matrices). There is always exactly one way of achieving this so that the dimensions work out. For example, <code>X</code> is of size [10 x 3] and <code>dD</code> of size [5 x 3], so if we want <code>dW</code> and <code>W</code> has shape [5 x 10], then the only way of achieving this is with <code>dD.dot(X.T)</code>, as shown above.</p>"},{"location":"ai/deep_learning_for_computer_vision/backpropagation/#additional-references","title":"Additional References","text":"<ul> <li>Automatic differentiation in machine learning: a survey</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/","title":"Image Classification with Linear Classifiers","text":""},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#the-image-classification-task","title":"The Image Classification Task","text":"<p>Image classification is a fundamental computer vision task where, given an image and a set of labels, the goal is to assign the image to one of the labels. This is essentially a supervised learning problem where we want to learn a mapping from input images to discrete class labels.</p> <p>Formally given a dataset \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n\\) where:</p> <ul> <li> <p>\\(x_i\\) is an input image (typically represented as a high-dimensional vector of pixel values)</p> </li> <li> <p>\\(y_i\\) is the corresponding class label from a predefined set of categories \\(\\{1, 2, \\ldots, C\\}\\)</p> </li> <li> <p>The task is to learn a function \\(f: \\mathbb{R}^d \\rightarrow \\{1, 2, \\ldots, C\\}\\) that can accurately predict the class label for new, unseen images</p> </li> </ul> <p>Key challenges:</p> <ul> <li> <p>High dimensionality: Images are typically represented as very high-dimensional vectors (e.g., a 224\u00d7224 RGB image has 150,528 dimensions)</p> </li> <li> <p>Variability: The same object can appear in different poses, lighting conditions, scales, backgrounds, camera movements, with background clutter, at different scales (zoom) within the image, with partial occlusion, deformation, and varying contextual information</p> </li> <li> <p>Intra-class variation: Objects within the same class can look very different</p> </li> <li> <p>Inter-class similarity: Objects from different classes can sometimes look very similar</p> </li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#machine-learning-data-driven-approach","title":"Machine Learning: Data-Driven Approach","text":"<p>The machine learning approach to image classification follows a systematic, data-driven methodology that can be broken down into three main steps:</p> <p>1. Collect a Dataset of Images and Labels: The first step involves gathering a comprehensive dataset where each image is paired with its corresponding class label.</p> <p>2. Use ML Algorithms to Train a Classifier: Once we have the dataset, we employ machine learning algorithms like Linear Regression, Support Vector Machines (SVM), or Logistic Regression to learn a mapping from images to class labels. The choice of algorithm depends on the complexity of the problem, dataset size, and computational constraints. </p> <p>3. Evaluate the Classifier on New Images: The final step is to assess how well the trained classifier performs on previously unseen images. This evaluation process includes measuring accuracy, precision, recall, and F1-score on the held-out test set. Also includes validation techniques like k-fold cross-validation to get robust performance estimates.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#k-nearest-neighbors-k-nn-algorithm","title":"k-Nearest Neighbors (k-NN) Algorithm","text":"<p>The k-Nearest Neighbors algorithm is one of the simplest and most intuitive machine learning algorithms for classification. It's a non-parametric, instance-based learning method that makes predictions based on the similarity of new examples to previously seen training examples.</p> <p>Training Phase: k-NN is a \"lazy learner\" - it doesn't actually learn a model during training. Instead, it simply stores all the training examples and their labels.</p> <p>Prediction Phase: For a new test image, k-NN:</p> <ol> <li> <p>Computes the distance between the test image and all training images</p> </li> <li> <p>Identifies the k nearest training examples (neighbors)</p> </li> <li> <p>Assigns the class label that appears most frequently among these k neighbors</p> </li> </ol> <p>The choice of distance metric is crucial for k-NN performance:</p> <ul> <li> <p>Euclidean Distance: \\(d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\\)</p> </li> <li> <p>Manhattan Distance: \\(d(x, y) = \\sum_{i=1}^{n}|x_i - y_i|\\)</p> </li> <li> <p>Cosine Similarity: \\(d(x, y) = 1 - \\frac{x \\cdot y}{||x|| \\cdot ||y||}\\)</p> </li> </ul> <p>Understanding the computational efficiency of k-NN requires analyzing its time complexity using Big O notation.</p> <p>Big O Notation: \\(O(f(n))\\) describes how the runtime of an algorithm grows as the input size \\(n\\) increases. It provides an upper bound on the worst-case performance, focusing on the dominant term and ignoring constants and lower-order terms.</p> <p>k-NN Time Complexity:</p> <ul> <li> <p>Training Time: \\(O(1)\\) - k-NN doesn't perform any computation during training; it simply stores the training data</p> </li> <li> <p>Prediction Time: \\(O(N)\\) - For each prediction, k-NN must compute distances to all \\(N\\) training examples</p> </li> </ul> <p>In real-world applications, we typically want classifiers that are fast at prediction and can tolerate slow training because:</p> <ol> <li>Training happens once: We train the model offline, often overnight or over days, so training time is less critical</li> <li>Prediction happens repeatedly: Once deployed, the model makes thousands or millions of predictions per day</li> <li>Real-time requirements: Many applications (autonomous vehicles, medical diagnosis, security systems) need immediate predictions</li> <li>Scalability: As the dataset grows, k-NN prediction time grows linearly, making it impractical for large-scale systems</li> </ol> <p>This is why we often prefer parametric models (like linear classifiers) that invest computational effort upfront during training to enable fast predictions later.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#linear-classifiers","title":"Linear Classifiers","text":"<p>kNN has a number of disadvantages:</p> <ul> <li>The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.</li> <li>Classifying a test image is expensive since it requires a comparison to all training images.</li> </ul> <p>We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to Neural Networks. The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#parameterized-mapping-from-images-to-label-scores","title":"Parameterized mapping from images to label scores","text":"<p>The first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class. We will develop the approach with a concrete example. Let's assume a training dataset of images \\(x_i \\in \\mathbb{R}^D\\), each associated with a label \\(y_i\\). Here \\(i = 1 \\ldots N\\) and \\(y_i \\in 1 \\ldots K\\). That is, we have \\(N\\) examples (each with a dimensionality \\(D\\)) and \\(K\\) distinct categories. For example, in CIFAR-10 we have a training set of \\(N = 50,000\\) images, each with \\(D = 32 \\times 32 \\times 3 = 3072\\) pixels, and \\(K = 10\\), since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function \\(f: \\mathbb{R}^D \\mapsto \\mathbb{R}^K\\) that maps the raw image pixels to class scores.</p> <p>We will start out with arguably the simplest possible function, a linear mapping:</p> \\[f(x_i, W, b) = Wx_i + b\\] <p>In the above equation, we are assuming that the image \\(x_i\\) has all of its pixels flattened out to a single column vector of shape \\([D \\times 1]\\). The matrix \\(W\\) (of size \\([K \\times D]\\)), and the vector \\(b\\) (of size \\([K \\times 1]\\)) are the parameters of the function. In CIFAR-10, \\(x_i\\) contains all pixels in the \\(i\\)-th image flattened into a single \\([3072 \\times 1]\\) column, \\(W\\) is \\([10 \\times 3072]\\) and \\(b\\) is \\([10 \\times 1]\\), so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in \\(W\\) are often called the weights, and \\(b\\) is called the bias vector because it influences the output scores, but without interacting with the actual data \\(x_i\\).</p> <p>There are a few things to note:</p> <ol> <li> <p>First, note that the single matrix multiplication \\(Wx_i\\) is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of \\(W\\).</p> </li> <li> <p>Notice also that we think of the input data \\((x_i, y_i)\\) as given and fixed, but we have control over the setting of the parameters \\(W, b\\). Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set. We will go into much more detail about how this is done, but intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes.</p> </li> <li> <p>An advantage of this approach is that the training data is used to learn the parameters \\(W, b\\), but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores.</p> </li> <li> <p>Lastly, note that classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images.</p> </li> </ol> <p>Geometric Interpretation</p> <p>To understand linear classifiers geometrically, let's first consider the general form of a plane in \u211d\u00b3:</p> \\[ax + by + cz = d\\] <p>where \\((a, b, c)\\) is the normal vector to the plane and \\(d\\) determines the plane's position in space.</p> <ul> <li> <p>The normal vector \\((a, b, c)\\) is perpendicular to the plane</p> </li> <li> <p>The plane divides 3D space into two half-spaces</p> </li> <li> <p>Points on one side of the plane satisfy \\(ax + by + cz &gt; d\\)</p> </li> <li> <p>Points on the other side satisfy \\(ax + by + cz &lt; d\\)</p> </li> <li> <p>Points on the plane satisfy \\(ax + by + cz = d\\)</p> </li> </ul> <p>Example: The plane \\(2x - 3y + 4z = 12\\) has normal vector \\((2, -3, 4)\\).</p> <p>Now, let's see how this relates to linear classifiers. In a binary classification problem, our linear classifier computes:</p> \\[f(x) = w^T x + b\\] <p>where \\(w\\) is the weight vector and \\(b\\) is the bias.</p> <p>The Decision Boundary: The equation \\(w^T x + b = 0\\) defines a hyperplane in the input space that separates the two classes. This is exactly analogous to the plane equation \\(ax + by + cz = d\\). This is how:</p> <ul> <li> <p>\\(w\\) is the normal vector to the hyperplane (just like \\((a, b, c)\\) in the plane equation)</p> </li> <li> <p>\\(b\\) determines the position of the hyperplane (just like \\(d\\) in the plane equation)</p> </li> <li> <p>The hyperplane divides the input space into two half-spaces</p> </li> <li> <p>Points in one half-space are classified as class 1 (\\(w^T x + b &gt; 0\\))</p> </li> <li> <p>Points in the other half-space are classified as class 2 (\\(w^T x + b &lt; 0\\))</p> </li> </ul> <p>Multi-class extension: For \\(K\\) classes, we have \\(K\\) hyperplanes, each defined by a row of the weight matrix \\(W\\). </p> <p>In the multi-class case, we have \\(K\\) score functions:</p> \\[f_1(x) = w_1^T x + b_1\\] \\[f_2(x) = w_2^T x + b_2\\] \\[\\vdots\\] \\[f_K(x) = w_K^T x + b_K\\] <p>The classifier predicts class \\(i\\) if \\(f_i(x) &gt; f_j(x)\\) for all \\(j \\neq i\\).</p> <p>The decision boundary between classes \\(i\\) and \\(j\\) is the set of points where the classifier is indifferent between the two classes, i.e., where \\(f_i(x) = f_j(x)\\).</p> <p>Starting with:</p> \\[f_i(x) = f_j(x)\\] <p>Substituting the score functions:</p> \\[w_i^T x + b_i = w_j^T x + b_j\\] <p>Rearranging terms:</p> \\[w_i^T x - w_j^T x = b_j - b_i\\] <p>Factoring out \\(x\\):</p> \\[(w_i - w_j)^T x = b_j - b_i\\] <p>Moving all terms to one side:</p> \\[(w_i - w_j)^T x + (b_i - b_j) = 0\\] <p>The decision boundary between classes \\(i\\) and \\(j\\) is the hyperplane:</p> \\[(w_i - w_j)^T x + (b_i - b_j) = 0\\] <p>Note:</p> <ul> <li> <p>The normal vector to this decision boundary is \\((w_i - w_j)\\)</p> </li> <li> <p>The bias term is \\((b_i - b_j)\\)</p> </li> <li> <p>Points on one side of this hyperplane are classified as class \\(i\\)</p> </li> <li> <p>Points on the other side are classified as class \\(j\\)</p> </li> <li> <p>Points on the hyperplane are exactly at the decision boundary</p> </li> </ul> <p>Why this matters: This geometric interpretation helps us understand that:</p> <ol> <li> <p>Linear classifiers create linear decision boundaries</p> </li> <li> <p>The weight vector \\(w\\) determines the orientation of the decision boundary</p> </li> <li> <p>The bias \\(b\\) determines the position of the decision boundary</p> </li> <li> <p>The classifier's performance depends on how well the data can be separated by these linear boundaries</p> </li> </ol> <p>This connection between planes in \u211d\u00b3 and hyperplanes in linear classifiers provides an intuitive way to visualize and understand how linear classifiers work geometrically.</p> <p>Coming back to CIFAR-10, we cannot visualize 3072-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing.</p> <p></p> <p>Above figure shows a cartoon representation of the image space, where each image is a single point, and three classifiers are visualized. For example, take the car classifier (in red), where the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores.</p> <p>As we saw above, every row of \\(W\\) is a classifier for one of the classes. The geometric interpretation of these numbers is that as we change one of the rows of \\(W\\), the corresponding line in the pixel space will rotate in different directions. The biases \\(b\\), on the other hand, allow our classifiers to translate the lines.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#template-matching-interpretation","title":"Template Matching interpretation","text":"<p>Another interpretation for the weights \\(W\\) is that each row of \\(W\\) corresponds to a template (or sometimes also called a prototype) for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using an inner product (or dot product) one by one to find the one that \"fits\" best. With this terminology, the linear classifier is doing template matching, where the templates are learned. Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#bias-trick","title":"Bias Trick","text":"<p>Recall that we defined the score function as:</p> \\[f(x_i, W, b) = Wx_i + b\\] <p>It is a little cumbersome to keep track of two sets of parameters (the biases \\(b\\) and weights \\(W\\)) separately. A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector \\(x_i\\) with one additional dimension that always holds the constant 1- a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply:</p> \\[f(x_i, W) = Wx_i\\] <p>With our CIFAR-10 example, \\(x_i\\) is now \\([3073 \\times 1]\\) instead of \\([3072 \\times 1]\\) - (with the extra dimension holding the constant 1), and \\(W\\) is now \\([10 \\times 3073]\\) instead of \\([10 \\times 3072]\\). The extra column that \\(W\\) now corresponds to the bias \\(b\\). An illustration might help clarify this transformation.</p> <p></p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#loss-function","title":"Loss Function","text":"<p>We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if we\u2019re doing a poor job of classifying the training data, and it will be low if we\u2019re doing well.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#multiclass-support-vector-machine-loss","title":"Multiclass Support Vector Machine loss","text":"<p>The SVM loss is set up so that the SVM \"wants\" the correct class for each image to have a score higher than the incorrect classes by some fixed margin \\(\\Delta\\).</p> <p>Recall that for the \\(i\\)-th example we are given the pixels of image \\(x_i\\) and the label \\(y_i\\) that specifies the index of the correct class. The score function takes the pixels and computes the vector \\(f(x_i,W)\\) of class scores, which we will abbreviate to \\(s\\) (short for scores). For example, the score for the \\(j\\)-th class is the \\(j\\)-th element: \\(s_j = f(x_i,W)_j\\). The Multiclass SVM loss for the \\(i\\)-th example is then formalized as follows:</p> \\[L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)\\] <p>Example. Let's unpack this with an example to see how it works. Suppose that we have three classes that receive the scores \\(s = [13, -7, 11]\\), and that the first class is the true class (i.e. \\(y_i = 0\\)). Also assume that \\(\\Delta\\) (a hyperparameter we will go into more detail about soon) is 10. The expression above sums over all incorrect classes (\\(j \\neq y_i\\)), so we get two terms:</p> \\[L_i = \\max(0, -7 - 13 + 10) + \\max(0, 11 - 13 + 10)\\] <p>You can see that the first term gives zero since \\([-7 - 13 + 10]\\) gives a negative number, which is then thresholded to zero with the \\(\\max(0, -)\\) function. We get zero loss for this pair because the correct class score (13) was greater than the incorrect class score (-7) by at least the margin 10. In fact the difference was 20, which is much greater than 10 but the SVM only cares that the difference is at least 10; any additional difference above the margin is clamped at zero with the max operation. The second term computes \\([11 - 13 + 10]\\) which gives 8. That is, even though the correct class had a higher score than the incorrect class (13 &gt; 11), it was not greater by the desired margin of 10. The difference was only 2, which is why the loss comes out to 8 (i.e. how much higher the difference would have to be to meet the margin). In summary, the SVM loss function wants the score of the correct class \\(y_i\\) to be larger than the incorrect class scores by at least by \\(\\Delta\\) (delta). If this is not the case, we will accumulate loss.</p> <p></p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#softmax-classifier","title":"Softmax Classifier","text":"<p>Unlike the SVM which treats the outputs \\(f(x_i,W)\\) as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping \\(f(x_i;W) = Wx_i\\) stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss. The function \\(f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\\) is called the softmax function: it takes a vector of arbitrary real-valued scores (in \\(z\\)) and squashes it to a vector of values between zero and one that sum to one.</p> <p>The cross-entropy between a \"true\" distribution \\(p\\) and an estimated distribution \\(q\\) is defined as:</p> \\[H(p, q) = -\\sum_x p(x) \\log q(x)\\] <p>Note: This derivation is for the \\(i\\)-th sample, where \\(y_i\\) is the correct class label for that sample.</p> <p>For the Softmax classifier, we have:</p> <ul> <li> <p>True distribution: \\(p_i = [0, \\ldots, 1, \\ldots, 0]\\) (one-hot vector with 1 at position \\(y_i\\))</p> </li> <li> <p>Estimated distribution: \\(q_{i,j} = \\frac{e^{f_j}}{\\sum_k e^{f_k}}\\) (softmax probabilities)</p> </li> </ul> <p>Substituting into the cross-entropy formula:</p> \\[H(p_i,q_i) = -\\sum_{j=1}^{K} p_{i,j} \\log q_{i,j}\\] <p>Since \\(p_i\\) is a one-hot vector, only \\(p_{i,y_i} = 1\\) and all other \\(p_{i,j} = 0\\):</p> \\[H(p_i,q_i) = -p_{i,y_i} \\log q_{i,y_i} - \\sum_{j \\neq y_i} p_{i,j} \\log q_{i,j} = -1 \\cdot \\log q_{i,y_i} - \\sum_{j \\neq y_i} 0 \\cdot \\log q_{i,j}\\] \\[H(p_i,q_i) = -\\log q_{i,y_i} = -\\log\\left(\\frac{e^{f_{y_i}}}{\\sum_k e^{f_k}}\\right) = L_i\\] \\[L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\\right) \\quad \\text{or equivalently} \\quad L_i = -f_{y_i} + \\log\\sum_j e^{f_j}\\] <p>where we are using the notation \\(f_j\\) to mean the \\(j\\)-th element of the vector of class scores \\(f\\). As before, the full loss for the dataset is the mean of \\(L_i\\) over all training examples together with a regularization term \\(R(W)\\). The function \\(f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\\) is called the softmax function: it takes a vector of arbitrary real-valued scores (in \\(z\\)) and squashes it to a vector of values between zero and one that sum to one.</p> <p>Therefore, the cross-entropy between the true one-hot distribution and the estimated softmax distribution is exactly the Softmax loss \\(L_i\\).</p> <p>The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities (\\(q_{i,j} = e^{f_{y_i}}/\\sum_j e^{f_j}\\) as seen above) and the \"true\" distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. \\(p_i = [0, \\ldots, 1, \\ldots, 0]\\) contains a single 1 at the \\(y_i\\)-th position.).</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#the-kl-divergence-connection","title":"The KL Divergence connection","text":"<p>Before diving into KL divergence, let's understand entropy. The entropy \\(H(p)\\) of a distribution \\(p\\) measures the average amount of information (or uncertainty) in the distribution:</p> \\[H(p) = -\\sum_x p_{x} \\log p_{x}\\] <ul> <li> <p>High entropy: Distribution is spread out, high uncertainty (e.g., uniform distribution)</p> </li> <li> <p>Low entropy: Distribution is concentrated, low uncertainty (e.g., one-hot distribution)</p> </li> </ul> <p>Note: This derivation is for the \\(i\\)-th sample, where \\(y_i\\) is the correct class label for that sample.</p> <p>For our one-hot true distribution \\(p_i\\) where \\(p_{i,y_i} = 1\\) and \\(p_{i,j} = 0\\) for \\(j \\neq y_i\\):</p> \\[H(p_i) = -p_{i,y_i} \\log p_{i,y_i} - \\sum_{j \\neq y_i} p_{i,j} \\log p_{i,j} = -1 \\cdot \\log 1 - \\sum_{j \\neq y_i} 0 \\cdot \\log 0 = 0\\] <p>The entropy is zero because there's no uncertainty - we know exactly which class is correct.</p> <p>The KL divergence \\(D_{KL}(p||q)\\) measures how much information is lost when we use distribution \\(q\\) to approximate distribution \\(p\\). It's defined as:</p> \\[D_{KL}(p||q) = \\sum_x p_{x} \\log\\left(\\frac{p_{x}}{q_{x}}\\right)\\] <p>Note: This derivation is for the \\(i\\)-th sample, where \\(y_i\\) is the correct class label for that sample.</p> <p>For our one-hot true distribution \\(p_i\\) where \\(p_{i,y_i} = 1\\) and \\(p_{i,j} = 0\\) for \\(j \\neq y_i\\):</p> \\[D_{KL}(p_i||q_i) = p_{i,y_i} \\log\\left(\\frac{p_{i,y_i}}{q_{i,y_i}}\\right) + \\sum_{j \\neq y_i} p_{i,j} \\log\\left(\\frac{p_{i,j}}{q_{i,j}}\\right)\\] \\[D_{KL}(p_i||q_i) = 1 \\cdot \\log\\left(\\frac{1}{q_{i,y_i}}\\right) + \\sum_{j \\neq y_i} 0 \\cdot \\log\\left(\\frac{0}{q_{i,j}}\\right) = -\\log q_{i,y_i}\\] <p>Since \\(H(p_i) = 0\\) (the entropy of a deterministic distribution is zero), we have:</p> \\[H(p_i,q_i) = H(p_i) + D_{KL}(p_i||q_i) = 0 + (-\\log q_{i,y_i}) = -\\log q_{i,y_i} = L_i\\] <p>This shows that minimizing the cross-entropy loss is exactly equivalent to minimizing the KL divergence between the true one-hot distribution and the predicted softmax distribution. The KL divergence is zero only when \\(q_{i,y_i} = 1\\) (perfect confidence in the correct class), and it increases as the predicted probability of the correct class decreases.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#practical-issues-numeric-stability","title":"Practical issues: numeric stability","text":"<p>When you're writing code for computing the Softmax function in practice, the intermediate terms \\(e^{f_{y_i}}\\) and \\(\\sum_j e^{f_j}\\) may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant \\(C\\) and push it into the sum, we get the following (mathematically equivalent) expression:</p> \\[\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} = \\frac{C e^{f_{y_i}}}{C \\sum_j e^{f_j}} = \\frac{e^{f_{y_i} + \\log C}}{\\sum_j e^{f_j + \\log C}}\\] <p>We are free to choose the value of \\(C\\). This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for \\(C\\) is to set \\(\\log C = -\\max_j f_j\\). This simply states that we should shift the values inside the vector \\(f\\) so that the highest value is zero.</p> <p>Example code:</p> <pre><code>f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\np = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n\n# instead: first shift the values of f so that the highest number is 0:\nf -= np.max(f) # f becomes [-666, -333, 0]\np = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#svm-vs-softmax","title":"SVM vs. Softmax","text":"<p>A picture might help clarify the distinction between the Softmax and SVM classifiers using an example.</p> <p></p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#final-notes-on-terminology","title":"Final notes on terminology","text":"<p>1. Softmax</p> <p>The softmax function is not a classifier itself, but it is an essential component of a classifier. The term \"softmax classifier\" is often used informally to describe a multi-class classification model that uses the softmax function in its final layer. However, the actual classification work\u2014the process of learning to predict the correct classes\u2014is done by the entire model, not just the softmax layer.</p> <p>2. SVM</p> <p>A \"linear classifier + SVM loss function\" is a Linear SVM. This is not a new or different kind of model; it is simply a Support Vector Machine that uses a linear function to find the optimal decision boundary.</p> <p>The key components of a Linear SVM are:</p> <ul> <li> <p>Linear decision function: \\(f(x) = Wx + b\\)</p> </li> <li> <p>Hinge loss: \\(L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)\\)</p> </li> <li> <p>Margin maximization: The SVM tries to maximize the margin between classes</p> </li> </ul> <p>The term \"Linear SVM\" emphasizes that the decision boundary is linear (a hyperplane), as opposed to kernel SVMs that can have non-linear decision boundaries through the kernel trick.</p>"},{"location":"ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/#additional-references","title":"Additional References","text":"<p>Here are some (optional) links you may find interesting for further reading:</p> <p>A Few Useful Things to Know about Machine Learning, where especially section 6 is related but the whole paper is a warmly recommended reading.</p> <p>Recognizing and Learning Object Categories, a short course of object categorization at ICCV 2005.</p> <p>Deep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.</p>"},{"location":"ai/deep_learning_for_computer_vision/introduction/","title":"Introduction","text":"<p>Machine Learning is a subset of artificial intelligence that focuses on algorithms and statistical models that enable computer systems to improve their performance on a specific task through experience, without being explicitly programmed for every scenario. At its core, ML is about finding patterns in data and using those patterns to make predictions or decisions. The field encompasses supervised learning (learning from labeled examples), unsupervised learning (finding hidden patterns in unlabeled data), and reinforcement learning (learning through interaction with an environment).</p> <p>Deep Learning is a specialized branch of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to model and understand complex patterns in data. These networks are inspired by the structure and function of the human brain, with interconnected nodes (neurons) that process information through weighted connections. Deep learning excels at tasks involving high-dimensional data like images, audio, and text, where traditional machine learning methods often struggle. Key architectures include convolutional neural networks (CNNs) for computer vision, recurrent neural networks (RNNs) and transformers for natural language processing, and generative adversarial networks (GANs) for creating synthetic data. The success of deep learning is largely due to the availability of large datasets, powerful computing resources (especially GPUs), and sophisticated optimization techniques like backpropagation. Deep learning has revolutionized fields like computer vision, natural language processing, speech recognition, and has enabled breakthroughs in areas ranging from protein folding prediction to creative AI applications.</p>"},{"location":"ai/deep_learning_for_computer_vision/introduction/#hubel-and-wiesels-experiments","title":"Hubel and Wiesel's Experiments","text":"<p>The foundation of modern computer vision and convolutional neural networks can be traced back to the groundbreaking work of David Hubel and Torsten Wiesel in the 1950s and 1960s. Their experiments on the visual cortex of cats and monkeys revealed fundamental principles about how the brain processes visual information.</p> <p>Hubel and Wiesel inserted microelectrodes into the primary visual cortex (V1) of cats and monkeys to record the electrical activity of individual neurons while presenting various visual stimuli. They discovered that neurons in the visual cortex respond selectively to specific features rather than responding to all visual input uniformly.</p> <p>Simple Cells: These neurons respond to oriented edges and lines at specific locations in the visual field. Each simple cell has a preferred orientation (e.g., horizontal, vertical, or diagonal) and responds most strongly when a line or edge of that orientation is presented at a particular location.</p> <p>Complex Cells: These neurons also respond to oriented edges but are less sensitive to the exact position of the stimulus. They maintain their response even when the stimulus is shifted slightly within their receptive field, making them more invariant to position.</p> <p>Hierarchical Organization: Hubel and Wiesel discovered that the visual cortex is organized hierarchically, with simple cells feeding into complex cells, which in turn feed into even more complex feature detectors. This hierarchical processing allows the brain to build increasingly complex representations from simple visual features.</p> <p>The experiments revealed several fundamental principles that directly inspired the design of convolutional neural networks:</p> <ol> <li> <p>Local Receptive Fields: Neurons respond to stimuli in small, localized regions of the visual field, not the entire image.</p> </li> <li> <p>Feature Detection: The visual system detects specific features (edges, orientations) rather than processing raw pixel values.</p> </li> <li> <p>Hierarchical Processing: Complex visual patterns are built up from simpler features through multiple layers of processing.</p> </li> <li> <p>Translation Invariance: Higher-level neurons become increasingly invariant to the exact position of features.</p> </li> <li> <p>Shared Weights: Similar feature detectors are replicated across different spatial locations.</p> </li> </ol> <p>These insights provided the biological inspiration for convolutional neural networks, where convolutional layers detect local features (like edges), pooling layers provide translation invariance, and multiple layers build up hierarchical representations of increasing complexity. Hubel and Wiesel's work earned them the Nobel Prize in Physiology or Medicine in 1981 and remains foundational to our understanding of both biological and artificial vision systems.</p>"},{"location":"ai/deep_learning_for_computer_vision/introduction/#what-solving-computer-vision-means","title":"What solving Computer Vision means","text":"<p>Computer vision aims to enable machines to interpret and understand visual information from the world, essentially replicating the human ability to \"see\" and make sense of visual data.</p> <p>Core Tasks: Object detection and recognition, scene understanding, motion analysis, 3D reconstruction, and visual reasoning. The ultimate goal is to extract meaningful information from images and videos that can be used for decision-making, navigation, interaction, and understanding.</p>"},{"location":"ai/deep_learning_for_computer_vision/introduction/#how-nature-has-solved-vision","title":"How Nature has solved Vision","text":"<p>Compound Eyes (Insects): Insects like bees and dragonflies have compound eyes composed of thousands of individual photoreceptor units (ommatidia). Each unit captures light from a small portion of the visual field, creating a mosaic image. This design provides excellent motion detection and wide field of view, crucial for navigation and predator avoidance. The honeybee's visual system can detect polarized light, enabling navigation using the sun's position even on cloudy days.</p> <p>Camera Eyes (Vertebrates): Most vertebrates, including humans, have camera-like eyes with a single lens focusing light onto a retina. The retina contains specialized photoreceptor cells (rods for low-light vision, cones for color vision) and complex neural processing layers. This design provides high resolution and excellent color discrimination, enabling detailed object recognition and complex visual tasks.</p> <p>Specialized Vision Systems: Different species have evolved unique adaptations. Mantis shrimp have 12-16 different photoreceptor types (compared to humans' 3), enabling them to see a vast spectrum of colors including ultraviolet and polarized light. Birds of prey have exceptional visual acuity - eagles can spot prey from kilometers away. Nocturnal animals like cats have enhanced low-light vision with reflective tapetum layers.</p> <p>Neural Processing: All these systems share common principles: hierarchical feature detection (from simple edges to complex objects), parallel processing of different visual attributes (motion, color, depth), and extensive neural plasticity allowing adaptation to environmental changes. The visual cortex processes information in specialized regions - V1 for basic features, V2-V4 for intermediate processing, and higher areas for object recognition and scene understanding.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/","title":"Neural Networks: Setting up the Architecture","text":""},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#modeling-one-neuron","title":"Modeling one neuron","text":"<p>The area of Neural Networks has originally been primarily inspired by the goal of modeling biological neural systems, but has since diverged and become a matter of engineering and achieving good results in Machine Learning tasks. Nonetheless, we begin our discussion with a very brief and high-level description of the biological system that a large portion of this area has been inspired by.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#biological-motivation-and-connections","title":"Biological motivation and connections","text":"<p>The basic computational unit of the brain is a neuron. Approximately 86 billion neurons can be found in the human nervous system and they are connected with approximately 10^14 - 10^15 synapses. Each neuron receives input signals from its dendrites and produces output signals along its (single) axon. The axon eventually branches out and connects via synapses to dendrites of other neurons. In the computational model of a neuron, the signals that travel along the axons (e.g. \\(x_0\\)) interact multiplicatively (e.g. \\(w_0 x_0\\)) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. \\(w_0\\)). The idea is that the synaptic strengths (the weights \\(w\\)) are learnable and control the strength of influence (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another. In the basic model, the dendrites carry the signal to the cell body where they all get summed. If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon. In the computational model, we assume that the precise timings of the spikes do not matter, and that only the frequency of the firing communicates information. Based on this rate code interpretation, we model the firing rate of the neuron with an activation function \\(f\\), which represents the frequency of the spikes along the axon. Historically, a common choice of activation function is the sigmoid function \\(\\sigma\\), since it takes a real-valued input (the signal strength after the sum) and squashes it to range between 0 and 1.</p> <p> </p> <p>An example code for forward-propagating a single neuron might look as follows:</p> <pre><code>class Neuron(object):\n  # ... \n  def forward(self, inputs):\n    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n    cell_body_sum = np.sum(inputs * self.weights) + self.bias\n    firing_rate = 1.0 / (1 + math.exp(-cell_body_sum)) # sigmoid activation function\n    return firing_rate\n</code></pre> <p>In other words, each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity (or activation function), in this case the sigmoid \\(\\sigma(x) = 1/(1+e^{-x})\\).</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#single-neuron-as-a-linear-classifier","title":"Single neuron as a linear classifier","text":"<p>The mathematical form of the model Neuron's forward computation might look familiar to you. As we saw with linear classifiers, a neuron has the capacity to \"like\" (activation near one) or \"dislike\" (activation near zero) certain linear regions of its input space. Hence, with an appropriate loss function on the neuron's output, we can turn a single neuron into a linear classifier:</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#binary-softmax-classifier","title":"Binary Softmax classifier","text":"<p>For example, we can interpret \\(\\sigma(\\sum_iw_ix_i + b)\\) to be the probability of one of the classes \\(P(y_i = 1 \\mid x_i; w)\\). The probability of the other class would be \\(P(y_i = 0 \\mid x_i; w) = 1 - P(y_i = 1 \\mid x_i; w)\\), since they must sum to one. With this interpretation, we can formulate the cross-entropy loss, and optimizing it would lead to a binary Softmax classifier (also known as logistic regression). Since the sigmoid function is restricted to be between 0-1, the predictions of this classifier are based on whether the output of the neuron is greater than 0.5.</p> <p>A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers)</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#commonly-used-activation-functions","title":"Commonly used activation functions","text":"<p>Every activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it.</p> <p> </p> <p>Top: Sigmoid non-linearity squashes real numbers to range between [0,1] Bottom: The tanh non-linearity squashes real numbers to range between [-1,1].</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#sigmoid","title":"Sigmoid","text":"<p>The sigmoid non-linearity has the mathematical form \\(\\sigma(x) = 1 / (1 + e^{-x})\\) and takes a real-valued number and \"squashes\" it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1. The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). In practice, the sigmoid non-linearity has fallen out of favor and it is rarely ever used.</p> <p>The Saturation Problem: The most critical issue with sigmoid neurons is that they suffer from gradient saturation. When the neuron's activation saturates at either tail of 0 or 1, the gradient at these regions becomes almost zero. The derivative of the sigmoid function is:</p> \\[\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x))\\] <p>Notice that when \\(\\sigma(x) \\approx 0\\) (saturated at low values) or \\(\\sigma(x) \\approx 1\\) (saturated at high values), the derivative approaches zero. The consequence of this can be shown with an example below.</p> <p>Example: Consider a simple neural network with one sigmoid neuron:</p> <p>Forward pass: Input \\(x = 5\\), weight \\(w = 2\\), bias \\(b = 0\\)</p> <ul> <li> <p>Pre-activation: \\(z = wx + b = 2 \\times 5 + 0 = 10\\)</p> </li> <li> <p>Sigmoid output: \\(\\sigma(10) = \\frac{1}{1 + e^{-10}} \\approx 0.99995\\) (saturated!)</p> </li> </ul> <p>Backward pass: Suppose the gradient from the output layer is \\(d_{out} = 0.1\\)</p> <ul> <li> <p>Local gradient: \\(\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1 - \\sigma(z)) = 0.99995 \\times (1 - 0.99995) \\approx 0.00005\\)</p> </li> <li> <p>Gradient through linear layer: \\(\\frac{dz}{dw} = \\frac{d(wx + b)}{dw} = x = 5\\)</p> </li> <li> <p>Gradient to weight: \\(\\frac{dL}{dw} = d_{out} \\times \\frac{d\\sigma(z)}{dz} \\times \\frac{dz}{dw} = 0.1 \\times 0.00005 \\times 5 = 0.000025\\)</p> </li> </ul> <p>The gradient is now 4000 times smaller than the original error signal! This means the weight will barely update, making learning extremely slow or impossible.</p> <p>In deep networks, this gradient vanishing effect compounds across layers. If multiple sigmoid neurons are saturated, the gradients can become so small that they're lost to numerical precision, effectively stopping learning entirely. This is why sigmoid activations are rarely used in modern deep learning architectures.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#tanh","title":"Tanh","text":"<p>The tanh non-linearity squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: \\(\\tanh(x) = 2 \\sigma(2x) -1\\).</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#relu","title":"ReLU","text":"<p>The Rectified Linear Unit is very popular. It computes the function \\(f(x) = \\max(0, x)\\). In other words, the activation is simply thresholded at zero.</p> <p> </p> <p>Top: Rectified Linear Unit (ReLU) activation function, which is zero when x &lt; 0 and then linear with slope 1 when x &gt; 0. Down: A plot from Krizhevsky et al. (pdf) paper indicating the 6x improvement in convergence with the ReLU unit compared to the tanh unit.</p> <p>There are several pros and cons to using the ReLUs:</p> <ul> <li>(+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions.</li> <li>(+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.</li> <li>(-) Unfortunately, ReLU units can be fragile during training and can \"die\". For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. A high learning rate can do this. Once the ReLU neuron cannot activate, it is considered \"dead\". For example, you may find that as much as 40% of your network can be \"dead\" (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#leaky-relu","title":"Leaky ReLU","text":"<p>Leaky ReLUs are one attempt to fix the \"dying ReLU\" problem. Instead of the function being zero when x &lt; 0, a leaky ReLU will instead have a small positive slope (of 0.01, or so). That is, the function computes \\(f(x) = \\mathbb{1}(x &lt; 0) (\\alpha x) + \\mathbb{1}(x&gt;=0) (x)\\) where \\(\\alpha\\) is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear.</p> <p>As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.</p> <p>TLDR: \"What neuron type should I use?\" Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of \"dead\" units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#neural-network-architectures","title":"Neural Network architectures","text":""},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#layer-wise-organization","title":"Layer-wise organization","text":"<p>Neural Networks as neurons in graphs: Neural Networks are modeled as collections of neurons that are connected in an acyclic graph. In other words, the outputs of some neurons can become inputs to other neurons. Cycles are not allowed since that would imply an infinite loop in the forward pass of a network. For regular neural networks, the most common layer type is the fully-connected layer in which neurons between two adjacent layers are fully pairwise connected, but neurons within a single layer share no connections.</p> <p> </p> <p>Top: A 2-layer Neural Network (one hidden layer of 4 neurons (or units) and one output layer with 2 neurons), and three inputs. Bottom: A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Notice that in both cases there are connections (synapses) between neurons across layers, but not within a layer.</p> <p>The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with example networks:</p> <ul> <li>A 2-layer network might have 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters.</li> <li>A 3-layer network might have 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters.</li> </ul> <p>To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#representational-power","title":"Representational power","text":"<p>One way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. A natural question that arises is: What is the representational power of this family of functions? In particular, are there functions that cannot be modeled with a Neural Network?</p> <p>It turns out that Neural Networks with at least one hidden layer are universal approximators. That is, it can be shown (e.g. see Approximation by Superpositions of Sigmoidal Function from 1989 (pdf), or this intuitive explanation from Michael Nielsen) that given any continuous function \\(f(x)\\) and some \\(\\epsilon &gt; 0\\), there exists a Neural Network \\(g(x)\\) with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that \\(\\forall x, \\mid f(x) - g(x) \\mid &lt; \\epsilon\\). In other words, the neural network can approximate any continuous function.</p> <p>If one hidden layer suffices to approximate any function, why use more layers and go deeper? The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice. In one dimension, the \"sum of indicator bumps\" function \\(g(x) = \\sum_i c_i \\mathbb{1}(a_i &lt; x &lt; b_i)\\) where \\(a,b,c\\) are parameter vectors is also a universal approximator, but noone would suggest that we use this functional form in Machine Learning.</p> <p>Consider Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.</p> <p>If you are interested in these topics we recommend for further reading:</p> <ul> <li>Deep Learning book in press by Bengio, Goodfellow, Courville, in particular Chapter 6.4.</li> <li>Do Deep Nets Really Need to be Deep?</li> <li>FitNets: Hints for Thin Deep Nets</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#setting-number-of-layers-and-their-sizes","title":"Setting number of layers and their sizes","text":"<p>How do we decide on what architecture to use when faced with a practical problem? Should we use no hidden layers? One hidden layer? Two hidden layers? How large should each layer be? First, note that as we increase the size and number of layers in a Neural Network, the capacity of the network increases. That is, the space of representable functions grows since the neurons can collaborate to express many different functions. For example, suppose we had a binary classification problem in two dimensions. We could train three separate neural networks, each with one hidden layer of some size and obtain the following classifiers as shown below.</p> <p></p> <p>Larger Neural Networks can represent more complicated functions. The data are shown as circles colored by their class, and the decision regions by a trained neural network are shown underneath.</p> <p>Neural Networks with more neurons can express more complicated functions. However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. For example, a model with 20 hidden neurons might fit all the training data but at the cost of segmenting the space into many disjoint decision regions. A model with 3 hidden neurons only has the representational power to classify the data in broad strokes. It models the data as two blobs and interprets the few outliers as noise. In practice, this could lead to better generalization on the test set.</p> <p>Based on our discussion above, it seems that smaller neural networks can be preferred if the data is not complex enough to prevent overfitting. However, this is incorrect- there are many other preferred ways to prevent overfitting in Neural Networks such as L2 regularization, dropout, etc. In practice, it is always better to use these methods to control overfitting instead of the number of neurons.</p> <p>The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: It's clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. On the other hand, if you train a large network you'll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.</p> <p>To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network.</p> <p></p> <p>The effects of regularization strength: Each neural network above has 20 hidden neurons, but changing the regularization strength makes its final decision regions smoother with a higher regularization.</p> <p>The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/#additional-references","title":"Additional References","text":"<ul> <li>ConvNetJS demos for intuitions</li> <li>Michael Nielsen's tutorials</li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/","title":"Neural Networks: Setting up the Data","text":"<p>A Neural Network performs a sequence of linear mappings with interwoven non-linearities.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#data-preprocessing","title":"Data Preprocessing","text":"<p>There are three common forms of data preprocessing a data matrix <code>X</code>, where we will assume that <code>X</code> is of size <code>[N x D]</code> (<code>N</code> is the number of data, <code>D</code> is their dimensionality).</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#mean-subtraction","title":"Mean subtraction","text":"<p>This is the most common form of preprocessing. It involves subtracting the mean across every individual feature in the data, and has the geometric interpretation of centering the cloud of data around the origin along every dimension. In numpy, this operation would be implemented as: <code>X -= np.mean(X, axis = 0)</code>. With images specifically, for convenience it can be common to subtract a single value from all pixels (e.g. <code>X -= np.mean(X)</code>), or to do so separately across the three color channels.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#normalization","title":"Normalization","text":"<p>This refers to normalizing the data dimensions so that they are of approximately the same scale. There are two common ways of achieving this normalization. One is to divide each dimension by its standard deviation, once it has been zero-centered: (<code>X /= np.std(X, axis = 0)</code>). Another form of this preprocessing normalizes each dimension so that the min and max along the dimension is -1 and 1 respectively. It only makes sense to apply this preprocessing if you have a reason to believe that different input features have different scales (or units), but they should be of approximately equal importance to the learning algorithm. In case of images, the relative scales of pixels are already approximately equal (and in range from 0 to 255), so it is not strictly necessary to perform this additional preprocessing step.</p> <p></p> <p>Common data preprocessing pipeline. Left: Original toy, 2-dimensional input data. Middle: The data is zero-centered by subtracting the mean in each dimension. The data cloud is now centered around the origin. Right: Each dimension is additionally scaled by its standard deviation. The red lines indicate the extent of the data - they are of unequal length in the middle, but of equal length on the right.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#pca-and-whitening","title":"PCA and Whitening","text":"<p>This is another form of preprocessing. Out of scope for this document.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#weight-initialization","title":"Weight Initialization","text":"<p>Before we can begin to train the network we have to initialize its parameters.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#pitfall-all-zero-initialization","title":"Pitfall: all zero initialization","text":"<p>Lets start with what we should not do. Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative (with proper normalization, the input features have been centered so that each feature has mean \u2248 0). A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the \"best guess\" in expectation. This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#small-random-numbers","title":"Small random numbers","text":"<p>Therefore, we still want the weights to be very close to zero, but as we have argued above, not identically zero. As a solution, it is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. The implementation for one weight matrix might look like <code>W = 0.01* np.random.randn(D,H)</code>, where <code>randn</code> samples from a zero mean, unit standard deviation gaussian. With this formulation, every neuron's weight vector is initialized as a random vector sampled from a multi-dimensional gaussian, so the neurons point in random direction in the input space. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.</p> <p>Warning: It's not necessarily the case that smaller numbers will work strictly better. For example, a Neural Network layer that has very small weights will during backpropagation compute very small gradients on its data (since this gradient, \\(\\frac{\\partial L}{\\partial x}\\) is proportional to the value of the weights). This could greatly diminish the \"gradient signal\" flowing backward through a network, and could become a concern for deep networks. The gradient being discussed here is \\(\\frac{\\partial L}{\\partial x}\\), where \\(x\\) represents the input to the current layer (which is the output of the previous layer). This gradient is needed to train the entire network - it flows backward from layer to layer, enabling all layers to learn. While the original training data is constant, the intermediate activations (outputs of each layer) change during training, and we need gradients with respect to these to update the weights of previous layers. Small weights make these gradients small, which can slow down or prevent learning in earlier layers of deep networks.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#calibrating-the-variances-with-1sqrtn","title":"Calibrating the variances with 1/sqrt(n)","text":""},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#the-problem-with-randomly-initialized-weights","title":"The problem with randomly initialized weights","text":"<p>One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. Based on the principle that the sum of more independent random variables increases variance, a neuron with more inputs will produce a larger variance in its initial output. This has several consequences for training a neural network.</p> <p>A neuron's output is calculated by a weighted sum of its inputs, followed by an activation function. Before training, the weights and inputs can be considered independent random variables.</p> \\[\\text{pre-activation} = z = \\sum_{i=1}^{n} w_i x_i\\] <p>If the number of inputs (\\(n\\)) increases, the variance of this sum increases.</p> \\[\\text{Var}(z) = \\text{Var}\\left(\\sum_{i=1}^{n} w_i x_i\\right)\\] <p>Assuming the weights (\\(w_i\\)) and inputs (\\(x_i\\)) are independent and identically distributed, the variance of the pre-activation will be proportional to the number of inputs, \\(n\\).</p> \\[\\text{Var}(z) \\propto n\\] <p>The vanishing/exploding gradient problem: This uncontrolled increase in variance is the primary problem:</p> <ul> <li> <p>Deep Networks: In deep networks, the variance of the pre-activation will grow exponentially with each successive layer.</p> </li> <li> <p>Exploding Gradients: During backpropagation, the gradients are multiplied by the weights of each layer. If the weights are too large (due to high variance from initialization), the gradients will also grow exponentially, leading to instability during training. This is known as the exploding gradient problem.</p> </li> <li> <p>Vanishing Gradients: Conversely, if the initial weights are too small, the gradients can shrink exponentially with each layer, becoming too small to update the weights effectively. This is the vanishing gradient problem.</p> </li> </ul>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#the-solution","title":"The solution","text":"<p>It turns out that we can normalize the variance of each neuron's output to 1 by scaling its weight vector by the square root of its fan-in (i.e. its number of inputs). That is, the recommended heuristic is to initialize each neuron's weight vector as: <code>w = np.random.randn(n) / sqrt(n)</code>, where <code>n</code> is the number of its inputs. This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence.</p> <p>The sketch of the derivation is as follows: Consider the inner product \\(s = \\sum_i^n w_i x_i\\) between the weights \\(w\\) and input \\(x\\), which gives the raw activation of a neuron before the non-linearity. We can examine the variance of \\(s\\):</p> \\[\\begin{align} \\text{Var}(s) &amp;= \\text{Var}(\\sum_i^n w_ix_i) \\\\\\\\ &amp;= \\sum_i^n \\text{Var}(w_ix_i) \\\\\\\\ &amp;= \\sum_i^n [E(w_i)]^2\\text{Var}(x_i) + [E(x_i)]^2\\text{Var}(w_i) + \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\ &amp;= \\sum_i^n \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\ &amp;= \\left( n \\text{Var}(w) \\right) \\text{Var}(x) \\end{align}\\] <p>where in the first 2 steps we have used properties of variance. In third step we assumed zero mean inputs and weights, so \\(E[x_i] = E[w_i] = 0\\). Note that this is not generally the case: for example ReLU units will have a positive mean. In the last step we assumed that all \\(w_i, x_i\\) are identically distributed. From this derivation we can see that if we want \\(s\\) to have the same variance as all of its inputs \\(x\\), then during initialization we should make sure that the variance of every weight \\(w\\) is \\(1/n\\). And since \\(\\text{Var}(aX) = a^2\\text{Var}(X)\\) for a random variable \\(X\\) and a scalar \\(a\\), this implies that we should draw from unit gaussian and then scale it by \\(a = \\sqrt{1/n}\\), to make its variance \\(1/n\\). This gives the initialization <code>w = np.random.randn(n) / sqrt(n)</code>.</p> <p>A more recent paper on this topic, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification by He et al., derives an initialization specifically for ReLU neurons, reaching the conclusion that the variance of neurons in the network should be \\(2.0/n\\). This gives the initialization <code>w = np.random.randn(n) * sqrt(2.0/n)</code>, and is the current recommendation for use in practice in the specific case of neural networks with ReLU neurons.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#initializing-the-biases","title":"Initializing the biases","text":"<p>It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#batch-normalization","title":"Batch Normalization","text":"<p>A recently developed technique by Ioffe and Szegedy called Batch Normalization alleviates a lot of headaches with properly initializing neural networks by explicitly forcing the activations throughout a network to take on a unit gaussian distribution at the beginning of the training. The core observation is that this is possible because normalization is a simple differentiable operation. In the implementation, applying this technique usually amounts to insert the BatchNorm layer immediately after fully connected layers (or convolutional layers, as we'll soon see), and before non-linearities. We do not expand on this technique here because it is well described in the linked paper, but note that it has become a very common practice to use Batch Normalization in neural networks. In practice networks that use Batch Normalization are significantly more robust to bad initialization. Additionally, batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network architecture in a differentiable manner.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#regularization","title":"Regularization","text":"<p>There are several ways of controlling the capacity of Neural Networks to prevent overfitting.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#l2-regularization","title":"L2 regularization","text":"<p>This is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. That is, for every weight \\(w\\) in the network, we add the term \\(\\frac{1}{2} \\lambda w^2\\) to the objective, where \\(\\lambda\\) is the regularization strength. It is common to see the factor of \\(\\frac{1}{2}\\) in front because then the gradient of this term with respect to the parameter \\(w\\) is simply \\(\\lambda w\\) instead of \\(2 \\lambda w\\). The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the appealing property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. Lastly, notice that during gradient descent parameter update, using the L2 regularization ultimately means that every weight is decayed linearly: \\(W += -\\lambda \\cdot W\\) towards zero.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#l1-regularization","title":"L1 regularization","text":"<p>This is another relatively common form of regularization, where for each weight \\(w\\) we add the term \\(\\lambda |w|\\) to the objective. It is possible to combine the L1 regularization with the L2 regularization: \\(\\lambda_1 |w| + \\lambda_2 w^2\\) (this is called Elastic net regularization). The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the \"noisy\" inputs. In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give better performance than L1.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#max-norm-constraints","title":"Max norm constraints","text":"<p>Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector \\(\\vec{w}\\) of every neuron to satisfy \\(||\\vec{w}||_2 &lt; c\\). Typical values of \\(c\\) are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that even if the learning rate is set too high, the network cannot \"explode\" because the updates are always bounded.</p>"},{"location":"ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/#dropout","title":"Dropout","text":"<p>This is an extremely effective, simple and recently introduced regularization technique by Srivastava et al. in Dropout: A Simple Way to Prevent Neural Networks from Overfitting (pdf) that complements the other methods (L1, L2, maxnorm). While training, dropout is implemented by only keeping a neuron active with some probability \\(p\\) (a hyperparameter), or setting it to zero otherwise.</p> <p></p> <p>Left: A standard 2-layer Neural Network. Right: An example of a 2-layer Neural Network with dropout applied. Crossed units have been randomly \"dropped out\" of the network. During testing there is no dropout applied</p> <p>Vanilla dropout in an example 3-layer Neural Network would be implemented as follows:</p> <pre><code>\"\"\" Vanilla Dropout: Not recommended implementation (see notes below) \"\"\"\n\np = 0.5 # probability of keeping a unit active. higher = less dropout\n\ndef train_step(X):\n  \"\"\" X contains the data \"\"\"\n\n  # forward pass for example 3-layer neural network\n  H1 = np.maximum(0, np.dot(W1, X) + b1)\n  U1 = np.random.rand(*H1.shape) &lt; p # first dropout mask\n  H1 *= U1 # drop!\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  U2 = np.random.rand(*H2.shape) &lt; p # second dropout mask\n  H2 *= U2 # drop!\n  out = np.dot(W3, H2) + b3\n\n  # backward pass: compute gradients... (not shown)\n  # perform parameter update... (not shown)\n\ndef predict(X):\n  # ensembling forward pass\n  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # scale the activations\n  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # scale the activations\n  out = np.dot(W3, H2) + b3\n</code></pre> <p>Crucially, note that in the predict function we are not dropping anymore, but we are performing a scaling of both hidden layer outputs by \\(p\\). This is important because at test time all neurons see all their inputs, so we want the outputs of neurons at test time to be identical to their expected outputs at training time. For example, in case of \\(p=0.5\\), the neurons must halve their outputs at test time to have the same output as they had during training time (in expectation). To see this, consider an output of a neuron \\(x\\) (before dropout). With dropout, the expected output from this neuron will become \\(px+(1-p)0\\), because the neuron's output will be set to zero with probability \\(1-p\\). At test time, when we keep the neuron always active, we must adjust \\(x \\rightarrow px\\) to keep the same expected output.</p> <p>The undesirable property of the scheme presented above is that we must scale the activations by \\(p\\) at test time. Since test-time performance is so critical, it is always preferable to use inverted dropout, which performs the scaling at train time, leaving the forward pass at test time untouched. Additionally, this has the appealing property that the prediction code can remain untouched when you decide to tweak where you apply dropout, or if at all. Inverted dropout looks as follows.</p> <pre><code>\"\"\" \nInverted Dropout: Recommended implementation example.\nWe drop and scale at train time and don't do anything at test time.\n\"\"\"\n\np = 0.5 # probability of keeping a unit active. higher = less dropout\n\ndef train_step(X):\n  # forward pass for example 3-layer neural network\n  H1 = np.maximum(0, np.dot(W1, X) + b1)\n  U1 = (np.random.rand(*H1.shape) &lt; p) / p # first dropout mask. Notice /p!\n  H1 *= U1 # drop!\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  U2 = (np.random.rand(*H2.shape) &lt; p) / p # second dropout mask. Notice /p!\n  H2 *= U2 # drop!\n  out = np.dot(W3, H2) + b3\n\n  # backward pass: compute gradients... (not shown)\n  # perform parameter update... (not shown)\n\ndef predict(X):\n  # ensembled forward pass\n  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  out = np.dot(W3, H2) + b3\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/optimization/","title":"Optimization","text":""},{"location":"ai/deep_learning_for_computer_vision/optimization/#visualizing-the-loss-function","title":"Visualizing the loss function","text":"<p>We can generate a random weight matrix \\(W\\), then march along a ray and record the loss function value along the way. That is, we can generate a random direction \\(W_1\\) and compute the loss along this direction by evaluating \\(L(W + aW_1)\\) for different values of \\(a\\). This process generates a simple plot with the value of \\(a\\) as the x-axis and the value of the loss function as the y-axis. We can also carry out the same procedure with two dimensions by evaluating the loss \\(L(W + aW_1 + bW_2)\\) as we vary \\(a, b\\). In a plot, \\(a, b\\) could then correspond to the x-axis and the y-axis, and the value of the loss function can be visualized with a color.</p> <p></p> <p>To reiterate, the loss function lets us quantify the quality of any particular set of weights W. The goal of optimization is to find W that minimizes the loss function.</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#strategies","title":"Strategies","text":""},{"location":"ai/deep_learning_for_computer_vision/optimization/#strategy-1-a-first-very-bad-idea-solution-random-search","title":"Strategy #1: A first very bad idea solution: Random search","text":"<p>Since it is so simple to check how good a given set of parameters \\(W\\) is, the first (very bad) idea that may come to mind is to simply try out many different random weights and keep track of what works best. This procedure might look as follows.</p> <pre><code># assume X_train is the data where each column is an example (e.g. 3073 x 50,000)\n# assume Y_train are the labels (e.g. 1D array of 50,000)\n# assume the function L evaluates the loss function\n\nbestloss = float(\"inf\") # Python assigns the highest possible float value\nfor num in range(1000):\n  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n  if loss &lt; bestloss: # keep track of the best solution\n    bestloss = loss\n    bestW = W\n  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)\n\n# prints:\n# in attempt 0 the loss was 9.401632, best 9.401632\n# in attempt 1 the loss was 8.959668, best 8.959668\n# in attempt 2 the loss was 9.044034, best 8.959668\n# in attempt 3 the loss was 9.278948, best 8.959668\n# in attempt 4 the loss was 8.857370, best 8.857370\n# in attempt 5 the loss was 8.943151, best 8.857370\n# in attempt 6 the loss was 8.605604, best 8.605604\n# ... (trunctated: continues for 1000 lines)\n\n# Assume X_test is [3073 x 10000], Y_test [10000 x 1]\nscores = bestW.dot(Xte_cols) # 10 x 10000, the class scores for all test examples\n# find the index with max score in each column (the predicted class)\nYte_predict = np.argmax(scores, axis = 0)\n# and calculate accuracy (fraction of predictions that are correct)\nnp.mean(Yte_predict == Yte)\n# returns 0.1555\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#strategy-2-random-local-search","title":"Strategy #2: Random Local Search","text":"<p>The first strategy you may think of is to try to extend one foot in a random direction and then take a step only if it leads downhill. Concretely, we will start out with a random \\(W\\), generate random perturbations \\(\\delta W\\) to it and if the loss at the perturbed \\(W + \\delta W\\) is lower, we will perform an update. The code for this procedure is as follows.</p> <pre><code>W = np.random.randn(10, 3073) * 0.001 # generate random starting W\nbestloss = float(\"inf\")\nfor i in range(1000):\n  step_size = 0.0001\n  Wtry = W + np.random.randn(10, 3073) * step_size\n  loss = L(Xtr_cols, Ytr, Wtry)\n  if loss &lt; bestloss:\n    W = Wtry\n    bestloss = loss\n  print 'iter %d loss is %f' % (i, bestloss)\n</code></pre>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#strategy-3-following-the-gradient","title":"Strategy #3: Following the Gradient","text":"<p>In the previous strategy we tried to find a direction in the weight-space that would improve our weight vector (and give us a lower loss). It turns out that there is no need to randomly search for a good direction: we can compute the best direction along which we should change our weight vector that is mathematically guaranteed to be the direction of the steepest descent of the loss function. This direction will be related to the gradient of the loss function. In a hiking analogy, this approach roughly corresponds to feeling the slope of the hill below our feet and stepping down the direction that feels steepest.</p> <p>In one-dimensional functions, the slope is the instantaneous rate of change of the function at any point you might be interested in. The gradient is just a vector of slopes (more commonly referred to as derivatives) for each dimension in the input space. The mathematical expression for the derivative of a 1-D function with respect its input is:</p> \\[\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\\] <p>When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension.</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#computing-the-gradient","title":"Computing the gradient","text":"<p>There are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone (implementation wise) way that requires calculus (analytic gradient).</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#computing-the-gradient-numerically-with-finite-differences","title":"Computing the gradient numerically with finite differences","text":"<pre><code>def eval_numerical_gradient(f, x):\n  \"\"\"\n  a naive implementation of numerical gradient of f at x\n  - f should be a function that takes a single argument\n  - x is the point (numpy array) to evaluate the gradient at\n  \"\"\"\n\n  fx = f(x) # evaluate function value at original point\n  grad = np.zeros(x.shape)\n  h = 0.00001\n\n  # iterate over all indexes in x\n  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n  while not it.finished:\n\n    # evaluate function at x+h\n    ix = it.multi_index\n    old_value = x[ix]\n    x[ix] = old_value + h # increment by h\n    fxh = f(x) # evalute f(x + h)\n    x[ix] = old_value # restore to previous value (very important!)\n\n    # compute the partial derivative\n    grad[ix] = (fxh - fx) / h # the slope\n    it.iternext() # step to next dimension\n\n  return grad\n</code></pre> <p>You may have noticed that evaluating the numerical gradient has complexity linear in the number of parameters. If our example had 30730 parameters in total, we therefore would have had to perform 30,731 evaluations of the loss function to evaluate the gradient and to perform only a single parameter update. This problem only gets worse, since modern Neural Networks can easily have tens of millions of parameters. Clearly, this strategy is not scalable and we need something better.</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#computing-the-gradient-analytically-with-calculus","title":"Computing the gradient analytically with Calculus","text":"<p>The numerical gradient is very simple to compute using the finite difference approximation, but the downside is that it is approximate (since we have to pick a small value of h, while the true gradient is defined as the limit as h goes to zero), and that it is very computationally expensive to compute. The second way to compute the gradient is analytically using Calculus, which allows us to derive a direct formula for the gradient (no approximations) that is also very fast to compute. However, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a gradient check.</p> <p>Let's use the example of the SVM loss function for a single datapoint:</p> \\[L_i = \\sum_{j \\neq y_i} \\left[\\max(0, w_j^T x_i - w_{y_i}^T x_i + \\Delta)\\right]\\] <p>We can differentiate the function with respect to the weights. For example, taking the gradient with respect to \\(w_{y_i}\\) we obtain:</p> \\[\\nabla_{w_{y_i}} L_i = -\\left(\\sum_{j \\neq y_i} \\mathbb{1}(w_j^T x_i - w_{y_i}^T x_i + \\Delta &gt; 0)\\right) x_i\\] <p>where \\(\\mathbb{1}\\) is the indicator function that is one if the condition inside is true or zero otherwise. While the expression may look scary when it is written out, when you're implementing this in code you'd simply count the number of classes that didn't meet the desired margin (and hence contributed to the loss function) and then the data vector \\(x_i\\) scaled by this number is the gradient. Notice that this is the gradient only with respect to the row of \\(W\\) that corresponds to the correct class. For the other rows where \\(j \\neq y_i\\) the gradient is:</p> \\[\\nabla_{w_j} L_i = \\mathbb{1}(w_j^T x_i - w_{y_i}^T x_i + \\Delta &gt; 0) x_i\\] <p>Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update.</p>"},{"location":"ai/deep_learning_for_computer_vision/optimization/#gradient-descent","title":"Gradient Descent","text":"<p>Now that we can compute the gradient of the loss function, the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent. Its vanilla version looks as follows.</p> <pre><code># Vanilla Gradient Descent\n\nwhile True:\n  weights_grad = evaluate_gradient(loss_fun, data, weights)\n  weights += - step_size * weights_grad # perform parameter update\n</code></pre> <p>In large-scale applications, the training data can have on order of millions of examples. Hence, it seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update. A very common approach to addressing this challenge is to compute the gradient over batches of the training data. For example, in current state of the art ConvNets, a typical batch contains 256 examples from the entire training set of 1.2 million. This batch is then used to perform a parameter update.</p> <pre><code># Vanilla Minibatch Gradient Descent\n\nwhile True:\n  data_batch = sample_training_data(data, 256) # sample 256 examples\n  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n  weights += - step_size * weights_grad # perform parameter update\n</code></pre> <p>The extreme case of this is a setting where the mini-batch contains only a single example. This process is called Stochastic Gradient Descent (SGD) (or also sometimes on-line gradient descent). Even though SGD technically refers to using a single example at a time to evaluate the gradient, you will hear people use the term SGD even when referring to mini-batch gradient descent. The size of the mini-batch is a hyperparameter but it is not very common to cross-validate it. It is usually based on memory constraints (if any), or set to some value, e.g. 32, 64 or 128. We use powers of 2 in practice because many vectorized operation implementations work faster when their inputs are sized in powers of 2.</p>"},{"location":"ai/deep_learning_for_computer_vision/regularization/","title":"Regularization","text":"<p>Increasing the amount of training data is one way of reducing overfitting. Fortunately, there are other techniques which can reduce overfitting, even when we have a fixed network and fixed training data. These are known as regularization techniques.</p>"},{"location":"ai/deep_learning_for_computer_vision/regularization/#cross-entropy-loss-with-l2-regularization","title":"Cross-Entropy Loss with L2 Regularization","text":"<p>The complete loss function for a softmax classifier with L2 regularization combines the cross-entropy loss with a regularization term:</p> \\[L = \\frac{1}{N} \\sum_{i=1}^{N} L_i + \\lambda R(W)\\] <p>where:</p> <ul> <li> <p>\\(L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\\right)\\) is the cross-entropy loss for the \\(i\\)-th sample</p> </li> <li> <p>\\(R(W) = \\sum_k \\sum_l W_{k,l}^2\\) is the L2 regularization term (sum of squared weights)</p> </li> <li> <p>\\(\\lambda\\) is the regularization strength hyperparameter</p> </li> <li> <p>\\(N\\) is the number of training samples</p> </li> </ul> <p>Intuitively, the effect of regularization is to make it so the network prefers to learn small weights, all other things being equal. Put another way, regularization can be viewed as a way of compromising between finding small weights and minimizing the original loss function. The relative importance of the two elements of the compromise depends on the value of \\(\\lambda\\): when \\(\\lambda\\) is small we prefer to minimize the original loss function, but when is large we prefer small weights.</p> <p>The compromise exists because the two objectives in our loss function can conflict with each other:</p> <ol> <li> <p>Minimizing the original loss function: This encourages the model to fit the training data as well as possible, which often requires large weights to capture complex patterns and noise in the data. To understand why large weights are often needed, consider what happens when we try to fit training data perfectly. Real training data often contains noise, outliers, or mislabeled examples. To achieve near zero training error, the model must learn to classify these noisy examples correctly. The model must thus be sensitive to noisy samples. Having large weights makes the model sensitive to variation in input data. The model might learn to recognize very specific features of individual training examples (like particular pixel patterns, lighting conditions, or background elements) rather than generalizable features. This often requires large weights to amplify these specific signals. In essence, the original loss function asks: \"How can I classify every training example correctly?\". The answer often involves learning very specific, complex patterns with noise that require large weights to implement.</p> </li> <li> <p>Minimizing the regularization term: This encourages small weights, which constrains the model's capacity to fit complex patterns.</p> </li> </ol> <p>Example: Which weights does the regularizer prefer?</p> <p>Consider the following example with input \\(x = [1, 1, 1, 1]\\) and two different weight vectors:</p> <ul> <li>\\(w_1 = [1, 0, 0, 0]\\) </li> <li>\\(w_2 = [0.25, 0.25, 0.25, 0.25]\\)</li> </ul> <p>Both weight vectors produce the same output: \\(w_1 \\cdot x = 1\\) and \\(w_2 \\cdot x = 1\\). However, the L2 regularizer strongly prefers \\(w_2\\) over \\(w_1\\).</p> <p>L2 regularization calculation:</p> <ul> <li> <p>\\(R(w_1) = \\sum_{i} w_{1,i}^2 = 1^2 + 0^2 + 0^2 + 0^2 = 1\\)</p> </li> <li> <p>\\(R(w_2) = \\sum_{i} w_{2,i}^2 = 0.25^2 + 0.25^2 + 0.25^2 + 0.25^2 = 4 \\times 0.0625 = 0.25\\)</p> </li> </ul> <p>Even though both weight vectors produce identical predictions, the regularizer penalizes \\(w_1\\) four times more heavily than \\(w_2\\) because \\(w_1\\) concentrates all its \"weight\" in a single dimension, while \\(w_2\\) distributes the same total \"influence\" more evenly across all dimensions.</p> <p>This example illustrates why regularization encourages weight spreading rather than weight concentration- it prefers solutions that use many small weights rather than a few large ones, even when both approaches achieve the same functional result.</p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/","title":"Applications of Projections in \u211d\u207f: Orthogonal Bases of Planes and Linear Regression","text":"<p>Linear regression refers to the problem of finding a function \\(f(x) = mx + b\\) which best fits a collection of given data points \\((x_i, y_i)\\).</p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/#finding-an-orthogonal-basis-special-case","title":"Finding an orthogonal basis: special case","text":"<p>Theorem: Suppose \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) are nonzero, and not scalar multiples of each other. The vectors \\(\\mathbf{y}\\) and \\(\\mathbf{x}' = \\mathbf{x} - \\text{Proj}_{\\mathbf{y}} \\mathbf{x}\\) constitute an orthogonal basis of \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\). In particular, \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\) is 2-dimensional.</p> <p>The setup is symmetric in \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), so \\(\\{\\mathbf{x}, \\mathbf{y}' = \\mathbf{y} - \\text{Proj}_{\\mathbf{x}} \\mathbf{y}\\}\\) is also an orthogonal basis of \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\).</p> <p></p> <p>Note: This is similar to the situation of projection of \\(\\mathbf{x}\\) onto a linear subspace \\(V\\). The displacement vector between the projection and \\(\\mathbf{x}\\) is perpendicular to everything in \\(V\\). In our case, when we project \\(\\mathbf{x}\\) onto the span of \\(\\mathbf{y}\\), the resulting vector \\(\\mathbf{x}' = \\mathbf{x} - \\text{Proj}_{\\mathbf{y}} \\mathbf{x}\\) is orthogonal to \\(\\mathbf{y}\\), which means it's perpendicular to everything in the span of \\(\\mathbf{y}\\). This is why \\(\\mathbf{y}\\) and \\(\\mathbf{x}'\\) form an orthogonal basis - they are perpendicular to each other and together span the same 2-dimensional space as the original vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).</p> <p>Example: Consider the plane \\(V\\) in \\(\\mathbb{R}^3\\) through \\(0\\) spanned by the vectors</p> \\[\\mathbf{v} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 3 \\\\ 4 \\end{bmatrix}\\] <p>Imagine that this plane is a metal sheet on which an electric charge is uniformly distributed. An iron particle placed at the point \\(\\mathbf{p} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) would then be attracted to the metal sheet, and by the symmetry of the situation this particle would move straight towards the point on the plane closest to the initial position of the particle. What is that point?</p> <p>In other words, we seek to compute the projection \\(\\text{Proj}_V(\\mathbf{p})\\). To compute this, we first seek an orthogonal basis for the plane \\(V\\). By the theorem above, such an orthogonal basis is given by \\(\\mathbf{w}\\) and \\(\\mathbf{v}' = \\mathbf{v} - \\text{Proj}_{\\mathbf{w}}(\\mathbf{v})\\). We first compute \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{v})\\). This is given by</p> \\[\\text{Proj}_{\\mathbf{w}}(\\mathbf{v}) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = \\frac{3}{25} \\begin{bmatrix} 0 \\\\ 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\frac{9}{25} \\\\ \\frac{12}{25} \\end{bmatrix}\\] <p>Thus \\(\\mathbf{v}' = \\mathbf{v} - \\begin{bmatrix} 0 \\\\ \\frac{9}{25} \\\\ \\frac{12}{25} \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ \\frac{16}{25} \\\\ -\\frac{12}{25} \\end{bmatrix}\\). As a safety check, \\(\\mathbf{w}\\) and \\(\\mathbf{v}'\\) are indeed orthogonal.</p> <p>The vector \\(\\mathbf{v}'\\) is a bit ugly due to the fractions, and for the purposes of having an orthogonal basis it is harmless to replace it with a nonzero scalar multiple, such as</p> \\[\\mathbf{v}'' = 25\\mathbf{v}' = \\begin{bmatrix} 50 \\\\ 16 \\\\ -12 \\end{bmatrix}\\] <p>Since \\(\\{\\mathbf{w}, \\mathbf{v}''\\}\\) is an orthogonal basis of the plane \\(V\\), we have</p> \\[\\text{Proj}_V(\\mathbf{p}) = \\text{Proj}_V \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\text{Proj}_{\\mathbf{w}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} + \\text{Proj}_{\\mathbf{v}''} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\text{Proj}_{\\mathbf{w}}(\\mathbf{p}) + \\text{Proj}_{\\mathbf{v}''}(\\mathbf{p})\\] <p>To compute these projections, we first work out some relevant dot products:</p> \\[\\mathbf{w} \\cdot \\mathbf{w} = 25, \\quad \\mathbf{v}'' \\cdot \\mathbf{v}'' = 2900, \\quad \\mathbf{p} \\cdot \\mathbf{w} = 7, \\quad \\mathbf{p} \\cdot \\mathbf{v}'' = 54\\] <p>Hence</p> \\[\\text{Proj}_{\\mathbf{w}}(\\mathbf{p}) = \\frac{\\mathbf{p} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = \\frac{7}{25} \\begin{bmatrix} 0 \\\\ 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\frac{21}{25} \\\\ \\frac{28}{25} \\end{bmatrix}\\] \\[\\text{Proj}_{\\mathbf{v}''}(\\mathbf{p}) = \\frac{\\mathbf{p} \\cdot \\mathbf{v}''}{\\mathbf{v}'' \\cdot \\mathbf{v}''} \\mathbf{v}'' = \\frac{54}{2900} \\begin{bmatrix} 50 \\\\ 16 \\\\ -12 \\end{bmatrix} = \\begin{bmatrix} \\frac{27}{29} \\\\ \\frac{216}{725} \\\\ -\\frac{162}{725} \\end{bmatrix}\\] <p>Thus, the place on the metal sheet that the particle ends up at is</p> \\[\\text{Proj}_V \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\text{Proj}_{\\mathbf{w}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} + \\text{Proj}_{\\mathbf{v}''} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\frac{21}{25} \\\\ \\frac{28}{25} \\end{bmatrix} + \\begin{bmatrix} \\frac{27}{29} \\\\ \\frac{216}{725} \\\\ -\\frac{162}{725} \\end{bmatrix} = \\begin{bmatrix} \\frac{27}{29} \\\\ \\frac{33}{29} \\\\ \\frac{26}{29} \\end{bmatrix} \\approx \\begin{bmatrix} 0.931 \\\\ 1.138 \\\\ 0.897 \\end{bmatrix}\\] <p>Example: Let \\(\\mathbf{w}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{w}_2 = \\begin{bmatrix} 1 \\\\ -3 \\\\ 1 \\\\ 1 \\end{bmatrix}\\). Define \\(U\\) to be the collection of all 4-vectors \\(\\mathbf{u}\\) that are orthogonal to both \\(\\mathbf{w}_1\\) and \\(\\mathbf{w}_2\\). Show that \\(U\\) is a linear subspace of \\(\\mathbb{R}^4\\) by writing it as a span of finitely many vectors. Explain why \\(\\dim(U) = 2\\).</p> <p>Solution:</p> <p>First, let's understand what \\(U\\) represents. A vector \\(\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4 \\end{bmatrix}\\) belongs to \\(U\\) if and only if:</p> \\[\\mathbf{u} \\cdot \\mathbf{w}_1 = 0 \\quad \\text{and} \\quad \\mathbf{u} \\cdot \\mathbf{w}_2 = 0\\] <p>This gives us the system of equations:</p> \\[u_1 + u_2 + u_3 + u_4 = 0\\] \\[u_1 - 3u_2 + u_3 + u_4 = 0\\] <p>Subtracting the second equation from the first:</p> \\[(u_1 + u_2 + u_3 + u_4) - (u_1 - 3u_2 + u_3 + u_4) = 0 - 0\\] \\[4u_2 = 0\\] \\[u_2 = 0\\] <p>Substituting \\(u_2 = 0\\) back into the first equation:</p> \\[u_1 + 0 + u_3 + u_4 = 0\\] \\[u_1 + u_3 + u_4 = 0\\] <p>This means \\(u_1 = -u_3 - u_4\\). So any vector in \\(U\\) must have the form:</p> \\[\\mathbf{u} = \\begin{bmatrix} -u_3 - u_4 \\\\ 0 \\\\ u_3 \\\\ u_4 \\end{bmatrix} = u_3 \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + u_4 \\begin{bmatrix} -1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] <p>Let's define:</p> \\[\\mathbf{v}_1 = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] <p>Then \\(U = \\text{span}(\\mathbf{v}_1, \\mathbf{v}_2)\\), which shows that \\(U\\) is indeed a linear subspace of \\(\\mathbb{R}^4\\).</p> <p>The dimension of \\(U\\) is 2 because we found that \\(U\\) is spanned by two vectors: \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\). These vectors are linearly independent (neither is a scalar multiple of the other). Therefore, \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) is a basis for \\(U\\). Since the basis has 2 elements, \\(\\dim(U) = 2\\).</p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/#fitting-a-function-to-data","title":"Fitting a function to data","text":"<p>What does \"best fit\" mean? Informally, we want \\(f(x_i)\\) to be as close as possible to \\(y_i\\) for all \\(i\\). The error</p> \\[\\text{error}_i = y_i - (mx_i + b)\\] <p>measures in absolute value how close the line \\(y = mx + b\\) is vertically to \\((x_i, y_i)\\).</p> <p></p> <p>Suppose the line is given by the equation \\(y = mx + b\\). Suppose the \\(i\\)th data point is denoted \\((x_i, y_i)\\). The \\(i\\)th error is given by \\(\\text{error}_i = e_i = y_i - (mx_i + b)\\). These errors are shown as blue line segments in the figure.</p> <p>To be a \"good fit\" means to choose \\((m, b)\\) so that the errors are collectively small. There are many ways to specify what \"collectively small\" means. The meaning in the least squares method is this: choose \\((m, b)\\) to minimize the sum of the squares of the errors; i.e., choose \\((m, b)\\) to minimize</p> \\[\\sum_{i=1}^n (y_i - (mx_i + b))^2\\] <p>Why use the sum of squares of the errors? The errors themselves might be positive and might be negative; we want to penalize a large negative error as well as a large positive error, so squaring errors removes the sign.</p> <p>But sometimes other ways to define the \"total error\" are indeed more appropriate, such as summing the absolute values of the errors (used in computational statistics, geophysics, and the important signal processing algorithm called \"compressed sensing\"). The absolute value function is inconvenient for our purposes; e.g., from a calculus viewpoint, \\(|x|\\) has the defect relative to \\(x^2\\) that it is not differentiable at \\(x = 0\\). Always remember that we choose how to define \"total error\" for any particular application, and experience determines the appropriateness of that choice; mathematics is a creation of the human mind.</p> <p>Put the data of all \\(x\\)-values into a single \\(n\\)-vector, and the data of all \\(y\\)-values into a single \\(n\\)-vector:</p> \\[X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\quad Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\] <p>Also, define \\(\\mathbf{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\in \\mathbb{R}^n\\) to be the vector with all entries equal to 1 (analogous to \\(\\mathbf{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^n\\)), so</p> \\[mX + b\\mathbf{1} = \\begin{bmatrix} mx_1 \\\\ mx_2 \\\\ \\vdots \\\\ mx_n \\end{bmatrix} + \\begin{bmatrix} b \\\\ b \\\\ \\vdots \\\\ b \\end{bmatrix} = \\begin{bmatrix} mx_1 + b \\\\ mx_2 + b \\\\ \\vdots \\\\ mx_n + b \\end{bmatrix}\\] <p>and hence</p> \\[Y - (mX + b\\mathbf{1}) = \\begin{bmatrix} y_1 - (mx_1 + b) \\\\ y_2 - (mx_2 + b) \\\\ \\vdots \\\\ y_n - (mx_n + b) \\end{bmatrix} = \\text{\"vector of errors\"}\\] <p>Thus, since \\(\\sum_{i=1}^n v_i^2 = \\|\\mathbf{v}\\|^2\\) for any \\(\\mathbf{v} \\in \\mathbb{R}^n\\) (by definition of \\(\\|\\mathbf{v}\\|\\)!), the sum of the squares of the errors is</p> \\[\\sum_{i=1}^n (y_i - (mx_i + b))^2 = \\|Y - (mX + b\\mathbf{1})\\|^2\\] <p>So we seek \\(m\\) and \\(b\\) that minimizes the squared length of \\(Y - (mX + b\\mathbf{1})\\), which is the same as minimizing the length of that difference.</p> <p>The length \\(\\|Y - (mX + b\\mathbf{1})\\|\\) is the distance from \\(Y\\) to \\(mX + b\\mathbf{1}\\) since \"distance\" between any \\(n\\)-vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is \\(\\|\\mathbf{v} - \\mathbf{w}\\|\\) by definition. As \\(m\\) and \\(b\\) vary, the vectors of the form \\(mX + b\\mathbf{1}\\) are exactly the vectors in \\(\\text{span}(X, \\mathbf{1})\\), due to the definition of \"span\". Hence, the least-squares minimization problem for \\(n\\) data points is equivalent to the following geometric problem:</p> <p>find the vector in \\(\\text{span}(X, \\mathbf{1})\\) that is closest to the vector \\(Y \\in \\mathbb{R}^n\\).</p> <p>Our task is now an instance of finding the point of a linear subspace of \\(\\mathbb{R}^n\\) closest to a given \\(n\\)-vector.</p> <p>The vectors \\(X\\) and \\(\\mathbf{1}\\) are not scalar multiples of each other because the hypothesis that the \\(n\\) data points do not lie in a common vertical line (i.e., the \\(x_i\\)'s are not all equal to each other) says that \\(X\\) is not a scalar multiple of the nonzero vector \\(\\mathbf{1}\\).</p> <p>By using the Theorem above, an orthogonal basis of \\(\\text{span}(X, \\mathbf{1})\\) is given by \\(\\mathbf{1}\\) and \\(\\hat{X} = X - \\text{Proj}_{\\mathbf{1}}X\\) with</p> \\[\\text{Proj}_{\\mathbf{1}}(X) = \\frac{X \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}} \\mathbf{1} = \\frac{\\sum_{i=1}^n x_i \\cdot 1}{\\sum_{i=1}^n 1 \\cdot 1} \\mathbf{1} = \\frac{\\sum_{i=1}^n x_i}{n} \\mathbf{1} = \\bar{x} \\mathbf{1} = \\begin{bmatrix} \\bar{x} \\\\ \\bar{x} \\\\ \\vdots \\\\ \\bar{x} \\end{bmatrix}\\] <p>equal to the \\(n\\)-vector each of whose entries is equal to the average \\(\\bar{x}\\) of the \\(x_i\\)'s. Hence,</p> \\[\\hat{X} = X - \\text{Proj}_{\\mathbf{1}}(X) = \\begin{bmatrix} x_1 - \\bar{x} \\\\ x_2 - \\bar{x} \\\\ \\vdots \\\\ x_n - \\bar{x} \\end{bmatrix}\\] <p>is obtained from \\(X\\) by subtracting the average \\(\\bar{x}\\) from all entries.</p> <p>By applying to this span the formula for the nearest point on a linear subspace in terms of an orthogonal basis, we obtain that the closest vector to \\(Y\\) in \\(\\text{span}(X, \\mathbf{1})\\) is</p> \\[\\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}} \\mathbf{1} = \\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\bar{y} \\mathbf{1}\\] <p>where \\(\\bar{y} = (1/n) \\sum_{i=1}^n y_i\\) is the average of the \\(y_i\\)'s.</p> <p>In the expression \\(\\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\bar{y} \\mathbf{1}\\) on the right side, we can expand \\(\\hat{X}\\) in terms of \\(X\\) and \\(\\mathbf{1}\\) using the definition of \\(\\text{Proj}_{\\mathbf{1}}(X)\\) and collect terms to rewrite this as a linear combination \\(mX + b\\mathbf{1}\\) of \\(X\\) and \\(\\mathbf{1}\\). Those coefficients \\(m\\) and \\(b\\) are exactly the desired \"\\(m\\)\" and \"\\(b\\)\" for the best-fit line!</p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/#correlation-coefficient-and-quality-of-fit","title":"Correlation Coefficient and quality of fit","text":"<p>Let the best-fit line be \\(y = mx + b\\), and let \\(r\\) be the correlation coefficient for the recentered data \\((x_i - \\bar{x}, y_i - \\bar{y})\\) (whose coordinates average to 0) with associated \\(n\\)-vectors \\(\\hat{X}\\) and \\(\\hat{Y}\\). Then the role of nearness of \\(r^2\\) to 1 (or equivalently of nearness of \\(1 - r^2\\) to 0) as a measure of quality of fit is expressed by the following identity:</p> \\[\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2 (1 - r^2)\\] <p>This equation will be proven later.</p> <p>where \\(\\hat{Y}\\) is the \"recentered\" version of \\(Y\\) (subtracting \\(\\bar{y}\\) from all \\(y_i\\)'s).</p> <p>To explain the meaning of the above equation, expand out the left side (and use that \\(t^2 = |t|^2\\) for any \\(t\\)) to get</p> \\[|y_1 - (mx_1 + b)|^2 + |y_2 - (mx_2 + b)|^2 + \\cdots + |y_n - (mx_n + b)|^2\\] <p>The number \\(|y_i - (mx_i + b)|\\) is the vertical distance between the data point \\((x_i, y_i)\\) and the best fit line \\(y = mx + b\\). When \\(r^2 \\approx 1\\), the equation therefore says that these vertical distances are \"collectively small\": the sum of their squares is tiny since \\(1 - r^2\\) on the right side of the equation is small, so the data points are all close to the best fit line. When \\(r^2 \\approx 0\\) then (at least informally) the opposite happens since the right side is approximately \\(\\|\\hat{Y}\\|^2\\), which is typically quite far from 0 (even though the average of the entries in \\(\\hat{Y}\\) is 0 by design).</p> <p>Example: Sometimes a quantity \\(y\\) of interest is expected to be (approximately \"linearly\") related to a pair of quantities \\(x\\) and \\(v\\) rather than just a single quantity \\(x\\). In such cases, as a variant on linear regression, we seek three constants \\(a, b, c\\) for which</p> \\[y \\approx a + bx + cv\\] <p>as measured by data.</p> <p>Suppose we make \\(n\\) measurements of \\(x, v, y\\), yielding data points \\((x_i, v_i, y_i)\\). Let \\(X, V, Y \\in \\mathbb{R}^n\\) be the corresponding \\(n\\)-vectors for the \\(n\\) measurements of each of \\(x, v, y\\). Assume \\(W = \\text{span}(\\mathbf{1}, X, V)\\) is 3-dimensional (a reasonable assumption when neither \\(x\\) nor \\(v\\) determines the other).</p> <p>(a) Explain in words how the vector \\(\\text{Proj}_W(Y) \\in W\\) encodes a \"least squares\" choice of \\((a, b, c)\\) in terms of the data.</p> <p>(b) What is the practical difficulty in using the equation of finding a projection of a vector to compute \\(\\text{Proj}_W(Y)\\), whereas we had no difficulty in computing the analogous such projection for linear regression?</p> <p>Solution:</p> <p>(a) Least Squares Interpretation</p> <p>The vector \\(\\text{Proj}_W(Y) \\in W\\) represents the closest point in the subspace \\(W = \\text{span}(\\mathbf{1}, X, V)\\) to the data vector \\(Y\\). Since \\(W\\) is 3-dimensional, any vector in \\(W\\) can be written as a linear combination:</p> \\[\\text{Proj}_W(Y) = a\\mathbf{1} + bX + cV\\] <p>for some constants \\(a, b, c \\in \\mathbb{R}\\).</p> <p>This projection minimizes the distance \\(\\|Y - (a\\mathbf{1} + bX + cV)\\|\\), which is equivalent to minimizing the sum of squared errors:</p> \\[\\sum_{i=1}^n (y_i - (a + bx_i + cv_i))^2\\] <p>Therefore, the coefficients \\((a, b, c)\\) in the expression \\(\\text{Proj}_W(Y) = a\\mathbf{1} + bX + cV\\) represent the least squares solution to the multiple linear regression problem \\(y \\approx a + bx + cv\\).</p> <p>(b) Practical Difficulty</p> <p>The practical difficulty in computing \\(\\text{Proj}_W(Y)\\) for multiple linear regression compared to simple linear regression is the dimensionality of the subspace.</p> <p>Simple Linear Regression (2D subspace):</p> <ul> <li> <p>We had \\(W = \\text{span}(\\mathbf{1}, X)\\), a 2-dimensional subspace</p> </li> <li> <p>We could easily construct an orthogonal basis using the Gram-Schmidt process</p> </li> <li> <p>The projection formula was straightforward: \\(\\text{Proj}_W(Y) = \\text{Proj}_{\\mathbf{1}}(Y) + \\text{Proj}_{\\hat{X}}(Y)\\)</p> </li> <li> <p>We could compute this step-by-step with simple projections</p> </li> </ul> <p>Multiple Linear Regression (3D subspace):</p> <ul> <li> <p>We now have \\(W = \\text{span}(\\mathbf{1}, X, V)\\), a 3-dimensional subspace</p> </li> <li> <p>Constructing an orthogonal basis becomes more complex</p> </li> <li> <p>The Gram-Schmidt process requires more steps and can lead to numerical instability</p> </li> <li> <p>The projection formula involves more terms and becomes computationally intensive</p> </li> </ul> <p>Specific Challenges:</p> <ol> <li> <p>Orthogonal Basis Construction: We need to find three mutually orthogonal vectors spanning \\(W\\), which requires applying Gram-Schmidt to three vectors instead of two.</p> </li> <li> <p>Numerical Stability: As the dimension increases, small errors in computations can accumulate, leading to less accurate results.</p> </li> <li> <p>Computational Complexity: The projection involves more dot products and vector operations, making it computationally expensive for large datasets.</p> </li> <li> <p>Matrix Methods: For higher dimensions, it becomes more practical to use matrix methods (like QR decomposition or solving the normal equations) rather than geometric projection formulas.</p> </li> </ol> <p>This is why in practice, multiple linear regression is typically solved using matrix algebra and computational algorithms rather than the geometric projection approach, even though the geometric interpretation remains valid and insightful.</p> <p>Example: The vectors \\(\\mathbf{v} = \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{w} = \\begin{bmatrix} 11 \\\\ 5 \\\\ -10 \\\\ 1 \\end{bmatrix}\\) span a plane \\(P\\) through the origin in \\(\\mathbb{R}^4\\). Let</p> \\[L = \\left\\{\\begin{bmatrix} 4-t \\\\ 4+4t \\\\ 4-t \\\\ -7-2t \\end{bmatrix} : t \\in \\mathbb{R}\\right\\}\\] <p>be a line in \\(\\mathbb{R}^4\\).</p> <p>(a) Consider the displacement vector \\(\\mathbf{x}\\) between any two different points of \\(L\\) (all such displacements are scalar multiples of each other since \\(L\\) is a line). Show that \\(\\mathbf{x}\\) belongs to \\(P\\); this is described in words by saying \\(L\\) is parallel to \\(P\\).</p> <p>Solution:</p> <p>Let's take two different points on the line \\(L\\) by choosing two different values of \\(t\\). Let's use \\(t = 0\\) and \\(t = 1\\):</p> <ul> <li> <p>Point 1 (when \\(t = 0\\)): \\(\\begin{bmatrix} 4 \\\\ 4 \\\\ 4 \\\\ -7 \\end{bmatrix}\\)</p> </li> <li> <p>Point 2 (when \\(t = 1\\)): \\(\\begin{bmatrix} 3 \\\\ 8 \\\\ 3 \\\\ -9 \\end{bmatrix}\\)</p> </li> </ul> <p>The displacement vector between these two points is:</p> \\[\\mathbf{x} = \\begin{bmatrix} 3 \\\\ 8 \\\\ 3 \\\\ -9 \\end{bmatrix} - \\begin{bmatrix} 4 \\\\ 4 \\\\ 4 \\\\ -7 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix}\\] <p>We need to find scalars \\(a, b \\in \\mathbb{R}\\) such that:</p> \\[\\mathbf{x} = a\\mathbf{v} + b\\mathbf{w}\\] <p>This gives us the system of equations:</p> \\[\\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix} = a\\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} + b\\begin{bmatrix} 11 \\\\ 5 \\\\ -10 \\\\ 1 \\end{bmatrix}\\] <p>Which expands to:</p> \\[-1 = 2a + 11b \\quad \\text{(1)}\\] \\[4 = -a + 5b \\quad \\text{(2)}\\] \\[-1 = -a - 10b \\quad \\text{(3)}\\] \\[-2 = a + b \\quad \\text{(4)}\\] <p>Let's solve equations (1) and (2) first:</p> <p>From equation (2): \\(4 = -a + 5b\\), so \\(a = 5b - 4\\)</p> <p>Substitute into equation (1):</p> \\[-1 = 2(5b - 4) + 11b = 10b - 8 + 11b = 21b - 8\\] \\[21b = 7\\] \\[b = \\frac{1}{3}\\] <p>Now substitute \\(b = \\frac{1}{3}\\) back to find \\(a\\):</p> \\[a = 5\\left(\\frac{1}{3}\\right) - 4 = \\frac{5}{3} - 4 = \\frac{5}{3} - \\frac{12}{3} = -\\frac{7}{3}\\] <p>Let's check if \\(a = -\\frac{7}{3}\\) and \\(b = \\frac{1}{3}\\) satisfy all four equations:</p> <p>Equation (1): \\(2a + 11b = 2\\left(-\\frac{7}{3}\\right) + 11\\left(\\frac{1}{3}\\right) = -\\frac{14}{3} + \\frac{11}{3} = -\\frac{3}{3} = -1\\) \u2713</p> <p>Equation (2): \\(-a + 5b = -\\left(-\\frac{7}{3}\\right) + 5\\left(\\frac{1}{3}\\right) = \\frac{7}{3} + \\frac{5}{3} = \\frac{12}{3} = 4\\) \u2713</p> <p>Equation (3): \\(-a - 10b = -\\left(-\\frac{7}{3}\\right) - 10\\left(\\frac{1}{3}\\right) = \\frac{7}{3} - \\frac{10}{3} = -\\frac{3}{3} = -1\\) \u2713</p> <p>Equation (4): \\(a + b = -\\frac{7}{3} + \\frac{1}{3} = -\\frac{6}{3} = -2\\) \u2713</p> <p>Since we found scalars \\(a = -\\frac{7}{3}\\) and \\(b = \\frac{1}{3}\\) such that:</p> \\[\\mathbf{x} = -\\frac{7}{3}\\mathbf{v} + \\frac{1}{3}\\mathbf{w}\\] <p>This proves that the displacement vector \\(\\mathbf{x} = \\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix}\\) belongs to the plane \\(P = \\text{span}(\\mathbf{v}, \\mathbf{w})\\).</p> <p>This means that the line \\(L\\) is parallel to the plane \\(P\\). In \\(\\mathbb{R}^4\\), just as in \\(\\mathbb{R}^3\\), a line is parallel to a plane if the direction vector of the line (which is a scalar multiple of any displacement vector between two points on the line) lies in the plane.</p> <p>(b) Whenever one has a linear subspace \\(V\\) of \\(\\mathbb{R}^n\\) and a line \\(\\ell\\) in \\(\\mathbb{R}^n\\) (possibly not through the origin) that is parallel to \\(V\\), it is a fact (not difficult to show, but you may take it on faith) that all points in \\(\\ell\\) have the same distance to \\(V\\). That is, for every point \\(\\mathbf{y} \\in \\ell\\) and the point \\(\\mathbf{y}' \\in V\\) nearest to \\(\\mathbf{y}\\), the distance \\(\\|\\mathbf{y} - \\mathbf{y}'\\|\\) is the same regardless of which \\(\\mathbf{y}\\) on \\(\\ell\\) we consider. Taking \\(V\\) and \\(\\ell\\) to be \\(P\\) and \\(L\\) above, compute the common distance \\(\\|\\mathbf{y} - \\mathbf{y}'\\|\\) (since it is independent of \\(\\mathbf{y}\\), you may pick whatever you consider to be the most convenient point \\(\\mathbf{y}\\) in \\(L\\) to do the calculation).</p> <p>Solution:</p> <p>Since all points on the line \\(L\\) have the same distance to the plane \\(P\\), we can choose the most convenient point. Let's use the point when \\(t = 0\\): \\(\\mathbf{y} = \\begin{bmatrix} 4 \\\\ 4 \\\\ 4 \\\\ -7 \\end{bmatrix}\\).</p> <p>The distance from \\(\\mathbf{y}\\) to the plane \\(P\\) is the distance from \\(\\mathbf{y}\\) to its projection onto \\(P\\). To find this projection, we need an orthogonal basis for \\(P\\).</p> <p>Step 1: Find an orthogonal basis for \\(P\\)</p> <p>Using the theorem from earlier, we can construct an orthogonal basis for \\(P = \\text{span}(\\mathbf{v}, \\mathbf{w})\\):</p> <p>Let \\(\\mathbf{v}_1 = \\mathbf{v} = \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\)</p> <p>Then \\(\\mathbf{v}_2 = \\mathbf{w} - \\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w})\\)</p> <p>First, compute \\(\\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w})\\):</p> \\[\\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w}) = \\frac{\\mathbf{w} \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1} \\mathbf{v}_1\\] \\[\\mathbf{w} \\cdot \\mathbf{v}_1 = 11(2) + 5(-1) + (-10)(-1) + 1(1) = 22 - 5 + 10 + 1 = 28\\] \\[\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = 2^2 + (-1)^2 + (-1)^2 + 1^2 = 4 + 1 + 1 + 1 = 7\\] <p>So:</p> \\[\\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w}) = \\frac{28}{7} \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} = 4 \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ -4 \\\\ -4 \\\\ 4 \\end{bmatrix}\\] <p>Therefore:</p> \\[\\mathbf{v}_2 = \\mathbf{w} - \\text{Proj}_{\\mathbf{v}_1}(\\mathbf{w}) = \\begin{bmatrix} 11 \\\\ 5 \\\\ -10 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 8 \\\\ -4 \\\\ -4 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 9 \\\\ -6 \\\\ -3 \\end{bmatrix}\\] <p>Step 2: Compute the projection of \\(\\mathbf{y}\\) onto \\(P\\)</p> <p>Using the orthogonal basis \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\), the projection is:</p> \\[\\text{Proj}_P(\\mathbf{y}) = \\frac{\\mathbf{y} \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1} \\mathbf{v}_1 + \\frac{\\mathbf{y} \\cdot \\mathbf{v}_2}{\\mathbf{v}_2 \\cdot \\mathbf{v}_2} \\mathbf{v}_2\\] <p>Compute the dot products:</p> \\[\\mathbf{y} \\cdot \\mathbf{v}_1 = 4(2) + 4(-1) + 4(-1) + (-7)(1) = 8 - 4 - 4 - 7 = -7\\] \\[\\mathbf{y} \\cdot \\mathbf{v}_2 = 4(3) + 4(9) + 4(-6) + (-7)(-3) = 12 + 36 - 24 + 21 = 45\\] \\[\\mathbf{v}_2 \\cdot \\mathbf{v}_2 = 3^2 + 9^2 + (-6)^2 + (-3)^2 = 9 + 81 + 36 + 9 = 135\\] <p>So:</p> \\[\\text{Proj}_P(\\mathbf{y}) = \\frac{-7}{7} \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} + \\frac{45}{135} \\begin{bmatrix} 3 \\\\ 9 \\\\ -6 \\\\ -3 \\end{bmatrix}\\] \\[= -1 \\begin{bmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} + \\frac{1}{3} \\begin{bmatrix} 3 \\\\ 9 \\\\ -6 \\\\ -3 \\end{bmatrix}\\] \\[= \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ -1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 3 \\\\ -2 \\\\ -1 \\end{bmatrix}\\] \\[= \\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix}\\] <p>Step 3: Compute the distance</p> <p>The distance from \\(\\mathbf{y}\\) to the plane \\(P\\) is:</p> \\[\\|\\mathbf{y} - \\text{Proj}_P(\\mathbf{y})\\| = \\left\\|\\begin{bmatrix} 4 \\\\ 4 \\\\ 4 \\\\ -7 \\end{bmatrix} - \\begin{bmatrix} -1 \\\\ 4 \\\\ -1 \\\\ -2 \\end{bmatrix}\\right\\| = \\left\\|\\begin{bmatrix} 5 \\\\ 0 \\\\ 5 \\\\ -5 \\end{bmatrix}\\right\\|\\] \\[\\|\\mathbf{y} - \\text{Proj}_P(\\mathbf{y})\\| = \\sqrt{5^2 + 0^2 + 5^2 + (-5)^2} = \\sqrt{25 + 0 + 25 + 25} = \\sqrt{75} = \\sqrt{25 \\times 3} = 5\\sqrt{3}\\] <p>Answer: The common distance from any point on the line \\(L\\) to the plane \\(P\\) is \\(\\sqrt{75}\\).</p>"},{"location":"math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/#orthogonal-basis-formula-and-relation-of-correlation-coefficient-to-best-fit-lines","title":"Orthogonal basis formula and relation of correlation coefficient to best fit lines","text":"<p>In this section we prove some results discussed earlier.</p> <p>Theorem: Suppose \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) are nonzero, and not scalar multiples of each other. The vectors \\(\\mathbf{y}\\) and \\(\\mathbf{x}' = \\mathbf{x} - \\text{Proj}_{\\mathbf{y}} \\mathbf{x}\\) constitute an orthogonal basis of \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\). In particular, \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\) is 2-dimensional.</p> <p>The setup is symmetric in \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), so \\(\\{\\mathbf{x}, \\mathbf{y}' = \\mathbf{y} - \\text{Proj}_{\\mathbf{x}} \\mathbf{y}\\}\\) is also an orthogonal basis of \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\).</p> <p>Proof: Write \\(\\mathbf{x}' = \\mathbf{x} - \\text{Proj}_{\\mathbf{y}} \\mathbf{x}\\).</p> \\[\\mathbf{x}' \\cdot \\mathbf{y} = \\left(\\mathbf{x} - \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\mathbf{y} \\cdot \\mathbf{y}}\\mathbf{y}\\right) \\cdot \\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{y} - \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\mathbf{y} \\cdot \\mathbf{y}}\\mathbf{y} \\cdot \\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{y} - \\mathbf{x} \\cdot \\mathbf{y} = 0.\\] <p>Next, \\(\\mathbf{y}\\) is not zero (we have assumed this). Also, \\(\\mathbf{x}'\\) is not zero: if it were zero then \\(\\mathbf{x} = \\text{Proj}_{\\mathbf{y}}(\\mathbf{x})\\), yet such a projection is always a scalar multiple of \\(\\mathbf{y}\\) and we have assumed \\(\\mathbf{x}\\) is not a scalar multiple of \\(\\mathbf{y}\\). Therefore \\(\\{\\mathbf{x}', \\mathbf{y}\\}\\) is a pair of nonzero orthogonal vectors belonging to \\(\\text{span}(\\mathbf{x}, \\mathbf{y})\\) by design (note that \\(\\mathbf{y} = 0 \\cdot \\mathbf{x} + 1 \\cdot \\mathbf{y}\\)), and they exhaust that span since we can also write each of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) as linear combinations of \\(\\mathbf{x}'\\) and \\(\\mathbf{y}\\): \\(\\mathbf{x} = \\mathbf{x}' + \\text{Proj}_{\\mathbf{y}}(\\mathbf{x}) = \\mathbf{x}' + ((\\mathbf{x} \\cdot \\mathbf{y})/(\\mathbf{y} \\cdot \\mathbf{y}))\\mathbf{y}\\) and \\(\\mathbf{y} = 0 \\cdot \\mathbf{x}' + 1 \\cdot \\mathbf{y}\\). Since any collection of pairwise orthogonal nonzero vectors is a basis for its span, we conclude that \\(\\{\\mathbf{x}', \\mathbf{y}\\}\\) is an orthogonal basis of \\(\\text{span}(\\mathbf{x}', \\mathbf{y}) = \\text{span}(\\mathbf{x}, \\mathbf{y})\\).</p> <p>Now suppose we are given \\(n\\) data points \\((x_i, y_i)\\), assembled into \\(n\\)-vectors</p> \\[X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad \\text{and} \\quad Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\] <p>Earlier, we described the relationship between the correlation coefficient \\(r\\) for the recentered data (corresponding to the \\(n\\)-vectors \\(\\hat{X}\\) and \\(\\hat{Y}\\)) and the line of best fit. Let's restate that in terms of \\(r^2\\), which we expressed as the formula</p> \\[r^2 = \\frac{(\\hat{X} \\cdot \\hat{Y})^2}{\\|\\hat{X}\\|^2\\|\\hat{Y}\\|^2} = \\frac{(\\hat{X} \\cdot \\hat{Y})^2}{(\\hat{X} \\cdot \\hat{X})(\\hat{Y} \\cdot \\hat{Y})}\\] <p>We stated that \\(r^2\\) is near 0 when the line of best fit is a bad fit, and near 1 when it is a good fit (note that this could happen either when \\(r\\) is near 1, or when \\(r\\) is near \u22121). We made the role of \\(r^2\\) as a measure of quality of fit precise. Here is the derivation of \\(\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2 (1 - r^2)\\).</p> <p>Proof: We know that the closest vector to \\(Y\\) in \\(\\text{span}(X, \\mathbf{1})\\) is</p> \\[\\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}} \\mathbf{1} = \\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}} \\hat{X} + \\bar{y} \\mathbf{1}\\] <p>where \\(\\bar{y} = (1/n) \\sum_{i=1}^n y_i\\) is the average of the \\(y_i\\)'s.</p> \\[Y - (mX + b\\mathbf{1}) = Y - \\left(\\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X} + \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}}\\mathbf{1}\\right) = \\left(Y - \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}}\\mathbf{1}\\right) - \\frac{Y \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X}\\] <p>where \\(Y - \\frac{Y \\cdot \\mathbf{1}}{\\mathbf{1} \\cdot \\mathbf{1}}\\mathbf{1} = Y - \\bar{y}\\mathbf{1}\\) is indeed equal to \\(\\hat{Y}\\).</p> <p>Note that \\(\\hat{Y} \\cdot \\hat{X} = Y \\cdot \\hat{X}\\) because the difference \\(\\hat{Y} - Y = -\\bar{y}\\mathbf{1}\\) is orthogonal to \\(\\hat{X}\\).</p> <p>To understand why \\(\\hat{Y} \\cdot \\hat{X} = Y \\cdot \\hat{X}\\), let's examine the orthogonality of \\(\\hat{Y} - Y = -\\bar{y}\\mathbf{1}\\) to \\(\\hat{X}\\). Recall that \\(\\hat{X} = X - \\bar{x}\\mathbf{1}\\), which means \\(\\hat{X}\\) is the vector \\(X\\) with the mean \\(\\bar{x}\\) subtracted from each component. We need to show that \\((-\\bar{y}\\mathbf{1}) \\cdot \\hat{X} = 0\\). This is:</p> \\[(-\\bar{y}\\mathbf{1}) \\cdot \\hat{X} = -\\bar{y}\\mathbf{1} \\cdot (X - \\bar{x}\\mathbf{1}) = -\\bar{y}(\\mathbf{1} \\cdot X) + \\bar{y}\\bar{x}(\\mathbf{1} \\cdot \\mathbf{1})\\] <p>But \\(\\mathbf{1} \\cdot X = \\sum_{i=1}^n x_i = n\\bar{x}\\) and \\(\\mathbf{1} \\cdot \\mathbf{1} = n\\).</p> <p>So: \\(-\\bar{y}(\\mathbf{1} \\cdot X) + \\bar{y}\\bar{x}(\\mathbf{1} \\cdot \\mathbf{1}) = -\\bar{y}(n\\bar{x}) + \\bar{y}\\bar{x}(n) = -n\\bar{x}\\bar{y} + n\\bar{x}\\bar{y} = 0\\)</p> \\[\\hat{Y} \\cdot \\hat{X} = (Y + (\\hat{Y} - Y)) \\cdot \\hat{X} = Y \\cdot \\hat{X} + (\\hat{Y} - Y) \\cdot \\hat{X} = Y \\cdot \\hat{X} + 0 = Y \\cdot \\hat{X}\\] <p>Putting this into the numerator of the final coefficient on the right side yields</p> \\[Y - (mX + b\\mathbf{1}) = \\hat{Y} - \\frac{\\hat{Y} \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X} = \\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}\\] <p>The vectors \\(\\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}\\) and \\(\\text{Proj}_{\\hat{X}}\\hat{Y}\\) are perpendicular to each other. Therefore, by the Pythagorean Theorem in \\(\\mathbb{R}^n\\), we have</p> \\[\\|\\hat{Y}\\|^2 = \\|(\\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}) + \\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2 = \\|\\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2 + \\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2\\] <p>so \\(\\|\\hat{Y} - \\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2 = \\|\\hat{Y}\\|^2 - \\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2\\). But the vector difference on the left side is exactly \\(Y - (mX + b\\mathbf{1})\\), so</p> \\[\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2 - \\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2\\] <p>Finally, using the definition of \\(\\text{Proj}_{\\hat{X}}\\hat{Y}\\), we have</p> \\[\\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2 = \\left(\\frac{\\hat{Y} \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X}\\right) \\cdot \\left(\\frac{\\hat{Y} \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\hat{X}\\right) = \\left(\\frac{\\hat{Y} \\cdot \\hat{X}}{\\hat{X} \\cdot \\hat{X}}\\right)^2\\hat{X} \\cdot \\hat{X} = \\frac{(\\hat{Y} \\cdot \\hat{X})^2}{\\hat{X} \\cdot \\hat{X}} = r^2(\\hat{Y} \\cdot \\hat{Y})\\] <p>so plugging into \\(\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2 - \\|\\text{Proj}_{\\hat{X}}\\hat{Y}\\|^2\\) yields \\(\\|Y - (mX + b\\mathbf{1})\\|^2 = \\|\\hat{Y}\\|^2(1 - r^2)\\), which is exactly the desired identity.</p>"},{"location":"math/linear_algebra/basis_and_orthogonality/","title":"Basis and orthogonality","text":"<p>A basis for a nonzero linear subspace \\(V\\) in \\(\\mathbb{R}^n\\) is a spanning set for \\(V\\) consisting of exactly \\(\\dim(V)\\) vectors.</p> <p>If \\(\\dim(V) = 2\\) then a basis for \\(V\\) consists of any v, w for which \\(\\text{span}(v,w) = V\\).</p> <p>One basis of \\(\\mathbb{R}^3\\) is given by \\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\), \\(\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\\), \\(\\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\); this is called the standard basis of \\(\\mathbb{R}^3\\). But many other triples of vectors are also a basis of \\(\\mathbb{R}^3\\).</p> <p>Although we have a way to figure out the dimension of the span of 2 or 3 nonzero vectors, we have to confront the reality that for the span of 4 or more nonzero vectors in \\(\\mathbb{R}^n\\) it becomes rather cumbersome to figure out the dimension via algebra alone; we need another way.</p> <p>A collection of vectors v\\(_1\\), . . . , v\\(_k\\) in \\(\\mathbb{R}^n\\) is called orthogonal if \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) whenever \\(i \\neq j\\). In words, the vectors are all perpendicular to one another.</p> <p>If v\\(_1\\), . . . , v\\(_k\\) is an orthogonal collection of nonzero vectors in \\(\\mathbb{R}^n\\) then it is a basis for \\(\\text{span}(v_1, \\ldots, v_k)\\). In particular, \\(\\text{span}(v_1, \\ldots, v_k)\\) then has dimension \\(k\\) and we call v\\(_1\\), . . . , v\\(_k\\) an orthogonal basis for its span (a single nonzero vector is always an orthogonal basis for its span!).</p> <p>The span of a collection of \\(k\\) vectors in \\(\\mathbb{R}^n\\) has dimension at most \\(k\\) (e.g., three vectors in \\(\\mathbb{R}^3\\) lying in a common plane through 0 have span with dimension less than 3). Orthogonality is a useful way to guarantee that \\(k\\) given nonzero \\(n\\)-vectors have a \\(k\\)-dimensional span.</p> <p>Example: Consider the span \\(V\\) of the following three vectors in \\(\\mathbb{R}^5\\):</p> \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\end{bmatrix}\\] <p>This collection of three vectors is not orthogonal, since, for example, \\(\\mathbf{v}_1 \\cdot \\mathbf{v}_2 = 1 + 0 + 6 + 0 + 3 = 10\\). We can show that \\(\\dim(V) = 3\\), so the triple \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) is a basis of \\(V\\) (if \\(\\dim(V) = 2\\), then the triple would not be a basis of V, just a regular spanning set), but not an orthogonal basis of \\(V\\).</p> <p>Note: There is a systematic process for finding an orthogonal basis for the span of \\(k\\) vectors in \\(\\mathbb{R}^n\\) called the \"Gram\u2013Schmidt process\".</p> <p>Every nonzero linear subspace of \\(\\mathbb{R}^n\\) has an orthogonal basis.</p> <p>There is an especially convenient type of orthogonal basis for a nonzero linear subspace of \\(\\mathbb{R}^n\\). A collection of vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\) is called orthonormal if they are orthogonal to each other and in addition they are all unit vectors; that is, \\(\\mathbf{v}_i \\cdot \\mathbf{v}_i = 1\\) for all \\(i\\) (ensuring \\(\\|\\mathbf{v}_i\\| = \\sqrt{\\mathbf{v}_i \\cdot \\mathbf{v}_i} = \\sqrt{1} = 1\\) for all \\(i\\)).</p> <p>Any orthonormal collection of vectors is a basis of its span.</p> <p>For any \\(n\\) the analogous orthonormal collection of \\(n\\) vectors \\(\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\) in \\(\\mathbb{R}^n\\) can be written down (i.e., \\(\\mathbf{e}_i\\) has its \\(i\\)th entry equal to \\(1\\) and all other entries are \\(0\\)), and this spans \\(\\mathbb{R}^n\\); it is called the standard basis of \\(\\mathbb{R}^n\\), and shows \\(\\dim(\\mathbb{R}^n) = n\\) (as we expect). In particular, (with \\(V = \\mathbb{R}^n\\)), every linear subspace of \\(\\mathbb{R}^n\\) has dimension at most \\(n\\) and the only \\(n\\)-dimensional one is \\(\\mathbb{R}^n\\) itself (as geometric intuition may suggest).</p> <p>In the special case \\(n = 3\\), the vectors \\(\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3 \\in \\mathbb{R}^3\\) are often respectively denoted as \\(\\mathbf{i}, \\mathbf{j}, \\mathbf{k}\\) in physics and engineering contexts.</p> <p>Example: The triple</p> \\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\end{bmatrix}, \\quad \\begin{bmatrix} -6 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} -2 \\\\ -25 \\\\ 13 \\end{bmatrix}\\] <p>is an orthogonal basis for \\(\\mathbb{R}^3\\). How can one \"see\" this? One can check by hand that it is an orthogonal collection of vectors, so this collection of \\(3\\) nonzero vectors must be a basis of its span by, and hence its span has dimension \\(3\\).</p>"},{"location":"math/linear_algebra/basis_and_orthogonality/#fourier-formula","title":"Fourier formula","text":"<p>If \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) are nonzero vectors in \\(\\mathbb{R}^n\\), by definition any vector \\(\\mathbf{v} \\in \\text{span}(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k)\\) can be written as a linear combination</p> \\[\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i\\] <p>for some scalars \\(c_1, \\ldots, c_k\\). If the collection of \\(\\mathbf{v}_i\\)'s is orthogonal, we can actually solve for the \\(c_i\\)'s in terms of \\(\\mathbf{v}\\) by the following slick technique that has useful generalizations throughout mathematics (with Fourier series, special function theory, and so on).</p> <p>For instance, if we form the dot product against \\(\\mathbf{v}_1\\) then we obtain</p> \\[\\mathbf{v} \\cdot \\mathbf{v}_1 = c_1(\\mathbf{v}_1 \\cdot \\mathbf{v}_1) + c_2(\\mathbf{v}_2 \\cdot \\mathbf{v}_1) + c_3(\\mathbf{v}_3 \\cdot \\mathbf{v}_1) + \\cdots = c_1(\\mathbf{v}_1 \\cdot \\mathbf{v}_1),\\] <p>where the tremendous cancellation at the final equality is precisely due to the orthogonality of the collection of \\(\\mathbf{v}_i\\)'s. Since \\(\\mathbf{v}_1\\) is nonzero, so \\(\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = \\|\\mathbf{v}_1\\|^2\\) is nonzero, we can now divide by it at both ends of our string of equalities above to obtain</p> \\[\\frac{\\mathbf{v} \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1} = c_1.\\] <p>In this way we have solved for \\(c_1\\)!</p> <p>The same procedure works likewise to solve for each \\(c_i\\) via forming dot products against \\(\\mathbf{v}_i\\), yielding the general formula</p> \\[c_i = \\frac{\\mathbf{v} \\cdot \\mathbf{v}_i}{\\mathbf{v}_i \\cdot \\mathbf{v}_i}\\] <p>for each \\(i\\). Substituting back into the right side of the equation for \\(\\mathbf{v}\\), we obtain the following result.</p> <p>Theorem (Fourier formula). For any orthogonal collection of nonzero vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\) and vector \\(\\mathbf{v}\\) in their span,</p> \\[\\mathbf{v} = \\sum_{i=1}^k \\frac{\\mathbf{v} \\cdot \\mathbf{v}_i}{\\mathbf{v}_i \\cdot \\mathbf{v}_i} \\mathbf{v}_i.\\] <p>In particular, if the \\(\\mathbf{v}_i\\)'s are all unit vectors (so \\(\\mathbf{v}_i \\cdot \\mathbf{v}_i = 1\\) for all \\(i\\)) then \\(\\mathbf{v} = \\sum_{i=1}^k (\\mathbf{v} \\cdot \\mathbf{v}_i)\\mathbf{v}_i\\).</p> <p>Example: For the orthonormal basis \\(\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3, \\mathbf{e}_4\\) of \\(\\mathbb{R}^4\\) and any \\(\\mathbf{v} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} \\in \\mathbb{R}^4\\), the coefficients \\(\\mathbf{v} \\cdot \\mathbf{e}_i\\) work out as follows:</p> \\[\\mathbf{v} \\cdot \\mathbf{e}_1 = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = a_1\\] <p>and similarly \\(\\mathbf{v} \\cdot \\mathbf{e}_i = a_i\\) for each \\(i = 1, 2, 3, 4\\). Thus (since \\(\\mathbf{e}_i \\cdot \\mathbf{e}_i = 1\\)), \\(\\mathbf{v} = \\sum_{i=1}^4 (\\mathbf{v} \\cdot \\mathbf{e}_i)\\mathbf{e}_i = \\sum_{i=1}^4 a_i\\mathbf{e}_i\\). Unpacking the summation notation, this is just asserting</p> \\[\\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} = a_1\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + a_2\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + a_3\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + a_4\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\\] <p>which can be directly verified by hand since the right side is exactly</p> \\[\\begin{bmatrix} a_1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ a_2 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ a_3 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ a_4 \\end{bmatrix}.\\] <p>In other words, the Fourier formula in the special case that \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) is the orthonormal basis \\(\\{\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\}\\) of \\(\\mathbb{R}^n\\) is precisely the familiar fact that any vector in \\(\\mathbb{R}^n\\) can be decomposed as the sum of its \"components\" along the various coordinate directions. This is neither surprising nor perhaps particularly interesting, so we next give a more \"typical\" example.</p> <p>Example: Consider the span \\(V\\) of the following three vectors in \\(\\mathbb{R}^5\\):</p> \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\end{bmatrix}\\] <p>Consider the following three nonzero vectors in \\(V\\), which form an orthogonal basis for their span:</p> \\[\\mathbf{w}_1 = \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{w}_2 = -2\\mathbf{v}_1 + 3\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 3 \\\\ 0 \\\\ -4 \\\\ 7 \\end{bmatrix}, \\quad \\mathbf{w}_3 = -9\\mathbf{v}_1 - 24\\mathbf{v}_2 + 75\\mathbf{v}_3 = \\begin{bmatrix} -33 \\\\ 201 \\\\ -75 \\\\ 132 \\\\ -6 \\end{bmatrix}\\] <p>Consider the vector</p> \\[\\mathbf{v} = 2\\mathbf{v}_1 - \\mathbf{v}_2 + \\mathbf{v}_3 = \\begin{bmatrix} 2 \\\\ 0 \\\\ 6 \\\\ 4 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\\\ 6 \\\\ 0 \\end{bmatrix}\\] <p>in \\(V\\). Since \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}\\) is a basis of \\(V\\), we know that there is some expression of the form</p> \\[\\mathbf{v} = c_1\\mathbf{w}_1 + c_2\\mathbf{w}_2 + c_3\\mathbf{w}_3\\] <p>for unknown scalars \\(c_1, c_2, c_3\\). What are these scalars? A brute-force approach would be to write everything out as explicit vectors to obtain</p> \\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\\\ 6 \\\\ 0 \\end{bmatrix} = \\mathbf{v} = c_1\\mathbf{w}_1 + c_2\\mathbf{w}_2 + c_3\\mathbf{w}_3 = c_1\\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix} + c_2\\begin{bmatrix} 1 \\\\ 3 \\\\ 0 \\\\ -4 \\\\ 7 \\end{bmatrix} + c_3\\begin{bmatrix} -33 \\\\ 201 \\\\ -75 \\\\ 132 \\\\ -6 \\end{bmatrix} = \\begin{bmatrix} c_1 + c_2 - 33c_3 \\\\ 3c_2 + 201c_3 \\\\ 3c_1 - 75c_3 \\\\ 2c_1 - 4c_2 + 132c_3 \\\\ c_1 + 7c_2 - 6c_3 \\end{bmatrix},\\] <p>and then equate corresponding vector entries on the left and right sides to get a huge system of \\(5\\) equations in \\(3\\) unknowns. We can entirely bypass that by computing dot products for our specific \\(\\mathbf{v}\\)!</p> <p>To carry this out, we use the explicit descriptions of \\(\\mathbf{v}\\) and the \\(\\mathbf{w}_i\\)'s to compute</p> \\[\\mathbf{v} \\cdot \\mathbf{w}_1 = 25, \\quad \\mathbf{v} \\cdot \\mathbf{w}_2 = -17, \\quad \\mathbf{v} \\cdot \\mathbf{w}_3 = 861,\\] <p>so the Fourier formula says for this particular \\(\\mathbf{v}\\) that</p> \\[\\mathbf{v} = \\frac{25}{15}\\mathbf{w}_1 - \\frac{17}{75}\\mathbf{w}_2 + \\frac{861}{64575}\\mathbf{w}_3 = \\frac{5}{3}\\mathbf{w}_1 - \\frac{17}{75}\\mathbf{w}_2 + \\frac{1}{75}\\mathbf{w}_3.\\] <p>That's it! This is the expression for \\(\\mathbf{v}\\) as a linear combination of the orthogonal basis \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}\\) of \\(V\\).</p>"},{"location":"math/linear_algebra/planes_in_r3/","title":"Planes in \\(\\mathbb{R}^3\\)","text":"<p>The collection of points \\((x, y, z)\\) in \\(\\mathbb{R}^3\\) satisfying an equation of the form</p> \\[ax + by + cz = d\\] <p>with at least one of the constants \\(a\\), \\(b\\), or \\(c\\) nonzero, is a plane in \\(\\mathbb{R}^3\\). Note that although the equation \\(x = 0\\) on \\(\\mathbb{R}^2\\) defines a line (the \\(y\\)-axis, consisting of points \\((0, y)\\)), the \"same\" equation \\(x = 0\\) on \\(\\mathbb{R}^3\\) defines a plane, namely the vertical \\(yz\\)-plane consisting of points \\((0, y, z)\\).</p>"},{"location":"math/linear_algebra/planes_in_r3/#lines-in-mathbbr3","title":"Lines in \\(\\mathbb{R}^3\\)","text":"<p>In \\(\\mathbb{R}^3\\), a line can be represented in several forms:</p> <p>Parametric form (most common): \\(\\mathbf{r}(t) = \\mathbf{r}_0 + t\\mathbf{v}\\) where \\(\\mathbf{r}_0\\) is a point on the line and \\(\\mathbf{v}\\) is a direction vector.</p> <p>As an example, consider a line that passes through the point \\((1, 0, -2)\\) and has direction vector \\((3, 1, 4)\\). The parametric equation is:</p> \\[\\mathbf{r}(t) = (1, 0, -2) + t(3, 1, 4) = (1 + 3t, 0 + t, -2 + 4t)\\] <p>This means any point on the line has coordinates \\((1 + 3t, t, -2 + 4t)\\) for some real number \\(t\\). For example:</p> <ul> <li> <p>When \\(t = 0\\): \\((1, 0, -2)\\) (the base point)</p> </li> <li> <p>When \\(t = 1\\): \\((4, 1, 2)\\)</p> </li> <li> <p>When \\(t = -1\\): \\((-2, -1, -6)\\)</p> </li> </ul> <p>Parametric form through two points: Given two points \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\), the line passing through them has parametric equation:</p> \\[\\mathbf{r}(t) = \\mathbf{p} + t(\\mathbf{q} - \\mathbf{p}) = (1-t)\\mathbf{p} + t\\mathbf{q}\\] <p>This form uses the direction vector \\(\\mathbf{q} - \\mathbf{p}\\) and parameter \\(t\\) ranges from 0 to 1 to give all points between \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\).</p> <p>Example: For points \\(\\mathbf{p} = (1, 2, 3)\\) and \\(\\mathbf{q} = (4, 1, 0)\\), the line equation is:</p> \\[\\mathbf{r}(t) = (1, 2, 3) + t(3, -1, -3) = (1 + 3t, 2 - t, 3 - 3t)\\] <p>Symmetric form (when all components of \\(\\mathbf{v}\\) are nonzero): \\(\\frac{x - x_0}{v_1} = \\frac{y - y_0}{v_2} = \\frac{z - z_0}{v_3}\\)</p> <p>In the symmetric form:</p> <ul> <li> <p>\\((x_0, y_0, z_0)\\) is a point on the line (the base point)</p> </li> <li> <p>\\((v_1, v_2, v_3)\\) are the components of the direction vector \\(\\mathbf{v}\\)</p> </li> </ul> <p>So if we have a line with parametric form \\(\\mathbf{r}(t) = \\mathbf{r}_0 + t\\mathbf{v}\\), then \\((x_0, y_0, z_0) = \\mathbf{r}_0\\) and \\((v_1, v_2, v_3) = \\mathbf{v}\\).</p> <p>For example, if a line passes through the point \\((2, -1, 3)\\) and has direction vector \\((1, 2, -1)\\), then the symmetric form would be:</p> \\[\\frac{x - 2}{1} = \\frac{y - (-1)}{2} = \\frac{z - 3}{-1}\\] <p>This form eliminates the parameter \\(t\\) and gives a direct relationship between the coordinates, but it only works when all components of the direction vector are nonzero (to avoid division by zero).</p> <p>Intersection of two planes:  \\(\\begin{cases} a_1x + b_1y + c_1z = d_1 \\\\ a_2x + b_2y + c_2z = d_2 \\end{cases}\\)</p> <p>The key difference from \\(\\mathbb{R}^2\\) is that in \\(\\mathbb{R}^3\\), a single linear equation \\(ax + by + cz = d\\) defines a plane, not a line. To define a line in \\(\\mathbb{R}^3\\), you need either a parametric equation with one parameter, the intersection of two planes (two linear equations), or, a point and a direction vector.</p>"},{"location":"math/linear_algebra/planes_in_r3/#forms-of-planes","title":"Forms of Planes","text":"<p>Planes in \\(\\mathbb{R}^3\\) can be represented in several different forms, each useful for different purposes:</p> <p>1. Point-normal form: \\((x - x_0, y - y_0, z - z_0) \\cdot \\mathbf{n} = 0\\)</p> <p>This form uses a point \\((x_0, y_0, z_0)\\) on the plane and a normal vector \\(\\mathbf{n} = (a, b, c)\\) perpendicular to the plane.</p> <p></p> <p>Example: A plane (shown above) passing through the point \\((0, 1, 1)\\) with normal vector \\((3, -2, 1)\\) has equation:</p> \\[(x - 0, y - 1, z - 1) \\cdot (3, -2, 1) = 0\\] \\[3(x - 0) - 2(y - 1) + 1(z - 1) = 0\\] \\[3x - 2y + z + 1 = 0\\] <p>From the coefficients we again read off that (3, -2, 1) is a normal vector to the plane (not to the individual points in the plane, but rather to differences between such points). This is no surprise, in view of how the plane was originally defined.</p> <p>2. General form: \\(ax + by + cz + d = 0\\)</p> <p>This is the standard form where \\((a, b, c)\\) is a normal vector to the plane and \\(d\\) determines the position of the plane in space.</p> <p>Example: The plane \\(2x - 3y + 4z = 12\\) has normal vector \\((2, -3, 4)\\) and can be rewritten as \\(2x - 3y + 4z - 12 = 0\\).</p> <p>The plane \\(ax + by + cz + d = 0\\) is at a distance of \\(\\frac{|d|}{\\sqrt{a^2 + b^2 + c^2}}\\) from the origin. All planes with the same normal vector \\((a, b, c)\\) but different \\(d\\) values are parallel to each other.</p> <p>If you think of the normal vector \\((a,b,c)\\) as pointing in a fixed direction, then \\(d\\) tells you \"how far along that direction\" the plane is located. For example, the planes \\(2x - 3y + 4z = 0\\), \\(2x - 3y + 4z = 5\\), and \\(2x - 3y + 4z = -3\\) all have the same normal vector \\((2, -3, 4)\\) but are at different distances from the origin.</p> <p>3. Parametric form: \\(\\mathbf{r}(s,t) = \\mathbf{r}_0 + s\\mathbf{v}_1 + t\\mathbf{v}_2\\)</p> <p>This form uses a point \\(\\mathbf{r}_0\\) on the plane and two non-parallel direction vectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) that lie in the plane.</p> <p>Example: A plane through the point \\((1, 0, 2)\\) with direction vectors \\((1, 1, 0)\\) and \\((0, 1, 1)\\) has parametric equation: \\(\\mathbf{r}(s,t) = (1, 0, 2) + s(1, 1, 0) + t(0, 1, 1) = (1 + s, s + t, 2 + t)\\)</p> <p>An advantage of the parametric form is that as we independently vary values of the parameters \\(t\\) and \\(t\u2032\\), the vectors we get in the parametric form are guaranteed to lie exactly on the plane. For instance, if we want to trace out some path in the plane, then by varying the values of \\(t\\) and \\(t\u2032\\) continuously we trace out a continuous curve exactly in the plan.</p> <p></p> <p>The parametric description for planes and its analogues for more complicated surfaces (such as a sphere, a cylinder, etc.) is quite useful in computer graphics to generate the image of a path of motion lying exactly on a specific surface. For such applications a parametric form is far more useful than the general form; as with the parametric form we do not have to solve for anything.</p> <p>4. Three-point form: Using three non-collinear points</p> <p>Given three points \\((x_1, y_1, z_1)\\), \\((x_2, y_2, z_2)\\), and \\((x_3, y_3, z_3)\\), the plane equation is: \\(\\begin{vmatrix} x - x_1 &amp; y - y_1 &amp; z - z_1 \\\\ x_2 - x_1 &amp; y_2 - y_1 &amp; z_2 - z_1 \\\\ x_3 - x_1 &amp; y_3 - y_1 &amp; z_3 - z_1 \\end{vmatrix} = 0\\)</p> <p>Example: For points \\((1, 0, 0)\\), \\((0, 1, 0)\\), and \\((0, 0, 1)\\), the plane equation becomes:</p> \\[\\begin{vmatrix} x - 1 &amp; y &amp; z \\\\ -1 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 \\end{vmatrix} = 0\\] \\[(x - 1)(1 \\cdot 1 - 0 \\cdot 0) - y(-1 \\cdot 1 - 0 \\cdot (-1)) + z(-1 \\cdot 0 - 1 \\cdot (-1)) = 0\\] \\[(x - 1) + y + z = 0\\] \\[x + y + z = 1\\] <p>Note: We claim that the only way three different points can be on a common line in space is when the difference vectors from one of them to the other two lie along the same or opposite directions. This corresponds to those two difference vectors being scalar multiples of each other (a positive scalar when pointing in the same direction, and a negative scalar when pointing in opposite directions).</p> <p>5. Intercept form: \\(\\frac{x}{a} + \\frac{y}{b} + \\frac{z}{c} = 1\\)</p> <p>This form shows the intercepts of the plane with the coordinate axes: \\((a, 0, 0)\\), \\((0, b, 0)\\), and \\((0, 0, c)\\).</p> <p>Example: The plane \\(\\frac{x}{3} + \\frac{y}{2} + \\frac{z}{6} = 1\\) has intercepts at \\((3, 0, 0)\\), \\((0, 2, 0)\\), and \\((0, 0, 6)\\).</p> <p>Each form has its advantages. The Point-normal form is useful for finding distance from a point to a plane. The General form is standard for solving systems of equations. The Parametric form is convenient for generating points on the plane. Three-point form is natural when given three points. The Intercept form provides immediate geometric insight.</p>"},{"location":"math/linear_algebra/projections/","title":"Projections","text":"<p>For many applications in engineering, physics, and data-related problems in all scientific fields (genetics, economics, neuroscience, computer science, etc.) it is essential to go far beyond \\(\\mathbb{R}^3\\) and solve problems involving distance minimization to subspaces in \\(\\mathbb{R}^n\\) for any \\(n\\):</p> <p>If \\(V\\) is a linear subspace in \\(\\mathbb{R}^n\\) and \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is some point, then what point in \\(V\\) is closest to \\(\\mathbf{x}\\)? This closest point will be called the projection of \\(\\mathbf{x}\\) into \\(V\\).</p>"},{"location":"math/linear_algebra/projections/#the-closest-point-to-a-line","title":"The closest point to a line","text":"<p>The way we will solve the general problem of distance minimization from a point in \\(\\mathbb{R}^n\\) to a linear subspace \\(V\\) involves \"assembling\" a collection of solutions to distance minimization to certain \\(1\\)-dimensional linear subspaces of \\(V\\), or in other words, solving distance minimization problems to a collection of lines through \\(0\\) in \\(\\mathbb{R}^n\\).</p> <p>Since minimizing distance to lines will be the foundation for the general case, we begin by focusing on this special case. Consider a line \\(L\\) in \\(\\mathbb{R}^n\\) through \\(0\\), so \\(L = \\text{span}(\\mathbf{w}) = \\{c\\mathbf{w} : c \\in \\mathbb{R}\\}\\) where \\(\\mathbf{w} \\in \\mathbb{R}^n\\) is some nonzero vector. For any point \\(\\mathbf{x} \\in \\mathbb{R}^n\\), we want to show that there is a unique point in \\(L\\) closest to \\(\\mathbf{x}\\), and to actually give a formula for how to compute this nearest point to \\(\\mathbf{x}\\) in \\(L\\). </p> <p>Here is the fundamental idea: although our task (necessary for many applications!) takes place in \\(\\mathbb{R}^n\\) with completely general (and possibly huge) \\(n\\), we look at a low-dimensional instance of the problem in the hope that the low-dimensional case will suggest some feature that has a chance to adapt to the general situation. This balancing of insight from pictures in low-dimensional cases alongside algebraic work and geometric language developed for \\(\\mathbb{R}^n\\) with general \\(n\\) is an important part of linear algebra, giving visual insight into \\(\\mathbb{R}^n\\) for big \\(n\\). This doesn't justify results in \\(\\mathbb{R}^n\\) for general \\(n\\), but it inspires what we should expect and/or try to prove is true.</p> <p>Let's look at the case \\(n = 2\\) as shown in the figure below.</p> <p></p> <p>The key insight suggested by the figure above is that the point on the line \\(L\\) that is closest to \\(\\mathbf{x}\\) has another characterization (that in turn will allow us to compute it): it is the one point on \\(L\\) for which the displacement vector to \\(\\mathbf{x}\\) (the dotted line segment joining it to \\(\\mathbf{x}\\) as in the figure above) is perpendicular to \\(L\\), or equivalently is perpendicular to \\(\\mathbf{w}\\). Visually, the perpendicular direction to \\(L\\) from \\(\\mathbf{x}\\) is the \"most direct\" route. Or put another way, you may convince yourself by drawing some pictures that any deviation from perpendicularity entails a longer path from \\(\\mathbf{x}\\) to the line \\(L\\).</p> <p>To summarize, the figure above suggests a workable idea: the point \\(c\\mathbf{w} \\in L\\) for which \\(\\|\\mathbf{x} - c\\mathbf{w}\\|\\) is minimal should also have the property that \\(\\mathbf{x} - c\\mathbf{w}\\) is orthogonal to everything in \\(L\\). Although this idea is suggested by the picture in \\(\\mathbb{R}^2\\), as written it makes equally good sense in \\(\\mathbb{R}^n\\) for any \\(n\\) whatsoever. But is it true? And even once we know it is true, how can we exploit this property of the nearest point to \\(\\mathbf{x}\\) on \\(L\\) to actually compute this nearest point?</p> <p>The informal reasoning above may have already convinced you that the distance is minimized precisely when the displacement vector is orthogonal to \\(L\\).</p> <p>Method I (algebraic). In accordance with the idea inspired by the figure above in the case \\(n = 2\\), for general \\(n\\) we look for a scalar \\(c\\) for which \\(\\mathbf{x} - c\\mathbf{w}\\) is orthogonal to every vector in \\(L\\). The points of \\(L = \\text{span}(\\mathbf{w})\\) are those of the form \\(a\\mathbf{w}\\) for scalars \\(a\\), so we seek \\(c\\) making \\((\\mathbf{x} - c\\mathbf{w}) \\cdot (a\\mathbf{w}) = 0\\) for every scalar \\(a\\). The dot product has the property that \\((\\mathbf{x} - c\\mathbf{w}) \\cdot (a\\mathbf{w}) = a((\\mathbf{x} - c\\mathbf{w}) \\cdot \\mathbf{w})\\), so actually it suffices to make sure that \\((\\mathbf{x} - c\\mathbf{w}) \\cdot \\mathbf{w} = 0\\). We can use the further properties of the dot product to rewrite this as</p> \\[0 = (\\mathbf{x} - c\\mathbf{w}) \\cdot \\mathbf{w} = \\mathbf{x} \\cdot \\mathbf{w} - (c\\mathbf{w}) \\cdot \\mathbf{w}.\\] <p>We can rearrange this expression to write it as \\(c(\\mathbf{w} \\cdot \\mathbf{w}) = \\mathbf{x} \\cdot \\mathbf{w}\\). But \\(\\mathbf{w} \\cdot \\mathbf{w} = \\|\\mathbf{w}\\|^2 &gt; 0\\) (since \\(\\mathbf{w} \\neq 0\\)), so it makes sense to divide both sides by \\(\\mathbf{w} \\cdot \\mathbf{w}\\) to obtain that \\(c = \\frac{\\mathbf{x} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}}\\). This is the coefficient we had previously regarded as unknown! </p> <p>To summarize, we have shown through algebra and the properties of dot products that there is exactly one point in the line \\(L = \\text{span}(\\mathbf{w})\\) through \\(0\\) in \\(\\mathbb{R}^n\\) whose difference from \\(\\mathbf{x}\\) is orthogonal to everything in \\(L\\): it is \\(\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w}\\). We have not yet actually shown that this scalar multiple of \\(\\mathbf{w}\\) on \\(L\\) is closer to \\(\\mathbf{x}\\) than every other vector in \\(L\\), but if we believe the orthogonality insight inspired by the \\(2\\)-dimensional picture in the figure above then this must be that closest point. As a bonus, we have obtained an explicit formula for it!</p> <p>Method II (geometric). We next use some plane geometry via the figure above to obtain the same formula for the closest point. Strictly speaking, this argument only applies when \\(n = 2\\), but you might find that it gives the formula some visual meaning that is somehow lacking in the purely algebraic work in Method I.</p> <p>There is nothing to be done if \\(\\mathbf{x} \\cdot \\mathbf{w} = 0\\) (in that case \\(\\mathbf{x}\\) is perpendicular to \\(L\\) and we are thereby convinced that \\(0\\) is the closest point, as is also given by the desired formula). Hence, we can suppose \\(\\mathbf{x} \\neq 0\\) and the angle \\(\\theta\\) between \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\) satisfies either \\(0\u00b0 &lt; \\theta &lt; 90\u00b0\\) or \\(90\u00b0 &lt; \\theta &lt; 180\u00b0\\).</p> <p>The case of acute \\(\\theta\\) is shown in the figure above, whereas if \\(\\theta\\) is obtuse then the point we seek would be in the direction of \\(-\\mathbf{w}\\) (rather than in the direction of \\(\\mathbf{w}\\)). If \\(\\theta\\) is acute, as in the figure above, then by basic trigonometry, the leg along \\(L\\) for the right triangle as shown has length \\(\\|\\mathbf{x}\\| \\cos(\\theta)\\) and it points in the direction of the unit vector \\(\\mathbf{w}/\\|\\mathbf{w}\\|\\). This says that the endpoint on \\(L\\) of the dotted segment is the vector</p> \\[(\\|\\mathbf{x}\\| \\cos(\\theta)) \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}.\\] <p>But \\(\\cos(\\theta) = (\\mathbf{x} \\cdot \\mathbf{w})/(\\|\\mathbf{x}\\|\\|\\mathbf{w}\\|)\\), so plugging this into the equation above yields the desired formula since \\(\\|\\mathbf{w}\\|^2 = \\mathbf{w} \\cdot \\mathbf{w}\\). The case when \\(90\u00b0 &lt; \\theta &lt; 180\u00b0\\) goes very similarly, except now \\(\\cos(\\theta) &lt; 0\\) (so the endpoint on \\(L\\) of the dotted segment is in the direction of the opposite unit vector \\(-\\mathbf{w}/\\|\\mathbf{w}\\|\\)) and we have to work with the length \\(\\|\\mathbf{x}\\| |\\cos(\\theta)| = -\\|\\mathbf{x}\\| \\cos(\\theta)\\). Putting these together, the two signs cancel and we get the desired formula again.</p> <p>Proposition. Let \\(L = \\text{span}(\\mathbf{w}) = \\{c\\mathbf{w} : c \\in \\mathbb{R}\\}\\) be a \\(1\\)-dimensional linear subspace of \\(\\mathbb{R}^n\\) (so \\(\\mathbf{w} \\neq 0\\)), a \"line\". Choose any point \\(\\mathbf{x} \\in \\mathbb{R}^n\\). There is exactly one point in \\(L\\) closest to \\(\\mathbf{x}\\), and it is given by the scalar multiple</p> \\[\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w}\\] <p>of \\(\\mathbf{w}\\). This is called \"the projection of \\(\\mathbf{x}\\) into \\(\\text{span}(\\mathbf{w})\\)\"; we denote it by the symbol \\(\\text{Proj}_{\\mathbf{w}} \\mathbf{x}\\).</p> <p>Note: This can be seen as (dot product of \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\)), which is a scalar, times the unit vector in direction of \\(\\mathbf{w}\\).</p> <p>Example: Consider \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}\\). The numbers \\(1, 3, 4\\) represent the amount of \\(\\mathbf{v}\\) that points along the \\(x, y, z\\)-axes respectively. More precisely, \\(\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\mathbf{e}_1\\) is the component of \\(\\mathbf{v}\\) along the \\(x\\)-axis line, \\(\\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\end{bmatrix} = 3\\mathbf{e}_2\\) is the component of \\(\\mathbf{v}\\) along the \\(y\\)-axis line, and \\(\\begin{bmatrix} 0 \\\\ 0 \\\\ 4 \\end{bmatrix} = 4\\mathbf{e}_3\\) is the component of \\(\\mathbf{v}\\) along the \\(z\\)-axis line. These are the closest points to \\(\\mathbf{v}\\) on the \\(x\\)-, \\(y\\)- and \\(z\\)-axes, respectively.</p> <p>In terms of this data, we want to compute the projection of \\(\\mathbf{v}\\) on some line pointing with some other direction: if \\(\\mathbf{w}\\) is a (nonzero) vector along this new direction, we want to compute \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{v})\\). The key point is that the formula for \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{x})\\) behaves well for any linear combination of any \\(n\\)-vectors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_k\\): the projection of a linear combination of the \\(\\mathbf{x}_i\\)'s is equal to the corresponding linear combination of the projections.</p> <p>For example, with \\(k = 2\\) it says \\(\\text{Proj}_{\\mathbf{w}}(5\\mathbf{x}_1 - 7\\mathbf{x}_2) = 5 \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_1) - 7 \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_2)\\) and likewise with \\(5\\) and \\(-7\\) replaced by any two scalars. The reason this works is an algebraic calculation:</p> \\[\\text{Proj}_{\\mathbf{w}}(c_1\\mathbf{x}_1 + \\cdots + c_k\\mathbf{x}_k) = \\frac{(c_1\\mathbf{x}_1 + \\cdots + c_k\\mathbf{x}_k) \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = \\frac{c_1(\\mathbf{x}_1 \\cdot \\mathbf{w}) + \\cdots + c_k(\\mathbf{x}_k \\cdot \\mathbf{w})}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = c_1 \\frac{\\mathbf{x}_1 \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} + \\cdots + c_k \\frac{\\mathbf{x}_k \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = c_1 \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_1) + \\cdots + c_k \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_k).\\] <p>Applying this to the expression \\(\\mathbf{v} = \\mathbf{e}_1 + 3\\mathbf{e}_2 + 4\\mathbf{e}_3\\) yields \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{v}) = \\text{Proj}_{\\mathbf{w}}(\\mathbf{e}_1) + 3 \\text{Proj}_{\\mathbf{w}}(\\mathbf{e}_2) + 4 \\text{Proj}_{\\mathbf{w}}(\\mathbf{e}_3)\\).</p>"},{"location":"math/linear_algebra/projections/#projection-onto-a-general-subspace","title":"Projection onto a general subspace","text":"<p>Let's look at the case of a plane \\(V\\) through the origin in \\(\\mathbb{R}^3\\) equipped with a choice of orthogonal basis \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) of this plane. In the figure below, we draw the typical situation, indicating with the notation \\(\\text{Proj}_V(\\mathbf{x})\\) the point in \\(V\\) closest to \\(\\mathbf{x}\\). The first geometric insight, similar to our experience with lines, is that since this nearest point should have displacement vector to \\(\\mathbf{x}\\) that is the \"most direct\" route to \\(V\\) from \\(\\mathbf{x}\\), the displacement should involve \"no tilting\" relative to any direction within \\(V\\).</p> <p></p> <p>If you think about it, hopefully it seems plausible that if \\(\\mathbf{v} \\in V\\) makes the displacement \\(\\mathbf{x} - \\mathbf{v}\\) perpendicular to everything in \\(V\\) then \\(\\mathbf{v}\\) should be the point in \\(V\\) for which the direction of the displacement \\(\\mathbf{x} - \\mathbf{v}\\) is the \"most direct\" route from \\(\\mathbf{x}\\) to \\(V\\), making \\(\\mathbf{v}\\) the point in \\(V\\) nearest to \\(\\mathbf{x}\\).</p> <p>Theorem (Orthogonal Projection Theorem, version I). For any \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and linear subspace \\(V\\) of \\(\\mathbb{R}^n\\), there is a unique \\(\\mathbf{v}\\) in \\(V\\) closest to \\(\\mathbf{x}\\). In symbols, \\(\\|\\mathbf{x} - \\mathbf{v}\\| &lt; \\|\\mathbf{x} - \\mathbf{v}'\\|\\) for all \\(\\mathbf{v}'\\) in \\(V\\) with \\(\\mathbf{v}' \\neq \\mathbf{v}\\). This \\(\\mathbf{v}\\) is called the projection of \\(\\mathbf{x}\\) onto \\(V\\), and is denoted \\(\\text{Proj}_V(\\mathbf{x})\\); see the figure above. The projection \\(\\text{Proj}_V(\\mathbf{x})\\) is also the only vector \\(\\mathbf{v} \\in V\\) with the property that the displacement \\(\\mathbf{x} - \\mathbf{v}\\) is perpendicular to \\(V\\) (i.e., \\(\\mathbf{x} - \\mathbf{v}\\) is perpendicular to every vector in \\(V\\)).</p> <p>If \\(V\\) is nonzero then for any orthogonal basis \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) of \\(V\\) we have</p> \\[\\text{Proj}_V(\\mathbf{x}) = \\text{Proj}_{\\mathbf{v}_1}(\\mathbf{x}) + \\text{Proj}_{\\mathbf{v}_2}(\\mathbf{x}) + \\cdots + \\text{Proj}_{\\mathbf{v}_k}(\\mathbf{x}),\\] <p>where \\(\\text{Proj}_{\\mathbf{v}_i}(\\mathbf{x}) = \\frac{\\mathbf{x} \\cdot \\mathbf{v}_i}{\\mathbf{v}_i \\cdot \\mathbf{v}_i} \\mathbf{v}_i\\). For \\(\\mathbf{x} \\in V\\) we have \\(\\text{Proj}_V(\\mathbf{x}) = \\mathbf{x}\\) \u2013 the point in \\(V\\) closest to \\(\\mathbf{x}\\) is itself! \u2013 so the equation above for \\(\\mathbf{x} \\in V\\) recovers the Fourier formula!</p> <p>Theorem (Orthogonal Projection Theorem, version II). If \\(V\\) is a linear subspace of \\(\\mathbb{R}^n\\) then every vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) can be uniquely expressed as a sum</p> \\[\\mathbf{x} = \\mathbf{v} + \\mathbf{v}'\\] <p>with \\(\\mathbf{v} \\in V\\) and \\(\\mathbf{v}'\\) orthogonal to everything in \\(V\\). Explicitly, \\(\\mathbf{v} = \\text{Proj}_V(\\mathbf{x})\\) and \\(\\mathbf{v}' = \\mathbf{x} - \\text{Proj}_V(\\mathbf{x})\\).</p> <p>Since the \\(\\mathbf{v}_i\\)'s span \\(V\\), the point \\(\\mathbf{v} \\in V\\) closest to \\(\\mathbf{x}\\) can be written in the form \\(\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i\\) for some unknown coefficients \\(c_i\\). We are going to see that the perpendicularity of \\(\\mathbf{x} - \\mathbf{v}\\) to everything in \\(V\\) forces \\(c_i = (\\mathbf{x} \\cdot \\mathbf{v}_i)/(\\mathbf{v}_i \\cdot \\mathbf{v}_i)\\) for every \\(i\\). But then \\(c_i\\mathbf{v}_i\\) is exactly the formula for \\(\\text{Proj}_{\\mathbf{v}_i}(\\mathbf{x})\\), so we would obtain \\(\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i = \\sum_{i=1}^k \\text{Proj}_{\\mathbf{v}_i}(\\mathbf{x})\\) as asserted in the equation above.</p> <p>How can we show that the coefficients \\(c_i\\) are really given by the ratios \\((\\mathbf{x} \\cdot \\mathbf{v}_i)/(\\mathbf{v}_i \\cdot \\mathbf{v}_i)\\)? For this we have to do some algebra (rather than geometry): since \\(\\mathbf{x} - \\mathbf{v}\\) is perpendicular to everything in \\(V\\), it is in particular perpendicular to every \\(\\mathbf{v}_j\\), so</p> \\[0 = (\\mathbf{x} - \\mathbf{v}) \\cdot \\mathbf{v}_j = \\mathbf{x} \\cdot \\mathbf{v}_j - \\mathbf{v} \\cdot \\mathbf{v}_j\\] <p>for every \\(j\\). This says \\(\\mathbf{x} \\cdot \\mathbf{v}_j = \\mathbf{v} \\cdot \\mathbf{v}_j\\) for every \\(j\\). But \\(\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i\\) with some unknown \\(c_i\\)'s, so</p> \\[\\mathbf{v} \\cdot \\mathbf{v}_j = \\sum_{i=1}^k (c_i\\mathbf{v}_i) \\cdot \\mathbf{v}_j = \\sum_{i=1}^k c_i(\\mathbf{v}_i \\cdot \\mathbf{v}_j),\\] <p>and the terms for \\(i \\neq j\\) all vanish since \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) whenever \\(i \\neq j\\) (as \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) is an orthogonal basis of \\(V\\)!). In other words, \\(\\mathbf{v} \\cdot \\mathbf{v}_j = c_j(\\mathbf{v}_j \\cdot \\mathbf{v}_j)\\) for every \\(j\\). But we have seen that \\(\\mathbf{x} \\cdot \\mathbf{v}_j = \\mathbf{v} \\cdot \\mathbf{v}_j\\), so</p> \\[\\mathbf{x} \\cdot \\mathbf{v}_j = c_j(\\mathbf{v}_j \\cdot \\mathbf{v}_j)\\] <p>for every \\(j\\). We can divide by \\(\\mathbf{v}_j \\cdot \\mathbf{v}_j\\) since this is nonzero (it is equal to \\(\\|\\mathbf{v}_j\\|^2 &gt; 0\\), as \\(\\mathbf{v}_j \\neq 0\\)), so we thereby obtain the formula \\(c_j = (\\mathbf{x} \\cdot \\mathbf{v}_j)/(\\mathbf{v}_j \\cdot \\mathbf{v}_j)\\) for every \\(j\\), as desired.</p>"},{"location":"math/linear_algebra/span_subspaces_and_dimension/","title":"Span, subspaces, and dimension","text":""},{"location":"math/linear_algebra/span_subspaces_and_dimension/#span-and-linear-subspaces","title":"Span and linear subspaces","text":"<p>Consider a plane \\(P\\) in \\(\\mathbb{R}^3\\) passing through 0 = \\((0, 0, 0)\\). We want to express mathematically the idea that \\(P\\) is \"flat\" with \"two degrees of freedom\". Choose two other points in \\(P\\), denoted v and w, that do not lie on a common line through 0.</p> <p></p> <p>We can get to any point on \\(P\\) by starting at 0 and walking first some specific distance in the v-direction, then some specific distance in the w-direction. Symbolically,</p> \\[P = \\{\\text{all vectors of the form } a\\mathbf{v} + b\\mathbf{w}, \\text{ for scalars } a, b\\}\\] <p>\\(a &lt; 0\\) and \\(b &lt; 0\\) correspond to walking \"backwards\" relative to the directions of v and w respectively. The description as vectors \\(a\\mathbf{v} + b\\mathbf{w}\\) for varying scalars \\(a\\) and \\(b\\) is a way of encoding the flatness of \\(P\\) with two degrees of freedom.</p> <p>In other words, any vector that we can obtain from v and w repeatedly using the vector operations (addition and scalar multiplication) in any order is actually of the form \\(a\\mathbf{v} + b\\mathbf{w}\\) for some scalars \\(a\\), \\(b\\).</p> <p>Thus, the right side of the symbolic equation gives a parametric form of the plane through the 3 points 0, v, w and describes all vectors created from v, w using vector operations. If we instead allow the nonzero 3-vectors v, w to lie on a common line through 0, which is to say w is a scalar multiple of v, then the right side describes a line through 0 rather than a plane (as shown in the figure below).</p> <p></p> <p>The span of vectors v\\(_1\\), . . . , v\\(_k\\) in \\(\\mathbb{R}^n\\) is the collection of all vectors in \\(\\mathbb{R}^n\\) that one can obtain from v\\(_1\\), . . . , v\\(_k\\) by repeatedly using addition and scalar multiplication. In symbols,</p> \\[\\text{span}(v_1, \\ldots, v_k) = \\{\\text{all } n\\text{-vectors } \\mathbf{x} \\text{ of the form } c_1\\mathbf{v}_1 + \\cdots + c_k\\mathbf{v}_k\\}\\] <p>where \\(c_1\\), . . . , \\(c_k\\) are arbitrary scalars.</p> <p>In \\(\\mathbb{R}^3\\), for \\(k = 2\\) and nonzero v\\(_1\\), v\\(_2\\) not multiples of each other, this recovers the parametric form of a plane through \\(P = 0\\). In general, the span of a collection of finitely many \\(n\\)-vectors is the collection of all the \\(n\\)-vectors one can reach from those given \\(n\\)-vectors by forming linear combinations in every possible way.</p> <p>This is a very new kind of concept- considering such a collection of \\(n\\)-vectors all at the same time. But it is ultimately no different than how we may visualize a plane in our head yet it consists of a lot of different points. The span of two nonzero \\(n\\)-vectors that are not scalar multiples of each other should be visualized as a \"plane\" through 0 in \\(\\mathbb{R}^n\\).</p> <p>Example: Let's show that the set \\(U\\) of 4-vectors \\(\\begin{bmatrix} x \\\\ y \\\\ z \\\\ w \\end{bmatrix}\\) that are perpendicular to v = \\(\\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\\\ 7 \\end{bmatrix}\\) is a span of three 4-vectors (the same type of calculation as what we are about to do shows that the set of vectors in \\(\\mathbb{R}^n\\) perpendicular to any fixed nonzero vector in \\(\\mathbb{R}^n\\) is a span of \\(n-1\\) nonzero \\(n\\)-vectors). This is a \"higher-dimensional\" analogue of our visual experience in \\(\\mathbb{R}^3\\) that the collection of vectors in \\(\\mathbb{R}^3\\) perpendicular to a given nonzero vector is a plane through the origin (and hence is the span of two nonzero vectors in \\(\\mathbb{R}^3\\)).</p> <p>As a first step, we write out the condition of perpendicularity using the dot product: \\(2x + 3y + z + 7w = 0\\)</p> <p>Now we solve for \\(w\\) (say) to get \\(w = -(2/7)x - (3/7)y - z/7\\). Thus, points of \\(U\\) are precisely</p> \\[\\begin{bmatrix} x \\\\ y \\\\ z \\\\ w \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ z \\\\ -(2/7)x - (3/7)y - z/7 \\end{bmatrix} = x\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ -2/7 \\end{bmatrix}_{\\mathbf{a}} + y\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -3/7 \\end{bmatrix}_{\\mathbf{b}} + z\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -1/7 \\end{bmatrix}_{\\mathbf{c}}\\] <p>with arbitrary scalars \\(x\\), \\(y\\), \\(z\\). This shows that the vectors perpendicular to v are precisely those in the span of the vectors a, b, c that appear on the right side. This shows that \\(U\\) is the span of a, b, c in \\(\\mathbb{R}^4\\).</p> <p>We have noted that a line in \\(\\mathbb{R}^2\\) or \\(\\mathbb{R}^3\\) passing through 0 and a plane in \\(\\mathbb{R}^3\\) passing through 0 each arise as a span of one or two vectors. But lines and planes not passing through 0 are not a span of any collection of vectors! The reason is that the span of any collection of \\(n\\)-vectors always contains 0, by setting all coefficients \\(c_1\\), . . . , \\(c_k\\) in the span definition to be 0, since \\(0\\mathbf{v}_1 + 0\\mathbf{v}_2 + \\cdots + 0\\mathbf{v}_k = 0\\).</p> <p>If \\(V\\) is the span of some finite collection of vectors in \\(\\mathbb{R}^n\\) then there are generally many different collections of vectors v\\(_1\\), . . . , v\\(_k\\) in \\(\\mathbb{R}^n\\) with span equal to \\(V\\). For example, given one parametric form of a plane in \\(\\mathbb{R}^3\\) through 0 we get lots of others by rotating the resulting parallelogram grid around the origin in that plane by any nonzero angle we like. To refer to a span while suppressing the mention of any specific choice of vectors v\\(_1\\), . . . , v\\(_k\\) that create the span, some new terminology is convenient.</p> <p>A linear subspace of \\(\\mathbb{R}^n\\) is a subset of \\(\\mathbb{R}^n\\) that is the span of a finite collection of vectors in \\(\\mathbb{R}^n\\) (this is also referred to as a subspace, dropping the word \"linear\"). If \\(V\\) is a linear subspace of \\(\\mathbb{R}^n\\), a spanning set for \\(V\\) is a collection of \\(n\\)-vectors v\\(_1\\), . . . , v\\(_k\\) whose span equals \\(V\\) (a linear subspace has lots of spanning sets, akin to tiling a floor by parallelogram tiles in many ways).</p> <p>Planes and lines in \\(\\mathbb{R}^3\\) passing through 0 are the visual examples to keep in mind when you hear the phrase \"linear subspace\". You may wonder: what is the difference between a linear subspace and a span? There is no difference, but saying \"span\" emphasizes the input \u2013 a specific finite list v\\(_1\\), . . . , v\\(_k\\) and the dynamic process of forming their linear combinations \u2013 whereas saying \"linear subspace\" emphasizes the output collection \\(V\\) of \\(n\\)-vectors without choosing a specific v\\(_1\\), . . . , v\\(_k\\) whose span is \\(V\\). It is far more important to know that \\(V\\) can be obtained as a span of some list v\\(_1\\), . . . , v\\(_k\\) rather than to pick a specific such list.</p> <p>Proposition. If \\(V\\) is a linear subspace in \\(\\mathbb{R}^n\\) then for any vectors \\(\\mathbf{x}_1\\), . . . , \\(\\mathbf{x}_m \\in V\\) and scalars \\(a_1\\), . . . , \\(a_m\\) the linear combination \\(a_1\\mathbf{x}_1 + \\cdots + a_m\\mathbf{x}_m\\) also lies in \\(V\\). In words: all linear combinations of \\(n\\)-vectors chosen from a linear subspace of \\(\\mathbb{R}^n\\) belong to that same subspace.</p> <p>Some illustrations below of which are linear subspaces and which are not. Only the blue and green planes are linear subspaces by definition as well as the proposition above.</p> <p> </p>"},{"location":"math/linear_algebra/span_subspaces_and_dimension/#dimension","title":"Dimension","text":"<p>We would like to define the \"dimension\" of a linear subspace in a way that generalizes our familiar concept of dimension: a thread or a line is 1-dimensional, a sheet of paper is 2-dimensional, the world around us is 3-dimensional.</p> <p>In particular, we may informally say: the \"dimension\" of an object \\(X\\) tells us how many different numbers are needed to locate a point in \\(X\\).</p> <p>To turn this into something unambiguous and useful, we now focus on the case of a linear subspace \\(V\\) of \\(\\mathbb{R}^n\\), where vector algebra will provide a way to make that informal idea precise. The \"dimension\" of \\(V\\) will be, intuitively, the number of independent directions in \\(V\\). In other words, it will tell us how many numbers we need in order to specify a vector v in \\(V\\).</p> <p>More precisely, recall that by the definition of \"linear subspace\", \\(V\\) is the span of some finite collection of vectors v\\(_1\\), . . . , v\\(_k \\in \\mathbb{R}^n\\). That is, for any v \\(\\in V\\) we can write</p> \\[\\mathbf{v} = c_1\\mathbf{v}_1 + \\cdots + c_k\\mathbf{v}_k\\] <p>for some scalars \\(c_1\\), . . . , \\(c_k\\), so to determine v it is enough to tell us \\(k\\) numbers\u2013 the scalars \\(c_1\\), . . . , \\(c_k\\). But \\(V\\) can have another spanning set consisting of a different number of vectors.</p> <p>The span \\(V\\) of two nonzero vectors in \\(\\mathbb{R}^3\\) could be a line (such as if the two vectors point in the same or opposite directions), in which case \\(V\\) is also spanned by just one of those two vectors (e.g., the second vector is redundant).</p> <p>Similarly, the span of three nonzero vectors in \\(\\mathbb{R}^3\\) could be a plane in special circumstances (or even a line in especially degenerate circumstances).</p> <p>In both such cases, the initial spanning set has some redundancy. To define \"dimension\" for \\(V\\), we want to use ways of spanning \\(V\\) that (in a sense we need to make precise) don't have redundancy.</p> <p>Let \\(V\\) be a nonzero linear subspace of some \\(\\mathbb{R}^n\\). The dimension of \\(V\\), denoted as \\(\\dim(V)\\), is defined to be the smallest number of vectors needed to span \\(V\\). We define \\(\\dim(\\{0\\}) = 0\\).</p> <p>For \\(k \\geq 2\\), consider a collection v\\(_1\\), . . . , v\\(_k\\) of vectors spanning a linear subspace \\(V\\) in \\(\\mathbb{R}^n\\). We have \\(\\dim(V) = k\\) precisely when \"there is no redundancy\": each v\\(_i\\) is not a linear combination of the others, or in other words removing it from the list destroys the spanning property.</p> <p>Equivalently, \\(\\dim(V) &lt; k\\) precisely when \"there is redundancy\": some v\\(_i\\) is a linear combination of the others, or in other words removing some v\\(_i\\) from the list does not affect the span. If some v\\(_i\\) vanishes then it is a linear combination of the others and hence can be dropped from the span, so \\(\\dim(V) &lt; k\\).</p> <p>If \\(V\\) and \\(W\\) are linear subspaces of \\(\\mathbb{R}^n\\) with \\(W\\) contained in \\(V\\) (i.e., every vector in \\(W\\) also belongs to \\(V\\), much like a line inside a plane in \\(\\mathbb{R}^3\\)) then \\(\\dim(W) \\leq \\dim(V)\\), and equality holds precisely when \\(W = V\\).</p>"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/","title":"Vector geometry in \\(\\mathbb{R}^n\\) and correlation coefficients","text":""},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#angles","title":"Angles","text":"<p>The angle \\(0\u00b0 \\leq \\theta \\leq 180\u00b0\\) between nonzero 2-vectors a = \\((a_1, a_2)\\) and b = \\((b_1, b_2)\\) satisfies</p> \\[\\cos \\theta = \\frac{a_1b_1 + a_2b_2}{\\|a\\|\\|b\\|}\\] <p>The angle \\(0\u00b0 \\leq \\theta \\leq 180\u00b0\\) between two nonzero 3-vectors a = \\((a_1, a_2, a_3)\\) and b = \\((b_1, b_2, b_3)\\) satisfies</p> \\[\\cos \\theta = \\frac{a_1b_1 + a_2b_2 + a_3b_3}{\\|a\\|\\|b\\|}\\] <p>The preceding in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) motivates how to define appropriate concepts with \\(n\\)-vectors for any \\(n\\).</p> <p>Consider \\(n\\)-vectors x = \\(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\) and y = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\).</p> <p>(i) The dot product of x and y is defined to be the scalar</p> \\[x \\cdot y = x_1y_1 + x_2y_2 + \\cdots + x_ny_n = \\sum_{i=1}^n x_iy_i\\] <p>The dot product is only defined if the two vectors are \\(n\\)-vectors for the same value of \\(n\\).</p> <p>(ii) The angle \\(\\theta\\) between two nonzero \\(n\\)-vectors x, y is defined by the formula</p> \\[\\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\|\\|y\\|}\\] <p>with \\(0\u00b0 \\leq \\theta \\leq 180\u00b0\\). For emphasis: x and y must be nonzero \\(n\\)-vectors for a common \\(n\\).</p> <p>(iii) When \\(x \\cdot y = 0\\) (same as \\(\\theta = 90\u00b0\\) if x, y \u2260 0), we say x and y are perpendicular; the word orthogonal is often used for this (\"orthog\u014dnios\" is ancient Greek for \"right-angled\"), though only rarely at the U.S. Supreme Court.</p> <p>Always remember that the dot product of vectors is a scalar (it is not a vector).</p> <p>The notion of angle is a definition in \\(\\mathbb{R}^n\\) for general \\(n\\): it is motivated by the case when \\(n = 3\\), but for general \\(n\\) there is nothing to \"physically justify\". The real content making this definition for general \\(n\\) is that (as you will learn with experience) this notion of angle behaves like our visual experience in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) and so provides useful visual guidance with \\(n\\)-vectors for any \\(n\\).</p> <p>Whenever we speak of an angle between two lines through the origin, there is always an ambiguity (when they're not perpendicular) of whether we want the acute angle between them or the (supplementary) obtuse angle between them. This corresponds to the fact that when we set it up as a vector problem, we have to choose a direction along each line (coming out of the intersection point). Depending on the choice, we will get the acute or the obtuse angle.</p>"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#properties-of-dot-products","title":"Properties of dot products","text":"<p>For any \\(n\\)-vectors v, w, w\\(_1\\), and w\\(_2\\), the following hold:</p> <p>(i) \\(v \\cdot w = w \\cdot v\\),</p> <p>(ii) \\(v \\cdot v = \\|v\\|^2\\),</p> <p>(iii) \\(v \\cdot (cw) = c(v \\cdot w)\\) for any scalar \\(c\\), and \\(v \\cdot (w_1 + w_2) = v \\cdot w_1 + v \\cdot w_2\\).</p> <p>(iii\u2032) Combining both rules in (iii), for any scalars \\(c_1\\), \\(c_2\\) we have</p> \\[v \\cdot (c_1w_1 + c_2w_2) = c_1(v \\cdot w_1) + c_2(v \\cdot w_2)\\]"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#pythagorean-theorem-in-mathbbrn-and-the-cauchyschwarz-inequality","title":"Pythagorean Theorem in \\(\\mathbb{R}^n\\) and the Cauchy\u2013Schwarz Inequality","text":"<p>As an application of the dot product rules, we can establish a version of the Pythagorean Theorem for \\(n\\)-vectors with any \\(n\\) (not just \\(n = 2\\)) and we can show that the subtlety lurking in the definition of \"angle\" between \\(n\\)-vectors is really not a problem at all.</p> <p>Theorem (Pythagoras). If \\(n\\)-vectors v\\(_1\\) and v\\(_2\\) are nonzero and perpendicular (i.e., at an angle of \\(90\u00b0\\)) then</p> \\[\\|v_1 + v_2\\|^2 = \\|v_1\\|^2 + \\|v_2\\|^2 \\] <p>Proof. Expand the left side as a dot product:</p> \\[(v_1 + v_2) \\cdot (v_1 + v_2) = v_1 \\cdot (v_1 + v_2) + v_2 \\cdot (v_1 + v_2) = v_1 \\cdot v_1 + v_1 \\cdot v_2 + v_2 \\cdot v_1 + v_2 \\cdot v_2\\] <p>We've used the rules for dot products at each step to expand. But the common value \\(v_1 \\cdot v_2\\) and \\(v_2 \\cdot v_1\\) is 0 because v\\(_1\\) and v\\(_2\\) are assumed to be perpendicular (which means by definition that their dot product equals 0). Thus, the right side equals \\(v_1 \\cdot v_1 + v_2 \\cdot v_2 = \\|v_1\\|^2 + \\|v_2\\|^2\\), as we wanted. \\(\\square\\)</p> <p>The motivation for our definitions of perpendicularity and more generally angle between vectors in \\(\\mathbb{R}^n\\) and length of vectors in \\(\\mathbb{R}^n\\) for general \\(n\\) (especially \\(n &gt; 3\\)) came from our knowledge of how things work in \\(\\mathbb{R}^2\\) based on knowing the Pythagorean Theorem in plane geometry. Making up definitions of words cannot ever replace the work involved in proving a real theorem.</p> <p>Since we now have several good properties of dot products in hand, we can establish a fact that is needed to confirm that our definition of \"angle\" between nonzero \\(n\\)-vectors makes sense for any \\(n\\):</p> <p>Theorem (Cauchy\u2013Schwarz Inequality). For \\(n\\)-vectors v, w, we have</p> \\[-\\|v\\| \\|w\\| \\leq v \\cdot w \\leq \\|v\\| \\|w\\|\\] <p>(or equivalently the absolute value \\(|v \\cdot w|\\) is at most \\(\\|v\\| \\|w\\|\\)). Moreover, one of the inequalities is an equality precisely when one of v or w is a scalar multiple of the other.</p> <p>Proof. If v = 0 or w = 0 then everything is clear (note that 0 is a scalar multiple of any \\(n\\)-vector: multiply it by the scalar 0), so now we assume v, w \u2260 0. The idea of the proof is to explore how the length of v + \\(x\\)w depends on \\(x\\). This is most conveniently done by analyzing the squared-length, which is a dot product:</p> \\[\\|v + xw\\|^2 = (v + xw) \\cdot (v + xw)\\] <p>Using the linearity properties of dot products, we have</p> \\[(v + xw) \\cdot (v + xw) = (v + xw) \\cdot v + x((v + xw) \\cdot w) = v \\cdot v + (xw) \\cdot v + x((v \\cdot w) + x(w \\cdot w)) = v \\cdot v + x(w \\cdot v) + x(v \\cdot w) + x^2(w \\cdot w)\\] <p>But \\(w \\cdot v = v \\cdot w\\), so combining the middle two terms yields:</p> \\[\\|v + xw\\|^2 = \\|v\\|^2 + 2(v \\cdot w)x + \\|w\\|^2x^2\\] <p>The squared length of a vector is always \u2265 0, and it equals 0 precisely when the vector equals 0. But the vector v + \\(x\\)w equals 0 for some value \\(x = c\\) precisely when v = \\(-c\\)w, which is to say v is a scalar multiple of w, and that is the same as w being a scalar multiple of v (since the scalar multiplier can be brought to the other side as its reciprocal as long as the scalar cannot be 0, and indeed such a scalar cannot be 0 since we have arranged that v, w \u2260 0). So we just need to analyze what it means that the quadratic polynomial in \\(x\\) given by</p> \\[q(x) = \\|w\\|^2x^2 + 2(v \\cdot w)x + \\|v\\|^2\\] <p>is always non-negative, and determine when this polynomial does actually attain the value 0 for some value of \\(x\\).</p> <p>Let's review when a quadratic polynomial \\(ax^2 + bx + c\\) with positive leading coefficient (such as \\(a = \\|w\\|^2\\) for \\(q(x)\\) above) is \u2265 0 everywhere. This happens precisely when its concave-up parabolic graph lies entirely on one side of the \\(x\\)-axis (possibly touching the \\(x\\)-axis at one point), which is exactly the situation that the graph does not cross the \\(x\\)-axis at two different points. This is exactly the situation when the output of the quadratic formula does not yield two different real numbers. The opposite case of having two different real roots occurs exactly when the \"\\(b^2 - 4ac\\)\" part of the quadratic formula inside the square-root is &gt; 0, so in our situation we must have the exactly opposite situation: \\(b^2 - 4ac \\leq 0\\), with equality happening precisely when there is a real root.</p> <p>Applying the preceding review with \\(a = \\|w\\|^2\\), \\(b = 2(v \\cdot w)\\), \\(c = \\|v\\|^2\\) for \\(q(x)\\), we get</p> \\[(2(v \\cdot w))^2 - 4\\|w\\|^2\\|v\\|^2 \\leq 0\\] <p>with equality happening exactly when v and w are scalar multiples of each other. Bringing the second term on the left over to the other side, we conclude that</p> \\[(2(v \\cdot w))^2 \\leq 4\\|w\\|^2\\|v\\|^2\\] <p>with equality precisely when v and w are scalar multiples of each other. Dividing each side by 4, this is the same as the inequality</p> \\[|v \\cdot w|^2 \\leq (\\|w\\| \\|v\\|)^2\\] <p>so taking square roots of both sides gives what we want. \\(\\square\\)</p>"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#the-correlation-coefficient","title":"The correlation coefficient","text":"<p>Given data points \\((x_1, y_1)\\), . . . , \\((x_n, y_n)\\), it is often useful to seek a line which gives a \"best fit\" to this collection of points.</p> <p>The problem of finding a \"best fit\" line to some data is called linear regression. But at a more basic level we may seek a measure of the extent to which it is reasonable to try to find a line that could be regarded as a good fit to the data (setting aside what that specific line may be). There is a widely used measure of whether one should seek such a line: this measure is called the correlation coefficient of the data points.</p> <p>Consider \\(n\\) data points \\((x_1, y_1)\\), \\((x_2, y_2)\\), . . . , \\((x_n, y_n)\\) in \\(\\mathbb{R}^2\\). Assume they don't all lie on a common vertical line nor on a common horizontal line (i.e., the \\(x_i\\)'s are not all equal to each other, and the \\(y_i\\)'s are not all equal to each other, so in particular X, Y \u2260 0).</p> <p>In the above setup, assume furthermore that the averages \\(\\bar{x} = \\frac{1}{n}\\sum x_i\\) and \\(\\bar{y} = \\frac{1}{n}\\sum y_i\\) of the \\(x\\)-coordinates and of the \\(y\\)-coordinates both equal 0. The correlation coefficient \\(r\\) between the \\(x_i\\)'s and \\(y_i\\)'s is defined to be the cosine of the angle between X and Y, or equivalently between the unit vectors \\(\\frac{X}{\\|X\\|}\\) and \\(\\frac{Y}{\\|Y\\|}\\):</p> \\[r = \\text{cosine of the angle between X and Y} = \\frac{X \\cdot Y}{\\|X\\|\\|Y\\|} = \\frac{X}{\\|X\\|} \\cdot \\frac{Y}{\\|Y\\|}\\] <p></p> <p>Intuition: This definition makes perfect geometric sense. When the data points \\((x_i, y_i)\\) are centered (so their averages are 0), we can think of X and Y as vectors in \\(\\mathbb{R}^n\\) representing the \\(x\\)-coordinates and \\(y\\)-coordinates respectively. </p> <p>The correlation coefficient \\(r\\) measures how well the data points align along a line through the origin. When \\(r = 1\\), the vectors X and Y point in the same direction, meaning the data points lie perfectly on a line with positive slope. When \\(r = -1\\), the vectors point in opposite directions, meaning the data points lie perfectly on a line with negative slope. When \\(r = 0\\), the vectors are perpendicular, meaning there's no linear relationship between the variables.</p> <p>You may be bothered by the assumption that the averages \\(\\bar{x}\\) and \\(\\bar{y}\\) of the coordinates of the data both equal 0, since in practice it is rarely satisfied. What is done in real-world problems is that the data is recentered: we replace \\(x_i\\) with \\(\\hat{x}_i = x_i - \\bar{x}\\) and replacing \\(y_i\\) with \\(\\hat{y}_i = y_i - \\bar{y}\\). Such subtraction of the averages makes \"center of mass\" move to \\((0, 0)\\) (i.e., \\(\\hat{\\bar{x}}, \\hat{\\bar{y}} = 0\\)).</p> <p>Often people work with \\(r^2\\), which is always non-negative. This is</p> \\[r^2 = \\frac{(X \\cdot Y)^2}{\\|X\\|^2\\|Y\\|^2}\\] <p>it is near 0 when there is little correlation, and near 1 when there's a strong linear relationship (without specifying the sign of the slope: \\(r\\) may be near 1 or near \\(-1\\)).</p> <p>Don't confuse the value of \\(r\\) with the slope of a \"best-fit line\"! The nearness of \\(r^2\\) to 1 (or of \\(r\\) to \\(\\pm 1\\)) is a measure of quality of fit. The actual slope of the best-fit line (which could be any real number at all) has nothing whatsoever to do with the value of \\(r\\) (which is always between \\(-1\\) and 1).</p> <p>Correlation coefficients go hand in hand with linear regression (finding a \"best fit\" line for data) and help one to understand how meaningful the results of a linear regression are.</p> <p>Note: Let's see why the correlation coefficient equals 1 precisely when the points \\((x_i, y_i)\\) all lie exactly on a line \\(y = mx\\) whose slope \\(m\\) is positive. We assume as always that the data doesn't all lie on a common vertical line or a common horizontal line, and that the averages \\(\\bar{x}\\) and \\(\\bar{y}\\) equal 0. By then replacing \\(y_i\\) with \\(-y_i\\) everywhere, it would follow that the correlation coefficient equals \\(-1\\) precisely when the points \\((x_i, y_i)\\) all lie exactly on a line \\(y = mx\\) whose slope \\(m\\) is negative.</p> <p>Note that X, Y \u2260 0 since we assumed the data points aren't on a common horizontal line and aren't on a common vertical line. We want to show that the correlation coefficient is 1 precisely when Y = \\(m\\)X for some \\(m &gt; 0\\).</p> <p>But the correlation coefficient is the cosine of the angle between the nonzero vectors X and Y, so the correlation coefficient is equal to 1 precisely when the angle between X and Y is \\(0\u00b0\\). The angle \\(\\theta\\) between the (nonzero) vectors X and Y is \\(0\u00b0\\) precisely when Y = \\(m\\)X for some \\(m &gt; 0\\).</p>"},{"location":"math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/","title":"Vectors, vector addition, and scalar multiplication","text":"<p>Differential equations provided the mathematical framework for many of the advances of the 20th century, but linear algebra (the algebra and geometry of vectors and matrices in arbitrary dimensions) is the mathematical tool par excellence (alongside statistics) for the systematic analysis and management of the data-driven tasks of the 21st century. Even for modern applications of differential equations, linear algebra far beyond 3 dimensions is an important tool.</p> <p>Also, a good understanding of multivariable differential calculus requires first learning some ideas and computational techniques in linear algebra.</p>"},{"location":"math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/#vectors-and-their-linear-combinations","title":"Vectors and their linear combinations","text":"<p>In the physical sciences, vectors are used to represent quantities that have both a magnitude and a direction. Examples of such quantities include displacement, velocity, force, and angular momentum.</p> <p>In data science, economics, and many industrial applications of mathematics, vectors are used to keep track of collections of numerical data. This type of example is much more varied than the examples arising from natural sciences, and nearly always \\(n\\) is very large.</p> <p>The sum v + w of two vectors is defined precisely when v and w are \\(n\\)-vectors for the same \\(n\\). In that case, we define their sum by the rule</p> \\[\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} + \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} = \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\\\ \\vdots \\\\ v_n + w_n \\end{bmatrix}\\] <p>We multiply a scalar \\(c\\) against an \\(n\\)-vector v = \\(\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\) by the rule </p> \\[c\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} cv_1 \\\\ cv_2 \\\\ \\vdots \\\\ cv_n \\end{bmatrix}\\] <p></p> <p>As in the above for \\(n = 3\\), the vector u + v is represented by the diagonal arrow in the parallelogram with one vertex at the origin and two edges given by u and v. This description of vector addition for \\(n = 2\\) and \\(n = 3\\) is called the parallelogram law.</p> <p>We define subtraction as we did addition, or equivalently:</p> \\[v - w = v + (-1)w\\] <p>A linear combination of two \\(n\\)-vectors v,w is an \\(n\\)-vector \\(av + bw\\) for scalars \\(a, b\\). More generally, a linear combination of \\(k\\) such \\(n\\)-vectors v\\(_1\\), v\\(_2\\), . . . , v\\(_k\\) is \\(a_1\\)v\\(_1\\) + \\(a_2\\)v\\(_2\\) + \u00b7 \u00b7 \u00b7 + \\(a_k\\)v\\(_k\\) for scalars \\(a_1\\), \\(a_2\\), . . . , \\(a_k\\). In physical sciences, this is often called a \"superposition\" of v\\(_1\\), . . . , v\\(_k\\).</p> <p>Example: Suppose that T\\(_{2001}\\), T\\(_{2002}\\), . . . , T\\(_{2016}\\) are 365-vectors that describe the daily average temperatures in Palo Alto (say in Celsius) in years 2001, 2002, . . . , 2016 (let's ignore February 29 in leap years). Then \\(\\frac{1}{16}\\)(T\\(_{2001}\\) + \u00b7 \u00b7 \u00b7 + T\\(_{2016}\\)) is a 365-vector that tells us, for each given day, the average temperature in the years 2001\u20132016. For example, the first entry of this vector is the average January 1 temperature during this period.</p> <p>A special type of linear combination that arises in applications such as linear programming, weighted averages, and probability theory is convex combination: in the case of two \\(n\\)-vectors v and w, this means a linear combination of the form \\((1 - t)\\)v + \\(tw\\) = v + \\(t\\)(w - v) with \\(0 \\leq t \\leq 1\\). This adds to v a portion (given by \\(t\\)) of the displacement from v to w. It has the geometric interpretation (for \\(n = 2, 3\\)) of being a point on the line segment between the tips of v and w; e.g., it is v when \\(t = 0\\), it is the midpoint when \\(t = \\frac{1}{2}\\), and it is w when \\(t = 1\\).</p> <p></p> <p>For any \\(n\\)-vectors v\\(_1\\), . . . , v\\(_k\\), a convex combination of them means a linear combination \\(t_1\\)v\\(_1\\) + \u00b7 \u00b7 \u00b7 + \\(t_k\\)v\\(_k\\) for which all \\(t_j \\geq 0\\) and the sum of the coefficients is equal to 1; that is, \\(t_1\\) + \u00b7 \u00b7 \u00b7 + \\(t_k\\) = 1. When the coefficients are all equal, which is to say every \\(t_j\\) is equal to \\(\\frac{1}{k}\\), this is the average (sometimes called the centroid) of the vectors.</p> <p>In linear algebra, the phrase \"point in \\(\\mathbb{R}^n\\)\" means exactly the same thing as \"\\(n\\)-vector\" (as well as \"vector\", when we don't need to specify \\(n\\)). The mental image for a given situation may suggest a preference between the words \"point\" and \"vector\", such as \"displacement vector\" or \"closest point\", but there is absolutely no difference in the meanings of these words in linear algebra. You might imagine that a \"point\" is the tip of an arrow emanating from 0, or that a \"vector\" is a directed line segment with specified endpoints.</p> <p>In physics and engineering, a special \"cross product\" of 3-vectors (the output of which is also a 3-vector) shows up a lot. This has no analogue in \\(\\mathbb{R}^n\\) for \\(n \\neq 3\\), and it behaves very differently from products of numbers; e.g., it is neither commutative nor associative!</p>"},{"location":"math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/#length-for-vectors-and-distance-in-mathbbrn","title":"Length for vectors and distance in \\(\\mathbb{R}^n\\)","text":"<p>The length or magnitude of an \\(n\\)-vector v = \\(\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\), denoted \\(\\|v\\|\\), is the number</p> \\[\\|v\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\geq 0\\] <p>Note that the length is a scalar, and \\(\\|-v\\| = \\|v\\|\\) (in accordance with the visualization of \\(-v\\) as \"a copy of v pointing in the opposite direction\") because signs disappear when squaring each \\(-v_j\\).</p> <p>If \\(c\\) is any scalar then \\(\\|cv\\| = |c|\\|v\\|\\) (i.e., if we multiply a vector by \\(c\\) then the length scales by the factor \\(|c|\\)). For example, \\((-5)\\)v has length \\(5\\|v\\|\\).</p> <p>In other references, you may see \\(\\|v\\|\\) called the \"norm\" of v.</p> <p>The distance between two \\(n\\)-vectors x, y is defined to be \\(\\|x - y\\|\\).</p> <p>In general it also equals \\(\\|y - x\\|\\) since y - x = \\(-(x - y)\\) and any vector has the same length as its negative, so the order of subtraction doesn't matter.</p> <p>There is no \"physical justification\" to be given when \\(n &gt; 3\\). What is important is that (i) for \\(n = 2, 3\\) we convince ourselves that it is the usual notion of distance, and (ii) for general \\(n\\) it satisfies reasonable properties for a notion of \"distance\" to provide helpful geometric insight.</p> <p>The zero vector in \\(\\mathbb{R}^n\\) is 0 = \\(\\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\), and a unit vector is a vector with length 1.</p> <p>Always \\(\\|v\\| \\geq 0\\), and \\(\\|v\\| = 0\\) precisely when v = 0. If v \u2260 0 then dividing v by its length (i.e., multiplying by the scalar \\(\\frac{1}{\\|v\\|} &gt; 0\\)) yields a unit vector \"pointing in the same direction\" as v.</p> <p>In general the length of \\(cv\\) for \\(c &gt; 0\\) is \\(c\\|v\\|\\), so in order that \\(cv\\) be a unit vector the condition is precisely that \\(c\\|v\\| = 1\\), which is to say \\(c = \\frac{1}{\\|v\\|}\\). In other words, the scalar multiple \\(\\frac{1}{\\|v\\|}\\)v of v is indeed the unique unit vector pointing in the same direction as v (in the opposite direction we have the unit vector \\(-\\frac{1}{\\|v\\|}\\)v).</p>"},{"location":"math/probability/beta_distributions/","title":"Beta Distributions","text":"<p>The beta distribution is a family of continuous probability distributions defined on the interval [0, 1] or (0, 1). It's particularly useful for modeling random variables that represent proportions, probabilities, or percentages- anything that naturally falls between 0 and 1.</p> <p>Think of it this way: if you're trying to model a random variable as something like \"the probability of success in a trial\" or \"the proportion of people who like a new product,\" the beta distribution is often your go-to choice because it's specifically designed for values between 0 and 1.</p>"},{"location":"math/probability/beta_distributions/#pdf","title":"PDF","text":"<p>The probability density function (PDF) of the beta distribution, for \\(0 \\leq x \\leq 1\\) or \\(0 &lt; x &lt; 1\\), and shape parameters \\(\\alpha\\), \\(\\beta &gt; 0\\), is a power function of the variable \\(x\\) and of its reflection \\((1-x)\\) as follows:</p> \\[f(x;\\alpha,\\beta) = \\mathrm{constant} \\cdot x^{\\alpha-1}(1-x)^{\\beta-1} = \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\] <p>The beta function, \\(\\mathrm{B}\\), is a normalization constant to ensure that the total probability is 1.</p>"},{"location":"math/probability/beta_distributions/#case-study-biased-coin-selection","title":"Case study: Biased coin selection","text":"<p>Let's illustrate the beta distribution with a concrete example involving biased coins.</p> <p>Setup: We have 11 coins with different bias probabilities:</p> <ul> <li> <p>Coin 0: P(H) = 0.0</p> </li> <li> <p>Coin 1: P(H) = 0.1  </p> </li> <li> <p>Coin 2: P(H) = 0.2</p> </li> <li> <p>Coin 3: P(H) = 0.3</p> </li> <li> <p>Coin 4: P(H) = 0.4</p> </li> <li> <p>Coin 5: P(H) = 0.5</p> </li> <li> <p>Coin 6: P(H) = 0.6</p> </li> <li> <p>Coin 7: P(H) = 0.7</p> </li> <li> <p>Coin 8: P(H) = 0.8</p> </li> <li> <p>Coin 9: P(H) = 0.9</p> </li> <li> <p>Coin 10: P(H) = 1.0</p> </li> </ul> <p>We randomly select one coin and flip it 10 times, observing 7 heads and 3 tails.</p> <p>Question: What's the probability that we selected each of the 11 coins?</p> <p>Solution: Let \\(X\\) be the random variable representing the probability of heads for the selected coin. We want to find \\(P(X = p_i)\\) for each \\(p_i \\in \\{0, 0.1, 0.2, ..., 1.0\\}\\).</p> <p>Initial probabilities: Since we randomly selected a coin, each coin has equal probability: \\(P(X = p_i) = \\frac{1}{11}\\) for all \\(i\\).</p> <p>Probability of observing 7 heads: Given that we observed 7 heads out of 10 flips, the probability of this outcome for each coin is:</p> \\[P(\\text{7 heads out of 10} | X = p_i) = \\binom{10}{7} p_i^7 (1-p_i)^3\\] <p>Updated probabilities: Using Bayes' theorem, the probability that we selected coin \\(i\\) given the observed data is:</p> \\[P(X = p_i | \\text{7 heads out of 10}) = \\frac{P(\\text{7 heads out of 10} | X = p_i) \\cdot P(X = p_i)}{\\sum_{j=0}^{10} P(\\text{7 heads out of 10} | X = p_j) \\cdot P(X = p_j)}\\] <p>Since each coin was equally likely initially, we can simplify:</p> \\[P(X = p_i | \\text{7 heads out of 10}) = \\frac{p_i^7 (1-p_i)^3}{\\sum_{j=0}^{10} p_j^7 (1-p_j)^3}\\] <p>Let's calculate the numerator for each coin:</p> Coin P(H) \\(p_i^7 (1-p_i)^3\\) 0 0.0 \\(0^7 \\cdot 1^3 = 0\\) 1 0.1 \\(0.1^7 \\cdot 0.9^3 = 0.000000729\\) 2 0.2 \\(0.2^7 \\cdot 0.8^3 = 0.00001024\\) 3 0.3 \\(0.3^7 \\cdot 0.7^3 = 0.000250047\\) 4 0.4 \\(0.4^7 \\cdot 0.6^3 = 0.001179648\\) 5 0.5 \\(0.5^7 \\cdot 0.5^3 = 0.000976563\\) 6 0.6 \\(0.6^7 \\cdot 0.4^3 = 0.001194393\\) 7 0.7 \\(0.7^7 \\cdot 0.3^3 = 0.000250047\\) 8 0.8 \\(0.8^7 \\cdot 0.2^3 = 0.00001024\\) 9 0.9 \\(0.9^7 \\cdot 0.1^3 = 0.000000729\\) 10 1.0 \\(1^7 \\cdot 0^3 = 0\\) <p>Total sum: \\(\\sum_{j=0}^{10} p_j^7 (1-p_j)^3 \\approx 0.003872\\)</p> <p>Normalized probabilities:</p> <ul> <li> <p>P(X = 0.6) = \\(\\frac{0.001194393}{0.003872} \\approx 0.308\\) (30.8%)</p> </li> <li> <p>P(X = 0.4) = \\(\\frac{0.001179648}{0.003872} \\approx 0.305\\) (30.5%)</p> </li> <li> <p>P(X = 0.5) = \\(\\frac{0.000976563}{0.003872} \\approx 0.252\\) (25.2%)</p> </li> <li> <p>All other coins have negligible probability</p> </li> </ul> <p>After observing 7 heads out of 10 flips, we're most confident that we selected:</p> <ol> <li> <p>Coin 6 (P(H) = 0.6) with ~30.8% probability</p> </li> <li> <p>Coin 4 (P(H) = 0.4) with ~30.5% probability  </p> </li> <li> <p>Coin 5 (P(H) = 0.5) with ~25.2% probability</p> </li> </ol> <p>The beta distribution would give us a continuous version of this discrete problem, allowing us to consider any probability between 0 and 1, not just the 11 discrete values.</p> <p>Let's extend this to 101 coins with probabilities from 0 to 1 in steps of 0.01:</p> <p>Setup: We have 101 coins with probabilities:</p> <ul> <li>Coin 0: P(H) = 0.00</li> <li>Coin 1: P(H) = 0.01  </li> <li>Coin 2: P(H) = 0.02</li> <li>...</li> <li>Coin 50: P(H) = 0.50</li> <li>...</li> <li>Coin 99: P(H) = 0.99</li> <li>Coin 100: P(H) = 1.00</li> </ul> <p>We randomly select one coin and flip it 10 times, observing 7 heads and 3 tails.</p> <p>Solution: The same approach applies, but now we have 101 coins instead of 11.</p> <p>Updated probabilities: </p> \\[P(X = p_i | \\text{7 heads out of 10}) = \\frac{p_i^7 (1-p_i)^3}{\\sum_{j=0}^{100} p_j^7 (1-p_j)^3}\\] <p>The most likely coins after observing 7 heads out of 10 flips are:</p> Coin P(H) \\(p_i^7 (1-p_i)^3\\) Probability 70 0.70 \\(0.70^7 \\cdot 0.30^3 = 0.000250047\\) ~25.0% 69 0.69 \\(0.69^7 \\cdot 0.31^3 = 0.000248\\) ~24.8% 71 0.71 \\(0.71^7 \\cdot 0.29^3 = 0.000248\\) ~24.8% 68 0.68 \\(0.68^7 \\cdot 0.32^3 = 0.000240\\) ~24.0% 72 0.72 \\(0.72^7 \\cdot 0.28^3 = 0.000240\\) ~24.0% <p>This discrete problem with 101 coins closely approximates the continuous beta distribution. If we used a continuous beta distribution with:</p> <ul> <li> <p>\u03b1 = 7 + 1 = 8 (number of successes + 1)</p> </li> <li> <p>\u03b2 = 3 + 1 = 4 (number of failures + 1)</p> </li> </ul> <p>The beta distribution Beta(8, 4) would have:</p> <ul> <li> <p>Mean: \\(\\frac{8}{8+4} = \\frac{2}{3} \\approx 0.667\\)</p> </li> <li> <p>Mode: \\(\\frac{8-1}{8+4-2} = \\frac{7}{10} = 0.7\\)</p> </li> </ul> <p>Notice how the discrete results cluster around 0.7, which matches the mode of the continuous beta distribution!</p> <p>As we increase the number of discrete coins (from 11 to 101 to 1001, etc.), the discrete probability distribution approaches the continuous beta distribution.</p> <p>Let's see what happens when we flip the selected coin 100 times and observe 70 heads and 30 tails:</p> <p>Setup: Same 101 coins with probabilities from 0.00 to 1.00 in steps of 0.01.</p> <p>Updated probabilities: </p> \\[P(X = p_i | \\text{70 heads out of 100}) = \\frac{p_i^{70} (1-p_i)^{30}}{\\sum_{j=0}^{100} p_j^{70} (1-p_j)^{30}}\\] <p>The most likely coins after observing 70 heads out of 100 flips are:</p> Coin P(H) \\(p_i^{70} (1-p_i)^{30}\\) Probability 70 0.70 \\(0.70^{70} \\cdot 0.30^{30}\\) ~99.9% 69 0.69 \\(0.69^{70} \\cdot 0.31^{30}\\) ~0.1% 71 0.71 \\(0.71^{70} \\cdot 0.29^{30}\\) ~0.1% 68 0.68 \\(0.68^{70} \\cdot 0.32^{30}\\) ~0.0% 72 0.72 \\(0.72^{70} \\cdot 0.28^{30}\\) ~0.0% <p>Comparison: 10 flips vs 100 flips:</p> Scenario Most Likely Coin Probability Confidence 7 heads out of 10 Coin 70 (0.70) ~25% Low 70 heads out of 100 Coin 70 (0.70) ~99.9% Very High <p>With 70 successes and 30 failures, the corresponding beta distribution is Beta(71, 31):</p> <ul> <li> <p>Mean: \\(\\frac{71}{71+31} = \\frac{71}{102} \\approx 0.696\\)</p> </li> <li> <p>Mode: \\(\\frac{71-1}{71+31-2} = \\frac{70}{100} = 0.7\\)</p> </li> </ul> <p>Key insights:</p> <ol> <li> <p>More data = more confidence: With 100 flips, we're 99.9% confident it's coin 70, compared to only 25% confidence with 10 flips.</p> </li> <li> <p>Precision increases: The probability mass concentrates much more tightly around the true value.</p> </li> <li> <p>Beta distribution reflects this: Beta(71, 31) has a much sharper peak than Beta(8, 4), showing how more data leads to more precise estimates.</p> </li> <li> <p>Law of large numbers: As we get more data, our estimate converges to the true probability (0.7 in this case).</p> </li> </ol> <p>An animation of the beta distribution for different values of its parameters:</p> <p></p>"},{"location":"math/probability/central_limit_theorem/","title":"Central Limit Theorem","text":""},{"location":"math/probability/central_limit_theorem/#a-simplified-galton-board","title":"A simplified Galton Board","text":"<p>This is a Galton board. Maybe you've seen one before\u2014 it's a popular demonstration of how, even when a single event is chaotic and random with an effectively unknowable outcome, it's still possible to make precise statements about a large number of events, namely how the relative proportions for many different outcomes are distributed.</p> <p></p> <p>Let's take a model version of this. </p> <p></p> <p>In this model, we will assume that each ball falls directly onto a certain central peg and that it has a 50-50 probability of bouncing to the left or to the right. </p> <p></p> <p>We'll think of each of those outcomes as either adding one or subtracting one from its position. Once one of those is chosen, we make the highly unrealistic assumption that it happens to land dead center on the peg adjacent below it, where again it'll be faced with the same 50-50 choice of bouncing to the left or to the right.</p> <p> </p> <p>Here, there are five different rows of pegs, so the little hopping ball makes five different random choices between plus one and minus one. </p> <p></p> <p>We can think of its final position as basically being the sum of all of those different numbers, which in this case happens to be one.</p> <p></p> <p>We might label all of the different buckets with the sum that they represent. As we repeat this, we're looking at different possible sums for those five random numbers.</p> <p></p> <p>Let it be emphasized that the goal right now is not to accurately model physics\u2014 the goal is to give a simple example to illustrate the central limit theorem. For that purpose, idealized though this might be, it actually provides a really good example.</p> <p>If we let many different balls fall, making yet another unrealistic assumption that they don't influence each other (as if they're all ghosts), then the number of balls that fall into each different bucket gives us some loose sense for how likely each one of those buckets is.</p> <p> </p> <p>In this example, the numbers are simple enough that it's not too hard to explicitly calculate what the probability is for falling into each bucket. If you do want to think that through, you'll find it very reminiscent of Pascal's triangle.</p> <p></p> <p>The basic idea of the central limit theorem is that if you increase the size of that sum (for example, here would mean increasing the number of rows of pegs for each ball to bounce off), then the distribution that describes where that sum is going to fall looks more and more like a bell curve.</p> <p> </p>"},{"location":"math/probability/central_limit_theorem/#the-general-idea","title":"The general idea","text":"<p>It's actually worth taking a moment to write down that general idea. </p> <p>The setup is that we have a random variable. We'll call that random number \\(X\\). What we're doing is taking multiple different samples of that variable \\(X\\) (or we could take different random variables that do the exact same thing, like \\(X1\\), \\(X2\\),.. that are independent and are the exact same function, i.e., identical) and adding them all together. On our Galton board, that looks like letting the ball bounce off multiple different pegs on its way down to the bottom, and in the case of a die, you might imagine rolling many different dice and adding up the results. The claim of the central limit theorem is that as you let the size of that sum get bigger and bigger, then the distribution of that sum, how likely it is to fall into different possible values, will look more and more like a bell curve. That's it- that is the general idea.</p> <p></p>"},{"location":"math/probability/central_limit_theorem/#dice-simulations","title":"Dice simulations","text":"<p>Usually if you think of rolling a die, you think of the six outcomes as being equally probable, but the theorem actually doesn't care about that. We could start with a weighted die, something with a non-trivial distribution across the outcomes, and the core idea still holds.</p> <p>To better illustrate what the central limit theorem is all about, let's run four of these simulations in parallel, where on the upper left we're doing it where we're only adding two dice at a time, on the upper right we're doing it where we're adding five dice at a time, the lower left is the one that we just saw adding 10 dice at a time, and then we'll do another one with a bigger sum, 15 at a time.</p> <p></p> <p>Notice how on the upper left when we're just adding two dice, the resulting distribution doesn't really look like a bell curve\u2014 it looks a lot more reminiscent of the one we started with, skewed towards the left. But as we allow for more and more dice in each sum, the resulting shape that comes up in these distributions looks more and more symmetric. It has the lump in the middle and fade towards the tails shape of a bell curve.</p> <p> </p> <p>And let it be emphasized again: you can start with any different distribution.</p> <p>Illustrating things with a simulation like this is very fun, and it's kind of neat to see order emerge from chaos, but it also feels a little imprecise. How many samples do we need before we can be sure that what we're looking at is representative of the true distribution?</p>"},{"location":"math/probability/central_limit_theorem/#the-true-distributions-for-sums","title":"The true distributions for sums","text":"<p>Let's get a little more theoretical and show the precise shape these distributions will take on in the long run. The easiest case to make this calculation is if we have a uniform distribution, where each possible face of the die has an equal probability, 1/6th.</p> <p></p> <p>For example, if you then want to know how likely different sums are for a pair of dice, it's essentially a counting game, where you count up how many distinct pairs take on the same sum, which in the diagram drawn, you can conveniently think about by going through all the different diagonals. Since each such pair has an equal chance of showing up, 1 in 36, all you have to do is count the sizes of these buckets.</p> <p></p> <p>That gives us a definitive shape for the distribution describing a sum of two dice, and if we were to play the same game with all possible triplets, the resulting distribution would look like this. </p> <p></p> <p>Now what's more challenging, but a lot more interesting, is to ask what happens if we have a non-uniform distribution for that single die. </p> <p></p> <p>So just to be crystal clear on what's being represented here: if you imagine sampling two different values from that top distribution, the one describing a single die, and adding them together, then the second distribution being drawn represents how likely you are to see various different sums (this is equivalent to the convolution of the die's probability mass function with itself). Likewise, if you imagine sampling three distinct values from that top distribution, and adding them together, the next plot represents the probabilities for various different sums in that case. </p> <p></p> <p>So if we compute what the distributions for these sums look like for larger and larger sums, it looks more and more like a bell curve.</p> <p></p> <p>But before we get to that, let's make a couple of simple observations. For example, these distributions seem to be wandering to the right, and also they seem to be getting more spread out, and a little bit more flat. You cannot describe the central limit theorem quantitatively without taking into account both of those effects, which in turn requires describing the mean and the standard deviation.</p> <p></p>"},{"location":"math/probability/central_limit_theorem/#mean-variance-and-standard-deviation","title":"Mean, variance, and standard deviation","text":"<p>Looking back at our sequence of distributions, let's talk about the mean and standard deviation. If we call the mean of the initial distribution mu, which for the one illustrated happens to be 2.24, hopefully it won't be too surprising if it's said that the mean of the next one is 2 times mu. That is, you roll a pair of dice, you want to know the expected value of the sum, it's two times the expected value for a single die. Similarly, the expected value for our sum of size 3 is 3 times mu, and so on and so forth. The mean just marches steadily on to the right, which is why our distributions seem to be drifting off in that direction.</p> <p></p> <p>A little more challenging, but very important, is to describe how the standard deviation changes. The key fact here is that if you have two different random variables, then the variance for the sum of those variables is the same as just adding together the original two variances. The main thing to highlight is how it's the variance that adds\u2014 it's not the standard deviation that adds. </p> <p></p> <p>So, critically, if you were to take n different realizations of the same random variable and ask what the sum looks like, the variance of sum is n times the variance of your original variable, meaning the standard deviation, the square root of all this, is the square root of n times the original standard deviation.</p> <p></p> <p>For example, back in our sequence of distributions, if we label the standard deviation of our initial one with sigma, then the next standard deviation is going to be the square root of 2 times sigma, and after that it looks like the square root of 3 times sigma, and so on and so forth.</p> <p></p> <p>This, as mentioned, is very important. It means that even though our distributions are getting spread out, they're not spreading out all that quickly\u2014 they only do so in proportion to the square root of the size of the sum.</p> <p>As we prepare to make a more quantitative description of the central limit theorem, the core intuition to keep in your head is that we'll basically realign all of these distributions so that their means line up together, and then rescale them so that all of the standard deviations are just going to be equal to one. </p> <p> </p> <p>And when we do that, the shape that results gets closer and closer to a certain universal shape, described with an elegant little function that we'll unpack in just a moment.</p> <p></p> <p>And let it be said one more time: the real magic here is how we could have started with any distribution, describing a single roll of the die, and if we play the same game, considering what the distributions for the many different sums look like, and we realign them so that the means line up, and we rescale them so that the standard deviations are all one, we still approach that same universal shape, which is kind of mind-boggling.</p>"},{"location":"math/probability/central_limit_theorem/#unpacking-the-gaussian-formula","title":"Unpacking the Gaussian formula","text":"<p>The function e to the x, or anything to the x, describes exponential growth, and if you make that exponent negative, which flips around the graph horizontally, you might think of it as describing exponential decay. </p> <p> </p> <p>To make this decay in both directions, you could do something to make sure the exponent is always negative and growing, like taking the negative absolute value. </p> <p></p> <p>That would give us this kind of awkward sharp point in the middle, but if instead you make that exponent the negative square of x, you get a smoother version of the same thing, which decays in both directions.</p> <p></p> <p>This gives us the basic bell curve shape. Now if you throw a constant in front of that x, and you scale that constant up and down, it lets you stretch and squish the graph horizontally, allowing you to describe narrow and wider bell curves.</p> <p> </p> <p>And a quick thing to point out here is that based on the rules of exponentiation, as we tweak around that constant c, you could also think about it as simply changing the base of the exponentiation.</p> <p> </p> <p>And in that sense, the number e is not really all that special for our formula. We could replace it with any other positive constant, and you'll get the same family of curves as we tweak that constant. Make it a 2, same family of curves. Make it a 3, same family of curves. The reason we use e is that it gives that constant a very readable meaning.</p> <p></p> <p>Or rather, if we reconfigure things a little bit so that the exponent looks like negative 1/2 times x divided by a certain constant, which we'll suggestively call sigma squared, then once we turn this into a probability distribution, that constant sigma will be the standard deviation of that distribution. And that's very nice.</p> <p> </p> <p>But before we can interpret this as a probability distribution, we need the area under the curve to be 1.</p> <p></p> <p>As it stands with the basic bell curve shape of e to the negative x squared, the area is not 1, it's actually the square root of pi.</p> <p></p> <p>For our purposes right now, all it means is that we should divide this function by the square root of pi, and it gives us the area we want. </p> <p></p> <p>Throwing back in the constants we had earlier, the one half and the sigma, the effect there is to stretch out the graph by a factor of sigma times the square root of 2. </p> <p></p> <p>So we also need to divide out by that in order to make sure it has an area of 1, and combining those fractions, the factor out front looks like 1 divided by sigma times the square root of 2 pi.</p> <p> </p> <p>This, finally, is a valid probability distribution. As we tweak that value sigma, resulting in narrower and wider curves, that constant in the front always guarantees that the area equals 1. </p> <p> </p> <p>The special case where sigma equals 1 has a specific name\u2014 we call it the standard normal distribution.</p> <p></p> <p>And all possible normal distributions are not only parameterized with this value sigma, but we also subtract off another constant mu from the variable x, and this essentially just lets you slide the graph left and right so that you can prescribe the mean of this distribution. </p> <p> </p> <p>So in short, we have two parameters, one describing the mean, one describing the standard deviation, and they're all tied together in this big formula involving an e and a pi.</p> <p>Let's look back again at the idea of starting with some random variable and asking what the distributions for sums of that variable look like. When you increase the size of that sum, the resulting distribution will shift according to a growing mean, and it slowly spreads out according to a growing standard deviation. </p> <p></p> <p>And putting some actual formulas to it, if we know the mean of our underlying random variable, we call it mu, and we also know its standard deviation, and we call it sigma, then the mean for the sum on the bottom will be mu times the size of the sum, and the standard deviation will be sigma times the square root of that size.</p> <p></p> <p>So now, if we want to claim that this looks more and more like a bell curve, and a bell curve is only described by two different parameters, the mean and the standard deviation, you know what to do. You could plug those two values into the formula, and it gives you a highly explicit, albeit kind of complicated, formula for a curve that should closely fit our distribution.</p> <p> </p>"},{"location":"math/probability/central_limit_theorem/#the-more-elegant-formulation","title":"The more elegant formulation","text":"<p>But there's another way we can describe it that's a little more elegant and lends itself to a very fun visual that we can build up to. Instead of focusing on the sum of all of these random variables, let's modify this expression a little bit, where what we'll do is we'll look at the mean that we expect that sum to take, and we subtract it off so that our new expression has a mean of zero, and then we're going to look at the standard deviation we expect of our sum, and divide out by that, which basically just rescales the units so that the standard deviation of our expression will equal one.</p> <p>This might seem like a more complicated expression, but it actually has a highly readable meaning. It's essentially saying how many standard deviations away from the mean is this sum? For example, this bar here corresponds to a certain value that you might find when you roll 10 dice and you add them all up, and its position a little above negative one is telling you that that value is a little bit less than one standard deviation lower than the mean.</p> <p>Also, by the way, in anticipation for the animation being built to here, the way things are being represented on that lower plot is that the area of each one of these bars is telling us the probability of the corresponding value rather than the height. You might think of the y-axis as representing not probability but a kind of probability density. The reason for this is to set the stage so that it aligns with the way we interpret continuous distributions, where the probability of falling between a range of values is equal to an area under a curve between those values. In particular, the area of all the bars together is going to be one.</p> <p>Now, with all of that in place, let's have a little fun. Let's start by rolling things back so that the distribution on the bottom represents a relatively small sum, like adding together only three such random variables. Notice what happens as the distribution we start with changes. As it changes, the distribution on the bottom completely changes its shape. It's very dependent on what we started with.</p> <p>If we let the size of our sum get a little bit bigger, say going up to 10, and as the distribution for x changes, it largely stays looking like a bell curve, but some distributions can be found that get it to change shape. For example, the really lopsided one where almost all the probability is in the numbers 1 or 6 results in this kind of spiky bell curve. And if you'll recall, earlier on this was actually shown in the form of a simulation. Though if you were wondering whether that spikiness was an artifact of the randomness or reflected the true distribution, it turns out it reflects the true distribution. In this case, 10 is not a large enough sum for the central limit theorem to kick in.</p> <p>But if instead we let that sum grow and consider adding 50 different values, which is actually not that big, then no matter how the distribution for our underlying random variable changes, it has essentially no effect on the shape of the plot on the bottom. No matter where we start, all of the information and nuance for the distribution of x gets washed away, and we tend towards this single universal shape described by a very elegant function for the standard normal distribution: 1 over square root of 2 pi times e to the negative x squared over 2.</p> <p>This, this right here is what the central limit theorem is all about. Almost nothing you can do to this initial distribution changes the shape we tend towards.</p> <p>Now, the more theoretically minded among you might still be wondering what is the actual theorem, like what's the mathematical statement that could be proved or disproved that we're claiming here. If you want a nice formal statement, here's how it might go.</p> <p>Consider this value where we're summing up n different instantiations of our variable, but tweaked and tuned so that its mean and standard deviation are 1, again meaning you can read it as asking how many standard deviations away from the mean is the sum. Then the actual rigorous no-jokes-this-time statement of the central limit theorem is that if you consider the probability that this value falls between two given real numbers, a and b, and you consider the limit of that probability as the size of your sum goes to infinity, then that limit is equal to a certain integral, which basically describes the area under a standard normal distribution between those two values.</p> <p>Again, there are three underlying assumptions that have yet to be told, but other than those, in all of its gory detail, this right here is the central limit theorem.</p>"},{"location":"math/probability/central_limit_theorem/#a-concrete-example","title":"A concrete example","text":"<p>All of that is a bit theoretical, so it might be helpful to bring things back down to earth and turn back to the concrete example that was mentioned at the start, where you imagine rolling a die 100 times, and let's assume it's a fair die for this example, and you add together the results. The challenge is to find a range of values such that you're 95% sure that the sum will fall within this range.</p> <p>For questions like this, there's a handy rule of thumb about normal distributions, which is that about 68% of your values are going to fall within one standard deviation of the mean, 95% of your values, the thing we care about, fall within two standard deviations of the mean, and a whopping 99.7% of your values will fall within three standard deviations of the mean. It's a rule of thumb that's commonly memorized by people who do a lot of probability and stats.</p> <p>Naturally, this gives us what we need for our example, and let's go ahead and draw out what this would look like, where the distribution for a fair die is shown up at the top, and the distribution for a sum of 100 such dice on the bottom, which by now as you know looks like a certain normal distribution.</p> <p>Step 1 with a problem like this is to find the mean of your initial distribution, which in this case will look like 1/6th times 1 plus 1/6th times 2 on and on and on, and works out to be 3.5. We also need the standard deviation, which requires calculating the variance, which as you know involves adding all the squares of the differences between the values and the means, and it works out to be 2.92, square root of that comes out to be 1.71.</p> <p>Those are the only two numbers we need, and you are invited again to reflect on how magical it is that those are the only two numbers you need to completely understand the bottom distribution. Its mean will be 100 times mu, which is 350, and its standard deviation will be the square root of 100 times sigma, so 10 times sigma, 17.1.</p> <p>Remembering our handy rule of thumb, we're looking for values two standard deviations away from the mean, and when you subtract 2 sigma from mean, you end up with about 316, and when you add 2 sigma you end up with 384. There you go, that gives us the answer.</p> <p>Instead of just asking about the sum of 100 die rolls, let's say we had you divide that number by 100, which basically means all the numbers in our diagram in the bottom get divided by 100. Take a moment to interpret what this all would be saying then. The expression essentially tells you the empirical average for 100 different die rolls, and that interval we found is now telling you what range you are expecting to see for that empirical average.</p> <p>In other words, you might expect it to be around 3.5, that's the expected value for a die roll, but what's much less obvious and what the central limit theorem lets you compute is how close to that expected value you'll reasonably find yourself. In particular, it's worth your time to take a moment mulling over what the standard deviation for this empirical average is, and what happens to it as you look at a bigger and bigger sample of die rolls.</p>"},{"location":"math/probability/central_limit_theorem/#underlying-assumptions","title":"Underlying assumptions","text":"<p>Lastly, but probably most importantly, let's talk about the assumptions that go into this theorem. The first one is that all of these variables that we're adding up are independent from each other. The outcome of one process doesn't influence the outcome of any other process.</p> <p>The second is that all of these variables are drawn from the same distribution. Both of these have been implicitly assumed with our dice example. We've been treating the outcome of each die roll as independent from the outcome of all the others, and we're assuming that each die follows the same distribution. Sometimes in the literature you'll see these two assumptions lumped together under the initials IID for independent and identically distributed.</p> <p>One situation where these assumptions are decidedly not true would be the Galton board. Think about it. Is it the case that the way a ball bounces off of one of the pegs is independent from how it's going to bounce off the next peg? Absolutely not. Depending on the last bounce, it's coming in with a completely different trajectory. And is it the case that the distribution of possible outcomes off of each peg are the same for each peg that it hits? Again, almost certainly not.</p> <p>Maybe it hits one peg glancing to the left, meaning the outcomes are hugely skewed in that direction, and then hits the next one glancing to the right. When all those simplifying assumptions were made in the opening example, it wasn't just to make this easier to think about. It's also that those assumptions were necessary for this to actually be an example of the central limit theorem.</p> <p>Nevertheless, it seems to be true that for the real Galton board, despite violating both of these, a normal distribution does kind of come about? Part of the reason might be that there are generalizations of the theorem beyond the scope of this video that relax these assumptions, especially the second one. But there is a caution against the fact that many times people seem to assume that a variable is normally distributed, even when there's no actual justification to do so.</p> <p>The third assumption is actually fairly subtle. It's that the variance we've been computing for these variables is finite. This was never an issue for the dice example because there were only six possible outcomes. But in certain situations where you have an infinite set of outcomes, when you go to compute the variance, the sum ends up diverging off to infinity. These can be perfectly valid probability distributions, and they do come up in practice.</p> <p>But in those situations, as you consider adding many different instantiations of that variable and letting that sum approach infinity, even if the first two assumptions hold, it is very much a possibility that the thing you tend towards is not actually a normal distribution.</p>"},{"location":"math/probability/conditional_distributions/","title":"Conditional Distributions","text":"<p>Conditional distributions describe the probability distribution of one random variable given that another random variable takes on a specific value. They are fundamental to understanding how random variables relate to each other.</p>"},{"location":"math/probability/conditional_distributions/#discrete-case","title":"Discrete Case","text":"<p>For two discrete random variables \\(X\\) and \\(Y\\), the conditional probability mass function of \\(X\\) given \\(Y = y\\) is defined as:</p> \\[p_{X|Y}(x|y) = \\frac{p_{X,Y}(x,y)}{p_Y(y)}\\] <p>where \\(p_Y(y) &gt; 0\\).</p>"},{"location":"math/probability/conditional_distributions/#continuous-case","title":"Continuous Case","text":"<p>For two continuous random variables \\(X\\) and \\(Y\\), the conditional probability density function of \\(X\\) given \\(Y = y\\) is defined as:</p> \\[f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\\] <p>where \\(f_Y(y) &gt; 0\\).</p>"},{"location":"math/probability/conditional_distributions/#interpretation","title":"Interpretation","text":"<p>The conditional distribution \\(p_{X|Y}(x|y)\\) or \\(f_{X|Y}(x|y)\\) represents:</p> <ul> <li> <p>The probability (or probability density) that \\(X = x\\) given that we know \\(Y = y\\)</p> </li> <li> <p>How the distribution of \\(X\\) changes when we condition on different values of \\(Y\\)</p> </li> </ul> <p>The conditional distribution formulas work analogously to the familiar conditional probability formula:</p> \\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\] <p>In the discrete case, we have:</p> \\[p_{X|Y}(x|y) = \\frac{p_{X,Y}(x,y)}{p_Y(y)}\\] <p>This is exactly analogous to:</p> \\[P(X = x | Y = y) = \\frac{P(X = x, Y = y)}{P(Y = y)}\\] <p>The joint PMF \\(p_{X,Y}(x,y)\\) represents \\(P(X = x, Y = y)\\), and the marginal PMF \\(p_Y(y)\\) represents \\(P(Y = y)\\).</p> <p>For continuous variables, the PDF values themselves are not probabilities, but the ratio of PDFs works the same way:</p> \\[f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\\] <p>Think of it this way:</p> <ol> <li>Joint PDF \\(f_{X,Y}(x,y)\\) tells us how \"dense\" the probability is at point \\((x,y)\\)</li> <li> <p>Marginal PDF \\(f_Y(y)\\) tells us how \"dense\" the probability is along the entire line \\(Y = y\\)</p> </li> <li> <p>Conditional PDF \\(f_{X|Y}(x|y)\\) tells us how \"dense\" the probability is at \\(X = x\\) when we're restricted to \\(Y = y\\)</p> </li> </ol> <p>Example: Consider a chicken that lays eggs according to a Poisson process. Let \\(N\\) be the total number of eggs laid, where \\(N \\sim \\text{Poisson}(\\lambda)\\). Each egg has a probability \\(p\\) of hatching. Let \\(X\\) be the number of eggs that hatch, so \\(X|N \\sim \\text{Binomial}(N,p)\\). Let \\(Y\\) be the number of eggs that don't hatch, so \\(X + Y = N\\). Find the joint distribution of \\(X\\) and \\(Y\\), and determine if they are independent.</p> <p>We are given:  - \\(N \\sim \\text{Poisson}(\\lambda)\\): Total eggs laid</p> <ul> <li> <p>\\(X|N \\sim \\text{Binomial}(N,p)\\): Eggs that hatch, given \\(N\\) eggs</p> </li> <li> <p>\\(Y = N - X\\): Eggs that don't hatch</p> </li> <li> <p>We need to find \\(p_{X,Y}(x,y)\\)</p> </li> </ul> <p>Let's find the joint distribution. Since \\(X + Y = N\\), we can write:</p> \\[p_{X,Y}(x,y) = P(X = x, Y = y) = P(X = x, N = x + y)\\] <p>Using the law of total probability and the conditional distribution:</p> \\[p_{X,Y}(x,y) = P(X = x|N = x + y) \\cdot P(N = x + y)\\] <p>Given \\(X|N \\sim \\text{Binomial}(N,p)\\):</p> \\[P(X = x|N = x + y) = \\binom{x + y}{x} p^x (1-p)^y\\] <p>And \\(N \\sim \\text{Poisson}(\\lambda)\\):</p> \\[P(N = x + y) = \\frac{e^{-\\lambda} \\lambda^{x + y}}{(x + y)!}\\] <p>Therefore:</p> \\[p_{X,Y}(x,y) = \\binom{x + y}{x} p^x (1-p)^y \\cdot \\frac{e^{-\\lambda} \\lambda^{x + y}}{(x + y)!}\\] <p>Simplifying:</p> \\[p_{X,Y}(x,y) = \\frac{(x + y)!}{x! y!} p^x (1-p)^y \\cdot \\frac{e^{-\\lambda} \\lambda^{x + y}}{(x + y)!}\\] \\[p_{X,Y}(x,y) = \\frac{e^{-\\lambda} \\lambda^{x + y} p^x (1-p)^y}{x! y!}\\] \\[p_{X,Y}(x,y) = \\frac{e^{-\\lambda} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] <p>Let's find the marginal distributions.</p> \\[p_X(x) = \\sum_{y=0}^{\\infty} p_{X,Y}(x,y) = \\sum_{y=0}^{\\infty} \\frac{e^{-\\lambda} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] \\[p_X(x) = \\frac{e^{-\\lambda} (\\lambda p)^x}{x!} \\sum_{y=0}^{\\infty} \\frac{(\\lambda(1-p))^y}{y!}\\] \\[p_X(x) = \\frac{e^{-\\lambda} (\\lambda p)^x}{x!} e^{\\lambda(1-p)} = \\frac{e^{-\\lambda p} (\\lambda p)^x}{x!}\\] <p>This shows \\(X \\sim \\text{Poisson}(\\lambda p)\\).</p> <p>Similarly:</p> \\[p_Y(y) = \\frac{e^{-\\lambda(1-p)} (\\lambda(1-p))^y}{y!}\\] <p>So \\(Y \\sim \\text{Poisson}(\\lambda(1-p))\\).</p> <p>If \\(X\\) and \\(Y\\) were independent, then:</p> \\[p_{X,Y}(x,y) = p_X(x) \\cdot p_Y(y)\\] <p>Let's check:</p> \\[p_X(x) \\cdot p_Y(y) = \\frac{e^{-\\lambda p} (\\lambda p)^x}{x!} \\cdot \\frac{e^{-\\lambda(1-p)} (\\lambda(1-p))^y}{y!}\\] \\[p_X(x) \\cdot p_Y(y) = \\frac{e^{-\\lambda p - \\lambda(1-p)} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] \\[p_X(x) \\cdot p_Y(y) = \\frac{e^{-\\lambda} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] <p>This equals \\(p_{X,Y}(x,y)\\), so \\(X\\) and \\(Y\\) are independent!</p> <p>The joint distribution is:</p> \\[p_{X,Y}(x,y) = \\frac{e^{-\\lambda} (\\lambda p)^x (\\lambda(1-p))^y}{x! y!}\\] <p>And \\(X\\) and \\(Y\\) are independent random variables, each following Poisson distributions with parameters \\(\\lambda p\\) and \\(\\lambda(1-p)\\) respectively.</p> <p>Note: It's crucial to distinguish between \\(X|N \\sim \\text{Binomial}(N,p)\\) (conditional distribution where \\(N\\) is random) and \\(X \\sim \\text{Binomial}(N,p)\\) (unconditional distribution where \\(N\\) would be fixed). The conditional notation is essential here since \\(N\\) is a random variable.</p>"},{"location":"math/probability/conditional_expectation/","title":"Conditional Expectation","text":""},{"location":"math/probability/conditional_expectation/#the-two-envelope-paradox","title":"The Two Envelope Paradox","text":"<p>The two envelope paradox is a famous probability puzzle that challenges our intuition about conditional expectation and reveals subtle issues in reasoning about random variables.</p> <p>You are given two envelopes. One contains twice as much money as the other. You pick one envelope at random, open it, and see that it contains $X. You are then given the choice to either:</p> <ol> <li>Keep the envelope you have (containing $X)</li> <li>Switch to the other envelope</li> </ol> <p>The question: Should you switch?</p> <p>At first glance, it seems like switching should be beneficial.</p> <p>The other envelope contains either $X/2 or $2X, each with probability 1/2. The expected value of switching is:</p> \\[E[\\text{other envelope}] = \\frac{1}{2} \\cdot \\frac{X}{2} + \\frac{1}{2} \\cdot 2X = \\frac{X}{4} + X = \\frac{5X}{4}\\] <p>Since \\(\\frac{5X}{4} &gt; X\\), you should always switch!</p> <p>The paradox: This reasoning suggests you should always switch, regardless of which envelope you initially chose. But this is clearly wrong- if you always switch, you're back to a 50-50 choice!</p> <p>The error lies in assuming that the other envelope contains \\(X/2\\) or \\(2X\\) with equal probability 1/2.</p> <p>Let's be more careful about the random variables involved.</p> <p>Let \\(A\\) be the amount in the first envelope, and \\(2A\\) be the amount in the second envelope. The envelopes are chosen randomly, so:</p> <ul> <li> <p>\\(P(\\text{you pick first}) = \\frac{1}{2}\\)</p> </li> <li> <p>\\(P(\\text{you pick second}) = \\frac{1}{2}\\)</p> </li> </ul> <p>Case 1: You picked the first envelope (contains \\(A\\))</p> <ul> <li> <p>The other envelope contains \\(2A\\)</p> </li> <li> <p>If you switch, you gain \\(A\\)</p> </li> </ul> <p>Case 2: You picked the second envelope (contains \\(2A\\))</p> <ul> <li> <p>The other envelope contains \\(A\\)</p> </li> <li> <p>If you switch, you lose \\(A\\)</p> </li> </ul> <p>Expected gain from switching:</p> \\[E[\\text{gain from switching}] = \\frac{1}{2} \\cdot A + \\frac{1}{2} \\cdot (-A) = 0\\] <p>Conclusion: There is no expected gain from switching. The correct strategy is to be indifferent between keeping and switching.</p> <p>The original reasoning made a subtle error by treating \\(X\\) as if it were independent of which envelope you chose. But \\(X\\) is not independent- it depends on whether you picked the first or second envelope.</p>"},{"location":"math/probability/conditional_expectation/#definition-of-conditional-expectation","title":"Definition of Conditional Expectation","text":"<p>Conditional expectation is the expected value of a random variable given that we know the value of another random variable. It's a fundamental concept in probability theory that allows us to make predictions based on partial information.</p>"},{"location":"math/probability/conditional_expectation/#discrete-case","title":"Discrete Case","text":"<p>For discrete random variables \\(X\\) and \\(Y\\), the conditional expectation of \\(X\\) given \\(Y = y\\) is:</p> \\[E[X | Y = y] = \\sum_x x \\cdot P(X = x | Y = y) = \\sum_x x \\cdot \\frac{P(X = x, Y = y)}{P(Y = y)}\\] <p>Key properties:</p> <ul> <li> <p>\\(E[X | Y = y]\\) is a function of \\(y\\)</p> </li> <li> <p>It represents the average value of \\(X\\) when we know \\(Y = y\\)</p> </li> <li> <p>If \\(X\\) and \\(Y\\) are independent, then \\(E[X | Y = y] = E[X]\\) for all \\(y\\)</p> </li> </ul>"},{"location":"math/probability/conditional_expectation/#continuous-case","title":"Continuous Case","text":"<p>For continuous random variables \\(X\\) and \\(Y\\) with joint density \\(f_{X,Y}(x,y)\\) and marginal density \\(f_Y(y)\\), the conditional expectation of \\(X\\) given \\(Y = y\\) is:</p> \\[E[X | Y = y] = \\int_{-\\infty}^{\\infty} x \\cdot f_{X|Y}(x|y) \\, dx = \\int_{-\\infty}^{\\infty} x \\cdot \\frac{f_{X,Y}(x,y)}{f_Y(y)} \\, dx\\] <p>where \\(f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\\) is the conditional density of \\(X\\) given \\(Y = y\\).</p>"},{"location":"math/probability/conditional_expectation/#the-random-variable-ex-y-and-its-intuitive-meaning","title":"The Random Variable \\(E[X | Y]\\) and its intuitive meaning","text":"<p>Conditional expectation is itself a random variable! We write \\(E[X | Y]\\) to denote the random variable that takes the value \\(E[X | Y = y]\\) when \\(Y = y\\).</p> <p>Conditional expectation answers the question: \"What is the average value of \\(X\\) when we know that \\(Y\\) takes a specific value?\"</p> <p>Example: If \\(X\\) is your test score and \\(Y\\) is the number of hours you studied:</p> <ul> <li> <p>\\(E[X | Y = 0]\\) = average test score for students who didn't study</p> </li> <li> <p>\\(E[X | Y = 10]\\) = average test score for students who studied 10 hours</p> </li> <li> <p>\\(E[X | Y]\\) = a function that tells you the expected test score for any amount of study time</p> </li> </ul> <p>Example: Let \\(X \\sim N(0, 1)\\) and \\(Y = X^2\\). What is \\(E[Y | X]\\)?</p> <p>Since \\(Y = X^2\\), when we know the value of \\(X\\), we know exactly what \\(Y\\) is. Therefore:</p> \\[E[Y | X = x] = E[X^2 | X = x] = E[x^2 | X = x] = x^2\\] <p>This makes intuitive sense: if we know \\(X = x\\), then we know \\(Y = x^2\\) with certainty, so the expected value of \\(Y\\) given \\(X = x\\) is simply \\(x^2\\).</p> <p>Key insight: When \\(Y\\) is a function of \\(X\\), the conditional expectation \\(E[Y | X]\\) is just that function evaluated at \\(X\\). In this case, \\(E[Y | X] = X^2\\).</p> <p>It's important to distinguish between \\(X^2\\) and \\(x^2\\). \\(X^2\\) is a random variable- it represents the square of the random variable \\(X\\). Since \\(X\\) can take different values, \\(X^2\\) can also take different values. \\(x^2\\) is a specific number - it's the square of a particular observed value \\(x\\) of the random variable \\(X\\).</p> <p>In our example:</p> <ul> <li> <p>\\(X \\sim N(0,1)\\) - \\(X\\) is a random variable that can take any real value</p> </li> <li> <p>\\(X^2\\) - This is also a random variable (the square of \\(X\\))</p> </li> <li> <p>\\(x\\) - This is a specific observed value of \\(X\\) (like \\(x = 1.5\\) or \\(x = -0.3\\))</p> </li> <li> <p>\\(x^2\\) - This is the square of that specific value (like \\(1.5^2 = 2.25\\) or \\((-0.3)^2 = 0.09\\))</p> </li> </ul> <p>When we write \\(E[Y | X = x]\\), we're asking: \"What's the expected value of \\(Y\\) given that \\(X\\) takes the specific value \\(x\\)?\" Since \\(Y = X^2\\), when \\(X = x\\), we know \\(Y = x^2\\) (a specific number). So \\(E[Y | X = x] = E[x^2 | X = x] = x^2\\)</p> <p>Now let's compute \\(E[X | Y]\\) for the same example. This is more interesting because knowing \\(Y = y\\) doesn't completely determine \\(X\\)- there are two possible values of \\(X\\) that give \\(Y = y\\).</p> <p>If \\(Y = y\\), then \\(X = \\pm\\sqrt{y}\\) (since \\(Y = X^2\\)).</p> <p>Since \\(X \\sim N(0,1)\\) is symmetric about 0, both \\(+\\sqrt{y}\\) and \\(-\\sqrt{y}\\) are equally likely when \\(Y = y\\).</p> \\[E[X | Y = y] = \\sqrt{y} \\cdot P(X = \\sqrt{y} | Y = y) + (-\\sqrt{y}) \\cdot P(X = -\\sqrt{y} | Y = y)\\] <p>Since \\(X \\sim N(0,1)\\) is symmetric about 0:</p> <ul> <li> <p>\\(P(X = \\sqrt{y}) = P(X = -\\sqrt{y})\\) (by symmetry)</p> </li> <li> <p>Therefore: \\(P(X = \\sqrt{y} | Y = y) = P(X = -\\sqrt{y} | Y = y)\\)</p> </li> </ul> <p>Since these are the only two possibilities when \\(Y = y\\), and they're equal:</p> \\[P(X = \\sqrt{y} | Y = y) = P(X = -\\sqrt{y} | Y = y) = \\frac{1}{2}\\] <p>When we condition on \\(Y = y\\), we're essentially asking: \"Given that \\(X^2 = y\\), what's the probability that \\(X = \\sqrt{y}\\) vs \\(X = -\\sqrt{y}\\)?\"</p> <p>Since the normal distribution is symmetric about 0, the probability density at \\(+\\sqrt{y}\\) is exactly the same as the probability density at \\(-\\sqrt{y}\\). When we condition on \\(Y = y\\), we're restricting ourselves to these two equally likely points.</p> \\[E[X | Y = y] = \\sqrt{y} \\cdot \\frac{1}{2} + (-\\sqrt{y}) \\cdot \\frac{1}{2} = \\frac{\\sqrt{y} - \\sqrt{y}}{2} = 0\\] <p>Result: \\(E[X | Y = y] = 0\\) for all \\(y &gt; 0\\).</p> <p>Key insight: Even though knowing \\(Y\\) gives us information about the magnitude of \\(X\\) (since \\(|X| = \\sqrt{Y}\\)), it doesn't give us information about the sign of \\(X\\). Due to the symmetry of the normal distribution, the positive and negative values cancel out, giving an expected value of 0.</p> <p>Example: Let's consider a more complex example involving continuous random variables and uniform distributions.</p> <p>We have a stick of length \\(L\\). First break: Break the stick at a point \\(Y\\) chosen uniformly at random along its length. This means \\(Y \\sim \\text{Uniform}(0, L)\\). Second break: Take the remaining piece of the stick (which has length \\(Y\\)) and break it at a point \\(X\\) chosen uniformly at random along this remaining length.</p> <p>Question: What is \\(E[X | Y]\\)?</p> <p>Given that \\(Y = y\\), we know:</p> <ul> <li> <p>The remaining piece has length \\(y\\)</p> </li> <li> <p>\\(X\\) is chosen uniformly at random from \\([0, y]\\)</p> </li> <li> <p>Therefore: \\(X | Y = y \\sim \\text{Uniform}(0, y)\\)</p> </li> </ul> <p>For a uniform distribution on \\([0, y]\\), the expected value is the midpoint:</p> \\[E[X | Y = y] = \\frac{0 + y}{2} = \\frac{y}{2}\\] <p>Result: \\(E[X | Y] = \\frac{Y}{2}\\)</p> <p>This makes perfect sense! If we know the remaining piece has length \\(y\\), then the expected position of the second break is exactly in the middle of that piece, which is \\(\\frac{y}{2}\\).</p> <p>Let's verify this:</p> <p>Step 1: Find \\(E[X]\\) directly</p> <ul> <li> <p>\\(Y \\sim \\text{Uniform}(0, L)\\), so \\(E[Y] = \\frac{L}{2}\\)</p> </li> <li> <p>\\(E[X] = E[E[X | Y]] = E\\left[\\frac{Y}{2}\\right] = \\frac{E[Y]}{2} = \\frac{L}{4}\\) (we will prove later why \\(E[X] = E[E[X | Y]] = E\\left[\\frac{Y}{2}\\right]\\), but it is a property)</p> </li> </ul> <p>Step 2: Find \\(E[X]\\) using the joint distribution</p> <ul> <li>Calculate the joint density</li> </ul> \\[f_{X,Y}(x,y) = f_{X|Y}(x|y) \\cdot f_Y(y) = \\frac{1}{y} \\cdot \\frac{1}{L} = \\frac{1}{Ly}\\] <p>The joint density of \\((X, Y)\\) is \\(f_{X,Y}(x,y) = \\frac{1}{Ly}\\) for \\(0 \\leq x \\leq y \\leq L\\)</p> <ul> <li>\\(E[X] = \\int_0^L \\int_0^y x \\cdot \\frac{1}{Ly} \\, dx \\, dy = \\int_0^L \\frac{y}{2L} \\, dy = \\frac{L}{4}\\)</li> </ul> <p>Both methods give the same result, confirming our calculation is correct.</p>"},{"location":"math/probability/conditional_expectation/#general-properties","title":"General Properties","text":"<p>1. Taking out what's known</p> <p>One of the most important properties of conditional expectation is the ability to \"take out what's known\":</p> \\[E[h(X)Y | X] = h(X)E[Y | X]\\] <p>where \\(h(X)\\) is any function of \\(X\\).</p> <p>Intuitive explanation: If we know the value of \\(X\\), then \\(h(X)\\) is just a constant (not random), so we can factor it out of the conditional expectation.</p> <p>Proof for discrete case:</p> \\[E[h(X)Y | X = x] = \\sum_y h(x) \\cdot y \\cdot P(Y = y | X = x) = h(x) \\sum_y y \\cdot P(Y = y | X = x) = h(x)E[Y | X = x]\\] <p>Proof for continuous case:</p> \\[E[h(X)Y | X = x] = \\int_{-\\infty}^{\\infty} h(x) \\cdot y \\cdot f_{Y|X}(y|x) \\, dy = h(x) \\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) \\, dy = h(x)E[Y | X = x]\\] <p>Examples:</p> <ul> <li> <p>\\(E[X^2Y | X] = X^2E[Y | X]\\) (since \\(X^2\\) is a function of \\(X\\))</p> </li> <li> <p>\\(E[\\sin(X)Y | X] = \\sin(X)E[Y | X]\\) (since \\(\\sin(X)\\) is a function of \\(X\\))</p> </li> <li> <p>\\(E[3Y | X] = 3E[Y | X]\\) (since 3 is a constant function of \\(X\\))</p> </li> </ul> <p>2. Independence property</p> <p>If \\(X\\) and \\(Y\\) are independent, then:</p> \\[E[Y | X] = E[Y]\\] <p>Intuitive explanation: If \\(X\\) and \\(Y\\) are independent, then knowing the value of \\(X\\) provides no information about \\(Y\\). Therefore, the conditional expectation of \\(Y\\) given \\(X\\) is the same as the unconditional expectation of \\(Y\\).</p> <p>Proof for discrete case:</p> \\[E[Y | X = x] = \\sum_y y \\cdot P(Y = y | X = x) = \\sum_y y \\cdot P(Y = y) = E[Y]\\] <p>where the second equality uses the fact that \\(P(Y = y | X = x) = P(Y = y)\\) when \\(X\\) and \\(Y\\) are independent.</p> <p>Proof for continuous case:</p> \\[E[Y | X = x] = \\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) \\, dy = \\int_{-\\infty}^{\\infty} y \\cdot f_Y(y) \\, dy = E[Y]\\] <p>where the second equality uses the fact that \\(f_{Y|X}(y|x) = f_Y(y)\\) when \\(X\\) and \\(Y\\) are independent.</p> <p>Examples:</p> <ul> <li> <p>If \\(X \\sim N(0,1)\\) and \\(Y \\sim N(0,1)\\) are independent, then \\(E[Y | X] = E[Y] = 0\\)</p> </li> <li> <p>If \\(X\\) is the number of heads in 10 coin flips and \\(Y\\) is the temperature tomorrow, and they're independent, then \\(E[Y | X] = E[Y]\\)</p> </li> </ul> <p>3. Iterated expectation (Adam's Law)</p> <p>The expectation of a conditional expectation equals the original expectation:</p> \\[E[E[Y | X]] = E[Y]\\] <p>Proof for discrete case:</p> \\[E[E[Y | X]] = \\sum_x E[Y | X = x] \\cdot P(X = x) = \\sum_x \\left(\\sum_y y \\cdot P(Y = y | X = x)\\right) \\cdot P(X = x)\\] \\[= \\sum_x \\sum_y y \\cdot P(Y = y | X = x) \\cdot P(X = x) = \\sum_x \\sum_y y \\cdot P(X = x, Y = y) = \\sum_y y \\cdot P(Y = y) = E[Y]\\] <p>Proof for continuous case:</p> \\[E[E[Y | X]] = \\int_{-\\infty}^{\\infty} E[Y | X = x] \\cdot f_X(x) \\, dx = \\int_{-\\infty}^{\\infty} \\left(\\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) \\, dy\\right) f_X(x) \\, dx\\] \\[= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) f_X(x) \\, dy \\, dx = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} y \\cdot f_{X,Y}(x,y) \\, dy \\, dx = E[Y]\\] <p>Examples:</p> <ul> <li> <p>In our stick-breaking example: \\(E[E[X | Y]] = E\\left[\\frac{Y}{2}\\right] = \\frac{E[Y]}{2} = \\frac{L}{4} = E[X]\\)</p> </li> <li> <p>If \\(Y\\) is test score and \\(X\\) is study hours: \\(E[E[Y | X]] = E[Y]\\) (the average test score across all students)</p> </li> </ul> <p>Why this matters: This property is fundamental to many probability calculations. It allows us to compute expectations by first conditioning on another variable, then taking the expectation of the conditional expectation. It's often easier to compute \\(E[Y | X]\\) first, then take its expectation, rather than computing \\(E[Y]\\) directly.</p> <p>4. Residual property</p> \\[E[(Y - E[Y | X])h(X)] = 0\\] <p>Proof:</p> \\[E[(Y - E[Y | X])h(X)] = E[Y \\cdot h(X) - E[Y | X] \\cdot h(X)]\\] \\[= E[Y \\cdot h(X)] - E[E[Y | X] \\cdot h(X)]\\] \\[= E[Y \\cdot h(X)] - E[h(X) \\cdot E[Y | X]]\\] <p>Now, using the \"taking out what's known\" property:</p> \\[E[h(X) \\cdot E[Y | X]] = E[h(X) \\cdot E[Y | X]] = E[E[h(X) \\cdot Y | X]] = E[h(X) \\cdot Y]\\] <p>Therefore:</p> \\[E[(Y - E[Y | X])h(X)] = E[Y \\cdot h(X)] - E[h(X) \\cdot Y] = 0\\] <p>Correlation interpretation:</p> <p>Since \\(E[(Y - E[Y | X])h(X)] = 0\\), we have:</p> \\[\\text{Cov}(Y - E[Y | X], h(X)) = E[(Y - E[Y | X])h(X)] - E[Y - E[Y | X]] \\cdot E[h(X)] = 0 - 0 \\cdot E[h(X)] = 0\\] <p>This shows that the residual \\(Y - E[Y | X]\\) is uncorrelated with any function \\(h(X)\\) of \\(X\\).</p> <p>Intuitive explanation: The residual \\(Y - E[Y | X]\\) represents the part of \\(Y\\) that cannot be predicted from \\(X\\). Since we've already extracted all the information that \\(X\\) can provide about \\(Y\\) (in the form of \\(E[Y | X]\\)), the remaining part should be uncorrelated with any function of \\(X\\).</p> <p>Examples:</p> <ul> <li> <p>\\(E[(Y - E[Y | X])X] = 0\\) (residual is uncorrelated with \\(X\\))</p> </li> <li> <p>\\(E[(Y - E[Y | X])X^2] = 0\\) (residual is uncorrelated with \\(X^2\\))</p> </li> <li> <p>\\(E[(Y - E[Y | X])\\sin(X)] = 0\\) (residual is uncorrelated with \\(\\sin(X)\\))</p> </li> </ul> <p>Geometric interpretation (Projection): The conditional expectation \\(E[Y | X]\\) is the projection of \\(Y\\) onto the space of all functions of \\(X\\), shown below as a plane. This is because in a certain sense, \\(E[Y | X]\\) is the function of \\(X\\) that is closest to \\(Y\\); we say that \\(E[Y | X]\\) is the projection of \\(Y\\) into the space of all functions of \\(X\\).The residual \\(Y - E[Y | X]\\) is orthogonal to the plane: it's perpendicular to (uncorrelated with) any function of \\(X\\).</p> <p></p> <p>The \"line\" from \\(Y\\) to \\(E[Y | X]\\) in the figure is orthogonal (perpendicular) to the \"plane\", since any other route from \\(Y\\) to \\(E[Y | X]\\) would be longer. This orthogonality turns out to be the geometric interpretation of the residual property.</p>"},{"location":"math/probability/conditional_expectation/#conditional-expectation-as-the-best-predictor","title":"Conditional Expectation as the Best Predictor","text":"<p>We can think of \\(E[Y | X]\\) as a prediction for \\(Y\\) based on \\(X\\). This is an extremely common problem in statistics: predict or estimate the future observations or unknown parameters based on data. The projection interpretation of conditional expectation implies that \\(E[Y | X]\\) is the best predictor of \\(Y\\) based on \\(X\\), in the sense that it is the function of \\(X\\) with the lowest mean squared error (expected squared difference between \\(Y\\) and the prediction of \\(Y\\)).</p> <p>Mathematical statement: For any function \\(g(X)\\) of \\(X\\):</p> \\[E[(Y - E[Y | X])^2] \\leq E[(Y - g(X))^2]\\] <p>Proof: Let \\(g(X)\\) be any function of \\(X\\). Then:</p> \\[E[(Y - g(X))^2] = E[(Y - E[Y | X] + E[Y | X] - g(X))^2]\\] \\[= E[(Y - E[Y | X])^2] + E[(E[Y | X] - g(X))^2] + 2E[(Y - E[Y | X])(E[Y | X] - g(X))]\\] <p>The cross term is zero because \\(E[Y | X] - g(X)\\) is a function of \\(X\\), and we know that \\(Y - E[Y | X]\\) is uncorrelated with any function of \\(X\\). Therefore:</p> \\[E[(Y - g(X))^2] = E[(Y - E[Y | X])^2] + E[(E[Y | X] - g(X))^2] \\geq E[(Y - E[Y | X])^2]\\] <p>The equality holds if and only if \\(g(X) = E[Y | X]\\) (almost surely).</p> <p>Why this matters: This shows that conditional expectation is not just a convenient mathematical tool, but the optimal predictor in the mean squared error sense. This is why it's so fundamental to statistics, machine learning, and any field that involves prediction.</p>"},{"location":"math/probability/conditional_expectation/#case-study-linear-regression","title":"Case study: Linear Regression","text":"<p>An extremely widely used method for data analysis in statistics is linear regression. In its most basic form, the linear regression model uses a single explanatory variable \\(X\\) to predict a response variable \\(Y\\), and it assumes that the conditional expectation of \\(Y\\) is linear in \\(X\\):</p> \\[E[Y | X] = a + bX\\] <p>Show that an equivalent way to express this is to write</p> \\[Y = a + bX + \\epsilon\\] <p>where \\(\\epsilon\\) is an r.v. (called the error) with \\(E[\\epsilon | X] = 0\\).</p> <p>Solution: Let \\(Y = a + bX + \\epsilon\\), with \\(E[\\epsilon | X] = 0\\). Then by linearity:</p> \\[E[Y | X] = E[a | X] + E[bX | X] + E[\\epsilon | X] = a + bX\\] <p>Conversely, suppose that \\(E[Y | X] = a + bX\\), and define:</p> \\[\\epsilon = Y - (a + bX)\\] <p>Then \\(Y = a + bX + \\epsilon\\), with:</p> \\[E[\\epsilon | X] = E[Y | X] - E[a + bX | X] = E[Y | X] - (a + bX) = 0\\]"},{"location":"math/probability/conditional_probability/","title":"Conditional Probability","text":""},{"location":"math/probability/conditional_probability/#independent-events","title":"Independent Events","text":"<p>Two events \\(A\\) and \\(B\\) are independent if the occurrence of one event does not affect the probability of the occurrence of the other event.</p> <p>Mathematical Definition: Events \\(A\\) and \\(B\\) are independent if and only if:</p> \\[P(A \\cap B) = P(A) \\cdot P(B)\\] <p>Independence means that knowing whether event \\(B\\) occurred gives us no information about whether event \\(A\\) occurred, and vice versa.</p> <p>Example 1: coin flips Event \\(A\\): First coin flip is heads. Event \\(B\\): Second coin flip is heads. These events are independent because the result of the first flip doesn't affect the second flip. \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{1}{2}\\), \\(P(A \\cap B) = \\frac{1}{4} = \\frac{1}{2} \\cdot \\frac{1}{2}\\) \u2713</p> <p>Example 2: drawing cards (with replacement) Event \\(A\\): First card drawn is red. Event \\(B\\): Second card drawn is red (with replacement). These events are independent because we replace the first card. \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{1}{2}\\), \\(P(A \\cap B) = \\frac{1}{4} = \\frac{1}{2} \\cdot \\frac{1}{2}\\) \u2713</p> <p>Example 3: drawing cards (without replacement) - NOT independent Event \\(A\\): First card drawn is red. Event \\(B\\): Second card drawn is red (without replacement). These events are NOT independent because drawing a red card first affects the probability of drawing red second.</p> <p>It's crucial to understand the difference between independent events and disjoint (mutually exclusive) events.</p> <p>Independent Events:</p> <ul> <li> <p>Can occur together: \\(P(A \\cap B) = P(A) \\cdot P(B) &gt; 0\\) (if both \\(P(A) &gt; 0\\) and \\(P(B) &gt; 0\\))</p> </li> <li> <p>Knowledge of one event doesn't affect the probability of the other</p> </li> <li> <p>Example: Rolling a die twice - getting a 6 on the first roll and a 6 on the second roll</p> </li> </ul> <p>Disjoint Events:</p> <ul> <li> <p>Cannot occur together: \\(P(A \\cap B) = 0\\)</p> </li> <li> <p>If one event occurs, the other cannot occur</p> </li> <li> <p>Example: Getting heads and tails on a single coin flip</p> </li> </ul> <p>Key insight: If two events \\(A\\) and \\(B\\) are disjoint with \\(P(A) &gt; 0\\) and \\(P(B) &gt; 0\\), then they are NOT independent. This is because knowing that \\(A\\) occurred tells us that \\(B\\) definitely did not occur.</p> <p>Three events \\(A\\), \\(B\\), and \\(C\\) are mutually independent if and only if all of the following conditions hold:</p> <p>Pairwise independence: \\(P(A \\cap B) = P(A) \\cdot P(B)\\). \\(P(A \\cap C) = P(A) \\cdot P(C)\\). \\(P(B \\cap C) = P(B) \\cdot P(C)\\)</p> <p>Triple independence: \\(P(A \\cap B \\cap C) = P(A) \\cdot P(B) \\cdot P(C)\\)</p> <p>Important: Pairwise independence alone is not sufficient for mutual independence. All four conditions must be satisfied.</p> <p>How should you update your beliefs/probabilities based on new evidence? This is a pretty broad question.</p>"},{"location":"math/probability/conditional_probability/#definition-of-conditional-probability","title":"Definition of Conditional Probability","text":"<p>Conditional probability is the probability of an event \\(A\\) occurring given that another event \\(B\\) has already occurred.</p> <p>Mathematical Definition: The conditional probability of event \\(A\\) given event \\(B\\) is:</p> \\[P(A | B) = \\frac{P(A \\cap B)}{P(B)}\\] <p>provided that \\(P(B) &gt; 0\\).</p> <p>\\(P(A | B)\\) is read as \"the probability of \\(A\\) given \\(B\\)\". It represents the updated probability of \\(A\\) after we know that \\(B\\) has occurred. We restrict our sample space to only those outcomes where \\(B\\) occurs.</p> <p>Example: Consider drawing cards from a standard deck. Let \\(A\\) = \"card is an Ace\". Let \\(B\\) = \"card is red\"</p> <p>\\(P(A | B) = \\frac{P(\\text{card is red Ace})}{P(\\text{card is red})} = \\frac{2/52}{26/52} = \\frac{2}{26} = \\frac{1}{13}\\)</p> <p>This makes sense: among the 26 red cards, only 2 are Aces.</p> <p>Key Properties:</p> <ul> <li> <p>\\(0 \\leq P(A | B) \\leq 1\\)</p> </li> <li> <p>\\(P(B | B) = 1\\) (if \\(P(B) &gt; 0\\))</p> </li> <li> <p>If \\(A\\) and \\(B\\) are independent, then \\(P(A | B) = P(A)\\)</p> </li> </ul>"},{"location":"math/probability/conditional_probability/#joint-probability","title":"Joint Probability","text":"<p>Joint probability is the probability that two or more events occur simultaneously.</p>"},{"location":"math/probability/conditional_probability/#two-events","title":"Two events","text":"<p>For events \\(A\\) and \\(B\\), the joint probability is:</p> \\[P(A \\cap B) = P(A \\text{ and } B)\\] <p>Properties:</p> <ul> <li> <p>\\(0 \\leq P(A \\cap B) \\leq 1\\)</p> </li> <li> <p>\\(P(A \\cap B) = P(B \\cap A)\\) (commutative)</p> </li> <li> <p>\\(P(A \\cap B) = P(A | B) \\cdot P(B) = P(B | A) \\cdot P(A)\\) (multiplication rule)</p> </li> </ul>"},{"location":"math/probability/conditional_probability/#three-events","title":"Three events","text":"<p>For events \\(A\\), \\(B\\), and \\(C\\), the joint probability is:</p> \\[P(A \\cap B \\cap C) = P(A \\text{ and } B \\text{ and } C)\\] <p>Properties:</p> <ul> <li> <p>\\(0 \\leq P(A \\cap B \\cap C) \\leq 1\\)</p> </li> <li> <p>\\(P(A \\cap B \\cap C) = P(B \\cap A \\cap C) = P(C \\cap A \\cap B)\\) (commutative)</p> </li> <li> <p>\\(P(A \\cap B \\cap C) = P(A | B \\cap C) \\cdot P(B | C) \\cdot P(C)\\) (chain rule)</p> </li> </ul> <p>This can be shown as follows:</p> <p>\\(P(A | B \\cap C) = \\frac{P(A \\cap B \\cap C)}{P(B \\cap C)}\\).</p> <p>Thus, \\(P(A \\cap B \\cap C) = P(A | B \\cap C) \\cdot P(B \\cap C)\\).</p> <p>Applying multiplication rule to \\(P(B \\cap C)\\), we get: \\(P(B \\cap C) = P(B | C) \\cdot P(C)\\)</p> <p>Substituting into the equation for joint probability, \\(P(A \\cap B \\cap C) = P(A | B \\cap C) \\cdot P(B | C) \\cdot P(C)\\).</p> <p>This derivation shows how the chain rule naturally extends the multiplication rule to multiple events.</p>"},{"location":"math/probability/conditional_probability/#n-events","title":"n events","text":"<p>For events \\(A_1, A_2, \\ldots, A_n\\), the joint probability is:</p> \\[P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) = P(A_1 \\text{ and } A_2 \\text{ and } \\cdots \\text{ and } A_n)\\] <p>Properties:</p> <ul> <li> <p>\\(0 \\leq P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) \\leq 1\\)</p> </li> <li> <p>\\(P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) = P(A_1 | A_2 \\cap \\cdots \\cap A_n) \\cdot P(A_2 | A_3 \\cap \\cdots \\cap A_n) \\cdots P(A_{n-1} | A_n) \\cdot P(A_n)\\) (chain rule)</p> </li> <li> <p>If events are mutually independent, then \\(P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) = P(A_1) \\cdot P(A_2) \\cdots P(A_n)\\)</p> </li> </ul>"},{"location":"math/probability/conditional_probability/#law-of-total-probability","title":"Law of Total Probability","text":"<p>The law of total probability is a fundamental rule that allows us to calculate the probability of an event by considering all possible ways it can occur.</p> <p>We're breaking down the probability of \\(A\\) by considering how it can occur in each of the different scenarios \\(B_i\\). Since \\(\\{B_1, B_2, \\ldots, B_n\\}\\) is a partition of the sample space, we can write:</p> \\[A = A \\cap S = A \\cap \\left(\\bigcup_{i=1}^n B_i\\right) = \\bigcup_{i=1}^n (A \\cap B_i)\\] <p>Since the \\(B_i\\)'s are mutually exclusive, the events \\((A \\cap B_i)\\) are also mutually exclusive. Therefore:</p> \\[P(A) = P\\left(\\bigcup_{i=1}^n (A \\cap B_i)\\right) = \\sum_{i=1}^n P(A \\cap B_i)\\] <p>By the definition of conditional probability, \\(P(A \\cap B_i) = P(A | B_i) \\cdot P(B_i)\\) for each \\(i\\).</p> <p>Substituting into the previous equation:</p> \\[P(A) = \\sum_{i=1}^n P(A | B_i) \\cdot P(B_i)\\] <p>Mathematical Definition: For any event \\(A\\) and a partition \\(\\{B_1, B_2, \\ldots, B_n\\}\\) of the sample space (i.e., events that are mutually exclusive and exhaustive), we have:</p> \\[P(A) = \\sum_{i=1}^n P(A | B_i) \\cdot P(B_i)\\] <p>where \\(B_i \\cap B_j = \\emptyset\\) for all \\(i \\neq j\\) (mutually exclusive) and \\(\\bigcup_{i=1}^n B_i = S\\) (exhaustive - their union is the entire sample space)</p> <p>Example: Consider two urns. Urn 1 contains 3 red balls and 2 blue balls. Urn 2 contains 1 red ball and 4 blue balls. You randomly select an urn (each with probability \\(\\frac{1}{2}\\)) and then draw a ball from that urn. What is the probability of drawing a red ball?</p> <p>Let \\(A\\) = \"draw a red ball\". Let \\(B_1\\) = \"select Urn 1\". Let \\(B_2\\) = \"select Urn 2\". Then, \\(P(B_1) = \\frac{1}{2}\\), \\(P(B_2) = \\frac{1}{2}\\). \\(P(A | B_1) = \\frac{3}{5}\\) (3 red out of 5 balls in Urn 1). \\(P(A | B_2) = \\frac{1}{5}\\) (1 red out of 5 balls in Urn 2)</p> <p>By the law of total probability:</p> \\[P(A) = P(A | B_1) \\cdot P(B_1) + P(A | B_2) \\cdot P(B_2) = \\frac{3}{5} \\cdot \\frac{1}{2} + \\frac{1}{5} \\cdot \\frac{1}{2} = \\frac{3}{10} + \\frac{1}{10} = \\frac{4}{10} = \\frac{2}{5}\\]"},{"location":"math/probability/conditional_probability/#prior-and-posterior-probabilities","title":"Prior and Posterior Probabilities","text":"<p>In many probability problems, we distinguish between prior and posterior probabilities, which represent our beliefs before and after observing evidence.</p> <p>Prior Probability: The probability of an event before we observe any evidence. This represents our initial belief or knowledge about the event.</p> <p>Posterior Probability: The probability of an event after we observe evidence. This is our updated belief after incorporating new information.</p> <p>If we have Event \\(H\\) (hypothesis) and Event \\(E\\) (evidence), then:</p> <ul> <li> <p>Prior: \\(P(H)\\) is the prior probability of hypothesis \\(H\\)</p> </li> <li> <p>Posterior: \\(P(H | E)\\) is the posterior probability of hypothesis \\(H\\) given evidence \\(E\\)</p> </li> </ul>"},{"location":"math/probability/conditional_probability/#conditional-independence","title":"Conditional Independence","text":"<p>Conditional independence is a concept where two events are independent given knowledge of a third event, even if they might not be independent without that knowledge.</p> <p>Mathematical Definition: Events \\(A\\) and \\(B\\) are conditionally independent given event \\(C\\) if and only if:</p> \\[P(A \\cap B | C) = P(A | C) \\cdot P(B | C)\\] <p>provided that \\(P(C) &gt; 0\\).</p> <p>Equivalently, this can be written as:</p> \\[P(A | B \\cap C) = P(A | C)\\] <p>This means that knowing both \\(B\\) and \\(C\\) gives us no more information about \\(A\\) than knowing just \\(C\\) alone.</p> <p>Example: Consider drawing two cards from a standard deck without replacement.</p> <p>Let \\(A\\) = \"first card is red\". \\(B\\) = \"second card is red\". \\(C\\) = \"both cards are the same color\"</p> <p>Check regular independence:</p> <ul> <li> <p>\\(P(A) = \\frac{26}{52} = \\frac{1}{2}\\)</p> </li> <li> <p>\\(P(B) = \\frac{26}{52} = \\frac{1}{2}\\) (by symmetry)</p> </li> <li> <p>\\(P(A \\cap B) = \\frac{26 \\times 25}{52 \\times 51} = \\frac{25}{102}\\)</p> </li> </ul> <p>Since \\(P(A \\cap B) = \\frac{25}{102} \\neq \\frac{1}{4} = P(A) \\cdot P(B)\\), events \\(A\\) and \\(B\\) are not independent.</p> <p>But let's modify this: suppose we draw cards with replacement. Then:</p> <ul> <li> <p>\\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{1}{2}\\), \\(P(A \\cap B) = \\frac{1}{4}\\)</p> </li> <li> <p>So \\(A\\) and \\(B\\) are independent</p> </li> </ul> <p>Check conditional independence given \\(C\\): Given that both cards are the same color, we know they're either both red or both black.</p> <ul> <li> <p>\\(P(A | C) = P(\\text{first red} | \\text{both same color}) = \\frac{1}{2}\\)</p> </li> <li> <p>\\(P(B | C) = P(\\text{second red} | \\text{both same color}) = \\frac{1}{2}\\)</p> </li> <li> <p>\\(P(A \\cap B | C) = P(\\text{both red} | \\text{both same color}) = \\frac{1}{2}\\)</p> </li> </ul> <p>Since \\(P(A \\cap B | C) = \\frac{1}{2} \\neq \\frac{1}{4} = P(A | C) \\cdot P(B | C)\\), events \\(A\\) and \\(B\\) are not conditionally independent given \\(C\\).</p>"},{"location":"math/probability/continuous_distributions/","title":"Continuous Distributions","text":"<p>Continuous distributions are probability distributions for random variables that can take on any value in a continuous range (typically an interval of real numbers). Unlike discrete distributions, continuous random variables have probability density functions (PDFs) rather than probability mass functions (PMFs).</p> <p>For a continuous random variable \\(X\\), the probability of \\(X\\) taking any specific value is exactly 0:</p> \\[P(X = x) = 0 \\quad \\text{for any specific value } x\\] <p>Since \\(X\\) can take uncountably many values in a continuous range, the probability of any single value must be 0. Otherwise, the total probability would exceed 1.</p> <p>Consequence: We cannot use probability mass functions (PMFs) like we do for discrete random variables, because \\(P(X = x)\\) is always 0. Instead, we need probability density functions (PDFs) to describe the distribution.</p>"},{"location":"math/probability/continuous_distributions/#probability-density-function-pdf","title":"Probability Density Function (PDF)","text":"<p>Definition: A function \\(f_X(x)\\) such that \\(P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx\\) for all \\(a\\) and \\(b\\).</p> <p>When \\(a = b\\), the interval \\([a, b]\\) becomes a single point, and we have:</p> \\[P(a \\leq X \\leq a) = P(X = a) = \\int_a^a f_X(x) dx = 0\\] <p>This confirms our earlier statement that \\(P(X = x) = 0\\) for any specific value \\(x\\) in a continuous distribution. The integral over a single point (which has zero length) is always 0.</p> <p>Properties: </p> <ul> <li> <p>\\(f_X(x) \\geq 0\\) for all \\(x\\)</p> </li> <li> <p>\\(\\int_{-\\infty}^{\\infty} f_X(x) dx = 1\\)</p> </li> </ul> <p>What does \\(f_X(x)\\) actually mean?</p> <p>The PDF \\(f_X(x)\\) represents the probability density at point \\(x\\). To understand this, consider a small interval around \\(x\\):</p> \\[P(x - \\frac{\\epsilon}{2} \\leq X \\leq x + \\frac{\\epsilon}{2}) = \\int_{x - \\frac{\\epsilon}{2}}^{x + \\frac{\\epsilon}{2}} f_X(t) dt\\] <p>For very small \\(\\epsilon\\), this integral is approximately:</p> \\[P(x - \\frac{\\epsilon}{2} \\leq X \\leq x + \\frac{\\epsilon}{2}) \\approx f_X(x) \\cdot \\epsilon\\] \\[f_X(x) \\approx \\frac{P(x - \\frac{\\epsilon}{2} \\leq X \\leq x + \\frac{\\epsilon}{2})}{\\epsilon}\\] <p>Interpretation: \\(f_X(x)\\) tells us how much probability \"mass\" is concentrated around the point \\(x\\). The probability of falling in a small interval around \\(x\\) is approximately \\(f_X(x)\\) times the length of that interval. Think of probability density like physical density:</p> <ul> <li> <p>Probability mass = \\(P(x - \\frac{\\epsilon}{2} \\leq X \\leq x + \\frac{\\epsilon}{2})\\) (the \"amount\" of probability)</p> </li> <li> <p>Volume = \\(\\epsilon\\) (the \"size\" of the interval)</p> </li> <li> <p>Density = \\(f_X(x)\\) (how \"concentrated\" the probability is)</p> </li> </ul> <p>Just as \\(\\text{density} = \\frac{\\text{mass}}{\\text{volume}}\\) in physics, we have:</p> \\[\\text{probability density} = \\frac{\\text{probability mass}}{\\text{interval length}}\\] <p>This explains why \\(f_X(x)\\) can be greater than 1 - it's not a probability, but a density!</p> <p>Key insight: While \\(P(X = x) = 0\\), the density \\(f_X(x)\\) tells us how likely \\(X\\) is to fall near \\(x\\) relative to other points.</p>"},{"location":"math/probability/continuous_distributions/#cumulative-distribution-function-cdf","title":"Cumulative Distribution Function (CDF)","text":"<p>The Cumulative Distribution Function (CDF) of a continuous random variable \\(X\\) is defined as:</p> \\[F_X(x) = P(X \\leq x) = \\int_{-\\infty}^x f_X(t) dt\\] <p>What it represents: \\(F_X(x)\\) gives the probability that \\(X\\) takes a value less than or equal to \\(x\\). Here, \\(f_X(t)\\) is the probability density function (PDF) of \\(X\\).</p> <p>Properties</p> <ol> <li> <p>Non-decreasing: \\(F_X(x_1) \\leq F_X(x_2)\\) whenever \\(x_1 \\leq x_2\\)</p> </li> <li> <p>Limits: \\(\\lim_{x \\to -\\infty} F_X(x) = 0\\) and \\(\\lim_{x \\to \\infty} F_X(x) = 1\\)</p> </li> <li> <p>Right-continuous: \\(F_X(x) = \\lim_{h \\to 0^+} F_X(x + h)\\)</p> </li> <li> <p>Probability interpretation: \\(P(a &lt; X \\leq b) = F_X(b) - F_X(a)\\)</p> </li> </ol> <p>Since the CDF is the integral of the PDF, we can recover the PDF by differentiating the CDF:</p> \\[f_X(x) = \\frac{d}{dx} F_X(x) = F_X'(x)\\] <p>Example: If \\(F_X(x) = 1 - e^{-x}\\) for \\(x \\geq 0\\) (and \\(F_X(x) = 0\\) for \\(x &lt; 0\\)), then:</p> \\[f_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx}(1 - e^{-x}) = e^{-x}\\] <p>This gives us the PDF: \\(f_X(x) = e^{-x}\\) for \\(x \\geq 0\\) (and \\(f_X(x) = 0\\) for \\(x &lt; 0\\)).</p> <p>Key insight: The PDF tells us where the CDF is changing rapidly (high density) versus slowly (low density).</p> <p>Why CDFs are useful</p> <ol> <li> <p>Probability calculations: Easy to find \\(P(X \\leq x)\\) or \\(P(a &lt; X \\leq b)\\)</p> </li> <li> <p>Distribution comparison: Can compare distributions by plotting CDFs</p> </li> <li> <p>Quantiles: The \\(p\\)-th quantile \\(x_p\\) satisfies \\(F_X(x_p) = p\\)</p> </li> </ol>"},{"location":"math/probability/continuous_distributions/#expectation-of-a-continuous-random-variable","title":"Expectation of a Continuous Random Variable","text":"<p>The expectation (or expected value) of a continuous random variable \\(X\\) is defined as:</p> \\[E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) dx\\] <p>What it represents: \\(E[X]\\) is the \"center of mass\" or \"average value\" of the distribution, representing the long-run average if we were to sample from this distribution many times. Here, \\(f_X(x)\\) is the probability density function (PDF) of \\(X\\).</p> <p>Think of the PDF as a \"weight distribution\" along the real line:</p> <ul> <li> <p>\\(f_X(x)\\): How much \"weight\" (probability density) is at point \\(x\\)</p> </li> <li> <p>\\(x \\cdot f_X(x)\\): The \"weighted position\" at point \\(x\\)</p> </li> <li> <p>\\(\\int_{-\\infty}^{\\infty} x \\cdot f_X(x) dx\\): The total \"center of mass\" of all the weight</p> </li> </ul>"},{"location":"math/probability/continuous_distributions/#variance","title":"Variance","text":"<p>The variance of a random variable measures how spread out the distribution is around its mean. It's defined as the expected squared deviation from the mean.</p> <p>For any random variable \\(X\\) (discrete or continuous):</p> \\[\\text{Var}(X) = E[(X - E[X])^2]\\] <p>Why not other measures of deviation?</p> <p>Problem 1: \\(E[X - E[X]]\\)</p> <p>This would always equal 0 because:</p> \\[E[X - E[X]] = E[X] - E[E[X]] = E[X] - E[X] = 0\\] <p>The average deviation from the mean is always 0, so this tells us nothing about spread.</p> <p>Problem 2: \\(E[|X - E[X]|]\\) (Mean Absolute Deviation)</p> <p>While this measures spread, it has mathematical disadvantages:</p> <ul> <li> <p>Non-differentiable: The absolute value function isn't smooth, making calculus difficult</p> </li> <li> <p>Harder to work with: Properties like additivity are more complex</p> </li> </ul> <p>Why \\(E[(X - E[X])^2]\\) is perfect:</p> <ol> <li> <p>Always positive: \\((X - E[X])^2 \\geq 0\\) for all \\(X\\), so variance is always non-negative</p> </li> <li> <p>Mathematically tractable: Squaring gives smooth, differentiable functions</p> </li> <li> <p>Additivity: Variance of sum of independent variables equals sum of variances</p> </li> <li> <p>Theoretical elegance: Leads to beautiful results in probability theory</p> </li> <li> <p>Statistical properties: Optimal for many statistical procedures</p> </li> </ol> <p>Alternative formula (often easier to compute):</p> \\[\\text{Var}(X) = E[X^2] - (E[X])^2\\] <p>Discrete Case</p> <p>For a discrete random variable \\(X\\) with PMF \\(p_X(x)\\):</p> \\[\\text{Var}(X) = \\sum_x (x - E[X])^2 \\cdot p_X(x) = \\sum_x x^2 \\cdot p_X(x) - (E[X])^2\\] <p>Example: For a Bernoulli random variable \\(X \\sim \\text{Bernoulli}(p)\\):</p> <ul> <li> <p>\\(E[X] = p\\)</p> </li> <li> <p>\\(E[X^2] = 0^2 \\cdot (1-p) + 1^2 \\cdot p = p\\)</p> </li> <li> <p>\\(\\text{Var}(X) = E[X^2] - (E[X])^2 = p - p^2 = p(1-p)\\)</p> </li> </ul> <p>Continuous Case</p> <p>For a continuous random variable \\(X\\) with PDF \\(f_X(x)\\):</p> \\[\\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - E[X])^2 \\cdot f_X(x) dx = \\int_{-\\infty}^{\\infty} x^2 \\cdot f_X(x) dx - (E[X])^2\\] <p>Example: For an exponential random variable \\(X \\sim \\text{Exponential}(\\lambda)\\):</p> <ul> <li> <p>\\(E[X] = \\frac{1}{\\lambda}\\)</p> </li> <li> <p>\\(E[X^2] = \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} dx = \\frac{2}{\\lambda^2}\\) (using integration by parts)</p> </li> <li> <p>\\(\\text{Var}(X) = E[X^2] - (E[X])^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\\)</p> </li> </ul>"},{"location":"math/probability/continuous_distributions/#standard-deviation","title":"Standard Deviation","text":"<p>The standard deviation of a random variable \\(X\\) is the square root of its variance:</p> \\[\\sigma_X = \\sqrt{\\text{Var}(X)} = \\sqrt{E[(X - E[X])^2]}\\] <p>What it represents: Standard deviation measures spread in the same units as the original random variable, making it more interpretable than variance.</p> <p>Variance has units that are the square of the original units. For example, if \\(X\\) measures height in meters, \\(\\text{Var}(X)\\) is in square meters. If \\(X\\) measures time in seconds, \\(\\text{Var}(X)\\) is in square seconds. Standard deviation has the same units as \\(X\\).</p>"},{"location":"math/probability/continuous_distributions/#uniform-distribution","title":"Uniform Distribution","text":"<p>The uniform distribution is the simplest continuous distribution, where every value in an interval has equal probability density.</p> <p>A random variable \\(X\\) follows a uniform distribution on the interval \\([a, b]\\) (denoted \\(X \\sim \\text{Uniform}(a, b)\\)) if its PDF is:</p> \\[f_X(x) = \\begin{cases}  \\frac{1}{b-a} &amp; \\text{if } a \\leq x \\leq b \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>Parameters:</p> <ul> <li> <p>\\(a\\): Lower bound of the interval</p> </li> <li> <p>\\(b\\): Upper bound of the interval (\\(b &gt; a\\))</p> </li> <li> <p>Support: \\(X\\) takes values in \\([a, b]\\)</p> </li> </ul> <p>Every point in \\([a, b]\\) has the same probability density. The PDF is a horizontal line (rectangle). If you randomly pick a point from \\([a, b]\\), every point is equally likely</p> <p>Examples:</p> <ul> <li> <p>Random number generation between 0 and 1</p> </li> <li> <p>Random angle selection (0 to 2\u03c0)</p> </li> <li> <p>Random time selection within an hour</p> </li> <li> <p>Random position selection along a line segment</p> </li> </ul> <p>The cumulative distribution function is:</p> \\[F_X(x) = \\begin{cases} 0 &amp; \\text{if } x &lt; a \\\\ \\frac{x-a}{b-a} &amp; \\text{if } a \\leq x \\leq b \\\\ 1 &amp; \\text{if } x &gt; b \\end{cases}\\] <p>\\(F_X(x)\\) increases linearly from 0 to 1 as \\(x\\) goes from \\(a\\) to \\(b\\).</p> <p>Expectation:</p> \\[E[X] = \\int_a^b x \\cdot \\frac{1}{b-a} dx = \\frac{1}{b-a} \\int_a^b x dx = \\frac{1}{b-a} \\cdot \\frac{b^2 - a^2}{2} = \\frac{a + b}{2}\\] <p>Variance:</p> \\[\\text{Var}(X) = E[X^2] - (E[X])^2\\] <p>First, calculate \\(E[X^2]\\):</p> \\[E[X^2] = \\int_a^b x^2 \\cdot \\frac{1}{b-a} dx = \\frac{1}{b-a} \\cdot \\frac{b^3 - a^3}{3} = \\frac{b^3 - a^3}{3(b-a)}\\] <p>Then:</p> \\[\\text{Var}(X) = \\frac{b^3 - a^3}{3(b-a)} - \\left(\\frac{a + b}{2}\\right)^2 = \\frac{(b-a)^2}{12}\\] <p>Standard deviation:</p> \\[\\sigma_X = \\frac{b-a}{2\\sqrt{3}}\\]"},{"location":"math/probability/continuous_distributions/#universality-of-the-uniform-distribution","title":"Universality of the Uniform Distribution","text":"<p>Given a \\(\\text{Uniform}(0, 1)\\) random variable, we can construct a random variable with any continuous distribution we want.</p> <p>We call this the universality of the Uniform, because it tells us the Uniform is a universal starting point for building random variables with other distributions.</p> <p>Theorem (Universality of the Uniform). Let \\(F\\) be a CDF which is a continuous function and strictly increasing on the support of the distribution. This ensures that the inverse function \\(F^{-1}\\) exists, as a function from \\((0, 1)\\) to \\(\\mathbb{R}\\). We then have the following results:</p> <ol> <li> <p>Let \\(U \\sim \\text{Unif}(0, 1)\\) and \\(X = F^{-1}(U)\\). Then \\(X\\) is a random variable with CDF \\(F\\).</p> </li> <li> <p>Let \\(X\\) be a random variable with CDF \\(F\\). Then \\(F(X) \\sim \\text{Unif}(0, 1)\\).</p> </li> </ol> <p>Part 1 is the inverse CDF method we discussed earlier - it shows how to generate any distribution from uniform.</p> <p>Part 2 is the probability integral transform - it shows that applying any CDF to its own random variable gives a uniform distribution.</p> <p>Let's make sure we understand what each part of the theorem is saying. The first part says that if we start with \\(U \\sim \\text{Unif}(0, 1)\\) and a CDF \\(F\\), then we can create a random variable whose CDF is \\(F\\) by plugging \\(U\\) into the inverse CDF \\(F^{-1}\\). Since \\(F^{-1}\\) is a function (known as the quantile function), \\(U\\) is a random variable, and a function of a random variable is a random variable, \\(F^{-1}(U)\\) is a random variable; universality of the Uniform says its CDF is \\(F\\).</p> <p>The second part of the theorem goes in the reverse direction, starting from a random variable \\(X\\) whose CDF is \\(F\\) and then creating a \\(\\text{Unif}(0, 1)\\) random variable. Again, \\(F\\) is a function, \\(X\\) is a random variable, and a function of a random variable is a random variable, so \\(F(X)\\) is a random variable. Since any CDF is between 0 and 1 everywhere, \\(F(X)\\) must take values between 0 and 1. Universality of the Uniform says that the distribution of \\(F(X)\\) is Uniform on \\((0, 1)\\).</p> <p>The second part of universality of the Uniform involves plugging a random variable \\(X\\) into its own CDF \\(F\\). This may seem strangely self-referential, but it makes sense because \\(F\\) is just a function (that satisfies the properties of a valid CDF), and a function of a random variable is a random variable. There is a potential notational confusion, however: \\(F(x) = P(X \\leq x)\\) by definition, but it would be incorrect to say \"\\(F(X) = P(X \\leq X) = 1\\)\". Rather, we should first find an expression for the CDF as a function of \\(x\\), then replace \\(x\\) with \\(X\\) to obtain a random variable. For example, if the CDF of \\(X\\) is \\(F(x) = 1 - e^{-x}\\) for \\(x &gt; 0\\), then \\(F(X) = 1 - e^{-X}\\).</p> <p>Proof.</p> <p>Let \\(U \\sim \\text{Unif}(0, 1)\\) and \\(X = F^{-1}(U)\\). For all real \\(x\\),</p> \\[P(X \\leq x) = P(F^{-1}(U) \\leq x) = P(U \\leq F(x)) = F(x);\\] <p>Why does \\(P(F^{-1}(U) \\leq x) = P(U \\leq F(x))\\) hold?</p> <p>This is a key step that uses the properties of inverse functions. Since \\(F\\) is strictly increasing, we have:</p> \\[F^{-1}(U) \\leq x \\quad \\text{if and only if} \\quad U \\leq F(x)\\] <p>If the inverse function \\(F^{-1}\\) applied to \\(U\\) gives a value \\(\\leq x\\), then \\(U\\) must be \\(\\leq F(x)\\). This is because \\(F^{-1}\\) \"undoes\" what \\(F\\) does, so the inequality reverses when we apply \\(F\\) to both sides.</p> <p>This shows the two events are equivalent, so their probabilities are equal.</p> <p>For the last equality, we used the fact that \\(P(U \\leq u) = u\\) for \\(u \\in (0, 1)\\).</p> <p>This is the fundamental property of the uniform distribution on \\((0, 1)\\). Since \\(U \\sim \\text{Unif}(0, 1)\\), the probability that \\(U\\) falls in any interval \\([0, u]\\) is exactly the length of that interval, which is \\(u\\).</p> <p>In our proof: We had \\(P(U \\leq F(x))\\), and since \\(F(x)\\) is a value between 0 and 1 (because \\(F\\) is a CDF), this equals exactly \\(F(x)\\) by the uniform distribution property.</p> <p>Let \\(X\\) have CDF \\(F\\), and find the CDF of \\(Y = F(X)\\). Since \\(Y\\) takes values in \\((0, 1)\\), \\(P(Y \\leq y)\\) equals 0 for \\(y \\leq 0\\) and equals 1 for \\(y \\geq 1\\). For \\(y \\in (0, 1)\\),</p> \\[P(Y \\leq y) = P(F(X) \\leq y) = P(X \\leq F^{-1}(y)) = F(F^{-1}(y)) = y.\\] <p>Thus \\(Y\\) has the \\(\\text{Unif}(0, 1)\\) CDF.</p> <p>Example: A large number of students take a certain exam, graded on a scale from 0 to 100. Let \\(X\\) be the score of a random student. Continuous distributions are easier to deal with here, so let's approximate the discrete distribution of scores using a continuous distribution. Suppose that \\(X\\) is continuous, with a CDF \\(F\\) that is strictly increasing. In reality, there are only finitely many students and only finitely many possible scores, but a continuous distribution may be a good approximation.</p> <p>Suppose that the median score on the exam is 60, i.e., half of the students score above 60 and the other half score below 60 (a convenient aspect of assuming a continuous distribution is that we don't need to worry about how many students had scores equal to 60). That is, \\(F(60) = 1/2\\); or, equivalently, \\(F^{-1}(1/2) = 60\\).</p> <p>If Fred scores a 72 on the exam, then his percentile is the fraction of students who score below a 72. This is \\(F(72)\\), which is some number in \\((1/2, 1)\\) since 72 is above the median. In general, a student with score \\(x\\) has percentile \\(F(x)\\). Going the other way, if we start with a percentile, say 0.95, then \\(F^{-1}(0.95)\\) is the score that has that percentile. A percentile is also called a quantile, which is why \\(F^{-1}\\) is called the quantile function. The function \\(F\\) converts scores to quantiles, and the function \\(F^{-1}\\) converts quantiles to scores.</p> <p>The strange operation of plugging \\(X\\) into its own CDF now has a natural interpretation: \\(F(X)\\) is the percentile attained by a random student. It often happens that the distribution of scores on an exam looks very non-Uniform. For example, there is no reason to think that 10% of the scores are between 70 and 80, even though \\((70, 80)\\) covers 10% of the range of possible scores.</p> <p>On the other hand, the distribution of percentiles of the students is Uniform: the universality property says that \\(F(X) \\sim \\text{Unif}(0, 1)\\). For example, 50% of the students have a percentile of at least 0.5. Universality of the Uniform is expressing the fact that 10% of the students have a percentile between 0 and 0.1, 10% of the students have a percentile between 0.1 and 0.2, 10% of the students have a percentile between 0.2 and 0.3, and so on\u2014a fact that is clear from the definition of percentile.</p> <p>Example: The Logistic CDF is</p> \\[F(x) = \\frac{e^x}{1 + e^x}, \\quad x \\in \\mathbb{R}\\] <p>Suppose we have \\(U \\sim \\text{Unif}(0, 1)\\) and wish to generate a Logistic random variable. Part 1 of the universality property says that \\(F^{-1}(U) \\sim \\text{Logistic}\\), so we first invert the CDF to get \\(F^{-1}\\):</p> \\[F^{-1}(u) = \\log\\left(\\frac{u}{1 - u}\\right)\\] <p>Then we plug in \\(U\\) for \\(u\\):</p> \\[F^{-1}(U) = \\log\\left(\\frac{U}{1 - U}\\right)\\] <p>Therefore \\(\\log\\left(\\frac{U}{1-U}\\right) \\sim \\text{Logistic}\\).</p> <p>We can verify directly that \\(\\log\\left(\\frac{U}{1-U}\\right)\\) has the required CDF: start from the definition of CDF, do some algebra to isolate \\(U\\) on one side of the inequality, and then use the CDF of the Uniform distribution. Let's work through these calculations once for practice:</p> \\[P\\left(\\log\\left(\\frac{U}{1 - U}\\right) \\leq x\\right) = P\\left(\\frac{U}{1 - U} \\leq e^x\\right) = P(U \\leq e^x(1 - U))\\] \\[= P\\left(U \\leq \\frac{e^x}{1 + e^x}\\right) = \\frac{e^x}{1 + e^x}\\] <p>which is indeed the Logistic CDF.</p> <p>We can also use simulation to visualize how universality of the Uniform works. To this end, we generate 1 million \\(\\text{Unif}(0, 1)\\) random samples. We then transform each of these values \\(u\\) into \\(\\log\\left(\\frac{u}{1-u}\\right)\\); if the universality of the Uniform is correct, the transformed numbers should follow a Logistic distribution.</p>"},{"location":"math/probability/convolution/","title":"Convolution","text":""},{"location":"math/probability/convolution/#introduction","title":"Introduction","text":"<p>Consider two different lists of numbers, or perhaps two different functions, and think about all the ways one might combine those two lists to get a new list of numbers, or combine the two functions to get a new function. One simple approach is to add them together term by term. Likewise with functions, one can add all the corresponding outputs. </p> <p></p> <p>Similarly, one could multiply the two lists term by term and do the same thing with the functions.</p> <p></p> <p>But there's another kind of combination just as fundamental as both of those, yet much less commonly discussed, known as a convolution. Unlike the previous two cases, it's not something that's merely inherited from an operation one can do to numbers. It's something genuinely new for the context of lists of numbers or combining functions.</p> <p></p> <p>Convolutions appear everywhere\u2014 they are ubiquitous in image processing, they're a core construct in the theory of probability, they're used extensively in solving differential equations, and one context where they've almost certainly been encountered, if not by this name, is multiplying two polynomials together.</p>"},{"location":"math/probability/convolution/#motivation-rolling-dice","title":"Motivation: Rolling Dice","text":"<p>Let's begin with probability, and in particular one of the simplest examples that most people have thought about at some point in their life: rolling a pair of dice and figuring out the chances of seeing various different sums.</p> <p>Each of the two dice has six different possible outcomes, which gives a total of 36 distinct possible pairs of outcomes. </p> <p></p> <p>By examining all of them, one can count how many pairs have a given sum. Arranging all the pairs in a grid reveals that all pairs with a constant sum lie along different diagonals. Simply counting how many exist on each of those diagonals tells us how likely one is to see a particular sum.</p> <p></p> <p>But can one think of other ways to visualize the same question? Other images that can help think about all the distinct pairs that have a given sum?</p> <p>One approach is to picture these two different sets of possibilities each in a row, but flip around the second row. That way all the different pairs which add up to seven line up vertically. If we slide that bottom row all the way to the right, then the unique pair that adds up to two (snake eyes) are the only ones that align. If we shift it over one unit to the right, the pairs which align are the two different pairs that add up to three.</p> <p></p> <p>In general, different offset values of this lower array (which was flipped around first) reveal all the distinct pairs that have a given sum.</p> <p> </p> <p>As far as probability questions go, this still isn't especially interesting because all we're doing is counting how many outcomes there are in each of these categories. But that is with the implicit assumption that there's an equal chance for each of these faces to come up.</p> <p></p> <p>But what if we have a special set of dice that's not uniform? Maybe the blue die has its own set of numbers describing the probabilities for each face coming up, and the red die has its own unique distinct set of numbers. </p> <p></p> <p>In that case, if one wanted to figure out, say, the probability of seeing a 2, one would multiply the probability that the blue die is a 1 times the probability that the red die is a 1. For the chances of seeing a 3, one looks at the two distinct pairs where that's possible, and again multiplies the corresponding probabilities and then adds those two products together. Similarly, the chances of seeing a 4 involves multiplying together three different pairs of possibilities and adding them all together.</p> <p></p> <p>In the spirit of setting up some formulas, let's name these top probabilities \\(a_1, a_2, a_3\\), and so on, and name the bottom ones \\(b_1, b_2, b_3\\), and so on.</p> <p> </p> <p>In general, this process where we're taking two different arrays of numbers, flipping the second one around, and then lining them up at various different offset values, taking a bunch of pairwise products and adding them up\u2014 that's one of the fundamental ways to think about what a convolution is. To spell it out more exactly, through this process, we just generated probabilities for seeing 2, 3, 4, on and on up to 12, and we got them by mixing together one list of values, \\(a\\), and another list of values, \\(b\\). In the lingo, we'd say the convolution of those two sequences gives us this new sequence\u2014 the new sequence of 11 values, each of which looks like some sum of pairwise products.</p> <p></p> <p>If one prefers, another way to think about the same operation is to first create a table of all the pairwise products, and then add up along all these diagonals. Again, that's a way of mixing together these two sequences of numbers to get a new sequence of 11 numbers. It's the same operation as the sliding windows thought, just another perspective.</p> <p></p> <p>Putting a little notation to it, here's how one might see it written down. The convolution of \\(a\\) and \\(b\\), denoted with this little asterisk (\\(*\\)), is a new list, and the \\(n\\)th element of that list looks like a sum, and that sum goes over all different pairs of indices, \\(i\\) and \\(j\\), so that the sum of those indices is equal to \\(n\\).</p> <p></p> <p>Convolution is a mathematical operation on two functions. In our dice example, we had:</p> <ul> <li> <p>Blue die: Random variable \\(X\\) with PMF \\(P(X = i) = a_i\\) for \\(i = 1, 2, \\ldots, 6\\)</p> </li> <li> <p>Red die: Random variable \\(Y\\) with PMF \\(P(Y = j) = b_j\\) for \\(j = 1, 2, \\ldots, 6\\)</p> </li> </ul> <p>Here, we performed a convolution of their PMFs. </p> <p>The resulting distribution's PMF (\\(f_Z\\)) is the convolution of the input distributions' PMFs (\\(f_X\\) and \\(f_Y\\)).</p> <p>For a fixed value \\(x\\) of \\(X\\), \\(Y\\) must take the value \\(z-x\\). Since \\(X\\) and \\(Y\\) are independent, the joint probability mass at \\((x,z-x)\\) is the product of their individual masses: \\(f_X(x) \\cdot f_Y(z-x)\\).</p> <p>Take the sum of these products over all possible values of \\(x\\) to find the total probability mass when \\(Z\\) is equal to \\(z\\).</p> <ul> <li>Convolution: \\(Z = X + Y\\) with PMF \\(P(Z = z)\\) for \\(z = 2, 3, \\ldots, 12\\)</li> </ul> <p></p>"},{"location":"math/probability/convolution/#definition","title":"Definition","text":"<p>A convolution is a sum of independent random variables. We often add independent random variables because the sum is a useful summary of an experiment (in \\(n\\) Bernoulli trials, we may only care about the total number of successes), and because sums lead to averages, which are also useful (in \\(n\\) Bernoulli trials, the proportion of successes).</p> <p>The main task is to determine the distribution of \\(Z = X + Y\\), where \\(X\\) and \\(Y\\) are independent random variables whose distributions are known.</p> <p>The distribution of \\(Z\\) is found using a convolution sum or integral. As we'll see, a convolution sum is nothing more than the law of total probability, conditioning on the value of either \\(X\\) or \\(Y\\); a convolution integral is analogous.</p> <p>Let \\(X\\) and \\(Y\\) be independent random variables and \\(Z = X + Y\\) be their sum.</p> <p>Discrete Case: If \\(X\\) and \\(Y\\) are discrete, then the PMF of \\(Z\\) is:</p> \\[P(Z = z) = \\sum_x P(Y = z - x)P(X = x) = \\sum_y P(X = z - y)P(Y = y)\\] <p>Continuous Case: If \\(X\\) and \\(Y\\) are continuous, then the PDF of \\(Z\\) is:</p> \\[f_Z(z) = \\int_{-\\infty}^{\\infty} f_Y(z - x)f_X(x)dx = \\int_{-\\infty}^{\\infty} f_X(z - y)f_Y(y)dy\\] <p>Proof: For the discrete case, we use the Law of Total Probability (LOTP), conditioning on \\(X\\):</p> \\[P(Z = z) = \\sum_x P(X + Y = z|X = x)P(X = x) = \\sum_x P(Y = z - x|X = x)P(X = x) = \\sum_x P(Y = z - x)P(X = x)\\] <p>The last equality follows from the independence of \\(X\\) and \\(Y\\). Conditioning on \\(Y\\) instead, we obtain the second formula for the PMF of \\(Z\\).</p>"},{"location":"math/probability/convolution/#moving-averages","title":"Moving Averages","text":"<p>Convolution has been demonstrated in one case where it serves as a natural and desirable operation\u2014adding up two probability distributions. Moving away from probabilities, another common example is the moving average. Consider a long list of numbers and a smaller list of numbers that all add up to 1. </p> <p></p> <p>In this case, there is a small list of 5 values that are all equal to 1/5. When applying the sliding window convolution process, and ignoring what happens at the very beginning, once the smaller list of values entirely overlaps with the larger one, each term in this convolution has a clear meaning.</p> <p> </p> <p>At each iteration, the process multiplies each of the values from the data by 1/5 and adds them all together, which means taking an average of the data inside this small window. </p> <p> </p> <p>Overall, the process produces a smoothed version of the original data. </p> <p></p> <p>This can be modified by starting with a different small list of numbers, and as long as that small list adds up to 1, it can still be interpreted as a moving average.</p> <p></p> <p>In the example shown here, that moving average would give more weight towards the central value. This also results in a smoothed version of the data.</p> <p></p>"},{"location":"math/probability/convolution/#image-processing","title":"Image Processing","text":"<p>A two-dimensional analog of convolution provides an algorithm for blurring images. </p> <p></p> <p>A small 3\u00d73 grid of values marches along the original image. When zooming in, each value is 1/9, and at each iteration, each value is multiplied by the corresponding pixel it sits on top of. </p> <p></p> <p>In computer science, colors are represented as vectors of three values representing the red, green, and blue components. When multiplying all these values by 1/9 and adding them together, the result is an average along each color channel, and the corresponding pixel for the image on the right is defined to be that sum. </p> <p></p> <p>The overall effect, as this process is applied to every single pixel on the image, is that each pixel bleeds into all of its neighbors, creating a blurrier version than the original.</p> <p></p> <p>In technical terms, the image on the right is a convolution of the original image with a small grid of values. More precisely, it is the convolution with a 180-degree rotated version of that small grid of values. While this distinction does not matter when the grid is symmetric, it is worth noting that the definition of convolution, as inherited from the pure mathematics context, always involves flipping around the second array.</p> <p></p> <p>Blurring is far from the only application of this concept. Consider a small grid of values that involves some positive numbers on the left and some negative numbers on the right, colored blue and red respectively. Take a moment to predict what effect this will have on the final image.</p> <p> </p> <p>In this case, the image is considered grayscale instead of colored, so each pixel is represented by one number instead of three.</p> <p></p> <p>One thing worth noticing is that as this convolution is performed, negative values can result. For example, at a certain point, if zooming in, the left half of the small grid sits entirely on top of black pixels, which have a value of zero, but the right half of negative values all sit on top of white pixels, which have a value of one. </p> <p> </p> <p>When multiplying corresponding terms and adding them together, the results will be very negative. The way this is displayed in the image on the right is to color negative values red and positive values blue.</p> <p></p> <p>Another thing to notice is that when on a patch that is all the same color, everything goes to zero, since the sum of the values in the small grid is zero. This is very different from the previous example where the sum of the small grid was one, which allowed interpretation as a moving average and hence a blur.</p> <p></p> <p>Overall, this process basically detects wherever there is variation in the pixel value as one moves from left to right, providing a way to pick up on all the vertical edges in the image.</p> <p></p> <p>This smaller grid is often called a kernel, and the beauty lies in how just by choosing a different kernel, different image processing effects can be achieved\u2014 not just blurring or edge detection, but also things like sharpening. </p> <p></p> <p>For those familiar with convolutional neural networks, the idea is to use data to determine what the kernels should be in the first place, as determined by whatever the neural network wants to detect.</p> <p></p> <p>Convolutions as a pure mathematical operation always produce an array that is bigger than the two arrays that are started with, at least assuming one of them does not have a length of one. In certain computer science contexts, it is often desirable to deliberately truncate that output.</p> <p>Another thing worth highlighting is that in the computer science context, the notion of flipping around the kernel before letting it march across the original often feels unusual and unnecessary, but again, note that this is inherited from the pure mathematics context, where, as seen with the probabilities, it is an incredibly natural thing to do.</p>"},{"location":"math/probability/convolution/#polynomial-multiplication","title":"Polynomial Multiplication","text":"<p>Recall that with the probability example, another way to think about convolution was to create a table of all the pairwise products and then add up those pairwise products along the diagonals. There is nothing specific to probability about this approach. Any time two different lists of numbers are convolved, this method can be used. Create a multiplication table with all pairwise products, and then each sum along the diagonal corresponds to one of the final outputs.</p> <p></p> <p>One context where this view is especially natural is when multiplying together two polynomials. For example, take the small grid already established and replace the top terms with 1, 2x, and 3x\u00b2, and replace the other terms with 4, 5x, and 6x\u00b2. Now, consider what it means when creating all these different pairwise products between the two lists.</p> <p></p> <p>What is being done is essentially expanding out the full product of the two polynomials written down, and then when adding up along the diagonal, that corresponds to collecting all like terms. </p> <p></p> <p>This is quite elegant. Expanding a polynomial and collecting like terms is exactly the same process as convolution.</p> <p>This allows for something quite interesting, because consider what is being stated here. The statement is that if two different functions are taken and multiplied together, which is a simple pointwise operation, that is the same thing as if the coefficients were first extracted from each one of those, assuming they are polynomials, and then a convolution of those two lists of coefficients was taken.</p> <p></p>"},{"location":"math/probability/covariance_and_correlation/","title":"Covariance and Correlation","text":"<p>Covariance and correlation are fundamental measures that describe the relationship between two random variables. They help us understand how variables change together and the strength and direction of their linear relationship.</p>"},{"location":"math/probability/covariance_and_correlation/#covariance","title":"Covariance","text":"<p>The covariance between two random variables \\(X\\) and \\(Y\\) is defined as:</p> \\[\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\] <p>The covariance can also be expressed as:</p> \\[\\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\\] <p>This form is often more convenient for calculations.</p>"},{"location":"math/probability/covariance_and_correlation/#interpretation-and-properties","title":"Interpretation and Properties","text":"<ul> <li> <p>Positive Covariance: When \\(X\\) tends to be above its mean, \\(Y\\) also tends to be above its mean OR when \\(X\\) tends to be below its mean, \\(Y\\) also tends to be below its mean</p> </li> <li> <p>Negative Covariance: When \\(X\\) tends to be above its mean, \\(Y\\) tends to be below its mean OR vice versa</p> </li> <li> <p>Variance as a special case: \\(\\text{Cov}(X, X) = \\text{Var}(X)\\)</p> </li> <li> <p>Symmetry: \\(\\text{Cov}(X, Y) = \\text{Cov}(Y, X)\\)</p> </li> <li> <p>Independence: If \\(X\\) and \\(Y\\) are independent, this means \\(\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]\\). This results in \\(\\text{Cov}(X, Y) = 0\\), as expected</p> </li> <li> <p>\\(\\text{Cov}(X, c) = 0\\) where \\(c\\) is a constant</p> </li> <li> <p>\\(\\text{Cov}(cX, Y) = c \\cdot \\text{Cov}(X, Y)\\) where \\(c\\) is a constant</p> </li> <li> <p>\\(\\text{Cov}(X, Y + Z) = \\text{Cov}(X, Y) + \\text{Cov}(X, Z)\\)</p> </li> <li> <p>\\(\\text{Cov}(X + Y, Z + W) = \\text{Cov}(X, Z) + \\text{Cov}(X, W) + \\text{Cov}(Y, Z) + \\text{Cov}(Y, W)\\)</p> </li> <li> <p>For constants \\(a_1, a_2, \\ldots, a_n\\) and \\(b_1, b_2, \\ldots, b_m\\):</p> </li> </ul> \\[\\text{Cov}\\left(\\sum_{i=1}^n a_i X_i, \\sum_{j=1}^m b_j Y_j\\right) = \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j \\text{Cov}(X_i, Y_j)\\] <ul> <li>\\(\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2\\text{Cov}(X_1, X_2)\\):</li> </ul> <p>Using the fact that \\(\\text{Var}(X) = \\text{Cov}(X, X)\\):</p> \\[\\begin{align} \\text{Var}(X_1 + X_2) &amp;= \\text{Cov}(X_1 + X_2, X_1 + X_2) \\\\ &amp;= \\text{Cov}(X_1, X_1) + \\text{Cov}(X_1, X_2) + \\text{Cov}(X_2, X_1) + \\text{Cov}(X_2, X_2) \\\\ &amp;= \\text{Var}(X_1) + \\text{Cov}(X_1, X_2) + \\text{Cov}(X_2, X_1) + \\text{Var}(X_2) \\\\ &amp;= \\text{Var}(X_1) + \\text{Var}(X_2) + 2\\text{Cov}(X_1, X_2) \\end{align}\\] <p>\\(\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2)\\) is true when \\(\\text{Cov}(X_1, X_2) = 0\\) (which means when \\(X_1\\) and \\(X_2\\) are uncorrelated). In particular, if \\(X_1\\) and \\(X_2\\) are independent, then \\(\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2)\\). Note that \\(X_1\\) and \\(X_2\\) can be dependent and \\(\\text{Cov}(X_1, X_2) = 0\\). But when \\(X_1\\) and \\(X_2\\) are independent, it is always true that \\(\\text{Cov}(X_1, X_2) = 0\\).</p> <p>General Case - Variance of sum of n Random Variables:</p> <p>For \\(n\\) random variables \\(X_1, X_2, \\ldots, X_n\\):</p> \\[\\text{Var}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\text{Var}(X_i) + 2\\sum_{1 \\leq i &lt; j \\leq n} \\text{Cov}(X_i, X_j)\\] <p>Note: Zero Covariance does not imply independence</p> <p>Let \\(Z \\sim N(0, 1)\\) be a standard normal random variable, and define:</p> <ul> <li> <p>\\(X = Z\\)</p> </li> <li> <p>\\(Y = Z^2\\)</p> </li> </ul> <p>Covariance calculation:</p> \\[\\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = \\mathbb{E}[Z \\cdot Z^2] - \\mathbb{E}[Z]\\mathbb{E}[Z^2]\\] <p>Since \\(Z \\sim N(0, 1)\\):</p> <ul> <li> <p>\\(\\mathbb{E}[Z] = 0\\)</p> </li> <li> <p>\\(\\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2 = 1 + 0 = 1\\)</p> </li> <li> <p>\\(\\mathbb{E}[Z^3] = 0\\) (odd moments of standard normal are zero)</p> </li> </ul> <p>Therefore: \\(\\text{Cov}(X, Y) = 0 - 0 \\cdot 1 = 0\\)</p> <p>Dependence: \\(X\\) and \\(Y\\) are clearly dependent because knowing \\(X = Z\\) completely determines \\(Y = Z^2\\). For example, if \\(X = 2\\), then \\(Y\\) must be 4.</p>"},{"location":"math/probability/covariance_and_correlation/#correlation","title":"Correlation","text":"<p>The correlation coefficient (or Pearson correlation) between two random variables \\(X\\) and \\(Y\\) is defined as:</p> \\[\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X) \\text{Var}(Y)}} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\] <p>where \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X\\) and \\(Y\\) respectively.</p>"},{"location":"math/probability/covariance_and_correlation/#interpretation-and-properties_1","title":"Interpretation and Properties","text":"<ul> <li> <p>Range: \\(-1 \\leq \\rho_{X,Y} \\leq 1\\)</p> </li> <li> <p>Scale Invariance: \\(\\rho_{aX + b, cY + d} = \\rho_{X, Y}\\) for \\(a, c &gt; 0\\)</p> </li> <li> <p>\\(\\rho_{X,Y} = 1\\) when \\(Y = aX + b\\) with \\(a &gt; 0\\)</p> </li> <li> <p>\\(\\rho_{X,Y} = -1\\) when \\(Y = aX + b\\) with \\(a &lt; 0\\)</p> </li> <li> <p>\\(\\rho = 1\\): Perfect positive linear relationship</p> </li> <li> <p>\\(\\rho = -1\\): Perfect negative linear relationship  </p> </li> <li> <p>\\(\\rho = 0\\): No linear relationship</p> </li> <li> <p>\\(|\\rho| &gt; 0.7\\): Strong linear relationship</p> </li> <li> <p>\\(0.3 &lt; |\\rho| &lt; 0.7\\): Moderate linear relationship</p> </li> <li> <p>\\(|\\rho| &lt; 0.3\\): Weak linear relationship</p> </li> <li> <p>The correlation coefficient is essentially a normalized version of the covariance:</p> </li> </ul> \\[\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\]"},{"location":"math/probability/covariance_and_correlation/#connection-to-linear-algebra","title":"Connection to Linear Algebra","text":"<p>The correlation coefficient has a beautiful geometric interpretation in terms of the angle between vectors in \\(\\mathbb{R}^n\\):</p> <p>For centered data: If we have \\(n\\) observations \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\), and we center the data by subtracting means:</p> <ul> <li> <p>\\(\\mathbf{x} = (x_1 - \\bar{x}, x_2 - \\bar{x}, \\ldots, x_n - \\bar{x})\\)</p> </li> <li> <p>\\(\\mathbf{y} = (y_1 - \\bar{y}, y_2 - \\bar{y}, \\ldots, y_n - \\bar{y})\\)</p> </li> </ul> <p>Then the correlation coefficient equals the cosine of the angle between these centered vectors:</p> \\[\\rho_{X,Y} = \\cos \\theta = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}\\] <p>Key insights:</p> <ul> <li> <p>\\(\\rho = 1\\) corresponds to \\(\\theta = 0\u00b0\\) (vectors point in same direction)</p> </li> <li> <p>\\(\\rho = -1\\) corresponds to \\(\\theta = 180\u00b0\\) (vectors point in opposite directions)  </p> </li> <li> <p>\\(\\rho = 0\\) corresponds to \\(\\theta = 90\u00b0\\) (vectors are orthogonal)</p> </li> <li> <p>The correlation measures how \"aligned\" the centered data vectors are in \\(\\mathbb{R}^n\\)</p> </li> </ul>"},{"location":"math/probability/expectation/","title":"Expectation","text":"<p>Computing Averages: two approaches</p> <p>Let's explore how to compute averages using two different methods, which will help build intuition for expectation.</p> <p>Method 1: Arithmetic Mean (summation divided by n)</p> <p>Formula: \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\)</p> <p>Example: Consider the numbers \\(2, 5, 8, 8, 8, 11, 14\\)</p> <p>Calculation:</p> \\[\\bar{x} = \\frac{1}{7} \\sum_{i=1}^7 x_i = \\frac{1}{7}(2 + 5 + 8 + 8 + 8 + 11 + 14) = \\frac{56}{7} = 8\\] <p>Method 2: Weighted Sum</p> <p>Formula: \\(\\bar{x} = \\sum_{i=1}^k w_i x_i\\) where \\(\\sum_{i=1}^k w_i = 1\\) and \\(k\\) is the number of unique values</p> <p>Same example: Unique values \\(2, 5, 8, 11, 14\\) with weights based on frequency:</p> <ul> <li> <p>\\(w_1 = \\frac{1}{7}\\) (for value 2, appears 1 time)</p> </li> <li> <p>\\(w_2 = \\frac{1}{7}\\) (for value 5, appears 1 time)  </p> </li> <li> <p>\\(w_3 = \\frac{3}{7}\\) (for value 8, appears 3 times)</p> </li> <li> <p>\\(w_4 = \\frac{1}{7}\\) (for value 11, appears 1 time)</p> </li> <li> <p>\\(w_5 = \\frac{1}{7}\\) (for value 14, appears 1 time)</p> </li> </ul> <p>Verification: \\(\\frac{1}{7} + \\frac{1}{7} + \\frac{3}{7} + \\frac{1}{7} + \\frac{1}{7} = \\frac{7}{7} = 1\\) \u2713</p> <p>Calculation:</p> \\[\\bar{x} = \\sum_{i=1}^5 w_i x_i = \\frac{1}{7}(2) + \\frac{1}{7}(5) + \\frac{3}{7}(8) + \\frac{1}{7}(11) + \\frac{1}{7}(14)\\] \\[= \\frac{2}{7} + \\frac{5}{7} + \\frac{24}{7} + \\frac{11}{7} + \\frac{14}{7} = \\frac{56}{7} = 8\\] <p>The weighted average equals the arithmetic mean because the weights reflect the actual frequency of each value in the data.</p>"},{"location":"math/probability/expectation/#definition-of-expectation","title":"Definition of Expectation","text":"<p>Now we're ready to define the expectation (or expected value) of a discrete random variable. The key insight is that the weights in our weighted sum become the probabilities of each value.</p> <p>For a discrete random variable \\(X\\) with possible values \\(x_1, x_2, \\ldots, x_k\\) and probability mass function \\(P(X = x_i) = p_i\\), the expectation is defined as:</p> \\[E[X] = \\sum_{i=1}^k x_i \\cdot P(X = x_i) = \\sum_{i=1}^k x_i \\cdot p_i\\] <p>Why use probabilities as weights? Because we want to assign higher weights to values that are more likely to occur.</p> <p>Example: Consider a random variable \\(X\\) representing the outcome of a biased die:</p> <ul> <li> <p>\\(P(X = 1) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 2) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 3) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 4) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 5) = 0.1\\) (10% chance)</p> </li> <li> <p>\\(P(X = 6) = 0.5\\) (50% chance)</p> </li> </ul> <p>Expectation calculation:</p> \\[E[X] = 1(0.1) + 2(0.1) + 3(0.1) + 4(0.1) + 5(0.1) + 6(0.5)\\] \\[= 0.1 + 0.2 + 0.3 + 0.4 + 0.5 + 3.0 = 4.5\\] <p>Expectation is a weighted average where the weights are the probabilities of each possible value. Weights are probabilities \\(P(X = x_i)\\) that sum to 1. The expectation gives us a single number that summarizes the \"center\" of a random variable's distribution.</p>"},{"location":"math/probability/expectation/#expectation-of-a-bernoulli-random-variable","title":"Expectation of a Bernoulli Random Variable","text":"<p>Let's compute the expectation of a Bernoulli random variable \\(X \\sim \\text{Bernoulli}(p)\\).</p> <p>A Bernoulli random variable \\(X\\) takes only two values:</p> <ul> <li> <p>\\(X = 1\\) with probability \\(p\\) (success)</p> </li> <li> <p>\\(X = 0\\) with probability \\(1-p\\) (failure)</p> </li> </ul> <p>Using the definition of expectation:</p> \\[E[X] = \\sum_{i=1}^k x_i \\cdot P(X = x_i)\\] <p>For Bernoulli, we have \\(k = 2\\) possible values:</p> \\[E[X] = 0 \\cdot P(X = 0) + 1 \\cdot P(X = 1)\\] \\[E[X] = 0 \\cdot (1-p) + 1 \\cdot p\\] \\[E[X] = 0 + p = p\\] <p>The expectation of a Bernoulli random variable is \\(p\\):</p> \\[E[X] = p \\quad \\text{where } X \\sim \\text{Bernoulli}(p)\\] <p>Why does this make sense?</p> <ul> <li> <p>If \\(p = 0.8\\) (80% chance of success), we expect to see 1 about 80% of the time</p> </li> <li> <p>The long-run average of many Bernoulli trials will be approximately \\(p\\)</p> </li> <li> <p>Since \\(X\\) only takes values 0 and 1, the expectation represents the \"proportion of successes\"</p> </li> </ul>"},{"location":"math/probability/expectation/#expectation-of-a-binomial-random-variable","title":"Expectation of a Binomial Random Variable","text":"<p>Now let's compute the expectation of a binomial random variable \\(X \\sim \\text{Binomial}(n, p)\\).</p> <p>A binomial random variable \\(X\\) represents the number of successes in \\(n\\) independent Bernoulli trials, each with success probability \\(p\\).</p> <p>Possible values: \\(X\\) takes values in \\(\\{0, 1, 2, \\ldots, n\\}\\). PMF: \\(P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k}\\) for \\(k = 0, 1, 2, \\ldots, n\\)</p> <p>Using the definition of expectation:</p> \\[E[X] = \\sum_{k=0}^n k \\cdot P(X = k) = \\sum_{k=0}^n k \\cdot \\binom{n}{k} p^k(1-p)^{n-k}\\] \\[E[X] = \\sum_{k=0}^n k \\cdot \\binom{n}{k} p^k(1-p)^{n-k}\\] <p>We can use this identity:</p> \\[k \\cdot \\binom{n}{k} = k \\cdot \\frac{n!}{k!(n-k)!} = \\frac{n!}{(k-1)!(n-k)!} = n \\cdot \\frac{(n-1)!}{(k-1)!(n-k)!} = n \\cdot \\binom{n-1}{k-1}\\] \\[E[X] = \\sum_{k=0}^n n \\cdot \\binom{n-1}{k-1} p^k(1-p)^{n-k}\\] \\[E[X] = n \\cdot p \\cdot \\sum_{k=0}^n \\binom{n-1}{k-1} p^{k-1}(1-p)^{n-k}\\] <p>Let \\(j = k-1\\), so \\(k = j+1\\). When \\(k = 0\\), \\(j = -1\\); when \\(k = n\\), \\(j = n-1\\).</p> <p>Note: The term with \\(j = -1\\) contributes 0 because \\(\\binom{n-1}{-1} = 0\\) (combinatorial coefficients are 0 for negative indices). So we can adjust the range to start from \\(j = 0\\):</p> \\[E[X] = n \\cdot p \\cdot \\sum_{j=0}^{n-1} \\binom{n-1}{j} p^j(1-p)^{(n-1)-j}\\] <p>The sum \\(\\sum_{j=0}^{n-1} \\binom{n-1}{j} p^j(1-p)^{(n-1)-j}\\) is exactly the binomial expansion of \\((p + (1-p))^{n-1} = 1^{n-1} = 1\\).</p> \\[E[X] = n \\cdot p \\cdot 1 = np\\] <p>The expectation of a binomial random variable is \\(np\\):</p> \\[E[X] = np \\quad \\text{where } X \\sim \\text{Binomial}(n, p)\\] <p>Why does this make sense?</p> <ul> <li> <p>\\(n\\) trials: We perform \\(n\\) independent Bernoulli trials</p> </li> <li> <p>\\(p\\) probability: Each trial has success probability \\(p\\)</p> </li> <li> <p>Expected successes: We expect \\(p\\) proportion of trials to succeed</p> </li> <li> <p>Total expectation: \\(n \\cdot p\\) total expected successes</p> </li> </ul> <p>Example: If we flip a fair coin (\\(p = 0.5\\)) 100 times (\\(n = 100\\)):</p> <ul> <li> <p>\\(E[X] = 100 \\cdot 0.5 = 50\\)</p> </li> <li> <p>Interpretation: We expect about 50 heads in 100 flips</p> </li> </ul>"},{"location":"math/probability/expectation/#linearity-of-expectation","title":"Linearity of Expectation","text":"<p>Linearity of expectation is one of the most powerful and useful properties in probability theory. It states that expectation is a linear operator, regardless of whether the random variables are independent or not.</p> <p>For any random variables \\(X\\) and \\(Y\\) (discrete or continuous) and any constants \\(a\\) and \\(b\\):</p> \\[E[aX + bY] = aE[X] + bE[Y]\\] <p>Key insight: Linearity of expectation holds even when \\(X\\) and \\(Y\\) are dependent!</p> <p>Binomial Distribution Revisited</p> <p>If \\(X \\sim \\text{Binomial}(n, p)\\), we can think of \\(X\\) as the sum of \\(n\\) independent Bernoulli\\((p)\\) random variables:</p> \\[X = B_1 + B_2 + \\cdots + B_n\\] <p>where each \\(B_i \\sim \\text{Bernoulli}(p)\\).</p> <p>By linearity:</p> \\[E[X] = E[B_1 + B_2 + \\cdots + B_n] = E[B_1] + E[B_2] + \\cdots + E[B_n] = p + p + \\cdots + p = np\\] <p>This gives us the same result as our direct calculation, but much more simply!</p>"},{"location":"math/probability/expectation/#expectation-of-a-hypergeometric-random-variable","title":"Expectation of a Hypergeometric Random Variable","text":"<p>Now let's compute the expectation of a hypergeometric random variable \\(X \\sim \\text{Hypergeometric}(N, K, n)\\).</p> <p>A hypergeometric random variable \\(X\\) represents the number of \"success\" items when drawing \\(n\\) items without replacement from a population of \\(N\\) items, where \\(K\\) items are \"successes\".</p> <p>Possible values: \\(X\\) takes values in \\(\\{0, 1, 2, \\ldots, \\min(K, n)\\}\\). PMF: \\(P(X = k) = \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\)</p> <p>Using the definition of expectation:</p> \\[E[X] = \\sum_{k=0}^{\\min(K,n)} k \\cdot P(X = k) = \\sum_{k=0}^{\\min(K,n)} k \\cdot \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\] \\[E[X] = \\sum_{k=0}^{\\min(K,n)} k \\cdot \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\] <p>Use the identity \\(k \\cdot \\binom{K}{k} = K \\cdot \\binom{K-1}{k-1}\\)</p> <p>This identity comes from:</p> \\[k \\cdot \\binom{K}{k} = k \\cdot \\frac{K!}{k!(K-k)!} = \\frac{K!}{(k-1)!(K-k)!} = K \\cdot \\frac{(K-1)!}{(k-1)!(K-k)!} = K \\cdot \\binom{K-1}{k-1}\\] \\[E[X] = \\sum_{k=0}^{\\min(K,n)} K \\cdot \\binom{K-1}{k-1} \\cdot \\frac{\\binom{N-K}{n-k}}{\\binom{N}{n}}\\] \\[E[X] = K \\cdot \\sum_{k=0}^{\\min(K,n)} \\binom{K-1}{k-1} \\cdot \\frac{\\binom{N-K}{n-k}}{\\binom{N}{n}}\\] <p>Let \\(j = k-1\\), so \\(k = j+1\\). When \\(k = 0\\), \\(j = -1\\); when \\(k = \\min(K,n)\\), \\(j = \\min(K,n)-1\\).</p> <p>Note: The term with \\(j = -1\\) contributes 0 because \\(\\binom{K-1}{-1} = 0\\). So we can adjust the range to start from \\(j = 0\\):</p> \\[E[X] = K \\cdot \\sum_{j=0}^{\\min(K-1,n-1)} \\binom{K-1}{j} \\cdot \\frac{\\binom{N-K}{n-(j+1)}}{\\binom{N}{n}}\\] <p>The sum \\(\\sum_{j=0}^{\\min(K-1,n-1)} \\binom{K-1}{j} \\cdot \\binom{N-K}{n-(j+1)}\\) represents the total number of ways to choose \\(n-1\\) items from \\(N-1\\) items (since we're choosing \\(j\\) from \\(K-1\\) and \\(n-1-j\\) from \\(N-K\\)).</p> <p>This equals \\(\\binom{N-1}{n-1}\\).</p> \\[E[X] = K \\cdot \\frac{\\binom{N-1}{n-1}}{\\binom{N}{n}} = K \\cdot \\frac{n}{N} = n \\cdot \\frac{K}{N}\\] <p>The expectation of a hypergeometric random variable is \\(n \\cdot \\frac{K}{N}\\):</p> \\[E[X] = n \\cdot \\frac{K}{N} \\quad \\text{where } X \\sim \\text{Hypergeometric}(N, K, n)\\] <p>Example: If we have a population of 100 items with 30 successes, and we draw 20 items:</p> <ul> <li> <p>\\(N = 100\\), \\(K = 30\\), \\(n = 20\\)</p> </li> <li> <p>\\(E[X] = 20 \\cdot \\frac{30}{100} = 20 \\cdot 0.3 = 6\\)</p> </li> <li> <p>Interpretation: We expect about 6 successes in our sample of 20</p> </li> </ul> <p>This result connects beautifully to the binomial expectation:</p> <ul> <li> <p>Binomial: \\(E[X] = np\\) (with replacement, constant probability)</p> </li> <li> <p>Hypergeometric: \\(E[X] = n \\cdot \\frac{K}{N}\\) (without replacement, changing probability)</p> </li> <li> <p>Key difference: \\(\\frac{K}{N}\\) vs. \\(p\\) - the proportion of successes in the population</p> </li> </ul>"},{"location":"math/probability/expectation/#proof-of-linearity-of-expectation","title":"Proof of Linearity of Expectation","text":"<p>Let's prove that expectation is a linear operator: \\(E[aX + bY] = aE[X] + bE[Y]\\) for any random variables \\(X\\) and \\(Y\\) and constants \\(a\\) and \\(b\\).</p> <p>Step 1: Start with the definition</p> <p>Important: This step introduces a new concept - the joint expectation of random variables, which we haven't discussed yet in this document.</p> <p>The expectation of a function of two random variables is defined as:</p> \\[E[g(X,Y)] = \\sum_{x,y} g(x,y) \\cdot P(X = x, Y = y)\\] <p>This is called the joint expectation because it involves the joint probability distribution \\(P(X = x, Y = y)\\) of both random variables together.</p> <p>In our case, \\(g(X,Y) = aX + bY\\), so:</p> \\[E[aX + bY] = \\sum_{x,y} (ax + by) \\cdot P(X = x, Y = y)\\] <p>Step 2: Distribute the sum</p> \\[E[aX + bY] = \\sum_{x,y} ax \\cdot P(X = x, Y = y) + \\sum_{x,y} by \\cdot P(X = x, Y = y)\\] <p>Step 3: Factor out constants</p> \\[E[aX + bY] = a \\sum_{x,y} x \\cdot P(X = x, Y = y) + b \\sum_{x,y} y \\cdot P(X = x, Y = y)\\] <p>Step 4: Use the law of total probability</p> <p>For any event \\(A\\), the law of total probability states:</p> \\[P(A) = \\sum_B P(A \\cap B) = \\sum_B P(A, B)\\] <p>In our derivation, we're using this to \"marginalize out\" one variable:</p> <p>For the first sum:</p> \\[\\sum_{x,y} x \\cdot P(X = x, Y = y) = \\sum_x x \\sum_y P(X = x, Y = y) = \\sum_x x \\cdot P(X = x) = E[X]\\] <p>Here, we're summing over all possible \\(y\\) values for each fixed \\(x\\), which gives us \\(P(X = x)\\) by the law of total probability.</p> <p>For the second sum:</p> \\[\\sum_{x,y} y \\cdot P(X = x, Y = y) = \\sum_y y \\sum_x P(X = x, Y = y) = \\sum_y y \\cdot P(Y = y) = E[Y]\\] <p>Here, we're summing over all possible \\(x\\) values for each fixed \\(y\\), which gives us \\(P(Y = y)\\) by the law of total probability.</p> <p>Step 5: Final result</p> \\[E[aX + bY] = aE[X] + bE[Y]\\]"},{"location":"math/probability/exponential_distribution/","title":"Exponential Distribution","text":"<p>The Exponential distribution is a continuous probability distribution that describes the time between events in a Poisson process. It is characterized by its memoryless property and is fundamental in reliability theory, queuing theory, and survival analysis.</p> <p>A continuous random variable \\(X\\) has an exponential distribution with parameter \\(\\lambda &gt; 0\\) if its PDF is:</p> \\[f_X(x) = \\begin{cases} \\lambda e^{-\\lambda x} &amp; \\text{if } x \\geq 0 \\\\ 0 &amp; \\text{if } x &lt; 0 \\end{cases}\\] <p>We write this as \\(X \\sim \\text{Exponential}(\\lambda)\\) or \\(X \\sim \\text{Exp}(\\lambda)\\).</p> <p>Parameters:</p> <ul> <li>\\(\\lambda\\): rate parameter (events per unit time)</li> </ul>"},{"location":"math/probability/exponential_distribution/#memory-less-property","title":"Memory-less property","text":"<p>The exponential distribution is memoryless, meaning:</p> \\[P(X &gt; s + t \\mid X &gt; s) = P(X &gt; t)\\] <p>If you've already waited \\(s\\) units of time, the probability of waiting an additional \\(t\\) units is the same as if you were starting fresh.</p> <p>Proof:</p> \\[P(X &gt; s + t \\mid X &gt; s) = \\frac{P(X &gt; s + t \\text{ AND } X &gt; s)}{P(X &gt; s)}\\] <p>Since \\(s + t &gt; s\\), if \\(X &gt; s + t\\), then automatically \\(X &gt; s\\). Therefore:</p> \\[P(X &gt; s + t \\text{ AND } X &gt; s) = P(X &gt; s + t)\\] <p>Using the CDF of the exponential distribution:</p> \\[P(X &gt; s + t) = 1 - P(X \\leq s + t) = 1 - (1 - e^{-\\lambda(s + t)}) = e^{-\\lambda(s + t)}\\] \\[P(X &gt; s) = 1 - P(X \\leq s) = 1 - (1 - e^{-\\lambda s}) = e^{-\\lambda s}\\] \\[P(X &gt; s + t \\mid X &gt; s) = \\frac{P(X &gt; s + t)}{P(X &gt; s)} = \\frac{e^{-\\lambda(s + t)}}{e^{-\\lambda s}}\\] \\[\\frac{e^{-\\lambda(s + t)}}{e^{-\\lambda s}} = e^{-\\lambda(s + t) + \\lambda s} = e^{-\\lambda t} = P(X &gt; t)\\] <p>Therefore:</p> \\[P(X &gt; s + t \\mid X &gt; s) = P(X &gt; t)\\] <p>This proves the memoryless property!</p> <p>Example: If a light bulb has been working for 100 hours, the probability it works for another 50 hours is the same as the probability a new bulb works for 50 hours.</p>"},{"location":"math/probability/exponential_distribution/#connection-to-poisson-process","title":"Connection to Poisson Process","text":"<p>The exponential distribution describes the inter-arrival times in a Poisson process:</p> <ul> <li> <p>Poisson process: Events occur at a constant average rate \\(\\lambda\\)</p> </li> <li> <p>Exponential distribution: Time between consecutive events</p> </li> <li> <p>Relationship: If events occur at rate \\(\\lambda\\), then inter-arrival times follow \\(\\text{Exp}(\\lambda)\\)</p> </li> </ul> <p>Theorem: In a Poisson process with rate \\(\\lambda\\), the inter-arrival times (time between consecutive events) are independent and identically distributed exponential random variables with parameter \\(\\lambda\\).</p> <p>Proof: Let \\(T_1, T_2, T_3, \\ldots\\) be the inter-arrival times. We need to show that each \\(T_i \\sim \\text{Exp}(\\lambda)\\).</p> <p>In a Poisson process, events occur continuously over time. We need multiple random variables because:</p> <ol> <li>\\(T_1\\): Time from start (time 0) until the first event occurs</li> <li>\\(T_2\\): Time from the first event until the second event occurs  </li> <li>\\(T_3\\): Time from the second event until the third event occurs</li> <li>And so on...: Each \\(T_i\\) represents the time between the \\((i-1)\\)th and \\(i\\)th events</li> </ol> <p>Each \\(T_i\\) represents a different time interval between consecutive events. Since events occur randomly, each of these time intervals is itself a random variable.</p> <p>The first event occurs at time \\(T_1\\). The probability that no events occur in time interval \\([0, t]\\) is:</p> \\[P(T_1 &gt; t) = P(\\text{No events in } [0, t])\\] <p>Since the number of events in \\([0, t]\\) follows \\(\\text{Poisson}(\\lambda t)\\):</p> \\[P(T_1 &gt; t) = P(\\text{Poisson}(\\lambda t) = 0) = \\frac{(\\lambda t)^0}{0!} e^{-\\lambda t} = e^{-\\lambda t}\\] <p>Therefore:</p> \\[P(T_1 \\leq t) = 1 - P(T_1 &gt; t) = 1 - e^{-\\lambda t}\\] <p>This is exactly the CDF of \\(\\text{Exp}(\\lambda)\\), so \\(T_1 \\sim \\text{Exp}(\\lambda)\\).</p> <p>Key insight: Each \\(T_i\\) follows the same exponential distribution \\(\\text{Exp}(\\lambda)\\) because:</p> <ol> <li> <p>Stationary increments: The Poisson process has the same behavior regardless of when we start observing</p> </li> <li> <p>Memoryless property: The exponential distribution \"forgets\" how long we've been waiting</p> </li> <li> <p>Independent increments: Each time interval is independent of previous intervals</p> </li> </ol> <p>Example: Consider a Poisson process with rate \\(\\lambda = 2\\) events per hour.</p> <p>Poisson aspect: Number of events in 3 hours follows \\(\\text{Poisson}(2 \\times 3) = \\text{Poisson}(6)\\).</p> <p>Exponential aspect: Time between consecutive events follows \\(\\text{Exp}(2)\\).</p> <p>Verification: </p> <ul> <li> <p>Expected events in 3 hours: \\(E[\\text{Poisson}(6)] = 6\\)</p> </li> <li> <p>Expected time between events: \\(E[\\text{Exp}(2)] = \\frac{1}{2}\\) hour</p> </li> <li> <p>Consistency: \\(\\frac{3 \\text{ hours}}{6 \\text{ events}} = \\frac{1}{2} \\text{ hour per event}\\) \u2713</p> </li> </ul>"},{"location":"math/probability/exponential_distribution/#cdf-of-the-exponential-distribution","title":"CDF of the Exponential Distribution","text":"<p>The CDF of \\(X \\sim \\text{Exp}(\\lambda)\\) is:</p> \\[F_X(x) = \\begin{cases} 1 - e^{-\\lambda x} &amp; \\text{if } x \\geq 0 \\\\ 0 &amp; \\text{if } x &lt; 0 \\end{cases}\\] <p>Derivation: </p> \\[F_X(x) = \\int_0^x \\lambda e^{-\\lambda t} \\, dt = \\lambda \\int_0^x e^{-\\lambda t} \\, dt = \\lambda \\left[-\\frac{1}{\\lambda} e^{-\\lambda t}\\right]_0^x = 1 - e^{-\\lambda x}\\]"},{"location":"math/probability/exponential_distribution/#expectation-and-variance","title":"Expectation and Variance","text":"<p>Expectation</p> \\[E[X] = \\frac{1}{\\lambda}\\] <p>Proof:</p> \\[E[X] = \\int_0^{\\infty} x \\cdot \\lambda e^{-\\lambda x} \\, dx\\] <p>Using integration by parts with \\(u = x\\) and \\(dv = \\lambda e^{-\\lambda x} \\, dx\\):</p> \\[E[X] = \\left[-x e^{-\\lambda x}\\right]_0^{\\infty} - \\int_0^{\\infty} (-e^{-\\lambda x}) \\, dx\\] <p>The boundary term evaluates to 0:</p> <ul> <li> <p>At \\(x = 0\\): \\(-0 \\cdot e^0 = 0\\)</p> </li> <li> <p>At \\(x = \\infty\\): \\(-x \\cdot e^{-\\lambda x} \\to 0\\) (exponential decay dominates)</p> </li> </ul> <p>Therefore:</p> \\[E[X] = \\int_0^{\\infty} e^{-\\lambda x} \\, dx = \\left[-\\frac{1}{\\lambda} e^{-\\lambda x}\\right]_0^{\\infty} = \\frac{1}{\\lambda}\\] <p>Variance</p> \\[\\text{Var}(X) = \\frac{1}{\\lambda^2}\\] <p>Proof:</p> \\[\\text{Var}(X) = E[X^2] - (E[X])^2 = E[X^2] - \\frac{1}{\\lambda^2}\\] <p>We need to calculate \\(E[X^2]\\):</p> \\[E[X^2] = \\int_0^{\\infty} x^2 \\cdot \\lambda e^{-\\lambda x} \\, dx\\] <p>Using integration by parts twice:</p> \\[E[X^2] = \\frac{2}{\\lambda^2}\\] <p>Therefore:</p> \\[\\text{Var}(X) = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\\] <p>Standard Deviation</p> \\[\\sigma_X = \\frac{1}{\\lambda}\\] <p>Note: For the exponential distribution, the mean equals the standard deviation.</p>"},{"location":"math/probability/independence_of_random_variables/","title":"Independence of Random Variables","text":"<p>Independence is one of the most fundamental and important concepts in probability theory. It allows us to simplify complex calculations, understand the structure of random phenomena, and make powerful assumptions that lead to elegant mathematical results.</p> <p>Two random variables \\(X\\) and \\(Y\\) are independent if and only if their joint probability distribution factors into the product of their individual distributions.</p> <p>For discrete random variables:</p> \\[P(X = x, Y = y) = P(X = x) \\cdot P(Y = y) \\quad \\text{for all } x, y\\] <p>For continuous random variables:</p> \\[f_{X,Y}(x, y) = f_X(x) \\cdot f_Y(y) \\quad \\text{for all } x, y\\] <p>For n random variables:</p> <p>A collection of random variables \\(X_1, X_2, \\ldots, X_n\\) is mutually independent if and only if their joint CDF factors into the product of their individual CDFs:</p> \\[F_{X_1, X_2, \\ldots, X_n}(x_1, x_2, \\ldots, x_n) = F_{X_1}(x_1) \\cdot F_{X_2}(x_2) \\cdots F_{X_n}(x_n) = \\prod_{i=1}^n F_{X_i}(x_i) \\quad \\text{for all } x_1, x_2, \\ldots, x_n\\] <p>Equivalently, for continuous random variables, the joint PDF factors:</p> \\[f_{X_1, X_2, \\ldots, X_n}(x_1, x_2, \\ldots, x_n) = f_{X_1}(x_1) \\cdot f_{X_2}(x_2) \\cdots f_{X_n}(x_n) = \\prod_{i=1}^n f_{X_i}(x_i) \\quad \\text{for all } x_1, x_2, \\ldots, x_n\\] <p>And for discrete random variables, the joint PMF factors:</p> \\[P(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n) = P(X_1 = x_1) \\cdot P(X_2 = x_2) \\cdots P(X_n = x_n) = \\prod_{i=1}^n P(X_i = x_i) \\quad \\text{for all } x_1, x_2, \\ldots, x_n\\] <p>Independence means that knowing the value of one random variable gives you no information about the value of the other. The random variables are completely unrelated in their behavior.</p>"},{"location":"math/probability/indicator_random_variables/","title":"Indicator Random Variables","text":"<p>Indicator random variables are one of the most powerful and elegant tools in probability theory. They provide a bridge between probability and expectation, making complex problems surprisingly simple.</p> <p>An indicator random variable \\(I_A\\) for an event \\(A\\) is defined as:</p> \\[I_A = \\begin{cases}  1 &amp; \\text{if event } A \\text{ occurs} \\\\ 0 &amp; \\text{if event } A \\text{ does not occur} \\end{cases}\\] <p>Key properties:</p> <ul> <li> <p>Binary values: Only takes values 0 or 1</p> </li> <li> <p>Event representation: Directly represents whether an event occurs</p> </li> <li> <p>Probability connection: \\(E[I_A] = P(A)\\)</p> </li> </ul> <p>Bridge Between Probability and Expectation</p> <p>The fundamental connection: \\(E[I_A] = P(A)\\)</p> <p>Proof:</p> \\[E[I_A] = 1 \\cdot P(I_A = 1) + 0 \\cdot P(I_A = 0) = 1 \\cdot P(A) + 0 \\cdot P(A^c) = P(A)\\] <p>This simple result allows us to convert probability problems into expectation problems, which are often easier to solve.</p> <p>Linearity of Expectation</p> <p>Since indicators only take values 0 and 1, they work beautifully with linearity of expectation:</p> \\[E[I_{A_1} + I_{A_2} + \\cdots + I_{A_n}] = E[I_{A_1}] + E[I_{A_2}] + \\cdots + E[I_{A_n}] = P(A_1) + P(A_2) + \\cdots + P(A_n)\\] <p>Example: Birthday Problem</p> <p>Problem: In a group of \\(n\\) people, what's the expected number of people with a birthday on January 1st?</p> <p>Solution using indicators:</p> <ul> <li> <p>Let \\(I_i\\) be the indicator that person \\(i\\) has a birthday on January 1st</p> </li> <li> <p>\\(E[I_i] = P(\\text{person } i \\text{ born on Jan 1}) = \\frac{1}{365}\\)</p> </li> <li> <p>Total expected: \\(E[\\sum_{i=1}^n I_i] = \\sum_{i=1}^n E[I_i] = n \\cdot \\frac{1}{365}\\)</p> </li> </ul> <p>Result: We expect \\(\\frac{n}{365}\\) people to have a birthday on January 1st.</p> <p>Without using indicators, we can solve this directly using the definition of expectation:</p> <p>Let \\(X\\) be the number of people with a birthday on January 1st.</p> <p>\\(X\\) follows a binomial distribution: \\(X \\sim \\text{Binomial}(n, \\frac{1}{365})\\)</p> <p>For \\(X \\sim \\text{Binomial}(n, p)\\), we know \\(E[X] = np\\).</p> <p>Therefore:</p> \\[E[X] = n \\cdot \\frac{1}{365} = \\frac{n}{365}\\] <p>Result: Same answer, different method! The indicator method breaks down the problem into individual components, while the direct method recognizes the overall distribution. Both are valid approaches that lead to the same mathematical result.</p> <p>Example: A permutation of numbers 1 to \\(n\\) has a local maximum at the \\(j\\)-th position if the number at the \\(j\\)-th position is bigger than both its neighbors. For the first and last positions, a local maximum exists if that number is bigger than its only neighbor. Given that all \\(n!\\) permutations are equally likely, calculate the expected number of local maxima.</p> <p>Solution using Indicator Random Variables</p> <p>Let \\(I_j\\) be the indicator that position \\(j\\) has a local maximum.</p> <p>Case 1: Interior positions (\\(2 \\leq j \\leq n-1\\))</p> <p>For position \\(j\\) to be a local maximum:</p> <ul> <li> <p>The number at position \\(j\\) must be larger than the number at position \\(j-1\\)</p> </li> <li> <p>The number at position \\(j\\) must be larger than the number at position \\(j+1\\)</p> </li> </ul> <p>Probability calculation:</p> <ul> <li> <p>We need to choose 3 distinct numbers from \\(\\{1, 2, \\ldots, n\\}\\)</p> </li> <li> <p>The middle number must be the largest of the three</p> </li> <li> <p>\\(P(I_j = 1) = \\frac{1}{3}\\) (by symmetry, any of the three numbers is equally likely to be largest)</p> </li> </ul> <p>Case 2: Boundary positions (\\(j = 1\\) or \\(j = n\\))</p> <p>For position 1 to be a local maximum:</p> <ul> <li>The number at position 1 must be larger than the number at position 2</li> </ul> <p>Probability calculation:</p> <ul> <li> <p>We need to choose 2 distinct numbers from \\(\\{1, 2, \\ldots, n\\}\\)</p> </li> <li> <p>The first number must be larger than the second</p> </li> <li> <p>\\(P(I_1 = 1) = \\frac{1}{2}\\) (by symmetry, either number is equally likely to be larger)</p> </li> </ul> <p>Similarly, \\(P(I_n = 1) = \\frac{1}{2}\\).</p> <p>Using the bridge between Probability and Expectation for IRV:</p> <ul> <li> <p>Interior positions: \\(E[I_j] = \\frac{1}{3}\\) for \\(j = 2, 3, \\ldots, n-1\\)</p> </li> <li> <p>Boundary positions: \\(E[I_1] = E[I_n] = \\frac{1}{2}\\)</p> </li> </ul> <p>Use linearity of expectation</p> \\[E[\\text{Total local maxima}] = E\\left[\\sum_{j=1}^n I_j\\right] = \\sum_{j=1}^n E[I_j]\\] \\[= E[I_1] + \\sum_{j=2}^{n-1} E[I_j] + E[I_n]\\] \\[= \\frac{1}{2} + (n-2) \\cdot \\frac{1}{3} + \\frac{1}{2}\\] \\[= 1 + (n-2) \\cdot \\frac{1}{3} = 1 + \\frac{n-2}{3} = \\frac{3 + n - 2}{3} = \\frac{n+1}{3}\\] <p>The expected number of local maxima in a random permutation of \\(\\{1, 2, \\ldots, n\\}\\) is:</p> \\[E[\\text{Local maxima}] = \\frac{n+1}{3}\\] <p>This problem demonstrates the power of indicator random variables in complex combinatorial problems where direct counting would be extremely difficult.</p>"},{"location":"math/probability/jensens_inequality/","title":"Jensen's inequality","text":"<p>Jensen's inequality is one of the most fundamental and widely-used inequalities in probability theory and analysis. It provides a powerful tool for relating the expectation of a convex (or concave) function to the function of the expectation.</p>"},{"location":"math/probability/jensens_inequality/#convex-and-concave-functions","title":"Convex and Concave functions","text":""},{"location":"math/probability/jensens_inequality/#convex-functions","title":"Convex functions","text":"<p>Before stating Jensen's inequality, we need to understand what convex and concave functions are.</p> <p>A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is convex if for any two points \\(x_1, x_2\\) in its domain and any \\(\\lambda \\in [0,1]\\):</p> \\[f(\\lambda x_1 + (1-\\lambda) x_2) \\leq \\lambda f(x_1) + (1-\\lambda) f(x_2)\\] <p>Geometric interpretation: A function is convex if the line segment connecting any two points on its graph lies above or on the graph itself. In other words, the graph \"curves upward\" or is \"bowl-shaped.\"</p> <p>Let's see how the mathematical definition translates to the geometric property. Consider two points \\((x_1, f(x_1))\\) and \\((x_2, f(x_2))\\) on the graph of a convex function \\(f\\).</p> <p>The line segment connecting these points can be parameterized as:</p> \\[L(\\lambda) = (1-\\lambda)(x_1, f(x_1)) + \\lambda(x_2, f(x_2))\\] <p>This gives us:</p> <ul> <li> <p>x-coordinate: \\((1-\\lambda)x_1 + \\lambda x_2 = x_1 + \\lambda(x_2 - x_1)\\)</p> </li> <li> <p>y-coordinate: \\((1-\\lambda)f(x_1) + \\lambda f(x_2)\\)</p> </li> </ul> <p>The convexity condition states that for any point on this line segment (i.e., for any \\(\\lambda \\in [0,1]\\)), the y-coordinate of the line is greater than or equal to the function value at the corresponding x-coordinate:</p> \\[(1-\\lambda)f(x_1) + \\lambda f(x_2) \\geq f((1-\\lambda)x_1 + \\lambda x_2)\\] <p>Visual interpretation: This means that if you draw a straight line between any two points on the graph of a convex function, the entire line segment lies above or on the graph. The function \"holds water\" - it forms a bowl shape.</p>"},{"location":"math/probability/jensens_inequality/#concave-functions","title":"Concave functions","text":"<p>A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is concave if for any two points \\(x_1, x_2\\) in its domain and any \\(\\lambda \\in [0,1]\\):</p> \\[f(\\lambda x_1 + (1-\\lambda) x_2) \\geq \\lambda f(x_1) + (1-\\lambda) f(x_2)\\] <p>Geometric interpretation: A function is concave if the line segment connecting any two points on its graph lies below or on the graph itself. In other words, the graph \"curves downward\" or is \"cave-shaped.\"</p> <p>Think of convex functions as \"smiling\" curves (like a bowl) and concave functions as \"frowning\" curves (like a cave). </p> <p>For twice-differentiable functions, we have a convenient test:</p> <ul> <li> <p>\\(f\\) is convex if and only if \\(f''(x) \\geq 0\\) for all \\(x\\) in the domain</p> </li> <li> <p>\\(f\\) is concave if and only if \\(f''(x) \\leq 0\\) for all \\(x\\) in the domain</p> </li> </ul> <p>Examples:</p> <ul> <li> <p>\\(f(x) = x^2\\): \\(f''(x) = 2 &gt; 0\\) \u2192 convex</p> </li> <li> <p>\\(f(x) = \\log(x)\\): \\(f''(x) = -\\frac{1}{x^2} &lt; 0\\) \u2192 concave</p> </li> <li> <p>\\(f(x) = e^x\\): \\(f''(x) = e^x &gt; 0\\) \u2192 convex</p> </li> </ul>"},{"location":"math/probability/jensens_inequality/#theorem-jensens-inequality","title":"Theorem (Jensen's Inequality)","text":"<p>Let \\(X\\) be a random variable and let \\(\\phi\\) be a convex function. Then:</p> \\[\\phi(E[X]) \\leq E[\\phi(X)]\\] <p>If \\(\\phi\\) is strictly convex, then equality holds if and only if \\(X\\) is constant (i.e., \\(X = E[X]\\) with probability 1).</p> <p>For concave functions: If \\(\\phi\\) is concave, then the inequality is reversed:</p> \\[\\phi(E[X]) \\geq E[\\phi(X)]\\] <p>Jensen's inequality captures a fundamental geometric insight: the function of the average is less than or equal to the average of the function (for convex functions).</p> <p>Think of it this way: if you have a convex function (like \\(f(x) = x^2\\)), the graph \"curves upward.\" If you take two points on this curve and draw a line between them, the line lies above the curve. This means that the average of the function values at two points is greater than the function value at the average of those points.</p> <p>Example 1: Quadratic function Let \\(X\\) be any random variable with finite variance, and let \\(\\phi(x) = x^2\\). Since \\(x^2\\) is convex:</p> \\[(E[X])^2 \\leq E[X^2]\\] <p>This immediately gives us the relationship between mean and variance:</p> \\[\\text{Var}(X) = E[X^2] - (E[X])^2 \\geq 0\\] <p>Example 2: Logarithm function Let \\(X\\) be a positive random variable, and let \\(\\phi(x) = \\log(x)\\). Since \\(\\log(x)\\) is concave:</p> \\[\\log(E[X]) \\geq E[\\log(X)]\\] <p>Example 3: Exponential function Let \\(X\\) be any random variable, and let \\(\\phi(x) = e^x\\). Since \\(e^x\\) is convex:</p> \\[e^{E[X]} \\leq E[e^X]\\] <p>This inequality is crucial in proving concentration inequalities like Hoeffding's inequality.</p>"},{"location":"math/probability/jensens_inequality/#multivariate-version","title":"Multivariate version","text":"<p>Jensen's inequality also extends to multivariate functions. If \\(\\phi: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex and \\(\\mathbf{X}\\) is a random vector, then:</p> \\[\\phi(E[\\mathbf{X}]) \\leq E[\\phi(\\mathbf{X})]\\] <p>This multivariate version is particularly useful in machine learning and optimization contexts where we deal with vector-valued random variables.</p>"},{"location":"math/probability/joint_distributions/","title":"Joint Distributions","text":"<p>A joint distribution describes the probability distribution of two or more random variables simultaneously. It captures not only the individual behavior of each random variable but also how they relate to each other.</p> <p>For two discrete random variables \\(X\\) and \\(Y\\), the joint probability mass function (joint PMF) is defined as:</p> \\[p_{X,Y}(x,y) = P(X = x, Y = y)\\] <p>For continuous random variables, the joint probability density function (joint PDF) satisfies:</p> \\[P((X,Y) \\in A) = \\iint_A f_{X,Y}(x,y) \\, dx \\, dy\\] <p>The joint cumulative distribution function (joint CDF) is defined as:</p> \\[F_{X,Y}(x,y) = P(X \\leq x, Y \\leq y)\\] <p>For discrete random variables, this becomes:</p> \\[F_{X,Y}(x,y) = \\sum_{i \\leq x} \\sum_{j \\leq y} p_{X,Y}(i,j)\\] <p>Example: Let's consider two Bernoulli random variables \\(X\\) and \\(Y\\) with parameters \\(p\\) and \\(q\\) respectively:</p> <ul> <li> <p>\\(X \\sim \\text{Bernoulli}(p)\\) where \\(P(X = 1) = p\\) and \\(P(X = 0) = 1-p\\)</p> </li> <li> <p>\\(Y \\sim \\text{Bernoulli}(q)\\) where \\(P(Y = 1) = q\\) and \\(P(Y = 0) = 1-q\\)</p> </li> </ul> <p>The joint PMF \\(p_{X,Y}(x,y)\\) gives us the probability of each possible combination:</p> \\(X \\backslash Y\\) \\(0\\) \\(1\\) \\(0\\) \\(p_{X,Y}(0,0)\\) \\(p_{X,Y}(0,1)\\) \\(1\\) \\(p_{X,Y}(1,0)\\) \\(p_{X,Y}(1,1)\\) <p>If \\(X\\) and \\(Y\\) are independent, then:</p> \\[p_{X,Y}(x,y) = p_X(x) \\cdot p_Y(y)\\] <p>This means:</p> <ul> <li> <p>\\(p_{X,Y}(0,0) = (1-p)(1-q)\\)</p> </li> <li> <p>\\(p_{X,Y}(0,1) = (1-p)q\\)</p> </li> <li> <p>\\(p_{X,Y}(1,0) = p(1-q)\\)</p> </li> <li> <p>\\(p_{X,Y}(1,1) = pq\\)</p> </li> </ul> <p>If \\(X\\) and \\(Y\\) are dependent, the joint PMF cannot be factored this way, and we need additional information to specify the relationship between them.</p> <p>From the joint distribution, we can recover the individual (marginal) distributions:</p> \\[p_X(x) = \\sum_y p_{X,Y}(x,y)\\] \\[p_Y(y) = \\sum_x p_{X,Y}(x,y)\\] <p>For our Bernoulli example:</p> <ul> <li> <p>\\(p_X(0) = p_{X,Y}(0,0) + p_{X,Y}(0,1) = 1-p\\)</p> </li> <li> <p>\\(p_X(1) = p_{X,Y}(1,0) + p_{X,Y}(1,1) = p\\)</p> </li> <li> <p>\\(p_Y(0) = p_{X,Y}(0,0) + p_{X,Y}(1,0) = 1-q\\)</p> </li> <li> <p>\\(p_Y(1) = p_{X,Y}(0,1) + p_{X,Y}(1,1) = q\\)</p> </li> </ul> <p>For continuous random variables, the marginal PDFs are obtained by integrating the joint PDF:</p> \\[f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy\\] \\[f_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx\\] <p>Example: Consider two continuous random variables \\(X\\) and \\(Y\\) with joint PDF:</p> \\[f_{X,Y}(x,y) = \\frac{1}{2\\pi\\sigma_X\\sigma_Y\\sqrt{1-\\rho^2}} \\exp\\left(-\\frac{1}{2(1-\\rho^2)}\\left[\\frac{(x-\\mu_X)^2}{\\sigma_X^2} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y}\\right]\\right)\\] <p>To find the marginal PDF of \\(X\\), we integrate over \\(y\\):</p> \\[f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy\\] <p>After some algebra, this gives us:</p> \\[f_X(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_X} \\exp\\left(-\\frac{(x-\\mu_X)^2}{2\\sigma_X^2}\\right)\\] <p>This shows that the marginal distribution of \\(X\\) is \\(N(\\mu_X, \\sigma_X^2)\\).</p> <p>Example: Consider a uniform distribution over the disc \\(x^2 + y^2 \\leq c\\). The joint PDF is:</p> \\[f_{X,Y}(x,y) = \\begin{cases} \\frac{1}{\\pi c} &amp; \\text{if } x^2 + y^2 \\leq c \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>To find the marginal PDF of \\(X\\), we integrate over \\(y\\):</p> \\[f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy\\] <p>For a given \\(x\\) with \\(|x| \\leq \\sqrt{c}\\), the range of \\(y\\) is from \\(-\\sqrt{c-x^2}\\) to \\(\\sqrt{c-x^2}\\):</p> \\[f_X(x) = \\int_{-\\sqrt{c-x^2}}^{\\sqrt{c-x^2}} \\frac{1}{\\pi c} \\, dy = \\frac{2\\sqrt{c-x^2}}{\\pi c}\\] <p>For \\(|x| &gt; \\sqrt{c}\\), \\(f_X(x) = 0\\).</p> <p>Similarly, the marginal PDF of \\(Y\\) is:</p> \\[f_Y(y) = \\frac{2\\sqrt{c-y^2}}{\\pi c}\\] <p>This shows that the marginal distributions are not uniform - they follow a semi-circular distribution, even though the joint distribution is uniform over the disc.</p> <p>To show that \\(X\\) and \\(Y\\) are dependent, we need to verify that:</p> \\[f_{X,Y}(x,y) \\neq f_X(x) \\cdot f_Y(y)\\] <p>Let's check this for a point inside the disc, say \\((x,y) = (0,0)\\):</p> <ul> <li> <p>Joint PDF: \\(f_{X,Y}(0,0) = \\frac{1}{\\pi c}\\)</p> </li> <li> <p>Marginal PDFs: \\(f_X(0) = \\frac{2\\sqrt{c}}{\\pi c}\\) and \\(f_Y(0) = \\frac{2\\sqrt{c}}{\\pi c}\\)</p> </li> <li> <p>Product of marginals: \\(f_X(0) \\cdot f_Y(0) = \\frac{4c}{\\pi^2 c^2} = \\frac{4}{\\pi^2 c}\\)</p> </li> </ul> <p>Since \\(\\frac{1}{\\pi c} \\neq \\frac{4}{\\pi^2 c}\\), we have:</p> \\[f_{X,Y}(0,0) \\neq f_X(0) \\cdot f_Y(0)\\] <p>This proves that \\(X\\) and \\(Y\\) are dependent random variables. The dependence arises from the geometric constraint \\(x^2 + y^2 \\leq c\\) - knowing the value of \\(X\\) constrains the possible values of \\(Y\\) and vice versa.</p>"},{"location":"math/probability/markov_chains/","title":"Markov Chains","text":""},{"location":"math/probability/markov_chains/#introduction","title":"Introduction","text":"<p>To see where the Markov chain comes from, start by considering an i.i.d. sequence of random variables \\(X_0, X_1, \\ldots, X_n, \\ldots\\) where we think of \\(n\\) as time. An i.i.d. sequence has no dependence between any of the random variables- each \\(X_n\\) is independent of all previous values. A Markov chain is a sequence of r.v.s that exhibits one-step dependence.</p>"},{"location":"math/probability/markov_chains/#state-space-and-time","title":"State Space and Time","text":"<p>Markov chains \"live\" in both space and time.</p> <ul> <li> <p>State Space: The set of all possible values that the random variables \\(X_n\\) can take</p> </li> <li> <p>Time: The index \\(n\\) represents the evolution of some process over time</p> </li> </ul> <p>1. State Space Type:</p> <ul> <li> <p>Discrete State Space: States take values from a countable set (finite or infinite)</p> </li> <li> <p>Continuous State Space: States take values from a continuous set (e.g., real numbers)</p> </li> </ul> <p>2. Time Type:</p> <ul> <li> <p>Discrete Time: Process evolves at discrete time steps (\\(n = 0, 1, 2, \\ldots\\))</p> </li> <li> <p>Continuous Time: Process evolves continuously over time (\\(t \\geq 0\\))</p> </li> </ul>"},{"location":"math/probability/markov_chains/#definition-markov-chain","title":"Definition (Markov chain)","text":"<p>A sequence of random variables \\(X_0, X_1, X_2, \\ldots\\) taking values in the state space \\(\\{1, 2, \\ldots, M\\}\\) is called a Markov chain if for all \\(n \\geq 0\\),</p> \\[P(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1}, \\ldots, X_0 = i_0) = P(X_{n+1} = j | X_n = i)\\] <p>The quantity \\(P(X_{n+1} = j | X_n = i)\\) is called the transition probability from state \\(i\\) to state \\(j\\).</p> <p>If we think of time \\(n\\) as the present, times before \\(n\\) as the past, and times after \\(n\\) as the future, the Markov property says that given the present, the past and future are conditionally independent. The Markov property greatly simplifies computations of conditional probability: instead of having to condition on the entire past, we only need to condition on the most recent value.</p>"},{"location":"math/probability/markov_chains/#transition-matrix","title":"Transition matrix","text":"<p>Definition (Transition matrix): Let \\(X_0, X_1, X_2, \\ldots\\) be a Markov chain with state space \\(\\{1, 2, \\ldots, M\\}\\), and let \\(q_{ij} = P(X_{n+1} = j | X_n = i)\\) be the transition probability from state \\(i\\) to state \\(j\\). The \\(M \\times M\\) matrix \\(Q = (q_{ij})\\) is called the transition matrix of the chain.</p> <p>Note that \\(Q\\) is a nonnegative matrix in which each row sums to 1. This is because, starting from any state \\(i\\), the events \"move to 1\", \"move to 2\", \\(\\ldots\\), \"move to \\(M\\)\" are disjoint, and their union has probability 1 because the chain has to go somewhere.</p> <p>Example: Suppose that on any given day, the weather can either be rainy or sunny. If today is rainy, then tomorrow will be rainy with probability \\(1/3\\) and sunny with probability \\(2/3\\). If today is sunny, then tomorrow will be rainy with probability \\(1/2\\) and sunny with probability \\(1/2\\). Letting \\(X_n\\) be the weather on day \\(n\\), \\(X_0, X_1, X_2, \\ldots\\) is a Markov chain on the state space \\(\\{R, S\\}\\), where \\(R\\) stands for rainy and \\(S\\) for sunny. We know that the Markov property is satisfied because, from the description of the process, only today's weather matters for predicting tomorrow's.</p> <p>The transition matrix of the chain is</p> \\[\\begin{array}{c|cc}  &amp; R &amp; S \\\\ \\hline R &amp; 1/3 &amp; 2/3 \\\\ S &amp; 1/2 &amp; 1/2 \\\\ \\end{array}\\] <p>The transition probabilities of a Markov chain can also be represented with a diagram. Each state is represented by a circle, and the arrows indicate the possible one-step transitions; we can imagine a particle wandering around from state to state, randomly choosing which arrow to follow. Next to the arrows we write the corresponding transition probabilities.</p> <p></p> <p>Let's trace through a specific realization of the rainy-sunny Markov chain. Suppose we start with \\(X_0 = R\\) (rainy on day 0) and simulate the next 5 days:</p> <p>Day-by-day evolution:</p> <ul> <li> <p>\\(X_0 = R\\) (start rainy)</p> </li> <li> <p>\\(X_1 = S\\) (transition: R\u2192S with probability 2/3)</p> </li> <li> <p>\\(X_2 = R\\) (transition: S\u2192R with probability 1/2)  </p> </li> <li> <p>\\(X_3 = S\\) (transition: R\u2192S with probability 2/3)</p> </li> <li> <p>\\(X_4 = S\\) (transition: S\u2192S with probability 1/2)</p> </li> <li> <p>\\(X_5 = R\\) (transition: S\u2192R with probability 1/2)</p> </li> </ul> <p>Key observations:</p> <ul> <li> <p>Each transition depends only on the current state (Markov property)</p> </li> <li> <p>This is just one possible realization - different runs would produce different sequences</p> </li> <li> <p>The probabilities at each step are determined by the transition matrix</p> </li> </ul>"},{"location":"math/probability/markov_chains/#n-step-transition-probabilities","title":"n-Step Transition Probabilities","text":"<p>Once we have the transition matrix \\(Q\\) of a Markov chain, we can work out the transition probabilities for longer timescales.</p> <p>Definition (n-step transition probability): The n-step transition probability from \\(i\\) to \\(j\\) is the probability of being at \\(j\\) exactly \\(n\\) steps after being at \\(i\\). We denote this by \\(q^{(n)}_{ij}\\):</p> \\[q^{(n)}_{ij} = P(X_n = j | X_0 = i)\\] <p>Note that</p> \\[q^{(2)}_{ij} = \\sum_k q_{ik} q_{kj}\\] <p>since to get from \\(i\\) to \\(j\\) in two steps, the chain must go from \\(i\\) to some intermediary state \\(k\\), and then from \\(k\\) to \\(j\\); these transitions are independent because of the Markov property. Since the right-hand side is the \\((i, j)\\) entry of \\(Q^2\\) by definition of matrix multiplication, we conclude that the matrix \\(Q^2\\) gives the two-step transition probabilities.</p> <p>Example: For our rainy-sunny Markov chain with transition matrix:</p> \\[Q = \\begin{pmatrix} 1/3 &amp; 2/3 \\\\ 1/2 &amp; 1/2 \\\\ \\end{pmatrix}\\] <p>The two-step transition matrix is:</p> \\[Q^2 = \\begin{pmatrix} 1/3 &amp; 2/3 \\\\ 1/2 &amp; 1/2 \\\\ \\end{pmatrix} \\begin{pmatrix} 1/3 &amp; 2/3 \\\\ 1/2 &amp; 1/2 \\\\ \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\cdot \\frac{1}{3} + \\frac{2}{3} \\cdot \\frac{1}{2} &amp; \\frac{1}{3} \\cdot \\frac{2}{3} + \\frac{2}{3} \\cdot \\frac{1}{2} \\\\ \\frac{1}{2} \\cdot \\frac{1}{3} + \\frac{1}{2} \\cdot \\frac{1}{2} &amp; \\frac{1}{2} \\cdot \\frac{2}{3} + \\frac{1}{2} \\cdot \\frac{1}{2} \\\\ \\end{pmatrix}\\] \\[= \\begin{pmatrix} \\frac{1}{9} + \\frac{1}{3} &amp; \\frac{2}{9} + \\frac{1}{3} \\\\ \\frac{1}{6} + \\frac{1}{4} &amp; \\frac{1}{3} + \\frac{1}{4} \\\\ \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{9} + \\frac{3}{9} &amp; \\frac{2}{9} + \\frac{3}{9} \\\\ \\frac{2}{12} + \\frac{3}{12} &amp; \\frac{4}{12} + \\frac{3}{12} \\\\ \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{9} &amp; \\frac{5}{9} \\\\ \\frac{5}{12} &amp; \\frac{7}{12} \\\\ \\end{pmatrix}\\] <p>Let's compute \\(q^{(2)}_{RS}\\) (starting rainy, ending sunny after 2 steps) using the formula \\(q^{(2)}_{ij} = \\sum_k q_{ik} q_{kj}\\):</p> \\[q^{(2)}_{RS} = \\sum_{k} q_{Rk} q_{kS} = q_{RR} \\cdot q_{RS} + q_{RS} \\cdot q_{SS}\\] <p>Substituting the values from our transition matrix:</p> \\[q^{(2)}_{RS} = \\frac{1}{3} \\cdot \\frac{2}{3} + \\frac{2}{3} \\cdot \\frac{1}{2} = \\frac{2}{9} + \\frac{1}{3} = \\frac{2}{9} + \\frac{3}{9} = \\frac{5}{9}\\] <p>The total probability is \\(\\frac{5}{9}\\).</p> <p>Verification: The \\((R,S)\\) entry is indeed \\(\\frac{5}{9}\\), which matches our calculation!</p> <p>By induction, the \\(n\\)th power of the transition matrix gives the \\(n\\)-step transition probabilities:</p> <p>\\(q^{(n)}_{ij}\\) is the \\((i, j)\\) entry of \\(Q^n\\).</p>"},{"location":"math/probability/markov_chains/#conditional-distributions-encoded-in-transition-matrices","title":"Conditional Distributions encoded in Transition Matrices","text":"<p>The transition matrix \\(Q\\) encodes the conditional distribution of \\(X_1\\) given the initial state of the chain. Specifically, the \\(i\\)th row of \\(Q\\) is the conditional PMF of \\(X_1\\) given \\(X_0 = i\\), displayed as a row vector. Similarly, the \\(i\\)th row of \\(Q^n\\) is the conditional PMF of \\(X_n\\) given \\(X_0 = i\\).</p> <p>Example: For our rainy-sunny Markov chain with transition matrix:</p> \\[Q = \\begin{pmatrix} 1/3 &amp; 2/3 \\\\ 1/2 &amp; 1/2 \\\\ \\end{pmatrix}\\] <p>One-step conditional distributions:</p> <ul> <li>Given \\(X_0 = R\\) (rainy): The first row \\((1/3, 2/3)\\) gives the conditional PMF of \\(X_1\\):</li> <li>\\(P(X_1 = R | X_0 = R) = 1/3\\)</li> <li> <p>\\(P(X_1 = S | X_0 = R) = 2/3\\)</p> </li> <li> <p>Given \\(X_0 = S\\) (sunny): The second row \\((1/2, 1/2)\\) gives the conditional PMF of \\(X_1\\):</p> </li> <li>\\(P(X_1 = R | X_0 = S) = 1/2\\)</li> <li>\\(P(X_1 = S | X_0 = S) = 1/2\\)</li> </ul> <p>Two-step conditional distributions:</p> <p>From our calculated \\(Q^2 = \\begin{pmatrix} 4/9 &amp; 5/9 \\\\ 5/12 &amp; 7/12 \\end{pmatrix}\\):</p> <ul> <li>Given \\(X_0 = R\\): The first row \\((4/9, 5/9)\\) gives the conditional PMF of \\(X_2\\):</li> <li>\\(P(X_2 = R | X_0 = R) = 4/9\\)</li> <li> <p>\\(P(X_2 = S | X_0 = R) = 5/9\\)</p> </li> <li> <p>Given \\(X_0 = S\\): The second row \\((5/12, 7/12)\\) gives the conditional PMF of \\(X_2\\):</p> </li> <li>\\(P(X_2 = R | X_0 = S) = 5/12\\)</li> <li>\\(P(X_2 = S | X_0 = S) = 7/12\\)</li> </ul> <p>Key insight: Each row of \\(Q^n\\) sums to 1, representing a valid probability distribution over the state space, conditioned on the initial state.</p>"},{"location":"math/probability/markov_chains/#marginal-distributions","title":"Marginal Distributions","text":"<p>To get the marginal distributions of \\(X_0, X_1, \\ldots\\), we need to specify not only the transition matrix, but also the initial conditions of the chain. The initial state \\(X_0\\) can be specified deterministically, or randomly according to some distribution. Let \\((t_1, t_2, \\ldots, t_M)\\) be the PMF of \\(X_0\\) displayed as a vector, that is, \\(t_i = P(X_0 = i)\\). Then the marginal distribution of the chain at any time can be computed from the transition matrix, averaging over all the states using LOTP.</p> <p>Important note: The initial distribution vector \\(t\\) is completely independent of the transition matrix \\(Q\\). The vector \\(t\\) specifies how the chain starts (the probabilities of being in each state at time 0), while \\(Q\\) specifies how the chain evolves from one time step to the next. We can choose any initial distribution \\(t\\) we want - it doesn't need to be related to \\(Q\\) in any way. For example, we could start deterministically in state 1 with \\(t = (1, 0, 0, 0)\\), or with any other probability distribution over the four states.</p> <p>Proposition (Marginal distribution of \\(X_n\\)): Define \\(t = (t_1, t_2, \\ldots, t_M)\\) by \\(t_i = P(X_0 = i)\\), and view \\(t\\) as a row vector. Then the marginal distribution of \\(X_n\\) is given by the vector \\(tQ^n\\). That is, the \\(j\\)th component of \\(tQ^n\\) is \\(P(X_n = j)\\).</p> <p>Proof: By the law of total probability, conditioning on \\(X_0\\), the probability that the chain is in state \\(j\\) after \\(n\\) steps is</p> \\[P(X_n = j) = \\sum_{i=1}^{M} P(X_0 = i)P(X_n = j | X_0 = i) = \\sum_{i=1}^{M} t_i q^{(n)}_{ij}\\] <p>which is the \\(j\\)th component of \\(tQ^n\\) by definition of matrix multiplication.</p> <p>Example (Marginal distributions of 4-state Markov chain): Consider the 4-state Markov chain shown in the figure below. </p> <p></p> <p>Suppose that the initial conditions are \\(t = (1/4, 1/4, 1/4, 1/4)\\), meaning that the chain has equal probability of starting in each of the four states. When no probabilities are written over the arrows, as in this case, it means all arrows originating from a given state are equally likely. For example, there are 3 arrows originating from state 1, so the transitions 1-&gt;3, 1-&gt;2, and 1-&gt;1 all have probability 1/3.Let \\(X_n\\) be the position of the chain at time \\(n\\). Then the marginal distribution of \\(X_1\\) is</p> \\[tQ = \\begin{pmatrix} 1/4 &amp; 1/4 &amp; 1/4 &amp; 1/4 \\\\ \\end{pmatrix} \\begin{pmatrix} 1/3 &amp; 1/3 &amp; 1/3 &amp; 0 \\\\ 0 &amp; 0 &amp; 1/2 &amp; 1/2 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1/2 &amp; 0 &amp; 0 &amp; 1/2 \\\\ \\end{pmatrix} = \\begin{pmatrix} 5/24 &amp; 1/3 &amp; 5/24 &amp; 1/4 \\\\ \\end{pmatrix}\\] <p>The marginal distribution of \\(X_5\\) is</p> \\[tQ^5 = \\begin{pmatrix} 1/4 &amp; 1/4 &amp; 1/4 &amp; 1/4 \\\\ \\end{pmatrix} \\begin{pmatrix} 853/3888 &amp; 509/1944 &amp; 52/243 &amp; 395/1296 \\\\ 173/864 &amp; 85/432 &amp; 31/108 &amp; 91/288 \\\\ 37/144 &amp; 29/72 &amp; 1/9 &amp; 11/48 \\\\ 499/2592 &amp; 395/1296 &amp; 71/324 &amp; 245/864 \\\\ \\end{pmatrix} = \\begin{pmatrix} 3379/15552 &amp; 2267/7776 &amp; 101/486 &amp; 1469/5184 \\\\ \\end{pmatrix}\\]"},{"location":"math/probability/multinomial_distribution/","title":"Multinomial Distribution","text":"<p>The multinomial distribution is a generalization of the binomial distribution to multiple categories or outcomes. It describes the probability distribution of counts across several mutually exclusive and exhaustive categories.</p> <p>A random vector \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_k)\\) follows a multinomial distribution with parameters \\(n\\) and \\(\\mathbf{p} = (p_1, p_2, \\ldots, p_k)\\), denoted as \\(\\mathbf{X} \\sim \\text{Multinomial}(n, \\mathbf{p})\\), if its joint probability mass function is:</p> \\[P(X_1 = x_1, X_2 = x_2, \\ldots, X_k = x_k) = \\frac{n!}{x_1! x_2! \\cdots x_k!} p_1^{x_1} p_2^{x_2} \\cdots p_k^{x_k}\\] <p>where:</p> <ul> <li> <p>\\(n\\) is the total number of trials</p> </li> <li> <p>\\(x_i\\) is the number of outcomes in category \\(i\\) (with \\(x_i \\geq 0\\))</p> </li> <li> <p>\\(p_i\\) is the probability of category \\(i\\) (with \\(0 \\leq p_i \\leq 1\\))</p> </li> <li> <p>The constraint \\(\\sum_{i=1}^k x_i = n\\) must hold</p> </li> <li> <p>The constraint \\(\\sum_{i=1}^k p_i = 1\\) must hold</p> </li> </ul> <p>The multinomial distribution models:</p> <ul> <li> <p>\\(n\\) independent trials where each trial results in exactly one of \\(k\\) possible outcomes</p> </li> <li> <p>Fixed probabilities \\(p_1, p_2, \\ldots, p_k\\) for each outcome</p> </li> <li> <p>Counts \\(X_1, X_2, \\ldots, X_k\\) representing how many times each outcome occurred</p> </li> </ul> <p>When \\(k = 2\\), the multinomial distribution reduces to the binomial distribution:</p> <ul> <li> <p>\\(X_1 \\sim \\text{Binomial}(n, p_1)\\)</p> </li> <li> <p>\\(X_2 = n - X_1\\) (since \\(X_1 + X_2 = n\\))</p> </li> <li> <p>\\(p_2 = 1 - p_1\\)</p> </li> </ul>"},{"location":"math/probability/multinomial_distribution/#marginal-distribution","title":"Marginal Distribution","text":"<p>Let's take a look at the marginal distribution of \\(X_i\\), which is the \\(i\\)th component of \\(\\mathbf{X}\\). Were we to blindly apply the definition, we would have to sum the joint PMF over all components of \\(\\mathbf{X}\\) other than \\(X_i\\). The prospect of \\(k-1\\) summations is an unpleasant one, to say the least.</p> <p>Fortunately, we can avoid tedious calculations if we instead use the story of the Multinomial distribution: \\(X_i\\) is the number of objects in category \\(i\\), where each of the \\(n\\) objects independently belongs to category \\(i\\) with probability \\(p_i\\). Define success as landing in category \\(i\\). Then we just have \\(n\\) independent Bernoulli trials, so the marginal distribution of \\(X_i\\) is \\(\\text{Bin}(n, p_i)\\).</p> <p>The marginals of a Multinomial are Binomial. Specifically, if \\(\\mathbf{X} \\sim \\text{Mult}_k(n, \\mathbf{p})\\), then \\(X_i \\sim \\text{Bin}(n, p_i)\\).</p>"},{"location":"math/probability/multinomial_distribution/#merging-categories","title":"Merging Categories","text":"<p>More generally, whenever we merge multiple categories together in a Multinomial random vector, we get another Multinomial random vector. For example, suppose we randomly sample \\(n\\) people in a country with 5 political parties (if the sampling is done without replacement, the \\(n\\) trials are not independent, but independence is a good approximation as long as the population is large relative to the sample). Let \\(\\mathbf{X} = (X_1, \\ldots, X_5) \\sim \\text{Mult}_5(n, (p_1, \\ldots, p_5))\\) represent the political party affiliations of the sample, i.e., \\(X_j\\) is the number of people in the sample who support party \\(j\\).</p> <p>Suppose that parties 1 and 2 are the dominant parties, while parties 3 through 5 are minor third parties. If we decide that instead of keeping track of all 5 parties, we only want to count the number of people in party 1, party 2, or \"other\", then we can define a new random vector that lumps all the third parties into one category:</p> \\[\\mathbf{Y} = (X_1, X_2, X_3 + X_4 + X_5)\\] <p>By the story of the Multinomial,</p> \\[\\mathbf{Y} \\sim \\text{Mult}_3(n, (p_1, p_2, p_3 + p_4 + p_5))\\] <p>Of course, this idea applies to merging categories in any Multinomial, not just in the context of political parties.</p> <p>General Rule: If \\(\\mathbf{X} \\sim \\text{Mult}_k(n, \\mathbf{p})\\), then for any distinct \\(i\\) and \\(j\\), \\(X_i + X_j \\sim \\text{Bin}(n, p_i + p_j)\\). The random vector of counts obtained from merging categories \\(i\\) and \\(j\\) is still Multinomial. For example, merging categories 1 and 2 gives:</p> \\[(X_1 + X_2, X_3, \\ldots, X_k) \\sim \\text{Mult}_{k-1}(n, (p_1 + p_2, p_3, \\ldots, p_k))\\]"},{"location":"math/probability/multinomial_distribution/#conditional-distribution","title":"Conditional Distribution","text":"<p>Suppose we get to observe \\(X_1\\), the number of objects in category 1, and we wish to update our distribution for the other categories \\((X_2, \\ldots, X_k)\\). One way to do this is with the definition of conditional PMF:</p> \\[P(X_2 = n_2, \\ldots, X_k = n_k|X_1 = n_1) = \\frac{P(X_1 = n_1, X_2 = n_2, \\ldots, X_k = n_k)}{P(X_1 = n_1)}\\] <p>The numerator is the joint PMF of the Multinomial, and the denominator is the marginal PMF of \\(X_1\\), both of which we have already derived. However, we prefer to use the Multinomial story to deduce the conditional distribution of \\((X_2, \\ldots, X_k)\\) without algebra.</p> <p>Given that there are \\(n_1\\) objects in category 1, the remaining \\(n - n_1\\) objects fall into categories 2 through \\(k\\), independently of one another. By Bayes' rule (Bayes' rule: \\(P(\\text{in category } j|\\text{not in category } 1) = \\frac{P(\\text{in category } j \\text{ AND not in category } 1)}{P(\\text{not in category } 1)}\\)), the conditional probability of falling into category \\(j\\) is:</p> \\[P(\\text{in category } j|\\text{not in category } 1) = \\frac{P(\\text{in category } j)}{P(\\text{not in category } 1)} = \\frac{p_j}{p_2 + \\cdots + p_k}\\] <p>for \\(j = 2, \\ldots, k\\). This makes intuitive sense: the updated probabilities are proportional to the original probabilities \\((p_2, \\ldots, p_k)\\), but these must be renormalized to yield a valid probability vector.</p> <p>Putting it all together, we have the following result:</p> <p>If \\(\\mathbf{X} \\sim \\text{Mult}_k(n, \\mathbf{p})\\), then</p> \\[(X_2, \\ldots, X_k)|X_1 = n_1 \\sim \\text{Mult}_{k-1}(n - n_1, (p'_2, \\ldots, p'_k))\\] <p>where \\(p'_j = \\frac{p_j}{p_2 + \\cdots + p_k}\\).</p> <p>Finally, we know that components within a Multinomial random vector are dependent since they are constrained by \\(X_1 + \\cdots + X_k = n\\).</p>"},{"location":"math/probability/multivariate_normal_distribution/","title":"Multivariate Normal Distribution","text":"<p>The multivariate normal distribution is a generalization of the univariate normal distribution to multiple dimensions. It's one of the most important distributions in statistics and machine learning, serving as the foundation for many statistical methods and models.</p>"},{"location":"math/probability/multivariate_normal_distribution/#definition","title":"Definition","text":"<p>A random vector \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_d)^T\\) follows a multivariate normal distribution with mean vector \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^d\\) and covariance matrix \\(\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}\\) (denoted as \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)) if its probability density function is:</p> \\[f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right)\\] <p>where:</p> <ul> <li> <p>\\(\\mathbf{x} = (x_1, x_2, \\ldots, x_d)^T\\) is a \\(d\\)-dimensional vector</p> </li> <li> <p>\\(\\boldsymbol{\\mu} = (\\mu_1, \\mu_2, \\ldots, \\mu_d)^T\\) is the mean vector</p> </li> <li> <p>\\(\\boldsymbol{\\Sigma}\\) is the covariance matrix</p> </li> <li> <p>\\(|\\boldsymbol{\\Sigma}|\\) denotes the determinant of \\(\\boldsymbol{\\Sigma}\\)</p> </li> </ul> <p>Important: This PDF \\(f(\\mathbf{x})\\) is the joint probability density function of the random variables \\(X_1, X_2, \\ldots, X_d\\) in the vector. That is, \\(f(x_1, x_2, \\ldots, x_d)\\) gives the joint density of all \\(d\\) random variables simultaneously.</p> <p>Alternative Definition</p> <p>A random vector \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_d)^T\\) follows a multivariate normal distribution if and only if every linear combination of its components is univariate normal. That is, for any constants \\(t_1, t_2, \\ldots, t_d \\in \\mathbb{R}\\) (not all zero), the random variable:</p> \\[Y = t_1X_1 + t_2X_2 + \\cdots + t_dX_d\\] <p>follows a univariate normal distribution.</p> <p>Mean Vector \\(\\boldsymbol{\\mu}\\):</p> <p>The mean vector contains the expected values of each component:</p> \\[\\boldsymbol{\\mu} = E[\\mathbf{X}] = (E[X_1], E[X_2], \\ldots, E[X_d])^T\\] <p>Covariance Matrix \\(\\boldsymbol{\\Sigma}\\):</p> <p>The covariance matrix captures the relationships between variables:</p> \\[\\boldsymbol{\\Sigma}_{ij} = \\text{Cov}(X_i, X_j) = E[(X_i - \\mu_i)(X_j - \\mu_j)]\\] <p>Properties of \\(\\boldsymbol{\\Sigma}\\):</p> <ul> <li> <p>Symmetric: \\(\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}^T\\)</p> </li> <li> <p>Positive definite: \\(\\mathbf{v}^T \\boldsymbol{\\Sigma} \\mathbf{v} &gt; 0\\) for all non-zero vectors \\(\\mathbf{v}\\)</p> </li> <li> <p>Diagonal elements: \\(\\boldsymbol{\\Sigma}_{ii} = \\text{Var}(X_i) = \\sigma_i^2\\)</p> </li> </ul>"},{"location":"math/probability/multivariate_normal_distribution/#properties","title":"Properties","text":"<p>1. Linear Transformations</p> <p>If \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) and \\(\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}\\), then:</p> \\[\\mathbf{Y} \\sim \\mathcal{N}(\\mathbf{A}\\boldsymbol{\\mu} + \\mathbf{b}, \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^T)\\] <p>2. Marginal Distributions</p> <p>Any subset of components follows a multivariate normal distribution. For example, if \\(\\mathbf{X} = (X_1, X_2, X_3)^T \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), then:</p> \\[(X_1, X_2)^T \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2 \\end{pmatrix}\\right)\\]"},{"location":"math/probability/multivariate_normal_distribution/#case-study-bivariate-normal-distribution-d-2","title":"Case study: Bivariate Normal Distribution (\\(d = 2\\))","text":"<p>For two variables \\(X_1\\) and \\(X_2\\):</p> \\[\\mathbf{X} = \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2 \\end{pmatrix}\\right)\\] <p>The joint density becomes:</p> \\[f(x_1, x_2) = \\frac{1}{2\\pi\\sqrt{\\sigma_1^2\\sigma_2^2 - \\sigma_{12}^2}} \\exp\\left(-\\frac{1}{2(\\sigma_1^2\\sigma_2^2 - \\sigma_{12}^2)}\\left[\\sigma_2^2(x_1-\\mu_1)^2 - 2\\sigma_{12}(x_1-\\mu_1)(x_2-\\mu_2) + \\sigma_1^2(x_2-\\mu_2)^2\\right]\\right)\\] <p>Note: The determinant of the covariance matrix is \\(|\\boldsymbol{\\Sigma}| = \\sigma_1^2\\sigma_2^2 - \\sigma_{12}^2\\), and the inverse covariance matrix is:</p> \\[\\boldsymbol{\\Sigma}^{-1} = \\frac{1}{\\sigma_1^2\\sigma_2^2 - \\sigma_{12}^2}\\begin{pmatrix} \\sigma_2^2 &amp; -\\sigma_{12} \\\\ -\\sigma_{12} &amp; \\sigma_1^2 \\end{pmatrix}\\] <p></p>"},{"location":"math/probability/normal_distribution/","title":"Normal Distribution","text":"<p>The Normal distribution (also called the Gaussian distribution) is one of the most important continuous distributions in probability and statistics. It appears naturally in many contexts due to the Central Limit Theorem and provides a foundation for many statistical methods.</p>"},{"location":"math/probability/normal_distribution/#standard-normal-distribution","title":"Standard Normal Distribution","text":"<p>A continuous random variable \\(Z\\) is said to have the standard Normal distribution if its PDF \\(f_Z\\) is given by:</p> \\[f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}, \\quad -\\infty &lt; z &lt; \\infty\\] <p>We write this as \\(Z \\sim N(0, 1)\\) since, as we will show, \\(Z\\) has mean 0 and variance 1.</p> <p>The constant \\(\\frac{1}{\\sqrt{2\\pi}}\\) in front of the PDF may look surprising (why is something with \\(\\pi\\) needed in front of something with \\(e\\), when there are no circles in sight?), but it's exactly what is needed to make the PDF integrate to 1. Such constants are called normalizing constants because they normalize the total area under the PDF to 1.</p> <p>By symmetry, the mean of the standard normal distribution is 0. Here's why:</p> <p>The standard normal PDF is symmetric about 0:</p> \\[f_Z(z) = f_Z(-z) \\quad \\text{for all } z\\] <p>This means the distribution looks the same on both sides of 0. The mean is defined as:</p> \\[E[Z] = \\int_{-\\infty}^{\\infty} z \\cdot f_Z(z) \\, dz\\] <p>Let's split this integral into two parts:</p> \\[E[Z] = \\int_{-\\infty}^0 z \\cdot f_Z(z) \\, dz + \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz\\] <p>First integral (\\(-\\infty\\) to 0): Let \\(u = -z\\), so \\(z = -u\\) and \\(dz = -du\\)</p> \\[\\int_{-\\infty}^0 z \\cdot f_Z(z) \\, dz = \\int_{\\infty}^0 (-u) \\cdot f_Z(-u) \\cdot (-du) = \\int_0^{\\infty} u \\cdot f_Z(u) \\, du\\] <p>Second integral (0 to \\(\\infty\\)): This is already in the right form</p> \\[\\int_0^{\\infty} z \\cdot f_Z(z) \\, dz\\] \\[E[Z] = \\int_0^{\\infty} u \\cdot f_Z(u) \\, du + \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz\\] <p>Since \\(u\\) and \\(z\\) are just dummy variables, we can write this as:</p> \\[E[Z] = \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz + \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz = 2 \\int_0^{\\infty} z \\cdot f_Z(z) \\, dz\\] <p>The integrand \\(z \\cdot f_Z(z)\\) is odd because:</p> <ul> <li> <p>\\(f_Z(z) = f_Z(-z)\\) (even function)</p> </li> <li> <p>\\(z\\) is odd</p> </li> <li> <p>Product of even and odd functions is odd</p> </li> </ul> <p>For odd functions, the integral from \\(-\\infty\\) to \\(\\infty\\) equals 0.</p> <p>Therefore, \\(E[Z] = 0\\).</p> <p>Now let's show that the variance of the standard normal distribution is 1. The variance is defined as:</p> \\[\\text{Var}(Z) = E[(Z - E[Z])^2] = E[Z^2]\\] <p>Since we already showed that \\(E[Z] = 0\\), we have \\(\\text{Var}(Z) = E[Z^2]\\).</p> <p>Calculating \\(E[Z^2]\\):</p> \\[E[Z^2] = \\int_{-\\infty}^{\\infty} z^2 \\cdot f_Z(z) \\, dz = \\int_{-\\infty}^{\\infty} z^2 \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz\\] <p>Let's use integration by parts with:</p> <ul> <li> <p>\\(u = z\\) and \\(dv = z \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz\\)</p> </li> <li> <p>\\(du = dz\\) and \\(v = -\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\)</p> </li> </ul> <p>Integration by parts formula: \\(\\int u \\, dv = uv - \\int v \\, du\\)</p> \\[E[Z^2] = \\left[z \\cdot \\left(-\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\right)\\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} \\left(-\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\right) \\, dz\\] <p>The boundary term evaluates to 0:</p> <ul> <li> <p>At \\(z = \\infty\\): \\(z \\cdot e^{-z^2/2} \\to 0\\) (exponential decay dominates)</p> </li> <li> <p>At \\(z = -\\infty\\): \\(z \\cdot e^{-z^2/2} \\to 0\\) (exponential decay dominates)</p> </li> </ul> <p>Therefore:</p> \\[E[Z^2] = 0 - \\int_{-\\infty}^{\\infty} \\left(-\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\right) \\, dz = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz\\] <p>The remaining integral is exactly the integral of the PDF from \\(-\\infty\\) to \\(\\infty\\), which equals 1:</p> \\[E[Z^2] = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz = 1\\] <p>Therefore, \\(\\text{Var}(Z) = E[Z^2] = 1\\).</p> <p>The standard Normal CDF \\(\\Phi\\) is the accumulated area under the PDF:</p> \\[\\Phi(z) = \\int_{-\\infty}^z f_Z(t) \\, dt = \\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}} e^{-t^2/2} \\, dt\\] <p>Some people, upon seeing the function \\(\\Phi\\) for the first time, express dismay that it is left in terms of an integral. Unfortunately, we have little choice in the matter: it turns out to be mathematically impossible to find a closed-form expression for the antiderivative of \\(f_Z\\), meaning that we cannot express \\(\\Phi\\) as a finite sum of more familiar functions like polynomials or exponentials. But closed-form or no, it's still a well-defined function: if we give \\(\\Phi\\) an input \\(z\\), it returns the accumulated area under the PDF from \\(-\\infty\\) up to \\(z\\).</p>"},{"location":"math/probability/normal_distribution/#general-normal-distribution","title":"General Normal Distribution","text":"<p>The general Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) (or variance \\(\\sigma^2\\)) is obtained by applying a linear transformation to the standard normal distribution. If \\(Z \\sim N(0, 1)\\) is a standard normal random variable, then:</p> \\[X = \\mu + \\sigma Z\\] <p>has a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). We write this as \\(X \\sim N(\\mu, \\sigma^2)\\).</p> <p>Mean and Variance: </p> <ul> <li> <p>\\(E[X] = E[\\mu + \\sigma Z] = \\mu + \\sigma E[Z] = \\mu + \\sigma \\cdot 0 = \\mu\\)</p> </li> <li> <p>\\(\\text{Var}(X) = \\text{Var}(\\mu + \\sigma Z) = \\sigma^2 \\text{Var}(Z) = \\sigma^2 \\cdot 1 = \\sigma^2\\)</p> </li> </ul> <p>CDF: The CDF of \\(X \\sim N(\\mu, \\sigma^2)\\) is:</p> \\[F_X(x) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\\] <p>where \\(\\Phi\\) is the standard normal CDF.</p> <p>Derivation of the CDF: Since \\(X = \\mu + \\sigma Z\\), we can find the CDF by:</p> \\[F_X(x) = P(X \\leq x) = P(\\mu + \\sigma Z \\leq x) = P\\left(Z \\leq \\frac{x - \\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\\] <p>PDF: The PDF of \\(X \\sim N(\\mu, \\sigma^2)\\) is:</p> \\[f_X(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\quad -\\infty &lt; x &lt; \\infty\\] <p>Derivation of the PDF from the CDF: We can derive the PDF by taking the derivative of the CDF with respect to \\(x\\):</p> \\[f_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx} \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\\] <p>Using the chain rule and the fact that \\(\\frac{d}{dz} \\Phi(z) = \\phi(z)\\) (where \\(\\phi(z)\\) is the standard normal PDF):</p> \\[f_X(x) = \\phi\\left(\\frac{x - \\mu}{\\sigma}\\right) \\cdot \\frac{d}{dx}\\left(\\frac{x - \\mu}{\\sigma}\\right)\\] <p>Since \\(\\frac{d}{dx}\\left(\\frac{x - \\mu}{\\sigma}\\right) = \\frac{1}{\\sigma}\\), we have:</p> \\[f_X(x) = \\phi\\left(\\frac{x - \\mu}{\\sigma}\\right) \\cdot \\frac{1}{\\sigma}\\] <p>Substituting the standard normal PDF \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\):</p> \\[f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\cdot \\frac{1}{\\sigma} = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] <p>Standardization: Any normal random variable \\(X \\sim N(\\mu, \\sigma^2)\\) can be standardized to a standard normal by:</p> \\[Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\] <p>This is the inverse of the transformation \\(X = \\mu + \\sigma Z\\).</p>"},{"location":"math/probability/poisson_distribution/","title":"Poisson Distribution","text":"<p>The Poisson distribution is one of the most important discrete probability distributions, modeling the number of events occurring in a fixed interval of time or space when these events happen independently at a constant average rate.</p> <p>This is arguably the most important discrete distribution in Statistics.</p> <p>A random variable \\(X\\) follows a Poisson distribution with parameter \\(\\lambda &gt; 0\\) (denoted \\(X \\sim \\text{Poisson}(\\lambda)\\)) if its probability mass function is:</p> \\[P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\quad \\text{for } k = 0, 1, 2, 3, \\ldots\\] <p>Parameters:</p> <ul> <li> <p>\\(\\lambda\\): Average number of events in the interval (also called the rate parameter)</p> </li> <li> <p>Support: \\(X\\) takes values in \\(\\{0, 1, 2, 3, \\ldots\\}\\) (non-negative integers)</p> </li> </ul> <p>What does the Poisson distribution model?</p> <ul> <li> <p>Rare events: Events that occur infrequently but consistently</p> </li> <li> <p>Independent occurrences: Each event is independent of others</p> </li> <li> <p>Constant rate: Events happen at a steady average rate</p> </li> <li> <p>Fixed interval: We count events in a specific time period or region</p> </li> </ul> <p>Examples:</p> <ul> <li> <p>Number of phone calls received in an hour</p> </li> <li> <p>Number of defects in a square meter of fabric</p> </li> <li> <p>Number of accidents at an intersection per day</p> </li> <li> <p>Number of customers arriving at a store in 10 minutes</p> </li> </ul> <p>The Poisson distribution can be derived as a limiting case of the binomial distribution.</p> <p>Setup: Consider \\(n\\) independent Bernoulli trials, each with success probability \\(p = \\frac{\\lambda}{n}\\)</p> <p>Binomial PMF: \\(P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k}\\)</p> <p>Take the limit: As \\(n \\to \\infty\\) while keeping \\(np = \\lambda\\) constant</p> <p>Result: The binomial PMF converges to the Poisson PMF</p> <p>Mathematical details:</p> \\[\\lim_{n \\to \\infty} \\binom{n}{k} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1-\\frac{\\lambda}{n}\\right)^{n-k} = \\frac{e^{-\\lambda} \\lambda^k}{k!}\\] <p>Proof: Let's prove this step by step.</p> <p>Start with the binomial PMF:</p> \\[P(X = k) = \\binom{n}{k} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\] <p>Expand the binomial coefficient:</p> \\[\\binom{n}{k} = \\frac{n!}{k!(n-k)!} = \\frac{n(n-1)(n-2)\\cdots(n-k+1)}{k!}\\] <p>Substitute and rearrange:</p> \\[P(X = k) = \\frac{n(n-1)(n-2)\\cdots(n-k+1)}{k!} \\cdot \\left(\\frac{\\lambda}{n}\\right)^k \\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\] \\[= \\frac{\\lambda^k}{k!} \\cdot \\frac{n(n-1)(n-2)\\cdots(n-k+1)}{n^k} \\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\] <p>Analyze this factor as \\(n \\to \\infty\\):</p> \\[\\frac{n(n-1)(n-2)\\cdots(n-k+1)}{n^k} = \\frac{n}{n} \\cdot \\frac{n-1}{n} \\cdot \\frac{n-2}{n} \\cdots \\frac{n-k+1}{n}\\] \\[= 1 \\cdot \\left(1-\\frac{1}{n}\\right) \\cdot \\left(1-\\frac{2}{n}\\right) \\cdots \\left(1-\\frac{k-1}{n}\\right)\\] <p>As \\(n \\to \\infty\\), each term \\(\\left(1-\\frac{j}{n}\\right) \\to 1\\) for \\(j = 1, 2, \\ldots, k-1\\).</p> <p>Therefore:</p> \\[\\lim_{n \\to \\infty} \\frac{n(n-1)(n-2)\\cdots(n-k+1)}{n^k} = 1\\] <p>Analyze the last factor as \\(n \\to \\infty\\):</p> \\[\\left(1-\\frac{\\lambda}{n}\\right)^{n-k} = \\left(1-\\frac{\\lambda}{n}\\right)^n \\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}\\] <p>As \\(n \\to \\infty\\):</p> <ul> <li> <p>\\(\\left(1-\\frac{\\lambda}{n}\\right)^n \\to e^{-\\lambda}\\) (this is the definition of \\(e\\))</p> </li> <li> <p>\\(\\left(1-\\frac{\\lambda}{n}\\right)^{-k} \\to 1^{-k} = 1\\)</p> </li> </ul> <p>Combine all limits:</p> \\[\\lim_{n \\to \\infty} P(X = k) = \\frac{\\lambda^k}{k!} \\cdot 1 \\cdot e^{-\\lambda} = \\frac{e^{-\\lambda} \\lambda^k}{k!}\\] <p>Intuition: When we have many rare events (large \\(n\\), small \\(p\\)), the binomial distribution becomes approximately Poisson.</p> <p>Example: A student types a 1000-word essay and makes an average of 2 typos per 1000 words. What's the probability of making exactly 3 typos?</p> <p>Solution:</p> <ul> <li> <p>Rate: \\(\\lambda = 2\\) typos per 1000 words</p> </li> <li> <p>Random variable: \\(X \\sim \\text{Poisson}(2)\\)</p> </li> <li> <p>Probability: \\(P(X = 3) = \\frac{e^{-2} \\cdot 2^3}{3!} = \\frac{e^{-2} \\cdot 8}{6} \\approx 0.180\\)</p> </li> </ul> <p>Example: A coffee shop serves an average of 5 customers every 15 minutes. What's the probability of serving at least 2 customers in the next 15 minutes?</p> <p>Solution:</p> <ul> <li> <p>Rate: \\(\\lambda = 5\\) customers per 15 minutes</p> </li> <li> <p>Random variable: \\(X \\sim \\text{Poisson}(5)\\)</p> </li> <li> <p>Probability: \\(P(X \\geq 2) = 1 - P(X = 0) - P(X = 1)\\)</p> </li> </ul> <p>First, calculate individual probabilities:</p> <ul> <li> <p>\\(P(X = 0) = \\frac{e^{-5} \\cdot 5^0}{0!} = e^{-5} \\approx 0.0067\\)</p> </li> <li> <p>\\(P(X = 1) = \\frac{e^{-5} \\cdot 5^1}{1!} = 5e^{-5} \\approx 0.0337\\)</p> </li> </ul> <p>Therefore:</p> <ul> <li>\\(P(X \\geq 2) = 1 - 0.0067 - 0.0337 = 0.9596\\)</li> </ul> <p>Note: </p> <ul> <li> <p>\\(\\lambda\\) is the average rate of events occurring</p> </li> <li> <p>Expectation represents the long-run average of the random variable</p> </li> <li> <p>In a Poisson process, we expect \\(\\lambda\\) events per unit time or space on average</p> </li> </ul> <p>Let's see if  \\(E[X] = \\lambda\\) is the case.</p> <p>Start with the definition of expectation for a discrete random variable:</p> \\[E[X] = \\sum_{k=0}^{\\infty} k \\cdot P(X = k) = \\sum_{k=0}^{\\infty} k \\cdot \\frac{e^{-\\lambda} \\lambda^k}{k!}\\] <p>Notice that the first term (\\(k = 0\\)) is 0, so we can start from \\(k = 1\\):</p> \\[E[X] = \\sum_{k=1}^{\\infty} k \\cdot \\frac{e^{-\\lambda} \\lambda^k}{k!}\\] <p>Cancel the \\(k\\):</p> \\[E[X] = \\sum_{k=1}^{\\infty} \\frac{e^{-\\lambda} \\lambda^k}{(k-1)!}\\] <p>Factor out \\(e^{-\\lambda}\\) and \\(\\lambda\\):</p> \\[E[X] = e^{-\\lambda} \\lambda \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}\\] <p>Make a change of variable: let \\(j = k-1\\). Then \\(k = j+1\\) and when \\(k = 1\\), \\(j = 0\\):</p> \\[E[X] = e^{-\\lambda} \\lambda \\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!}\\] <p>Recognize the series as the Taylor series for \\(e^{\\lambda}\\):</p> \\[\\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!} = e^{\\lambda}\\] <p>Substitute and simplify:</p> \\[E[X] = e^{-\\lambda} \\lambda \\cdot e^{\\lambda} = \\lambda\\] <p>Final result: \\(E[X] = \\lambda\\)</p> <p>Let's verify that the Poisson PMF satisfies all required properties:</p> <p>Property 1: Non-negativity</p> <p>\\(P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\geq 0\\) for all \\(k \\geq 0\\) since:</p> <ul> <li> <p>\\(e^{-\\lambda} &gt; 0\\)</p> </li> <li> <p>\\(\\lambda^k \\geq 0\\) for \\(\\lambda &gt; 0\\)</p> </li> <li> <p>\\(k! &gt; 0\\) for all \\(k \\geq 0\\)</p> </li> </ul> <p>Property 2: Sum to 1</p> \\[\\sum_{k=0}^{\\infty} P(X = k) = \\sum_{k=0}^{\\infty} \\frac{e^{-\\lambda} \\lambda^k}{k!} = e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} = e^{-\\lambda} \\cdot e^{\\lambda} = 1\\] <p>The key insight is that \\(\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} = e^{\\lambda}\\) (the Taylor series for \\(e^{\\lambda}\\)).</p> <p>Property 3: Probability bounds</p> <p>\\(0 \\leq P(X = k) \\leq 1\\) for each \\(k\\), which follows from Properties 1 and 2.</p>"},{"location":"math/probability/probability_and_counting/","title":"Probability and counting","text":""},{"location":"math/probability/probability_and_counting/#sample-spaces","title":"Sample spaces","text":"<p>The mathematical framework for probability is built around sets. Imagine that an experiment is performed, resulting in one out of a set of possible outcomes. Before the experiment is performed, it is unknown which outcome will be the result; after, the result \"crystallizes\" into the actual outcome. The sample space S of an experiment is the set of all possible outcomes of the experiment. An event A is a subset of the sample space S, and we say that A occurred if the actual outcome is in A.  When the sample space is finite, we can visualize it as Pebble World (figure above). Each pebble represents an outcome, and an event is a set of pebbles. Performing the experiment amounts to randomly selecting one pebble. If all the pebbles are of the same mass, all the pebbles are equally likely to be chosen.</p> <p>Set theory is very useful in probability, since it provides a rich language for expressing and working with events. Set operations, especially unions, intersections, and complements, make it easy to build new events in terms of already defined events.</p> <p>Example:  A coin is flipped 10 times. Writing Heads as H and Tails as T, a possible outcome is: HHHTHHTTHT. The sample space is the set of all possible strings of length 10 consisting of H's and T's. We can (and will) encode H as <code>1</code> and T as <code>0</code>, so that an outcome is a sequence: $$ (s_1, s_2, \\dots, s_{10}) \\quad \\text{with} \\quad s_j \\in {0, 1} $$ The sample space is the set of all such sequences.</p> <p>Some events:</p> <ol> <li>Event A_1: the first flip is Heads. As a set: $$ A_1 =  (1, s_2, \\dots, s_{10}) \\; \\mid \\; s_j \\in {0,1} \\; \\text{and } 2 \\leq j \\leq 10  $$ This is a subset of the sample space, so it is indeed an event. Saying that A_1 occurs is equivalent to saying that the first flip is Heads. Similarly, let A_j be the event that the j-th flip is Heads, for: $$ j = 2, 3, \\dots, 10 $$</li> <li>Event B: at least one flip was Heads. As a set: $$ B = \\bigcup_{j=1}^{10} A_j $$</li> <li>Event C: all the flips were Heads. As a set: $$ C = \\bigcap_{j=1}^{10} A_j $$</li> <li>Event D: there were at least two consecutive Heads. As a set: $$ D = \\bigcup_{j=1}^{9} \\left( A_j \\cap A_{j+1} \\right) $$ </li> </ol>"},{"location":"math/probability/probability_and_counting/#naive-definition-of-probability","title":"Naive definition of probability","text":"<p>(Naive definition of probability). Let \\( A \\) be an event for an experiment with a finite sample space \\( S \\). The naive probability of \\( A \\) is</p> \\[ P_{\\text{naive}}(A) = \\frac{|A|}{|S|} = \\frac{\\text{number of outcomes favorable to } A}{\\text{total number of outcomes in } S} \\] <p>The naive definition is very restrictive in that it requires S to be finite, with equal mass for each pebble (all outcomes equally likely). It has often been misapplied by people who assume equally likely outcomes without justification and make arguments to the e\ufb00ect of \u201ceither it will happen or it won\u2019t, and we don\u2019t know which, so it\u2019s 50-50\u201d.</p>"},{"location":"math/probability/probability_and_counting/#counting","title":"Counting","text":"<p>The multiplication rule is a fundamental principle in counting that allows us to count the number of possible outcomes of a sequence of experiments.</p> <p>Multiplication Rule: If Experiment 1 has \\(n_1\\) possible outcomes, Experiment 2 has \\(n_2\\) possible outcomes, ..., and Experiment \\(r\\) has \\(n_r\\) possible outcomes, then the sequence of all \\(r\\) experiments has \\(n_1 \\times n_2 \\times \\cdots \\times n_r\\) possible outcomes.</p> <p>Example: A license plate consists of 3 letters followed by 3 digits. How many different license plates are possible?</p> <ul> <li>Experiment 1: Choose the first letter (26 outcomes)</li> <li>Experiment 2: Choose the second letter (26 outcomes) </li> <li>Experiment 3: Choose the third letter (26 outcomes)</li> <li>Experiment 4: Choose the first digit (10 outcomes)</li> <li>Experiment 5: Choose the second digit (10 outcomes)</li> <li>Experiment 6: Choose the third digit (10 outcomes)</li> </ul> <p>By the multiplication rule, the total number of possible license plates is: \\(26 \\times 26 \\times 26 \\times 10 \\times 10 \\times 10 = 26^3 \\times 10^3 = 17,576,000\\)</p> <p>Key insight: The multiplication rule applies when the experiments are independent- the number of possible outcomes of each experiment doesn't depend on the outcomes of previous experiments.</p> <p>Example: Probability of a Full House in Poker</p> <p>A full house in poker consists of three cards of one rank and two cards of another rank (e.g., three Kings and two 7s). Let's calculate the probability of being dealt a full house in a 5-card hand from a standard 52-card deck.</p> <p>Step 1: Count the total number of possible 5-card hands - This is the number of ways to choose 5 cards from 52 cards - Total hands = \\(\\binom{52}{5} = \\frac{52!}{5!(52-5)!} = \\frac{52!}{5! \\cdot 47!} = 2,598,960\\)</p> <p>Step 2: Count the number of full house hands We can break this down using the multiplication rule:</p> <ol> <li>Choose the rank for the three-of-a-kind: 13 choices (A, 2, 3, ..., 10, J, Q, K)</li> <li>Choose 3 cards of that rank: \\(\\binom{4}{3} = 4\\) ways (since there are 4 cards of each rank)</li> <li>Choose a different rank for the pair: 12 choices (must be different from the three-of-a-kind rank)</li> <li>Choose 2 cards of that rank: \\(\\binom{4}{2} = 6\\) ways</li> </ol> <p>By the multiplication rule, the number of full house hands is: \\(13 \\times 4 \\times 12 \\times 6 = 3,744\\)</p> <p>Step 3: Calculate the probability Using the naive definition of probability:</p> \\[P(\\text{Full House}) = \\frac{\\text{Number of full house hands}}{\\text{Total number of hands}} = \\frac{3,744}{2,598,960} = \\frac{6}{4,165} \\approx 0.00144\\] <p>So the probability of being dealt a full house is approximately 0.144%, or about 1 in 694 hands.</p>"},{"location":"math/probability/probability_and_counting/#sampling-choosing-k-items-from-n-items","title":"Sampling: Choosing \\(k\\) Items from \\(n\\) Items","text":"<p>When we want to choose \\(k\\) items from a set of \\(n\\) items, there are four fundamental cases depending on whether:</p> <ol> <li> <p>Replacement: Can we choose the same item multiple times?</p> </li> <li> <p>Order: Does the order of selection matter?</p> </li> </ol> <p>This gives us four distinct sampling scenarios:</p> <p>Case 1: Sampling with Replacement, Order Matters</p> <ul> <li> <p>Description: Choose \\(k\\) items from \\(n\\) items, allowing repeats, where order matters</p> </li> <li> <p>Example: Rolling a die 3 times (can get same number multiple times, order matters)</p> </li> <li> <p>Formula: \\(n^k\\)</p> </li> <li> <p>Derivation: By the multiplication rule, each of the \\(k\\) choices has \\(n\\) possible outcomes, so total outcomes = \\(n \\times n \\times \\cdots \\times n = n^k\\)</p> </li> </ul> <p>Case 2: Sampling without Replacement, Order Matters</p> <ul> <li> <p>Description: Choose \\(k\\) items from \\(n\\) items, no repeats, where order matters</p> </li> <li> <p>Example: Choosing 3 people from 10 to be president, vice-president, and treasurer</p> </li> <li> <p>Formula: \\(P(n,k) = \\frac{n!}{(n-k)!} = n \\times (n-1) \\times (n-2) \\times \\cdots \\times (n-k+1)\\)</p> </li> <li> <p>Derivation: First choice has \\(n\\) options, second choice has \\((n-1)\\) options (can't repeat), third choice has \\((n-2)\\) options, etc. By multiplication rule: \\(n \\times (n-1) \\times (n-2) \\times \\cdots \\times (n-k+1) = \\frac{n!}{(n-k)!}\\)</p> </li> </ul> <p>Case 3: Sampling without Replacement, Order Doesn't Matter</p> <ul> <li> <p>Description: Choose \\(k\\) items from \\(n\\) items, no repeats, where order doesn't matter</p> </li> <li> <p>Example: Choosing 3 people from 10 to be on a committee</p> </li> <li> <p>Formula: \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)</p> </li> <li> <p>Derivation: This is the number of combinations. We start with Case 2- (\\(P(n,k)\\)) and divide by \\(k!\\) because each group of \\(k\\) items can be arranged in \\(k!\\) different orders, but we only want to count each group once.</p> </li> </ul> <p>Case 4: Sampling with Replacement, Order Doesn't Matter</p> <ul> <li> <p>Description: Choose \\(k\\) items from \\(n\\) items, allowing repeats, where order doesn't matter</p> </li> <li> <p>Example: Choosing 3 scoops of ice cream from 4 flavors, where you can get multiple scoops of the same flavor (e.g., 2 chocolate and 1 vanilla)</p> </li> <li> <p>Formula: \\(\\binom{n+k-1}{k} = \\binom{n+k-1}{n-1}\\)</p> </li> <li> <p>Derivation: Let's derive the formula using a visual approach. Imagine we have 3 scoops (represented by x's) and 4 flavors (separated by dividers |). For example, xx|x|| means 2 scoops of flavor 1, 1 scoop of flavor 2, and 0 scoops of flavors 3 and 4. We need to arrange 3 x's and 3 dividers (to create 4 sections) in a total of 6 positions. Now, in these 6 positions, we place 3 scoops. Once the 3 scoops (x) are placed, the dividers (|) are automatically determined. A few examples to illustrate this:</p> </li> <li> <p>xxx||| means 3 scoops of flavor 1, 0 scoops of flavors 2, 3, and 4</p> </li> <li>x|x|x| means 1 scoop of each flavor and 0 scoops of flavor 4</li> <li>||xxx| means 0 scoops of flavors 1 and 2, 3 scoops of flavor 3, 0 scoops of flavor 4</li> </ul> <p>The key insight is that we're choosing 3 positions out of 6 total positions to place our scoops (x's). The number of ways to do this is \\(\\binom{6}{3} = 20\\). </p> <p>In general, for \\(k\\) scoops and \\(n\\) flavors, we have \\((k + n - 1)\\) total positions and we need to choose \\(k\\) positions for the scoops. This gives us \\(\\binom{k + n - 1}{k} = \\binom{n + k - 1}{k}\\).</p> <p>Summary Table:</p> Replacement Order Matters Formula Name With Yes \\(n^k\\) Permutations with replacement Without Yes \\(\\frac{n!}{(n-k)!}\\) Permutations without replacement Without No \\(\\frac{n!}{k!(n-k)!}\\) Combinations With No \\(\\binom{n+k-1}{k}\\) Combinations with replacement <p>Example: Splitting 10 People into teams</p> <p>Case 1: Distinguishable teams (Team A and Team B)</p> <p>Suppose we want to split 10 people into two teams: Team A with 6 people and Team B with 4 people.</p> <p>Question: How many ways can we do this?</p> <p>Solution: We need to choose 6 people out of 10 for Team A. The remaining 4 people automatically go to Team B. The number of ways is:</p> \\[\\binom{10}{6} = \\frac{10!}{6! \\cdot 4!} = \\frac{10 \\times 9 \\times 8 \\times 7}{4 \\times 3 \\times 2 \\times 1} = 210\\] <p>Case 2: Indistinguishable teams</p> <p>Now suppose we want to split 10 people into two teams of 5, but the teams are indistinguishable (no Team A vs Team B).</p> <p>Question: How many ways can we do this?</p> <p>Solution: Since the teams are indistinguishable, we've double-counted each arrangement. For example, the arrangement where people {1,2,3,4,5} are on one team and {6,7,8,9,10} are on the other is the same as the arrangement where {6,7,8,9,10} are on one team and {1,2,3,4,5} are on the other.</p> <p>The number of ways is:</p> \\[\\frac{\\binom{10}{6}}{2} = \\frac{210}{2} = 105\\] <p>For unequal-sized groups (like 6 and 4), the two sides of the split are inherently distinguishable due to their sizes.</p>"},{"location":"math/probability/random_variables/","title":"Random Variables","text":""},{"location":"math/probability/random_variables/#definition-of-a-random-variable","title":"Definition of a Random Variable","text":"<p>A random variable is a function that maps outcomes from a sample space to real numbers. Formally, if \\(S\\) is a sample space, then a random variable \\(X\\) is a function:</p> \\[X: S \\rightarrow \\mathbb{R}\\] <p>Example 1: Coin toss</p> <ul> <li> <p>Sample space: \\(S = \\{\\text{Heads}, \\text{Tails}\\}\\)</p> </li> <li> <p>Random variable: \\(X(\\text{Heads}) = 1\\), \\(X(\\text{Tails}) = 0\\)</p> </li> <li> <p>Interpretation: \\(X\\) counts the number of heads</p> </li> </ul> <p>Example 2: Rolling a die</p> <ul> <li> <p>Sample space: \\(S = \\{1, 2, 3, 4, 5, 6\\}\\)</p> </li> <li> <p>Random variable: \\(X(\\omega) = \\omega\\) (identity function)</p> </li> <li> <p>Interpretation: \\(X\\) gives the face value of the die</p> </li> </ul> <p>Example 3: Multiple coin tosses</p> <ul> <li> <p>Sample space: \\(S = \\{\\text{HH}, \\text{HT}, \\text{TH}, \\text{TT}\\}\\)</p> </li> <li> <p>Random variable: \\(X(\\text{HH}) = 2\\), \\(X(\\text{HT}) = 1\\), \\(X(\\text{TH}) = 1\\), \\(X(\\text{TT}) = 0\\)</p> </li> <li> <p>Interpretation: \\(X\\) counts the total number of heads</p> </li> </ul>"},{"location":"math/probability/random_variables/#probability-mass-function-pmf","title":"Probability Mass Function (PMF)","text":"<p>Before discussing distributions, let's understand what a Probability Mass Function (PMF) is.</p> <p>A Probability Mass Function is a function that gives the probability that a discrete random variable takes on a specific value. For a discrete random variable \\(X\\), the PMF is defined as:</p> \\[p_X(x) = P(X = x)\\] <p>Key Properties of PMF:</p> <ol> <li> <p>Non-negativity: \\(p_X(x) \\geq 0\\) for all possible values \\(x\\)</p> </li> <li> <p>Sum to 1: \\(\\sum_x p_X(x) = 1\\) (sum over all possible values)</p> </li> <li> <p>Probability interpretation: \\(0 \\leq p_X(x) \\leq 1\\) for each \\(x\\)</p> </li> </ol>"},{"location":"math/probability/random_variables/#cumulative-distribution-function-cdf","title":"Cumulative Distribution Function (CDF)","text":"<p>Another important function for describing random variables is the Cumulative Distribution Function (CDF).</p> <p>A Cumulative Distribution Function is a function that gives the probability that a random variable takes on a value less than or equal to a given number. For a random variable \\(X\\), the CDF is defined as:</p> \\[F_X(x) = P(X \\leq x)\\] <p>Key Properties of CDF:</p> <ol> <li> <p>Non-decreasing: \\(F_X(x) \\leq F_X(y)\\) whenever \\(x \\leq y\\) (monotonicity)</p> </li> <li> <p>Bounded: \\(0 \\leq F_X(x) \\leq 1\\) for all \\(x\\)</p> </li> <li> <p>Limits: \\(\\lim_{x \\to -\\infty} F_X(x) = 0\\) and \\(\\lim_{x \\to \\infty} F_X(x) = 1\\)</p> </li> </ol>"},{"location":"math/probability/random_variables/#distribution-of-a-random-variable","title":"Distribution of a Random Variable","text":"<p>The distribution of a random variable \\(X\\) describes how the probability mass (or density) is distributed across all possible values that \\(X\\) can take. It tells us the complete probabilistic behavior of \\(X\\). It can be represented in various ways, including a PMF, a cumulative distribution function (CDF), or other methods.</p> <p>The distribution answers the question: \"What is the probability that \\(X\\) takes on a particular value or falls within a particular range?\"</p> <p>Consider the die-rolling random variable \\(X\\):</p> <ul> <li> <p>Sample space: \\(S = \\{1, 2, 3, 4, 5, 6\\}\\)</p> </li> <li> <p>Random variable: \\(X(\\omega) = \\omega\\)</p> </li> <li> <p>Distribution: \\(P(X = k) = \\frac{1}{6}\\) for \\(k = 1, 2, 3, 4, 5, 6\\)</p> </li> </ul> <p>The distribution tells us that:</p> <ul> <li> <p>Each face is equally likely</p> </li> <li> <p>The probability of any specific value is \\(\\frac{1}{6}\\)</p> </li> <li> <p>The probability of rolling an even number is \\(P(X \\in \\{2, 4, 6\\}) = \\frac{1}{2}\\)</p> </li> </ul> <p>While a PMF is a key concept for describing discrete distributions, it's not the only way to define a distribution (e.g., CDF, table, etc.). </p>"},{"location":"math/probability/random_variables/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>The Bernoulli distribution is the simplest discrete probability distribution, modeling a random experiment with exactly two possible outcomes: success or failure.</p> <p>A random variable \\(X\\) follows a Bernoulli distribution with parameter \\(p\\) (denoted \\(X \\sim \\text{Bernoulli}(p)\\)) if:</p> <ul> <li> <p>Possible values: \\(X\\) takes only two values: \\(0\\) (failure) and \\(1\\) (success)</p> </li> <li> <p>Parameter: \\(p \\in [0, 1]\\) represents the probability of success</p> </li> <li> <p>Probability mass function:</p> </li> </ul> \\[P(X = 1) = p \\quad \\text{and} \\quad P(X = 0) = 1 - p\\] \\[P(X = k) = p^k(1-p)^{1-k} \\quad \\text{for } k \\in \\{0, 1\\}\\] <p>This compact formula gives:</p> <ul> <li> <p>\\(P(X = 1) = p^1(1-p)^0 = p\\)</p> </li> <li> <p>\\(P(X = 0) = p^0(1-p)^1 = 1-p\\)</p> </li> </ul> <p>Example: coin toss</p> <ul> <li> <p>Success: Heads (with probability \\(p = \\frac{1}{2}\\))</p> </li> <li> <p>Failure: Tails (with probability \\(1-p = \\frac{1}{2}\\))</p> </li> <li> <p>Random variable: \\(X(\\text{Heads}) = 1\\), \\(X(\\text{Tails}) = 0\\)</p> </li> </ul>"},{"location":"math/probability/random_variables/#binomial-distribution","title":"Binomial Distribution","text":"<p>The binomial distribution models the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success.</p> <p>A random variable \\(X\\) follows a binomial distribution with parameters \\(n\\) and \\(p\\) (denoted \\(X \\sim \\text{Binomial}(n, p)\\)) if:</p> <ul> <li> <p>Possible values: \\(X\\) takes values in \\(\\{0, 1, 2, \\ldots, n\\}\\)</p> </li> <li> <p>Parameters: </p> </li> <li> <p>\\(n \\in \\mathbb{N}\\) (number of trials)</p> </li> <li> <p>\\(p \\in [0, 1]\\) (probability of success in each trial)</p> </li> <li> <p>Probability mass function:</p> </li> </ul> \\[P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k} \\quad \\text{for } k = 0, 1, 2, \\ldots, n\\] <p>The PMF formula \\(P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k}\\) has three components:</p> <ol> <li> <p>\\(\\binom{n}{k}\\): Number of ways to choose \\(k\\) successes from \\(n\\) trials</p> </li> <li> <p>\\(p^k\\): Probability of \\(k\\) successes</p> </li> <li> <p>\\((1-p)^{n-k}\\): Probability of \\(n-k\\) failures</p> </li> </ol> <p>Example: Quality Control</p> <ul> <li> <p>Experiment: Test 100 products from a production line</p> </li> <li> <p>Random variable: \\(X\\) = number of defective products</p> </li> <li> <p>Distribution: \\(X \\sim \\text{Binomial}(100, 0.02)\\) (assuming 2% defect rate)</p> </li> <li> <p>Probability of at most 3 defects: \\(P(X \\leq 3) = \\sum_{k=0}^3 \\binom{100}{k} (0.02)^k (0.98)^{100-k}\\)</p> </li> </ul> <p>Let's derive the binomial PMF formula \\(P(X = k) = \\binom{n}{k} p^k(1-p)^{n-k}\\) using combinatorial reasoning.</p> <p>We want to find the probability of getting exactly \\(k\\) successes in \\(n\\) independent Bernoulli trials, where each trial has success probability \\(p\\).</p> <p>Since the trials are independent, the probability of any specific sequence of outcomes is the product of individual probabilities.</p> <p>Example: For \\(n = 3\\) trials with \\(k = 2\\) successes, one possible sequence is:</p> <ul> <li> <p>Trial 1: Success (probability \\(p\\))</p> </li> <li> <p>Trial 2: Success (probability \\(p\\))</p> </li> <li> <p>Trial 3: Failure (probability \\(1-p\\))</p> </li> </ul> <p>Probability of this specific sequence: \\(p \\cdot p \\cdot (1-p) = p^2(1-p)^1 = p^k(1-p)^{n-k}\\)</p> <p>The key insight is that there are multiple sequences that result in exactly \\(k\\) successes and \\(n-k\\) failures.</p> <p>Question: How many different ways can we arrange \\(k\\) successes and \\(n-k\\) failures in \\(n\\) positions?</p> <p>Answer: \\(\\binom{n}{k}\\) , that is, the number of ways to choose \\(k\\) positions out of \\(n\\) for the successes</p> <p>Probability of any specific sequence with \\(k\\) successes and \\(n-k\\) failures:</p> \\[P(\\text{specific sequence}) = p^k(1-p)^{n-k}\\] <p>Number of different sequences with exactly \\(k\\) successes:</p> \\[\\text{Number of sequences} = \\binom{n}{k}\\] <p>Total probability using the addition rule (sums probabilities of mutually exclusive sequences):</p> \\[P(X = k) = \\binom{n}{k} \\cdot p^k(1-p)^{n-k}\\] <p>Let's verify that the binomial PMF \\(p_X(k) = \\binom{n}{k} p^k(1-p)^{n-k}\\) satisfies all the required properties of a PMF.</p> <p>Property 1: Non-negativity</p> <p>We need to show that \\(p_X(k) \\geq 0\\) for all \\(k \\in \\{0, 1, 2, \\ldots, n\\}\\).</p> <p>Proof: </p> <ul> <li> <p>\\(\\binom{n}{k} \\geq 0\\) (combinatorial coefficient is always non-negative)</p> </li> <li> <p>\\(p^k \\geq 0\\) (since \\(p \\in [0, 1]\\) and \\(k \\geq 0\\))</p> </li> <li> <p>\\((1-p)^{n-k} \\geq 0\\) (since \\(1-p \\in [0, 1]\\) and \\(n-k \\geq 0\\))</p> </li> </ul> <p>Since all three factors are non-negative, their product \\(p_X(k) = \\binom{n}{k} p^k(1-p)^{n-k} \\geq 0\\) \u2713</p> <p>Property 2: Sum to 1</p> <p>We need to show that \\(\\sum_{k=0}^n p_X(k) = 1\\).</p> <p>Proof: </p> \\[\\sum_{k=0}^n p_X(k) = \\sum_{k=0}^n \\binom{n}{k} p^k(1-p)^{n-k}\\] <p>This is exactly the binomial expansion of \\((p + (1-p))^n\\):</p> \\[(p + (1-p))^n = \\sum_{k=0}^n \\binom{n}{k} p^k(1-p)^{n-k}\\] <p>But \\(p + (1-p) = 1\\), so:</p> \\[(p + (1-p))^n = 1^n = 1\\] <p>Therefore, \\(\\sum_{k=0}^n p_X(k) = 1\\) \u2713</p> <p>Property 3: Probability Interpretation</p> <p>We need to show that \\(0 \\leq p_X(k) \\leq 1\\) for each \\(k\\).</p> <p>Proof: </p> <ul> <li> <p>We already showed \\(p_X(k) \\geq 0\\) (Property 1)</p> </li> <li> <p>Since \\(\\sum_{k=0}^n p_X(k) = 1\\) (Property 2) and all terms are non-negative, no individual term can exceed 1</p> </li> <li> <p>Therefore, \\(0 \\leq p_X(k) \\leq 1\\) for each \\(k\\) \u2713</p> </li> </ul> <p>One of the most important properties of the binomial distribution is the addition property, which states that the sum of two independent binomial random variables is also binomial under certain conditions.</p> <p>Theorem: If \\(X \\sim \\text{Binomial}(n_1, p)\\) and \\(Y \\sim \\text{Binomial}(n_2, p)\\) are independent random variables with the same success probability \\(p\\), then:</p> \\[X + Y \\sim \\text{Binomial}(n_1 + n_2, p)\\] <p>This property makes intuitive sense:</p> <ul> <li> <p>\\(X\\): Random variable outputting number of successes in \\(n_1\\) trials with success probability \\(p\\)</p> </li> <li> <p>\\(Y\\): Random variable outputting number of successes in \\(n_2\\) trials with success probability \\(p\\)</p> </li> <li> <p>\\(X + Y\\): Random variable outputting number total number of successes in \\(n_1 + n_2\\) trials with success probability \\(p\\)</p> </li> </ul>"},{"location":"math/probability/random_variables/#hypergeometric-distribution","title":"Hypergeometric Distribution","text":"<p>The hypergeometric distribution models situations where we sample without replacement from a finite population, making it fundamentally different from the binomial distribution.</p> <p>Imagine you're on a treasure hunt with a group of friends. You've discovered an ancient chest containing 20 precious gems:</p> <ul> <li> <p>8 diamonds (the rare, valuable ones)</p> </li> <li> <p>12 emeralds (still beautiful, but less valuable)</p> </li> </ul> <p>The chest is sealed with a magical lock that only opens if you can correctly identify the composition of gems inside. Here's the catch: you can only draw 5 gems to examine them, and once you draw a gem, you cannot put it back (no replacement).</p> <p>Your mission: What's the probability that exactly 3 of your 5 draws are diamonds?</p> <p>Understanding the Problem:</p> <p>Population: 20 total gems</p> <ul> <li> <p>Success items: 8 diamonds (what we're interested in)</p> </li> <li> <p>Failure items: 12 emeralds</p> </li> <li> <p>Sample size: 5 gems drawn</p> </li> <li> <p>Target: Exactly 3 diamonds</p> </li> </ul> <p>Key difference from binomial: In the binomial case, you'd put each gem back after examining it, so the probability of drawing a diamond would stay constant at \\(\\frac{8}{20} = 0.4\\). But here, each draw affects the remaining population!</p> <p>For a hypergeometric random variable \\(X\\) with parameters:</p> <ul> <li> <p>\\(N\\) = total population size</p> </li> <li> <p>\\(K\\) = number of success items in population</p> </li> <li> <p>\\(n\\) = sample size</p> </li> </ul> <p>The PMF is:</p> \\[P(X = k) = \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\] <p>Where \\(k\\) is the number of success items in your sample.</p> <p>Numerator: \\(\\binom{K}{k} \\cdot \\binom{N-K}{n-k}\\)</p> <ul> <li> <p>\\(\\binom{K}{k}\\): Ways to choose \\(k\\) diamonds from \\(K\\) diamonds</p> </li> <li> <p>\\(\\binom{N-K}{n-k}\\): Ways to choose \\((n-k)\\) emeralds from \\((N-K)\\) emeralds</p> </li> <li> <p>Product: Total ways to get exactly \\(k\\) diamonds and \\((n-k)\\) emeralds</p> </li> </ul> <p>Denominator: \\(\\binom{N}{n}\\)</p> <ul> <li>Total ways to choose any \\(n\\) gems from \\(N\\) gems</li> </ul> <p>Result: Probability of getting exactly \\(k\\) diamonds</p> <p>Solution:</p> <ul> <li> <p>\\(N = 20\\) (total gems)</p> </li> <li> <p>\\(K = 8\\) (diamonds)</p> </li> <li> <p>\\(n = 5\\) (sample size)</p> </li> <li> <p>\\(k = 3\\) (target diamonds)</p> </li> </ul> <p>Calculation:</p> \\[P(X = 3) = \\frac{\\binom{8}{3} \\cdot \\binom{12}{2}}{\\binom{20}{5}}\\] <p>Computing the combinations:</p> <ul> <li> <p>\\(\\binom{8}{3} = \\frac{8!}{3! \\cdot 5!} = 56\\) (ways to choose 3 diamonds)</p> </li> <li> <p>\\(\\binom{12}{2} = \\frac{12!}{2! \\cdot 10!} = 66\\) (ways to choose 2 emeralds)</p> </li> <li> <p>\\(\\binom{20}{5} = \\frac{20!}{5! \\cdot 15!} = 15,504\\) (total ways to choose 5 gems)</p> </li> </ul> <p>Final probability:</p> \\[P(X = 3) = \\frac{56 \\cdot 66}{15,504} = \\frac{3,696}{15,504} \\approx 0.238\\] <p>So there's about a 23.8% chance of drawing exactly 3 diamonds!</p> <p>Key insight: The hypergeometric distribution captures the dependence between draws that occurs in finite populations, unlike the binomial distribution which assumes independence.</p> Aspect Binomial Hypergeometric Replacement With replacement Without replacement Probability Constant \\(p\\) Changes with each draw Independence Independent trials Dependent trials Population Infinite/very large Finite Formula \\(\\binom{n}{k} p^k(1-p)^{n-k}\\) \\(\\frac{\\binom{K}{k} \\binom{N-K}{n-k}}{\\binom{N}{n}}\\) <p>Let's verify that the hypergeometric PMF \\(P(X = k) = \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\) satisfies all the required properties of a valid PMF.</p> <p>Property 1: Non-negativity</p> <p>We need to show that \\(P(X = k) \\geq 0\\) for all valid values of \\(k\\).</p> <p>Proof: </p> <ul> <li> <p>\\(\\binom{K}{k} \\geq 0\\) (combinatorial coefficient is always non-negative)</p> </li> <li> <p>\\(\\binom{N-K}{n-k} \\geq 0\\) (combinatorial coefficient is always non-negative)</p> </li> <li> <p>\\(\\binom{N}{n} &gt; 0\\) (denominator is positive since \\(n \\leq N\\))</p> </li> </ul> <p>Since all factors are non-negative and the denominator is positive, the ratio \\(P(X = k) \\geq 0\\) \u2713</p> <p>Note: \\(k\\) must satisfy \\(0 \\leq k \\leq \\min(K, n)\\) and \\(n-k \\leq N-K\\) (i.e., \\(k \\geq \\max(0, n-(N-K))\\)).</p> <p>Property 2: Sum to 1</p> <p>We need to show that \\(\\sum_{k} P(X = k) = 1\\) over all valid values of \\(k\\).</p> <p>Proof: </p> \\[\\sum_{k} P(X = k) = \\sum_{k} \\frac{\\binom{K}{k} \\cdot \\binom{N-K}{n-k}}{\\binom{N}{n}}\\] <p>The valid range for \\(k\\) is \\(\\max(0, n-(N-K)) \\leq k \\leq \\min(K, n)\\). Let's call this range \\(k_{\\min}\\) to \\(k_{\\max}\\).</p> \\[\\sum_{k=k_{\\min}}^{k_{\\max}} P(X = k) = \\frac{1}{\\binom{N}{n}} \\sum_{k=k_{\\min}}^{k_{\\max}} \\binom{K}{k} \\cdot \\binom{N-K}{n-k}\\] <p>Key insight: This sum equals \\(\\binom{N}{n}\\) by the Vandermonde identity!</p> <p>The Vandermonde identity states:</p> \\[\\sum_{k=0}^{\\min(K,n)} \\binom{K}{k} \\binom{N-K}{n-k} = \\binom{N}{n}\\] <p>Therefore, \\(\\sum_{k=k_{\\min}}^{k_{\\max}} P(X = k) = \\frac{1}{\\binom{N}{n}} \\cdot \\binom{N}{n} = 1\\) \u2713</p> <p>Property 3: Probability Interpretation</p> <p>We need to show that \\(0 \\leq P(X = k) \\leq 1\\) for each valid \\(k\\).</p> <p>Proof: </p> <ul> <li> <p>We already showed \\(P(X = k) \\geq 0\\) (Property 1)</p> </li> <li> <p>Since \\(\\sum_{k} P(X = k) = 1\\) (Property 2) and all terms are non-negative, no individual term can exceed 1</p> </li> <li> <p>Therefore, \\(0 \\leq P(X = k) \\leq 1\\) for each valid \\(k\\) \u2713</p> </li> </ul> <p>Let's verify these properties for our treasure hunt example:</p> <ul> <li> <p>\\(N = 20\\) (total gems)</p> </li> <li> <p>\\(K = 8\\) (diamonds)</p> </li> <li> <p>\\(n = 5\\) (sample size)</p> </li> </ul> <p>Valid range for \\(k\\): \\(\\max(0, 5-(20-8)) = \\max(0, -7) = 0\\) to \\(\\min(8, 5) = 5\\)</p> <p>So \\(k\\) can be 0, 1, 2, 3, 4, or 5.</p> <p>PMF values:</p> <ul> <li> <p>\\(P(X = 0) = \\frac{\\binom{8}{0} \\cdot \\binom{12}{5}}{\\binom{20}{5}} = \\frac{1 \\cdot 792}{15,504} \\approx 0.051\\)</p> </li> <li> <p>\\(P(X = 1) = \\frac{\\binom{8}{1} \\cdot \\binom{12}{4}}{\\binom{20}{5}} = \\frac{8 \\cdot 495}{15,504} \\approx 0.255\\)</p> </li> <li> <p>\\(P(X = 2) = \\frac{\\binom{8}{2} \\cdot \\binom{12}{3}}{\\binom{20}{5}} = \\frac{28 \\cdot 220}{15,504} \\approx 0.397\\)</p> </li> <li> <p>\\(P(X = 3) = \\frac{\\binom{8}{3} \\cdot \\binom{12}{2}}{\\binom{20}{5}} = \\frac{56 \\cdot 66}{15,504} \\approx 0.238\\)</p> </li> <li> <p>\\(P(X = 4) = \\frac{\\binom{8}{4} \\cdot \\binom{12}{1}}{\\binom{20}{5}} = \\frac{70 \\cdot 12}{15,504} \\approx 0.054\\)</p> </li> <li> <p>\\(P(X = 5) = \\frac{\\binom{8}{5} \\cdot \\binom{12}{0}}{\\binom{20}{5}} = \\frac{56 \\cdot 1}{15,504} \\approx 0.004\\)</p> </li> <li> <p>Non-negativity: All values are positive \u2713</p> </li> <li> <p>Sum to 1: \\(0.051 + 0.255 + 0.397 + 0.238 + 0.054 + 0.004 = 0.999 \\approx 1\\) (small rounding error) \u2713</p> </li> <li> <p>Probability bounds: All values are between 0 and 1 \u2713</p> </li> </ul>"},{"location":"math/probability/random_variables/#independence-of-random-variables","title":"Independence of Random Variables","text":"<p>Independence is one of the most important concepts in probability theory, as it allows us to simplify complex calculations and understand the structure of random phenomena.</p> <p>Two discrete random variables \\(X\\) and \\(Y\\) are independent if and only if their joint probability mass function (PMF) factors into the product of their individual PMFs:</p> \\[P(X = x, Y = y) = P(X = x) \\cdot P(Y = y) \\quad \\text{for all } x, y\\] <p>Knowing the value of one random variable gives you no information about the value of the other.</p>"},{"location":"math/probability/some_famous_problems/","title":"Some famous problems","text":""},{"location":"math/probability/some_famous_problems/#the-monty-hall-problem","title":"The Monty Hall Problem","text":"<p>The Monty Hall Problem is a famous probability puzzle named after the host of the television game show \"Let's Make a Deal.\" It demonstrates how our intuition about probability can be misleading and how conditional probability can lead to counterintuitive results.</p> <p>You are a contestant on a game show. There are three doors:</p> <ul> <li> <p>Door 1: Behind one door is a car (the prize you want)</p> </li> <li> <p>Door 2: Behind another door is a goat</p> </li> <li> <p>Door 3: Behind the third door is another goat</p> </li> </ul> <p>You pick a door (say Door 1). The host, who knows what's behind each door, opens another door (say Door 3) to reveal a goat. The host then asks: \"Do you want to switch to the remaining unopened door (Door 2)?\"</p> <p>Question: Should you switch doors to maximize your probability of winning the car?</p> <p>Many people think that after one door is revealed, there are only two doors left, so the probability of winning the car is \\(\\frac{1}{2}\\) regardless of whether you switch or stay. This reasoning is incorrect.</p> <p>Correct Solution: always switch!</p> <p>The optimal strategy is to always switch. When you switch, your probability of winning the car is \\(\\frac{2}{3}\\), while if you stay with your original choice, your probability is only \\(\\frac{1}{3}\\).</p> <p></p> <p>The above tree diagram shows all the paths the game is played with the probabilities. The two diagrams below show the probabilities for winning and losing if we always switch.</p> <p></p> <p></p> <p>Let's look at some logical reasoning to gain intuition. If we picked a goat first (diagram below), and decided to always switch, we are guaranteed to win! So the probability of winning is the probability of picking a goat first. Picking a goat first has the probability \\(\\frac{2}{3}\\). And if we pick a car first, and decided to always switch, we are guaranteed to lose! So the probability of losing is the probability of picking a car first. Picking a car first has the probability \\(\\frac{1}{3}\\).</p> <p></p> <p>Let's use the Law of Total Probability to formally calculate the probability of success when our strategy is to always switch. Let's assume we initially pick door 1. The soultion is symmetric if we pick any other door initially.</p> <ul> <li> <p>Let \\(S\\) be the event \"We win by switching\"</p> </li> <li> <p>Let \\(D_j\\) be the event \"The car is behind door \\(j\\)\" for \\(j = 1, 2, 3\\)</p> </li> </ul> <p>The law of total probability states that:</p> \\[P(S) = P(S|D_1)P(D_1) + P(S|D_2)P(D_2) + P(S|D_3)P(D_3)\\] <p>1. \\(P(S|D_1)\\) - Probability of winning by switching given car is behind Door 1</p> <ul> <li> <p>If the car is behind Door 1, we initially picked the correct door</p> </li> <li> <p>When we switch, we must switch to a door with a goat</p> </li> <li> <p>Therefore, \\(P(S|D_1) = 0\\)</p> </li> </ul> <p>2. \\(P(S|D_2)\\) - Probability of winning by switching given car is behind Door 2</p> <ul> <li> <p>If the car is behind Door 2, we initially picked Door 1 (incorrect)</p> </li> <li> <p>Monty opens Door 3 (revealing a goat)</p> </li> <li> <p>When we switch, we switch to Door 2 (which has the car)</p> </li> <li> <p>Therefore, \\(P(S|D_2) = 1\\)</p> </li> </ul> <p>3. \\(P(S|D_3)\\) - Probability of winning by switching given car is behind Door 3</p> <ul> <li> <p>If the car is behind Door 3, we initially picked Door 1 (incorrect)</p> </li> <li> <p>Monty opens Door 2 (revealing a goat)</p> </li> <li> <p>When we switch, we switch to Door 3 (which has the car)</p> </li> <li> <p>Therefore, \\(P(S|D_3) = 1\\)</p> </li> </ul> <p>4. Prior Probabilities</p> <ul> <li> <p>\\(P(D_1) = \\frac{1}{3}\\) (car equally likely to be behind any door initially)</p> </li> <li> <p>\\(P(D_2) = \\frac{1}{3}\\)</p> </li> <li> <p>\\(P(D_3) = \\frac{1}{3}\\)</p> </li> </ul> <p>Substituting into the law of total probability:</p> \\[P(S) = P(S|D_1)P(D_1) + P(S|D_2)P(D_2) + P(S|D_3)P(D_3)\\] \\[P(S) = 0 \\cdot \\frac{1}{3} + 1 \\cdot \\frac{1}{3} + 1 \\cdot \\frac{1}{3}\\] \\[P(S) = 0 + \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}\\] <p>The law of total probability provides a rigorous mathematical foundation for why the switching strategy is optimal.</p> <p>To build stronger intuition for why switching is optimal, consider this thought experiment:</p> <p>Scenario: There are 1,000,000 doors. Behind one door is a car, and behind the other 999,999 doors are goats.</p> <ol> <li>You pick one door (say Door 1)</li> <li>The host opens 999,998 doors, revealing goats behind each one</li> <li>Two doors remain: Your original choice (Door 1) and one other door (say Door 535,780)</li> <li>The host asks: \"Do you want to switch to the remaining unopened door?\"</li> </ol> <p>Intuition: In this case, it seems very intuitive to always switch because the probability that your initial pick is the car is literally 1 in a million!</p> <p>The same logic applies to the original 3-door problem, just on a smaller scale. The million doors experiment helps us see that the 3-door problem is not a special case - it's a general principle that applies regardless of the number of doors.</p>"},{"location":"math/probability/some_famous_problems/#simpsons-paradox","title":"Simpson's Paradox","text":"<p>Simpson's Paradox is a statistical phenomenon where a trend appears in different groups of data but disappears or reverses when these groups are combined. This paradox demonstrates how aggregating data can sometimes hide important underlying relationships and why it's crucial to examine data at multiple levels.</p> <p>Simpson's Paradox occurs when Group A shows a trend in one direction, Group B shows the same trend in the same direction but the combined data shows the trend in the opposite direction.</p> <p>This happens because of confounding variables that affect the relationship between the variables of interest.</p> <p>Example:</p> Gender Department A Department B Overall Admit Rate Admit Rate Admit Rate Men 300/400 (75%) 50/100 (50%) 350/500 (70%) Women 100/100 (100%) 25/100 (25%) 125/200 (62.5%) <p>Analysis by Department: Department A: Women (100%) &gt; Men (75%) Department B: Women (25%) &lt; Men (50%)</p> <p>Overall Analysis: Combined: Men (70%) &gt; Women (62.5%)</p> <p>This is Simpson's Paradox! Women have higher admission rates in both departments individually, but men have a higher overall admission rate.</p> <p>Let's denote:</p> <ul> <li> <p>\\(p_{ij}\\) = admission rate for gender \\(i\\) in department \\(j\\)</p> </li> <li> <p>\\(n_{ij}\\) = number of applicants for gender \\(i\\) in department \\(j\\)</p> </li> </ul> <p>Overall admission rate for gender \\(i\\):</p> \\[P_i = \\frac{\\sum_j n_{ij} p_{ij}}{\\sum_j n_{ij}}\\] <p>The paradox occurs when:</p> <ul> <li> <p>\\(p_{1A} &gt; p_{2A}\\) and \\(p_{1B} &gt; p_{2B}\\) (women win in each department)</p> </li> <li> <p>But \\(P_1 &lt; P_2\\) (men win overall)</p> </li> </ul> <p>This happens because the \\(n_{ij}\\) values (group sizes) create different weights in the overall calculation.</p> <p>Simpson's Paradox teaches us that aggregated data can be misleading. The relationship between variables can change dramatically when we combine different groups, especially when those groups have different sizes or characteristics. This is why it's essential to examine data both individually and collectively to understand the true underlying relationships.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/","title":"Story Proofs and Axioms of Probability","text":""},{"location":"math/probability/story_proofs_and_axioms_of_probability/#story-proofs","title":"Story Proofs","text":"<p>Story proofs are a powerful technique in combinatorics where we prove identities by interpreting both sides of an equation as counting the same thing in different ways. This is also known as proof by interpretation or bijective proof.</p> <p>Example: \\(\\binom{n}{k} = \\binom{n}{n-k}\\)</p> <p>Identity: \\(\\binom{n}{k} = \\binom{n}{n-k}\\)</p> <p>Story Proof: Think of choosing \\(k\\) people from a group of \\(n\\) people to be on a committee. The left side \\(\\binom{n}{k}\\) counts the number of ways to choose \\(k\\) people for the committee. The right side \\(\\binom{n}{n-k}\\) counts the number of ways to choose \\(n-k\\) people to be left out of the committee. But choosing \\(k\\) people for the committee is exactly the same as choosing \\(n-k\\) people to leave out! Therefore, both sides count the same thing.</p> <p>Key insight: Every choice of \\(k\\) people corresponds uniquely to a choice of \\(n-k\\) people (the complement), and vice versa.</p> <p>Example: \\(n \\cdot \\binom{n-1}{k-1} = k \\cdot \\binom{n}{k}\\)</p> <p>Identity: \\(n \\cdot \\binom{n-1}{k-1} = k \\cdot \\binom{n}{k}\\)</p> <p>Story Proof: Think of choosing \\(k\\) people from \\(n\\) people, with one of them designated as President.</p> <p>Left side: \\(n \\cdot \\binom{n-1}{k-1}\\) First, choose who will be President (\\(n\\) choices). Then, from the remaining \\(n-1\\) people, choose \\(k-1\\) more people to complete the committee. Total: \\(n \\cdot \\binom{n-1}{k-1}\\)</p> <p>Right side: \\(k \\cdot \\binom{n}{k}\\) First, choose any \\(k\\) people from \\(n\\) people (\\(\\binom{n}{k}\\) ways). Then, from those \\(k\\) people, choose one to be President (\\(k\\) choices). Total: \\(k \\cdot \\binom{n}{k}\\)</p> <p>Example: Vandermonde Identity</p> <p>Identity: \\(\\sum_{k=0}^n \\binom{m}{k} \\binom{p}{n-k} = \\binom{m+p}{n}\\)</p> <p>Story Proof: Think of choosing \\(n\\) people from a group of \\(m\\) men and \\(p\\) women to form a committee.</p> <p>Left side: \\(\\sum_{k=0}^n \\binom{m}{k} \\binom{p}{n-k}\\) For each \\(k\\) from \\(0\\) to \\(n\\), choose \\(k\\) men from \\(m\\) men (\\(\\binom{m}{k}\\) ways). Then, choose \\(n-k\\) women from \\(p\\) women (\\(\\binom{p}{n-k}\\) ways). Total for this \\(k\\): \\(\\binom{m}{k} \\binom{p}{n-k}\\). Sum over all possible values of \\(k\\): \\(\\sum_{k=0}^n \\binom{m}{k} \\binom{p}{n-k}\\)</p> <p>Right side: \\(\\binom{m+p}{n}\\) Simply choose \\(n\\) people from the total group of \\(m+p\\) people</p> <p>Key insight: The left side partitions the counting by gender composition, while the right side ignores gender entirely. Both approaches must give the same result.</p> <p>Story proofs are powerful because they:</p> <ol> <li> <p>Provide intuition - You understand why the identity is true</p> </li> <li> <p>Are memorable - The story helps you remember the result</p> </li> <li> <p>Avoid algebra - No need for complex manipulations</p> </li> <li> <p>Generalize well - The same story often works for related problems</p> </li> </ol> <p>Key principle: If two expressions count the same thing, they must be equal.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/#formal-definition-of-probability","title":"Formal Definition of Probability","text":"<p>Let \\(S\\) be a sample space (the set of all possible outcomes of an experiment). An event \\(A\\) is a subset of \\(S\\) (i.e., \\(A \\subseteq S\\)).</p> <p>A probability function \\(P\\) is a function that takes an event \\(A\\) as input and returns a real number \\(P(A)\\) as output, where \\(P(A) \\in [0, 1]\\) for any event \\(A \\subseteq S\\).</p> <p>Philosophically, there are different interpretations of Probability, arguments even. But mathematically speaking, the formal definition of Probability along with the axioms is well defined without any ambiguity. And this definition coupled with the axioms constitute a foundation for this field where every theorem or result can be derived from.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/#axioms-of-probability","title":"Axioms of Probability","text":"<p>The probability function \\(P\\) must satisfy the following axioms:</p> <p>Axiom 1 (Non-negativity): For any event \\(A \\subseteq S\\),</p> \\[P(A) \\geq 0\\] <p>Axiom 2 (Normalization): For the entire sample space \\(S\\),</p> \\[P(S) = 1\\] <p>Axiom 3 (Additivity): For any collection of mutually exclusive events \\(A_1, A_2, A_3, \\ldots\\) (i.e., \\(A_i \\cap A_j = \\emptyset\\) for \\(i \\neq j\\)),</p> \\[P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i)\\] <p>From these axioms, we can derive several important properties:</p> <ol> <li>Probability of the empty set: \\(P(\\emptyset) = 0\\)</li> <li>Complement rule: \\(P(A^c) = 1 - P(A)\\)</li> <li>Monotonicity: If \\(A \\subseteq B\\), then \\(P(A) \\leq P(B)\\)</li> <li>Union rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)</li> </ol> <p>Inclusion-Exclusion Principle (3 events): </p> \\[P(A \\cup B \\cup C) = P(A) + P(B) + P(C) - P(A \\cap B) - P(A \\cap C) - P(B \\cap C) + P(A \\cap B \\cap C)\\] <p>Inclusion-Exclusion Principle (n events): For events \\(A_1, A_2, \\ldots, A_n\\),</p> \\[P\\left(\\bigcup_{i=1}^n A_i\\right) = \\sum_{i=1}^n P(A_i) - \\sum_{1 \\leq i &lt; j \\leq n} P(A_i \\cap A_j) + \\sum_{1 \\leq i &lt; j &lt; k \\leq n} P(A_i \\cap A_j \\cap A_k) - \\cdots + (-1)^{n+1} P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n)\\] <p>These axioms provide the mathematical foundation for probability theory and ensure that probability behaves in an intuitive and consistent way.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/#the-birthday-problem","title":"The Birthday Problem","text":"<p>The Birthday Problem is a classic probability puzzle that asks: What is the probability that in a group of \\(n\\) people, at least two people share the same birthday?</p> <p>This seemingly simple question leads to a surprising result that challenges our intuition about probability.</p> <p>Given: A group of \\(n\\) people chosen randomly from a population where birthdays are uniformly distributed across 365 days (ignoring leap years).</p> <p>Find: The probability that at least two people in the group share the same birthday.</p> <p>The result is counterintuitive: In a group of just 23 people, there is approximately a 50% chance that at least two people share the same birthday!</p> <p>This seems impossible at first glance- with 365 possible birthdays and only 23 people, how can there be a 50% chance of a match?</p> <p>We solve this using the complement rule: Instead of calculating the probability of at least one match directly, we calculate the probability of no matches and subtract from 1.</p> <p>Let \\(A\\) be the event \"at least two people share a birthday\". \\(P(A) = 1 - P(A^c)\\), where \\(A^c\\) is \"no two people share a birthday\". First person: Can have any birthday (365/365 = 1). Second person: Must have a different birthday (364/365). Third person: Must have a different birthday from the first two (363/365). And so on...</p> <p>General formula:</p> \\[P(A^c) = \\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\cdots \\times \\frac{365-n+1}{365}\\] <p>This can be written more compactly as:</p> \\[P(A^c) = \\frac{365!}{(365-n)! \\times 365^n}\\] \\[P(A) = 1 - \\frac{365!}{(365-n)! \\times 365^n}\\] <p>Let's calculate some key values:</p> Number of People (\\(n\\)) Probability of at least one match 10 11.7% 15 25.3% 20 41.1% 23 50.7% 30 70.6% 40 89.1% 50 97.0% 60 99.4% <p>The result feels wrong because we're thinking about individual comparisons rather than all possible pairs.</p> <p>Number of pairs: In a group of \\(n\\) people, there are \\(\\binom{n}{2} = \\frac{n(n-1)}{2}\\) possible pairs. With 23 people: \\(\\binom{23}{2} = 253\\) pairs. With 30 people: \\(\\binom{30}{2} = 435\\) pairs. With 50 people: \\(\\binom{50}{2} = 1,225\\) pairs.</p>"},{"location":"math/probability/story_proofs_and_axioms_of_probability/#de-montmorts-problem","title":"De Montmort's Problem","text":"<p>De Montmort's Problem is a classic probability puzzle that asks: What is the probability that when \\(n\\) cards are dealt from a deck, at least one card appears in a position that matches its value?</p> <p>This problem is also known as the matching problem or the coincidence problem and was first studied by Pierre Raymond de Montmort in the early 18th century.</p> <p>Given: A deck of \\(n\\) cards numbered from 1 to \\(n\\). The cards are shuffled and dealt face up in a row.</p> <p>Find: The probability that at least one card appears in the \\(k\\)-th position where the card's value is \\(k\\).</p> <p>Example: For \\(n = 4\\), we have cards [1, 2, 3, 4]. A deal of [2, 1, 4, 3] has no matches, but [1, 4, 2, 3] has a match (card 1 in position 1).</p> <p>We solve this using the inclusion-exclusion principle. Let \\(A_i\\) be the event that card \\(i\\) appears in position \\(i\\).</p> <p>Key insight: We want \\(P(A_1 \\cup A_2 \\cup \\cdots \\cup A_n)\\), which we can calculate using inclusion-exclusion.</p> <p>Step-by-Step Solution</p> <ol> <li>Individual probabilities: \\(P(A_i) = \\frac{1}{n}\\) for each card</li> <li>Pairwise intersections: \\(P(A_i \\cap A_j) = \\frac{1}{n(n-1)}\\) for \\(i \\neq j\\)</li> <li>Triple intersections: \\(P(A_i \\cap A_j \\cap A_k) = \\frac{1}{n(n-1)(n-2)}\\) for distinct \\(i, j, k\\)</li> <li>and so on...</li> </ol> <p>Using inclusion-exclusion:</p> \\[P(A_1 \\cup A_2 \\cup \\cdots \\cup A_n) = \\sum_{i=1}^n P(A_i) - \\sum_{1 \\leq i &lt; j \\leq n} P(A_i \\cap A_j) + \\sum_{1 \\leq i &lt; j &lt; k \\leq n} P(A_i \\cap A_j \\cap A_k) - \\cdots\\] <p>Calculating the terms: First term: \\(\\sum_{i=1}^n P(A_i) = n \\cdot \\frac{1}{n} = 1\\). Second term: \\(\\sum_{1 \\leq i &lt; j \\leq n} P(A_i \\cap A_j) = \\binom{n}{2} \\cdot \\frac{1}{n(n-1)} = \\frac{n(n-1)}{2} \\cdot \\frac{1}{n(n-1)} = \\frac{1}{2}\\). Third term: \\(\\sum_{1 \\leq i &lt; j &lt; k \\leq n} P(A_i \\cap A_j \\cap A_k) = \\binom{n}{3} \\cdot \\frac{1}{n(n-1)(n-2)} = \\frac{1}{3!} = \\frac{1}{6}\\). And so on...</p> <p>General pattern: The \\(k\\)-th term is \\(\\frac{1}{k!}\\)</p> <p>Final result:</p> \\[P(\\text{at least one match}) = 1 - \\frac{1}{2!} + \\frac{1}{3!} - \\frac{1}{4!} + \\cdots + (-1)^{n+1} \\frac{1}{n!}\\] <p>As \\(n\\) approaches infinity, the probability approaches:</p> \\[\\lim_{n \\to \\infty} P(\\text{at least one match}) = 1 - \\frac{1}{e} \\approx 0.632\\] <p>This means that even with infinitely many cards, there's still only about a 63.2% chance that at least one card appears in its \"correct\" position!</p>"},{"location":"math/probability/transformations_of_random_variables/","title":"Transformations of Random Variables","text":"<p>Let \\(X\\) be a continuous random variable with PDF \\(f_X\\), and let \\(Y = g(X)\\), where \\(g\\) is differentiable and strictly increasing (or strictly decreasing). Then the PDF of \\(Y\\) is given by:</p> \\[f_Y(y) = f_X(x) \\left|\\frac{dx}{dy}\\right|\\] <p>where \\(x = g^{-1}(y)\\).</p> <p>Proof. Let \\(g\\) be strictly increasing. The CDF of \\(Y\\) is:</p> \\[F_Y(y) = P(Y \\leq y) = P(g(X) \\leq y) = P(X \\leq g^{-1}(y)) = F_X(g^{-1}(y)) = F_X(x)\\] <p>So by the chain rule, the PDF of \\(Y\\) is:</p> \\[f_Y(y) = f_X(x) \\frac{dx}{dy}\\] <p>The proof for \\(g\\) strictly decreasing is analogous. In that case the PDF ends up as \\(-f_X(x) \\frac{dx}{dy}\\), which is nonnegative since \\(\\frac{dx}{dy} &lt; 0\\) if \\(g\\) is strictly decreasing. Using \\(\\left|\\frac{dx}{dy}\\right|\\), as in the statement of the theorem, covers both cases.</p> <p>Key points:</p> <ol> <li>Differentiability: The function \\(g\\) must be differentiable</li> <li>Monotonicity: \\(g\\) must be strictly increasing or strictly decreasing</li> <li>Absolute value: The absolute value ensures the PDF is non-negative</li> </ol> <p>Note: When applying the change of variables formula, we can choose whether to compute \\(\\frac{dx}{dy}\\), or compute \\(\\frac{dy}{dx}\\) and take the reciprocal. By the chain rule, these give the same result, so we can do whichever is easier.</p> <p>Example: Let \\(X \\sim N(0, 1)\\), \\(Y = e^X\\). We name the distribution of \\(Y\\) the Log-Normal. Now we can use the change of variables formula to find the PDF of \\(Y\\), since \\(g(x) = e^x\\) is strictly increasing.</p> <p>Let \\(y = e^x\\), so \\(x = \\log y\\) and \\(\\frac{dy}{dx} = e^x\\). Then:</p> \\[f_Y(y) = f_X(x) \\left|\\frac{dx}{dy}\\right| = \\phi(x) \\frac{1}{e^x} = \\phi(\\log y) \\frac{1}{y}, \\quad y &gt; 0\\] <p>Note that after applying the change of variables formula, we write everything on the right-hand side in terms of \\(y\\), and we specify the support of the distribution. To determine the support, we just observe that as \\(x\\) ranges from \\(-\\infty\\) to \\(\\infty\\), \\(e^x\\) ranges from \\(0\\) to \\(\\infty\\).</p> <p>We can get the same result by working from the definition of the CDF, translating the event \\(Y \\leq y\\) into an equivalent event involving \\(X\\). For \\(y &gt; 0\\):</p> \\[F_Y(y) = P(Y \\leq y) = P(e^X \\leq y) = P(X \\leq \\log y) = \\Phi(\\log y)\\] <p>So the PDF is again:</p> \\[f_Y(y) = \\frac{d}{dy} \\Phi(\\log y) = \\phi(\\log y) \\frac{1}{y}, \\quad y &gt; 0\\]"},{"location":"math/probability/transformations_of_random_variables/#change-of-variables-in-multiple-dimensions","title":"Change of Variables in multiple Dimensions","text":"<p>The change of variables formula generalizes to \\(n\\) dimensions, where it tells us how to use the joint PDF of a random vector \\(\\mathbf{X}\\) to get the joint PDF of the transformed random vector \\(\\mathbf{Y} = g(\\mathbf{X})\\). The formula is analogous to the one-dimensional version, but it involves a multivariate generalization of the derivative called a Jacobian matrix.</p> <p>Let \\(\\mathbf{X} = (X_1, \\ldots, X_n)\\) be a continuous random vector with joint PDF \\(f_{\\mathbf{X}}\\). Let \\(g : A_0 \\to B_0\\) be an invertible function, where \\(A_0\\) and \\(B_0\\) are open subsets of \\(\\mathbb{R}^n\\), \\(A_0\\) contains the support of \\(\\mathbf{X}\\), and \\(B_0\\) is the range of \\(g\\).</p> <p>Note: A set \\(C \\subseteq \\mathbb{R}^n\\) is open if for each \\(\\mathbf{x} \\in C\\), there exists \\(\\epsilon &gt; 0\\) such that all points with distance less than \\(\\epsilon\\) from \\(\\mathbf{x}\\) are contained in \\(C\\). Sometimes we take \\(A_0 = B_0 = \\mathbb{R}^n\\), but often we would like more flexibility for the domain and range of \\(g\\). For example, if \\(n = 2\\), and \\(X_1\\) and \\(X_2\\) have support \\((0,\\infty)\\), we may want to work with the open set \\(A_0 = (0,\\infty) \\times (0,\\infty)\\) rather than all of \\(\\mathbb{R}^2\\). When we say \"\\(A_0\\) contains the support,\" we mean that \\(A_0\\) must be a superset of the support of \\(\\mathbf{X}\\). In the example above, if the support is \\((0,\\infty) \\times (0,\\infty)\\), then \\(A_0 = (0,\\infty) \\times (0,\\infty)\\) does contain the support (in fact, it equals the support). We could also take \\(A_0 = \\mathbb{R}^2\\), which would definitely contain the support, but choosing \\(A_0\\) to be the minimal open set containing the support is often more convenient and natural.</p> <p>Let \\(\\mathbf{Y} = g(\\mathbf{X})\\), and mirror this by letting \\(\\mathbf{y} = g(\\mathbf{x})\\). Since \\(g\\) is invertible, we also have \\(\\mathbf{X} = g^{-1}(\\mathbf{Y})\\) and \\(\\mathbf{x} = g^{-1}(\\mathbf{y})\\).</p> <p>Suppose that all the partial derivatives \\(\\frac{\\partial x_i}{\\partial y_j}\\) exist and are continuous, so we can form the Jacobian matrix:</p> \\[\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{y}} = \\begin{pmatrix} \\frac{\\partial x_1}{\\partial y_1} &amp; \\frac{\\partial x_1}{\\partial y_2} &amp; \\cdots &amp; \\frac{\\partial x_1}{\\partial y_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial x_n}{\\partial y_1} &amp; \\frac{\\partial x_n}{\\partial y_2} &amp; \\cdots &amp; \\frac{\\partial x_n}{\\partial y_n} \\end{pmatrix}\\] <p>Also assume that the determinant of this Jacobian matrix is never 0. Then the joint PDF of \\(\\mathbf{Y}\\) is:</p> \\[f_{\\mathbf{Y}}(\\mathbf{y}) = f_{\\mathbf{X}}(g^{-1}(\\mathbf{y})) \\left|\\det\\left(\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{y}}\\right)\\right|\\] <p>for \\(\\mathbf{y} \\in B_0\\), and 0 otherwise.</p> <p>That is, to convert \\(f_{\\mathbf{X}}(\\mathbf{x})\\) to \\(f_{\\mathbf{Y}}(\\mathbf{y})\\) we express the \\(\\mathbf{x}\\) in \\(f_{\\mathbf{X}}(\\mathbf{x})\\) in terms of \\(\\mathbf{y}\\) and then multiply by the absolute value of the determinant of the Jacobian \\(\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{y}}\\).</p> <p>As in the 1D case, \\(\\left|\\det\\left(\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{y}}\\right)\\right| = \\left|\\det\\left(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\right)\\right|^{-1}\\), so we can compute whichever of the two Jacobians is easier, and then at the end express the joint PDF of \\(\\mathbf{Y}\\) as a function of \\(\\mathbf{y}\\).</p> <p>The idea is to apply the change of variables formula from multivariable calculus and the fact that if \\(A\\) is a region in \\(A_0\\) and \\(B = \\{g(\\mathbf{x}) : \\mathbf{x} \\in A\\}\\) is the corresponding region in \\(B_0\\), then \\(\\mathbf{X} \\in A\\) is equivalent to \\(\\mathbf{Y} \\in B\\)\u2014they are the same event. So \\(P(\\mathbf{X} \\in A) = P(\\mathbf{Y} \\in B)\\), which shows that:</p> \\[\\int_A f_{\\mathbf{X}}(\\mathbf{x}) d\\mathbf{x} = \\int_B f_{\\mathbf{Y}}(\\mathbf{y}) d\\mathbf{y}\\]"},{"location":"productivity/how_to_build_your_career_in_ai/make_every_day_count/","title":"Make Every Day Count","text":"<p>We could expect to live a total of 27,649 days. How small this number is! Print it in a large font and paste it somewhere as a daily reminder. That\u2019s all the days we have to spend with loved ones, learn, build for the future, and help others. Whatever you\u2019re doing today, is it worth 1/30,000 of your life?</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_1_learning/","title":"Phase 1: Learning","text":"<p>More research papers have been published on AI than anyone can read in a lifetime. So, when learning, it's critical to prioritize topic selection.</p> <p>Foundational Machine Learning Skills For example, it\u2019s important to understand models such as linear regression, logistic regression, decision trees, clustering, and anomaly detection. Beyond specific models, it\u2019s even more important to understand the core concepts behind how and why machine learning works, such as bias/variance, cost functions, regularization, optimization algorithms, and error analysis.</p> <p>Deep learning This has become such a large fraction of machine learning that it\u2019s hard to excel in the field without some understanding of it! It\u2019s valuable to know the basics of neural networks, practical skills for making them work (such as hyperparameter tuning), convolutional networks, sequence models, and transformers.</p> <p>Math relevant to machine learning Key areas include linear algebra (vectors, matrices, and various manipulations of them) as well as probability and statistics (including discrete and continuous probability, standard probability distributions, basic rules such as independence and Bayes\u2019 rule, and hypothesis testing). In addition, exploratory data analysis (EDA)\u2014 using visualizations and other methods to systematically explore a dataset\u2014 is an underrated skill. Finally, a basic intuitive understanding of calculus will also help. The math needed to do machine learning well has been changing. For instance, although some tasks require calculus, improved automatic differentiation software makes it possible to invent and implement new neural network architectures without doing any calculus.</p> <p>Software development  While you can get a job and make huge contributions with only machine learning modeling skills, your job opportunities will increase if you can also write good software to implement complex AI systems.</p> <p>A good course\u2014 in which a body of material has been organized into a coherent and logical form\u2014 is often the most time-efficient way to master a meaningful body of knowledge. When you\u2019ve absorbed the knowledge available in courses, you can switch over to research papers and other resources.</p> <p>Given how quickly our field is changing, there\u2019s little choice but to keep learning if you want to keep up. If you can cultivate the habit of learning a little bit every week, you can make significant progress with what feels like less effort.</p> <p>The best way to build a new habit is to start small and succeed, rather than start too big and fail.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/","title":"Phase 2: Projects","text":""},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#scoping-successful-ai-projects","title":"Scoping Successful AI Projects","text":"<p>One of the most important skills of an AI architect is the ability to identify ideas that are worth working on.</p> <p>Here are five steps to help you scope projects.</p> <p>Step 1: Identify a business problem (not an AI problem).</p> <p>Find a domain expert and ask, \"What are the top three things that you wish worked better? Why aren't they working yet?\"</p> <p>For example, if you want to apply AI to climate change, you might discover that power-grid operators can't accurately predict how much power intermittent sources like wind and solar might generate in the future.</p> <p>Step 2: Brainstorm AI solutions.</p> <p>Don't execute on the first idea you get excited about. Sometimes this works out okay, but sometimes you end up missing an even better idea that might not have taken any more effort to build.</p> <p>Once you understand a problem, you can brainstorm potential solutions more efficiently. For instance, to predict power generation from intermittent sources, we might consider:</p> <ul> <li> <p>Using satellite imagery to map the locations of wind turbines more accurately</p> </li> <li> <p>Using satellite imagery to estimate the height and generation capacity of wind turbines  </p> </li> <li> <p>Using weather data to better predict cloud cover and thus solar irradiance</p> </li> </ul> <p>Sometimes there isn't a good AI solution, and that's okay too.</p> <p>Step 3: Assess the feasibility and value of potential solutions.</p> <p>You can determine whether an approach is technically feasible by looking at:</p> <ul> <li> <p>Published work</p> </li> <li> <p>What competitors have done</p> </li> <li> <p>Building a quick proof of concept implementation</p> </li> </ul> <p>You can determine its value by consulting with domain experts (say, power-grid operators, who can advise on the utility of the potential solutions mentioned above).</p> <p>Step 4: Determine milestones.</p> <p>Once you've deemed a project sufficiently valuable, the next step is to determine the metrics to aim for. This includes both Machine learning metrics (such as accuracy) and Business metrics (such as revenue).</p> <p>Unfortunately, not every business problem can be reduced to optimizing test set accuracy! If you aren't able to determine reasonable milestones, it may be a sign that you need to learn more about the problem. A quick proof of concept can help supply the missing perspective.</p> <p>Step 5: Budget for resources.</p> <p>Think through everything you'll need to get the project done including:</p> <ul> <li> <p>Data: Raw data, labeled data, data cleaning tools</p> </li> <li> <p>Personnel: Team members, skills required, roles and responsibilities</p> </li> <li> <p>Time: Project timeline, milestones, deadlines</p> </li> <li> <p>Integrations: APIs, third-party services, system connections</p> </li> <li> <p>Support: Other teams, external vendors, domain experts</p> </li> </ul> <p>For example, if you need funds to purchase satellite imagery, make sure that's in the budget.</p> <p>Note: Working on projects is an iterative process. If, at any step, you find that the current direction is infeasible, return to an earlier step and proceed with your new understanding.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#finding-projects-that-complement-your-career-goals","title":"Finding Projects that Complement Your Career Goals","text":"<p>A fruitful career will include many projects, hopefully growing in scope, complexity, and impact over time. Thus, it is fine to start small. Use early projects to learn and gradually step up to bigger projects as your skills grow.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#what-if-you-dont-have-any-project-ideas","title":"What if you don't have any project ideas?","text":"<p>Here are a few ways to generate them:</p> <p>Join existing projects. If you find someone else with an idea, ask to join their project.</p> <p>Keep reading and talking to people. You can come up with new ideas whenever you spend a lot of time reading, taking courses, or talking with domain experts.</p> <p>Focus on an application area. Many researchers are trying to advance basic AI technology \u2014 say, by inventing the next generation of transformers or further scaling up language models \u2014 so, while this is an exciting direction, it is also very hard. But the variety of applications to which machine learning has not yet been applied is vast!</p> <p>Develop a side hustle. Even if you have a full-time job, a fun project that may or may not develop into something bigger can stir the creative juices and strengthen bonds with collaborators. Silicon Valley abounds with stories of startups that started as side projects.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#given-a-few-project-ideas-which-one-should-you-jump-into","title":"Given a few project ideas, which one should you jump into?","text":"<p>Here's a quick checklist of factors to consider:</p> <p>Will the project help you grow technically? Ideally, it should be challenging enough to stretch your skills but not so hard that you have little chance of success. This will put you on a path toward mastering ever-greater technical complexity.</p> <p>Do you have good teammates to work with? If not, are there people you can discuss things with? We learn a lot from the people around us, and good collaborators will have a huge impact on your growth.</p> <p>Can it be a stepping stone? If the project is successful, will its technical complexity and/or business impact make it a meaningful stepping stone to larger projects? If the project is bigger than those you've worked on before, there's a good chance it could be such a stepping stone.</p> <p>Building models is an iterative process. For many applications, the cost of training and conducting error analysis is not prohibitive. Furthermore, it is very difficult to carry out a study that will shed light on the appropriate model, data, and hyperparameters. So it makes sense to build an end-to-end system quickly and revise it until it works well.</p> <p>But when committing to a direction means making a costly investment or entering a one-way door (meaning a decision that's hard to reverse), it's often worth spending more time in advance to make sure it really is a good idea.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_2_projects/#building-a-portfolio-of-projects-that-shows-skill-progression","title":"Building a Portfolio of Projects that Shows Skill Progression","text":"<p>Over the course of a career, you\u2019re likely to work on projects in succession, each growing in scope and complexity. Each project is only one step on a longer journey, hopefully one that has a positive impact. Don\u2019t worry about starting too small. Communication is key. You need to be able to explain your thinking if you want others to see the value in your work and trust you with resources that you can invest in larger projects. Leadership isn\u2019t just for managers. When you reach the point of working on larger AI projects that require teamwork, your ability to lead projects will become more important, whether or not you are in a formal position of leadership.</p> <p>Building a portfolio of projects, especially one that shows progress over time from simple to complex undertakings, will be a big help when it comes to looking for a job.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_3_job/","title":"Phase 3: Job","text":""},{"location":"productivity/how_to_build_your_career_in_ai/phase_3_job/#a-simple-framework-for-starting-your-ai-job-search","title":"A Simple Framework for Starting Your AI Job Search","text":"<p>Are you switching roles? For example, if you\u2019re a software engineer, university student, or physicist who\u2019s looking to become a machine learning engineer, that\u2019s a role switch. Are you switching industries? For example, if you work for a healthcare company, financial services company, or a government agency and want to work for a software company, that\u2019s a switch in industries. If you\u2019re looking for your first job in AI, you\u2019ll probably find switching either roles or industries easier than doing both at the same time.</p> <p>If you\u2019re considering a role switch, a startup can be an easier place to do it than a big company. While there are exceptions, startups usually don\u2019t have enough people to do all the desired work. If you\u2019re able to help with AI tasks\u2014 even if it\u2019s not your official job \u2014 your work is likely to be appreciated. This lays the groundwork for a possible role switch without needing to leave the company. In contrast, in a big company, a rigid reward system is more likely to reward you for doing your job well (and your manager for supporting you in doing the job for which you were hired), but it\u2019s not as likely to reward contributions outside your job\u2019s scope.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_3_job/#using-informational-interviews-to-find-the-right-job","title":"Using Informational Interviews to Find the Right Job","text":"<p>An informational interview involves finding someone in a company or role you\u2019d like to know more about and informally interviewing them about their work. Such conversations are separate from searching for a job. In fact, it\u2019s helpful to interview people who hold positions that align with your interests well before you\u2019re ready to kick off a job search.</p> <p>Finding someone to interview isn\u2019t always easy, but many people who are in senior positions today received help when they were new from those who had entered the field ahead of them, and many are eager to pay it forward.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/phase_3_job/#overcoming-imposter-syndrome","title":"Overcoming Imposter Syndrome","text":"<p>AI is technically complex, and it has its fair share of smart and highly capable people. But it is easy to forget that to become good at anything, the first step is to suck at it. If you\u2019ve succeeded at sucking at AI\u2014 congratulations, you\u2019re on your way! It is guaranteed that everyone who has published a seminal AI paper struggled with simple technical challenges at some point.</p> <p>No one is an expert at everything! Recognize what you do well.</p>"},{"location":"productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/","title":"Three Steps to Career Growth","text":"<p>Three key steps of career growth are learning foundational skills, working on projects (to deepen your skills, build a portfolio, and create impact), and finding a job. These steps stack on top of each other.</p> <p></p> <p>Initially, you focus on learning foundational skills. After having gained foundational technical skills, you will begin working on projects. During this period, you\u2019ll also keep learning. Later, you will work on finding a job. Throughout this process, you\u2019ll continue to learn and work on meaningful projects.</p>"}]}