{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my wiki!","text":""},{"location":"AI/deep_generative_models/autoregressive_models/","title":"Autoregressive models","text":"<p>We assume we are given access to a dataset: $$ \\mathcal{D} = { \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_m } $$ where each datapoint is n-dimensional. For simplicity, we assume the datapoints are binary. $$ x_i \\in {0,1}^n $$</p>"},{"location":"AI/deep_generative_models/autoregressive_models/#representation","title":"Representation","text":"<p>If you have n random variables: $$ X_1, X_2, \\dots, X_n $$ then their joint probability can be written as a product of conditional probabilities: $$ P(X_1, X_2, \\dots, X_n) = P(X_1) \\cdot P(X_2 \\mid X_1) \\cdot P(X_3 \\mid X_1, X_2) \\cdot \\dots \\cdot P(X_n \\mid X_1, X_2, \\dots, X_{n-1}) $$ In words:</p> <p>The probability of all n variables taking particular values equals: \u2192 the probability of the first variable, \u2192 times the probability of the second variable given the first, \u2192 times the probability of the third variable given the first two, \u2192 and so on, until the n-th variable.</p> <p>By this chain rule of probability, we can factorize the joint distribution over the n-dimensions as:</p> \\[ p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i \\mid x_1, x_2, \\dots, x_{i-1}) = \\prod_{i=1}^n p(x_i \\mid x_{&lt;i}) \\] <p>where</p> \\[ x_{&lt;i} = [x_1, x_2, \\dots, x_{i-1}] \\] <p>denotes the vector of random variables with index less than i.</p> <p>The chain rule factorization can be expressed graphically as a Bayesian network.</p> <p></p> <p>Such a Bayesian network that makes no conditional independence assumptions is said to obey the autoregressive property. The term autoregressive originates from the literature on time-series models where observations from the previous time-steps are used to predict the value at the current time step. Here, we fix an ordering of the variables x1, x2, \u2026, xn and the distribution for the i-th random variable depends on the values of all the preceding random variables in the chosen ordering x1, x2, \u2026, xi\u22121.</p> <p>If we allow for every conditional p(xi|x&lt;i) to be specified in a tabular form, then such a representation is fully general and can represent any possible distribution over n random variables. However, the space complexity for such a representation grows exponentially with n.</p> <p>To see why, let us consider the conditional for the last dimension, given by p(xn|x&lt;n). In order to fully specify this conditional, we need to specify a probability for 2^(n\u22121) configurations of the variables x1, x2, \u2026, xn\u22121. Since the probabilities should sum to 1, the total number of parameters for specifying this conditional is given by 2^(n\u22121)\u22121. Hence, a tabular representation for the conditionals is impractical for learning the joint distribution factorized via chain rule.</p> <p>In an autoregressive generative model, the conditionals are specified as parameterized functions with a fixed number of parameters. Specifically, we assume that each conditional distribution corresponds to a Bernoulli random variable. We then learn a function that maps the preceding random variables to the parameter (mean) of this Bernoulli distribution. Hence, we have:</p> \\[ p_{\\theta_i}(x_i \\mid x_{&lt;i}) = \\text{Bern} \\left( f_i(x_1, x_2, \\dots, x_{i-1}) \\right) \\] <p>where the function is defined as:</p> \\[ f_i : \\{0,1\\}^{i-1} \\to [0,1] \\] <p>and theta_i denotes the set of parameters used to specify this function. This function takes in a vector of size (i-1) where each element is a 0 or a 1, and outputs a scalar bit.</p> <p>The total number of parameters in an autoregressive generative model is given by:</p> \\[ \\sum_{i=1}^n \\left| \\theta_i \\right| \\] <p>In the simplest case, we can specify the function as a linear combination of the input elements followed by a sigmoid non-linearity (to restrict the output to lie between 0 and 1). This gives us the formulation of a fully-visible sigmoid belief network (FVSBN).</p> \\[ f_i(x_1, x_2, \\dots, x_{i-1}) = \\sigma(\\alpha^{(i)}_0 + \\alpha^{(i)}_1 x_1 + \\dots + \\alpha^{(i)}_{i-1} x_{i-1}) \\] <p>where \\(\\sigma\\) denotes the sigmoid function and \\(\\theta_i = \\{\\alpha^{(i)}_0, \\alpha^{(i)}_1, \\dots, \\alpha^{(i)}_{i-1}\\}\\) denote the parameters of the mean function. The conditional for variable \\(i\\) requires \\(i\\) parameters, and hence the total number of parameters in the model is given by \\(\\sum_{i=1}^n i = O(n^2)\\). Note that the number of parameters are much fewer than the exponential complexity of the tabular case.</p> <p>A natural way to increase the expressiveness of an autoregressive generative model is to use more flexible parameterizations for the mean function e.g., multi-layer perceptrons (MLP). For example, consider the case of a neural network with 1 hidden layer. The mean function for variable \\(i\\) can be expressed as</p> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{A}_i \\mathbf{x}_{&lt;i} + \\mathbf{c}_i) \\\\ f_i(x_1, x_2, \\dots, x_{i-1}) &amp;= \\sigma(\\boldsymbol{\\alpha}^{(i)} \\mathbf{h}_i + b_i) \\end{align} \\] <p>where \\(\\mathbf{h}_i \\in \\mathbb{R}^d\\) denotes the hidden layer activations for the MLP and \\(\\theta_i = \\{\\mathbf{A}_i \\in \\mathbb{R}^{d \\times (i-1)}, \\mathbf{c}_i \\in \\mathbb{R}^d, \\boldsymbol{\\alpha}^{(i)} \\in \\mathbb{R}^d, b_i \\in \\mathbb{R}\\}\\) are the set of parameters for the mean function \\(\\mu_i(\\cdot)\\). The total number of parameters in this model is dominated by the matrices \\(\\mathbf{A}_i\\) and given by \\(O(n^2d)\\).</p> <p>Note: The term \"mean function\" here refers to the function that determines the mean (expected value) of the Bernoulli distribution for each variable. Since we're modeling binary variables, the mean of the Bernoulli distribution is the probability of the variable being 1. The sigmoid function \\(\\sigma\\) ensures that this probability lies between 0 and 1.</p> <p>For a Bernoulli random variable \\(X\\) with parameter \\(p\\), the expectation (mean) is given by:</p> \\[ \\mathbb{E}[X] = 1 \\cdot p + 0 \\cdot (1-p) = p \\] <p>This is because: - \\(X\\) takes value 1 with probability \\(p\\) - \\(X\\) takes value 0 with probability \\((1-p)\\) - The expectation is the weighted sum of all possible values, where the weights are their respective probabilities</p> <p>Therefore, when we say the mean function determines the mean of the Bernoulli distribution, we're saying it determines the probability \\(p\\) of the variable being 1.</p> <p>The Neural Autoregressive Density Estimator (NADE) provides an alternate MLP-based parameterization that is more statistically and computationally efficient than the vanilla approach. In NADE, parameters are shared across the functions used for evaluating the conditionals. In particular, the hidden layer activations are specified as</p> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{W}_{.,&lt;i} \\mathbf{x}_{&lt;i} + \\mathbf{c}) \\\\ f_i(x_1, x_2, \\dots, x_{i-1}) &amp;= \\sigma(\\boldsymbol{\\alpha}^{(i)} \\mathbf{h}_i + b_i) \\end{align} \\] <p>where \\(\\theta = \\{\\mathbf{W} \\in \\mathbb{R}^{d \\times n}, \\mathbf{c} \\in \\mathbb{R}^d, \\{\\boldsymbol{\\alpha}^{(i)} \\in \\mathbb{R}^d\\}_{i=1}^n, \\{b_i \\in \\mathbb{R}\\}_{i=1}^n\\}\\) is the full set of parameters for the mean functions \\(f_1(\\cdot), f_2(\\cdot), \\dots, f_n(\\cdot)\\). The weight matrix \\(\\mathbf{W}\\) and the bias vector \\(\\mathbf{c}\\) are shared across the conditionals. Sharing parameters offers two benefits:</p> <ol> <li> <p>The total number of parameters gets reduced from \\(O(n^2d)\\) to \\(O(nd)\\).</p> </li> <li> <p>The hidden unit activations can be evaluated in \\(O(nd)\\) time via the following recursive strategy:</p> </li> </ol> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{a}_i) \\\\ \\mathbf{a}_{i+1} &amp;= \\mathbf{a}_i + \\mathbf{W}_{[.,i]} x_i \\end{align} \\] <p>with the base case given by \\(\\mathbf{a}_1 = \\mathbf{c}\\).</p> <p>The RNADE algorithm extends NADE to learn generative models over real-valued data. Here, the conditionals are modeled via a continuous distribution such as a equi-weighted mixture of \\(K\\) Gaussians. Instead of learning a mean function, we now learn the means \\(\\mu_{i,1}, \\mu_{i,2}, \\dots, \\mu_{i,K}\\) and variances \\(\\Sigma_{i,1}, \\Sigma_{i,2}, \\dots, \\Sigma_{i,K}\\) of the \\(K\\) Gaussians for every conditional. For statistical and computational efficiency, a single function \\(g_i: \\mathbb{R}^{i-1} \\to \\mathbb{R}^{2K}\\) outputs all the means and variances of the \\(K\\) Gaussians for the \\(i\\)-th conditional distribution.</p> <p>The conditional distribution \\(p_{\\theta_i}(x_i \\mid \\mathbf{x}_{&lt;i})\\) in RNADE is given by:</p> \\[ p_{\\theta_i}(x_i \\mid \\mathbf{x}_{&lt;i}) = \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(x_i; \\mu_{i,k}, \\Sigma_{i,k}) \\] <p>where \\(\\mathcal{N}(x; \\mu, \\Sigma)\\) denotes the probability density of a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\Sigma\\) evaluated at \\(x\\). The parameters \\(\\{\\mu_{i,k}, \\Sigma_{i,k}\\}_{k=1}^K\\) are the outputs of the function \\(g_i(\\mathbf{x}_{&lt;i})\\).</p> <p>This is how RNADE is autoregressive. Example sequence showing autoregressive dependencies:</p> <p>\\(x_1\\):    - Input to \\(g_1\\): \\(\\mathbf{x}_{&lt;1} = []\\) (empty)   - Output: \\(\\{\\mu_{1,k}, \\Sigma_{1,k}\\}_{k=1}^K\\) for \\(p(x_1)\\)</p> <p>\\(x_2\\):    - Input to \\(g_2\\): \\(\\mathbf{x}_{&lt;2} = [x_1]\\)   - Output: \\(\\{\\mu_{2,k}, \\Sigma_{2,k}\\}_{k=1}^K\\) for \\(p(x_2 \\mid x_1)\\)</p> <p>\\(x_3\\):    - Input to \\(g_3\\): \\(\\mathbf{x}_{&lt;3} = [x_1, x_2]\\)   - Output: \\(\\{\\mu_{3,k}, \\Sigma_{3,k}\\}_{k=1}^K\\) for \\(p(x_3 \\mid x_1, x_2)\\)</p> <p>\\(x_4\\):    - Input to \\(g_4\\): \\(\\mathbf{x}_{&lt;4} = [x_1, x_2, x_3]\\)   - Output: \\(\\{\\mu_{4,k}, \\Sigma_{4,k}\\}_{k=1}^K\\) for \\(p(x_4 \\mid x_1, x_2, x_3)\\)</p> <p>This sequential, conditional generation process is what makes RNADE an autoregressive model. The mixture of Gaussians is just the form of the conditional distribution, but the autoregressive property comes from how these distributions are parameterized based on previous variables.</p>"},{"location":"AI/deep_generative_models/autoregressive_models/#learning-and-inference","title":"Learning and inference","text":"<p>Recall that learning a generative model involves optimizing the closeness between the data and model distributions. One commonly used notion of closeness is the KL divergence between the data and the model distributions:</p> \\[ \\min_{\\theta \\in \\Theta} d_{KL}(p_{data}, p_{\\theta}) = \\min_{\\theta \\in \\Theta} \\mathbb{E}_{x \\sim p_{data}}[\\log p_{data}(x) - \\log p_{\\theta}(x)] \\] <p>where: - \\(p_{data}\\) is the true data distribution - \\(p_{\\theta}\\) is our model distribution parameterized by \\(\\theta\\) - \\(\\Theta\\) is the set of all possible parameter values - \\(d_{KL}\\) is the Kullback-Leibler divergence</p> <p>Let's break down how this minimization works:</p> <ol> <li>For a fixed value of \\(\\theta\\), we compute:</li> <li>The expectation over all possible data points \\(x\\) from \\(p_{data}\\)</li> <li>For each \\(x\\), we compute \\(\\log p_{data}(x) - \\log p_{\\theta}(x)\\)</li> <li> <p>This gives us a single scalar value for this particular \\(\\theta\\)</p> </li> <li> <p>The minimization operator \\(\\min_{\\theta \\in \\Theta}\\) then:</p> </li> <li>Tries different values of \\(\\theta\\) in the parameter space \\(\\Theta\\)</li> <li> <p>Finds the \\(\\theta\\) that gives the smallest expected value</p> </li> <li> <p>Since \\(p_{data}\\) is constant with respect to \\(\\theta\\), minimizing the KL divergence is equivalent to maximizing the expected log-likelihood of the data under our model:</p> </li> </ol> \\[ \\max_{\\theta \\in \\Theta} \\mathbb{E}_{x \\sim p_{data}}[\\log p_{\\theta}(x)] \\] <p>This is because \\(\\log p_{data}(x)\\) doesn't depend on \\(\\theta\\), so it can be treated as a constant. Minimizing \\(-\\log p_{\\theta}(x)\\) is the same as maximizing \\(\\log p_{\\theta}(x)\\)</p> <p>To approximate the expectation over the unknown \\(p_{data}\\), we make an assumption: points in the dataset \\(\\mathcal{D}\\) are sampled i.i.d. from \\(p_{data}\\). This allows us to obtain an unbiased Monte Carlo estimate of the objective as:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\log p_{\\theta}(x) = \\mathcal{L}(\\theta | \\mathcal{D}) \\] <p>The maximum likelihood estimation (MLE) objective has an intuitive interpretation: pick the model parameters \\(\\theta \\in \\Theta\\) that maximize the log-probability of the observed datapoints in \\(\\mathcal{D}\\).</p> <p>In practice, we optimize the MLE objective using mini-batch gradient ascent. The algorithm operates in iterations. At every iteration \\(t\\), we sample a mini-batch \\(\\mathcal{B}_t\\) of datapoints sampled randomly from the dataset (\\(|\\mathcal{B}_t| &lt; |\\mathcal{D}|\\)) and compute gradients of the objective evaluated for the mini-batch. These parameters at iteration \\(t+1\\) are then given via the following update rule:</p> \\[ \\theta^{(t+1)} = \\theta^{(t)} + r_t \\nabla_{\\theta} \\mathcal{L}(\\theta^{(t)} | \\mathcal{B}_t) \\] <p>where \\(\\theta^{(t+1)}\\) and \\(\\theta^{(t)}\\) are the parameters at iterations \\(t+1\\) and \\(t\\) respectively, and \\(r_t\\) is the learning rate at iteration \\(t\\). Typically, we only specify the initial learning rate \\(r_1\\) and update the rate based on a schedule.</p> <p>Now that we have a well-defined objective and optimization procedure, the only remaining task is to evaluate the objective in the context of an autoregressive generative model. To this end, we first write the MLE objective in terms of the joint probability:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\log p_{\\theta}(x) \\] <p>Then, we substitute the factorized joint distribution of an autoregressive model. Since \\(p_{\\theta}(x) = \\prod_{i=1}^n p_{\\theta_i}(x_i | x_{&lt;i})\\), we have:</p> \\[ \\log p_{\\theta}(x) = \\log \\prod_{i=1}^n p_{\\theta_i}(x_i | x_{&lt;i}) = \\sum_{i=1}^n \\log p_{\\theta_i}(x_i | x_{&lt;i}) \\] <p>Substituting this into the MLE objective, we get:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\sum_{i=1}^n \\log p_{\\theta_i}(x_i | x_{&lt;i}) \\] <p>where \\(\\theta = \\{\\theta_1, \\theta_2, \\dots, \\theta_n\\}\\) now denotes the collective set of parameters for the conditionals.</p> <p>Inference in an autoregressive model is straightforward. For density estimation of an arbitrary point \\(x\\), we simply evaluate the log-conditionals \\(\\log p_{\\theta_i}(x_i | x_{&lt;i})\\) for each \\(i\\) and add these up to obtain the log-likelihood assigned by the model to \\(x\\). Since we have the complete vector \\(x = [x_1, x_2, \\dots, x_n]\\), we know all the values needed for each conditional \\(x_{&lt;i}\\), so each of the conditionals can be evaluated in parallel. Hence, density estimation is efficient on modern hardware.</p> <p>For example, given a 4-dimensional vector \\(x = [x_1, x_2, x_3, x_4]\\), we can compute all conditionals in parallel:</p> <ul> <li>\\(\\log p_{\\theta_1}(x_1)\\) (no conditioning needed)</li> <li>\\(\\log p_{\\theta_2}(x_2 | x_1)\\) (using known \\(x_1\\))</li> <li>\\(\\log p_{\\theta_3}(x_3 | x_1, x_2)\\) (using known \\(x_1, x_2\\))</li> <li>\\(\\log p_{\\theta_4}(x_4 | x_1, x_2, x_3)\\) (using known \\(x_1, x_2, x_3\\))</li> </ul> <p>Then sum them to get the total log-likelihood: \\(\\log p_{\\theta}(x) = \\sum_{i=1}^4 \\log p_{\\theta_i}(x_i | x_{&lt;i})\\)</p> <p>Sampling from an autoregressive model is a sequential procedure. Here, we first sample \\(x_1\\), then we sample \\(x_2\\) conditioned on the sampled \\(x_1\\), followed by \\(x_3\\) conditioned on both \\(x_1\\) and \\(x_2\\) and so on until we sample \\(x_n\\) conditioned on the previously sampled \\(x_{&lt;n}\\). For applications requiring real-time generation of high-dimensional data such as audio synthesis, the sequential sampling can be an expensive process.</p> <p>Finally, an autoregressive model does not directly learn unsupervised representations of the data. This is because:</p> <ol> <li>The model directly models the data distribution \\(p(x)\\) through a sequence of conditional distributions \\(p(x_i | x_{&lt;i})\\)</li> <li>There is no explicit latent space or bottleneck that forces the model to learn a compressed representation</li> <li>Each variable is modeled based on previous variables, but there's no mechanism to learn a global, compressed representation of the entire data point</li> <li>The model's parameters \\(\\theta_i\\) are specific to each conditional distribution and don't encode a meaningful representation of the data</li> </ol> <p>In contrast, latent variable models like variational autoencoders explicitly learn a compressed representation by: 1. Introducing a latent space \\(z\\) that captures the essential features of the data 2. Learning an encoder that maps data to this latent space 3. Learning a decoder that reconstructs data from the latent space 4. Using a bottleneck that forces the model to learn meaningful representations</p>"},{"location":"AI/deep_generative_models/introduction/","title":"Introduction","text":"<p>Natural agents excel at discovering patterns, extracting knowledge, and performing complex reasoning based on the data they observe. How can we build artificial learning systems to do the same?</p> <p>Generative models view the world under the lens of probability. In such a worldview, we can think of any kind of observed data, say , as a finite set of samples from an underlying distribution, say  pdata. At its very core, the goal of any generative model is then to approximate this data distribution given access to the dataset . The hope is that if we are able to  learn  a good generative model, we can use the learned model for downstream  inference.</p>"},{"location":"AI/deep_generative_models/introduction/#learning","title":"Learning","text":"<p>We will be primarily interested in parametric approximations (parametric models assume a specific data distribution (like a normal distribution) and estimate parameters (like the mean and standard deviation) of that distribution, while non-parametric models make no assumptions about the underlying distribution) to the data distribution, which summarize all the information about the dataset  in a finite set of parameters. In contrast with non-parametric models, parametric models scale more efficiently with large datasets but are limited in the family of distributions they can represent.</p> <p>In the parametric setting, we can think of the task of learning a generative model as picking the parameters within a family of model distributions that minimizes some notion of distance between the model distribution and the data distribution.</p> <p> </p> <p>For instance, we might be given access to a dataset of dog images  and our goal is to learn the parameters of a generative model \u03b8 within a model family M such that the model distribution p\u03b8 is close to the data distribution over dogs  pdata. Mathematically, we can specify our goal as the following optimization problem:</p> <p>min d(pdata,p\u03b8) where \u03b8\u2208M</p> <p>where pdata is accessed via the dataset  and  d(\u22c5) is a notion of distance between probability distributions.</p> <p>It is interesting to take note of the difficulty of the problem at hand. A typical image from a modern phone camera has a resolution of approximately  700\u00d71400700\u00d71400 pixels. Each pixel has three channels: R(ed), G(reen) and B(lue) and each channel can take a value between 0 to 255. Hence, the number of possible images is given by 256700\u00d71400\u00d73\u224810800000256700\u00d71400\u00d73\u224810800000. In contrast, Imagenet, one of the largest publicly available datasets, consists of only about 15 million images. Hence, learning a generative model with such a limited dataset is a highly underdetermined problem.</p> <p>Fortunately, the real world is highly structured and automatically discovering the underlying structure is key to learning generative models. For example, we can hope to learn some basic artifacts about dogs even with just a few images: two eyes, two ears, fur etc. Instead of incorporating this prior knowledge explicitly, we will hope the model learns the underlying structure directly from data.  We will be primarily interested in the following questions:</p> <ul> <li>What is the representation for the model family M?</li> <li>What is the objective function  d(\u22c5)?</li> <li>What is the optimization procedure for minimizing  d(\u22c5)?</li> </ul>"},{"location":"AI/deep_generative_models/introduction/#inference","title":"Inference","text":"<p>Discriminative models, also referred to as  conditional models, are a class of models frequently used for  classification. They are typically used to solve  binary classification  problems, i.e. assign labels, such as pass/fail, win/lose, alive/dead or healthy/sick, to existing datapoints.</p> <p>Types of discriminative models include  logistic regression  (LR),  conditional random fields  (CRFs),  decision trees  among many others.  Generative model  approaches which uses a joint probability distribution instead, include  naive Bayes classifiers,  Gaussian mixture models,  variational autoencoders,  generative adversarial networks  and others.</p> <p>Unlike generative modelling, which studies the joint probability  P(x,y), discriminative modeling studies the P(y|x) or maps the given unobserved variable (target) x to a class label y dependent on the observed variables (training samples). For example, in object recognition, x is likely to be a vector of raw pixels (or features extracted from the raw pixels of the image). Within a probabilistic framework, this is done by modeling the conditional probability distribution  P(y|x), which can be used for predicting y from x.</p> <p>For a discriminative model such as logistic regression, the fundamental inference task is to predict a label for any given datapoint. Generative models, on the other hand, learn a joint distribution over the entire data. While the range of applications to which generative models have been used continue to grow, we can identify three fundamental inference queries for evaluating a generative model.:</p> <ol> <li> <p>Density estimation:  Given a datapoint  x, what is the probability assigned by the model, i.e.,  p\u03b8(x)?</p> </li> <li> <p>Sampling:  How can we  generate  novel data from the model distribution, i.e.,  xnew\u223cp\u03b8(x)?</p> </li> <li> <p>Unsupervised representation learning:  How can we learn meaningful feature representations for a datapoint  x?</p> </li> </ol> <p>Going back to our example of learning a generative model over dog images, we can intuitively expect a good generative model to work as follows. For density estimation, we expect  p\u03b8(x) to be high for dog images and low otherwise. Alluding to the name  generative model, sampling involves generating novel images of dogs beyond the ones we observe in our dataset. Finally, representation learning can help discover high-level structure in the data such as the breed of dogs.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/","title":"Normalizing Flow Models","text":"<p>So far we have learned two types of likelihood based generative models:</p> <p>Autoregressive Models: \\(p_\\theta(x) = \\prod_{i=1}^N p_\\theta(x_i|x_{&lt;i})\\)</p> <p>Variational autoencoders: \\(p_\\theta(x) = \\int p_\\theta(x,z)dz\\)</p> <p>The two methods have relative strengths and weaknesses. Autoregressive models provide tractable likelihoods but no direct mechanism for learning features, whereas variational autoencoders can learn feature representations but have intractable marginal likelihoods.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#change-of-variables-formula","title":"Change of Variables Formula","text":"<p>In normalizing flows, we wish to map simple distributions (easy to sample and evaluate densities) to complex ones (learned via data). The change of variables formula describes how to evaluate densities of a random variable that is a deterministic transformation from another variable.</p> <p>Let's start with the univariate case and then generalize to multivariate random variables.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#univariate-case","title":"Univariate Case","text":"<p>Consider two random variables \\(Z\\) and \\(X\\) related by a strictly monotonic function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) such that \\(X = f(Z)\\). We want to find the probability density function of \\(X\\) in terms of the density of \\(Z\\).</p> <p>The key insight comes from the fact that probabilities must be preserved under the transformation. For any interval \\([a, b]\\) in the \\(X\\) space:</p> \\[P(a \\leq X \\leq b) = P(f^{-1}(a) \\leq Z \\leq f^{-1}(b))\\] <p>This can be written as:</p> \\[\\int_a^b p_X(x) dx = \\int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz\\] <p>To perform the substitution \\(z = f^{-1}(x)\\), we need to express \\(dz\\) in terms of \\(dx\\). Since \\(z = f^{-1}(x)\\), we can use the chain rule to find:</p> \\[\\frac{dz}{dx} = \\frac{d}{dx}f^{-1}(x) = \\frac{1}{f'(f^{-1}(x))}\\] <p>This follows from the inverse function theorem: if \\(y = f(x)\\), then \\(\\frac{dx}{dy} = \\frac{1}{f'(x)}\\).</p> <p>Therefore, \\(dz = \\frac{1}{f'(f^{-1}(x))} dx\\). However, we need to take the absolute value because probability densities must be non-negative. If \\(f'(f^{-1}(x)) &lt; 0\\) (meaning \\(f\\) is decreasing), then \\(\\frac{1}{f'(f^{-1}(x))} &lt; 0\\), which would make the density negative. Therefore, we use:</p> \\[dz = \\frac{1}{|f'(f^{-1}(x))|} dx\\] <p>Substituting this into our integral:</p> \\[\\int_a^b p_X(x) dx = \\int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz = \\int_a^b p_Z(f^{-1}(x)) \\cdot \\frac{1}{|f'(f^{-1}(x))|} dx\\] <p>Since this equality must hold for all intervals \\([a, b]\\), the integrands must be equal:</p> \\[p_X(x) = p_Z(f^{-1}(x)) \\cdot \\frac{1}{|f'(f^{-1}(x))|}\\] <p>This is the univariate change of variables formula. The factor \\(\\frac{1}{|f'(f^{-1}(x))|}\\) accounts for how the transformation stretches or compresses the probability mass.</p> <p>Why should \\(f\\) be monotonic? The monotonicity requirement ensures that \\(f\\) is invertible (one-to-one), which is crucial for the change of variables formula to work correctly. If \\(f\\) were not monotonic, there could be multiple values of \\(z\\) that map to the same value of \\(x\\), making the inverse function \\(f^{-1}\\) ill-defined. This would violate the fundamental assumption that we can uniquely determine the original variable \\(z\\) from the transformed variable \\(x\\).</p> <p>For example, if \\(f(z) = z^2\\) (which is not monotonic on \\(\\mathbb{R}\\)), then both \\(z = 2\\) and \\(z = -2\\) map to \\(x = 4\\). This creates ambiguity in the inverse mapping and would require special handling to account for multiple pre-images.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#multivariate-case","title":"Multivariate Case","text":"<p>For the multivariate case, we have random variables \\(\\mathbf{Z}\\) and \\(\\mathbf{X}\\) related by a bijective function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(\\mathbf{X} = f(\\mathbf{Z})\\).</p> <p>The key insight is that the probability mass in any region must be preserved under the transformation. For any region \\(A\\) in the \\(\\mathbf{X}\\) space:</p> \\[P(\\mathbf{X} \\in A) = P(\\mathbf{Z} \\in f^{-1}(A))\\] <p>This can be written as:</p> \\[\\int_A p_X(\\mathbf{x}) d\\mathbf{x} = \\int_{f^{-1}(A)} p_Z(\\mathbf{z}) d\\mathbf{z}\\] <p>To perform the multivariate substitution \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\), we need to understand how the volume element \\(d\\mathbf{z}\\) transforms. The Jacobian matrix \\(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\) is an \\(n \\times n\\) matrix where:</p> \\[\\left[\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right]_{ij} = \\frac{\\partial f^{-1}_i(\\mathbf{x})}{\\partial x_j}\\] <p>This matrix describes how small changes in \\(\\mathbf{x}\\) correspond to changes in \\(\\mathbf{z}\\). In multivariate calculus, when we perform a change of variables \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\), the volume element transforms as:</p> \\[d\\mathbf{z} = \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right| d\\mathbf{x}\\] <p>This is the multivariate generalization of the univariate substitution \\(dz = \\frac{1}{|f'(f^{-1}(x))|} dx\\). The determinant of the Jacobian matrix measures how the transformation affects the volume of a small region: - If \\(|\\det(J)| &gt; 1\\), the transformation expands volume - If \\(|\\det(J)| &lt; 1\\), the transformation contracts volume - If \\(|\\det(J)| = 1\\), the transformation preserves volume</p> <p>Substituting this into our integral:</p> \\[\\int_A p_X(\\mathbf{x}) d\\mathbf{x} = \\int_{f^{-1}(A)} p_Z(\\mathbf{z}) d\\mathbf{z} = \\int_A p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right| d\\mathbf{x}\\] <p>Since this equality must hold for all regions \\(A\\), the integrands must be equal:</p> \\[p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>This is the multivariate change of variables formula. The determinant of the Jacobian matrix accounts for how the transformation affects the volume of probability mass.</p> <p>Alternative Form Using Forward Mapping: Using the property that \\(\\det(A^{-1}) = \\det(A)^{-1}\\) for any invertible matrix \\(A\\), we can rewrite this as:</p> \\[p_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right|^{-1}\\] <p>This form is often more convenient in practice because it uses the forward mapping \\(f\\) rather than the inverse mapping \\(f^{-1}\\).</p> <p>Final result: Let \\(Z\\) and \\(X\\) be random variables which are related by a mapping \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(X = f(Z)\\) and \\(Z = f^{-1}(X)\\). Then</p> \\[p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>There are several things to note here:</p> <ul> <li>\\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) need to be continuous and have the same dimension.</li> <li>\\(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\) is a matrix of dimension \\(n \\times n\\), where each entry at location \\((i,j)\\) is defined as \\(\\frac{\\partial f^{-1}(\\mathbf{x})_i}{\\partial x_j}\\). This matrix is also known as the Jacobian matrix.</li> <li>\\(\\det(A)\\) denotes the determinant of a square matrix \\(A\\).</li> </ul> <p>For any invertible matrix \\(A\\), \\(\\det(A^{-1}) = \\det(A)^{-1}\\), so for \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\) we have</p> \\[p_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right|^{-1}\\] <p>If \\(\\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right| = 1\\), then the mapping is volume preserving, which means that the transformed distribution \\(p_X\\) will have the same \"volume\" compared to the original one \\(p_Z\\).</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#normalizing-flow-models-deep-dive","title":"Normalizing Flow Models deep dive","text":"<p>Let us consider a directed, latent-variable model over observed variables \\(X\\) and latent variables \\(Z\\). In a normalizing flow model, the mapping between \\(Z\\) and \\(X\\), given by \\(f_\\theta: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\), is deterministic and invertible such that \\(X = f_\\theta(Z)\\) and \\(Z = f^{-1}_\\theta(X)\\).</p> <p>Using change of variables, the marginal likelihood \\(p(x)\\) is given by</p> \\[p_X(\\mathbf{x}; \\theta) = p_Z(f^{-1}_\\theta(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}_\\theta(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>The name \"normalizing flow\" can be interpreted as the following:</p> <ul> <li> <p>\"Normalizing\" means that the change of variables gives a normalized density after applying an invertible transformation. When we transform a random variable through an invertible function, the resulting density automatically integrates to 1 (is normalized) because the change of variables formula preserves the total probability mass. This is different from other methods where we might need to explicitly normalize or approximate the density.</p> </li> <li> <p>\"Flow\" means that the invertible transformations can be composed with each other to create more complex invertible transformations. If we have two invertible functions \\(f_1\\) and \\(f_2\\), then their composition \\(f_2 \\circ f_1\\) is also invertible. This allows us to build complex transformations by chaining simpler ones, creating a \"flow\" of transformations.</p> </li> </ul> <p>Different from autoregressive models and variational autoencoders, deep normalizing flow models require specific architectural structures:</p> <ol> <li> <p>The input and output dimensions must be the same - This is necessary for the transformation to be invertible. If the dimensions don't match, we can't uniquely map back and forth between the spaces.</p> </li> <li> <p>The transformation must be invertible - This is fundamental to the change of variables formula and allows us to compute both the forward transformation (for sampling) and the inverse transformation (for density evaluation).</p> </li> <li> <p>Computing the determinant of the Jacobian needs to be efficient (and differentiable) - The change of variables formula requires computing the determinant of the Jacobian matrix. For high-dimensional spaces, this can be computationally expensive, so we need architectures that make this computation tractable.</p> </li> </ol> <p>Next, we introduce several popular forms of flow models that satisfy these properties.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#planar-flow","title":"Planar Flow","text":"<p>The Planar Flow introduces the following invertible transformation:</p> \\[\\mathbf{x} = f_\\theta(\\mathbf{z}) = \\mathbf{z} + \\mathbf{u}h(\\mathbf{w}^\\top\\mathbf{z} + b)\\] <p>where \\(\\mathbf{u}, \\mathbf{w}, b\\) are parameters.</p> <p>The absolute value of the determinant of the Jacobian is given by:</p> \\[\\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right| = |1 + h'(\\mathbf{w}^\\top\\mathbf{z} + b)\\mathbf{u}^\\top\\mathbf{w}|\\] <p>However, \\(\\mathbf{u}, \\mathbf{w}, b, h(\\cdot)\\) need to be restricted in order to be invertible. For example, \\(h = \\tanh\\) and \\(h'(\\mathbf{w}^\\top\\mathbf{z} + b)\\mathbf{u}^\\top\\mathbf{w} \\geq -1\\). Note that while \\(f_\\theta(\\mathbf{z})\\) is invertible, computing \\(f^{-1}_\\theta(\\mathbf{z})\\) could be difficult analytically. The following models address this problem, where both \\(f_\\theta\\) and \\(f^{-1}_\\theta\\) have simple analytical forms.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#nice-and-realnvp","title":"NICE and RealNVP","text":"<p>The Nonlinear Independent Components Estimation (NICE) model and Real Non-Volume Preserving (RealNVP) model compose two kinds of invertible transformations: additive coupling layers and rescaling layers. The coupling layer in NICE partitions a variable \\(\\mathbf{z}\\) into two disjoint subsets, say \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\). Then it applies the following transformation:</p> <p>Forward mapping \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\):</p> <ul> <li>\\(\\mathbf{x}_1 = \\mathbf{z}_1\\), which is an identity mapping.</li> <li>\\(\\mathbf{x}_2 = \\mathbf{z}_2 + m_\\theta(\\mathbf{z}_1)\\), where \\(m_\\theta\\) is a neural network.</li> </ul> <p>Inverse mapping \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\):</p> <ul> <li>\\(\\mathbf{z}_1 = \\mathbf{x}_1\\), which is an identity mapping.</li> <li>\\(\\mathbf{z}_2 = \\mathbf{x}_2 - m_\\theta(\\mathbf{x}_1)\\), which is the inverse of the forward transformation.</li> </ul> <p>Therefore, the Jacobian of the forward mapping is lower triangular, whose determinant is simply the product of the elements on the diagonal, which is 1. Therefore, this defines a volume preserving transformation. RealNVP adds scaling factors to the transformation:</p> \\[\\mathbf{x}_2 = \\exp(s_\\theta(\\mathbf{z}_1)) \\odot \\mathbf{z}_2 + m_\\theta(\\mathbf{z}_1)\\] <p>where \\(\\odot\\) denotes elementwise product. This results in a non-volume preserving transformation.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#autoregressive-flow-models","title":"Autoregressive Flow Models","text":"<p>Some autoregressive models can also be interpreted as flow models. For a Gaussian autoregressive model, one receives some Gaussian noise for each dimension of \\(\\mathbf{x}\\), which can be treated as the latent variables \\(\\mathbf{z}\\). Such transformations are also invertible, meaning that given \\(\\mathbf{x}\\) and the model parameters, we can obtain \\(\\mathbf{z}\\) exactly.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#masked-autoregressive-flow-maf","title":"Masked Autoregressive Flow (MAF)","text":"<p>Masked Autoregressive Flow (MAF) uses this interpretation, where the forward mapping is an autoregressive model. However, sampling is sequential and slow, in \\(O(n)\\) time where \\(n\\) is the dimension of the samples.</p> <p>MAF Architecture and Mathematical Formulation:</p> <p>The MAF is comprised of Masked Autoencoder for Distribution Estimation (MADE) blocks, which has a special masking scheme at each layer such that the autoregressive property is preserved. In particular, we consider a Gaussian autoregressive model:</p> \\[p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i | \\mathbf{x}_{&lt;i})\\] <p>such that the conditional Gaussians \\(p(x_i | \\mathbf{x}_{&lt;i}) = \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\) are parameterized by neural networks \\(\\mu_i = f_{\\mu_i}(\\mathbf{x}_{&lt;i})\\) and \\(\\alpha_i = f_{\\alpha_i}(\\mathbf{x}_{&lt;i})\\). Note that \\(\\alpha_i\\) denotes the log standard deviation of the Gaussian \\(p(x_i | \\mathbf{x}_{&lt;i})\\).</p> <p>As seen in the change of variables formula, a normalizing flow uses a series of deterministic and invertible mappings \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(\\mathbf{x} = f(\\mathbf{z})\\) and \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\) to transform a simple prior distribution \\(p_z\\) (e.g. isotropic Gaussian) into a more expressive one. In particular, a normalizing flow which composes \\(k\\) invertible transformations \\(\\{f_j\\}_{j=1}^k\\) such that \\(\\mathbf{x} = f_k \\circ f_{k-1} \\circ \\cdots \\circ f_1(\\mathbf{z}_0)\\) takes advantage of the change-of-variables property:</p> \\[\\log p(\\mathbf{x}) = \\log p_z(f^{-1}(\\mathbf{x})) + \\sum_{j=1}^k \\log \\left|\\det\\left(\\frac{\\partial f_j^{-1}(\\mathbf{x}_j)}{\\partial \\mathbf{x}_j}\\right)\\right|\\] <p>In MAF, the forward mapping is: \\(x_i = \\mu_i + z_i \\cdot \\exp(\\alpha_i)\\), and the inverse mapping is: \\(z_i = (x_i - \\mu_i)/\\exp(\\alpha_i)\\). The log of the absolute value of the determinant of the Jacobian is:</p> \\[\\log \\left|\\det\\left(\\frac{\\partial f^{-1}}{\\partial \\mathbf{x}}\\right)\\right| = -\\sum_{i=1}^n \\alpha_i\\] <p>where \\(\\mu_i\\) and \\(\\alpha_i\\) are as defined above.</p> <p>Connection between \\(p(\\mathbf{x})\\) and \\(\\log p(\\mathbf{x})\\) formulations:</p> <p>The two formulations are equivalent but serve different purposes:</p> <ol> <li>\\(p(\\mathbf{x})\\) formulation (autoregressive view):</li> </ol> \\[p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i | \\mathbf{x}_{&lt;i}) = \\prod_{i=1}^n \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\] <ol> <li>\\(\\log p(\\mathbf{x})\\) formulation (flow view):</li> </ol> \\[\\log p(\\mathbf{x}) = \\log p_z(f^{-1}(\\mathbf{x})) + \\sum_{j=1}^k \\log \\left|\\det\\left(\\frac{\\partial f_j^{-1}(\\mathbf{x}_j)}{\\partial \\mathbf{x}_j}\\right)\\right|\\] <p>How they relate:</p> <p>Taking the logarithm of the autoregressive formulation:</p> \\[\\log p(\\mathbf{x}) = \\sum_{i=1}^n \\log p(x_i | \\mathbf{x}_{&lt;i}) = \\sum_{i=1}^n \\log \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\] <p>For a Gaussian distribution \\(\\mathcal{N}(x | \\mu, \\sigma^2)\\), we have:</p> \\[\\log \\mathcal{N}(x | \\mu, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\] <p>Substituting \\(\\sigma = \\exp(\\alpha_i)\\) and using the inverse mapping \\(z_i = (x_i - \\mu_i)/\\exp(\\alpha_i)\\):</p> \\[\\log p(\\mathbf{x}) = \\sum_{i=1}^n \\left[-\\frac{1}{2}\\log(2\\pi) - \\alpha_i - \\frac{z_i^2}{2}\\right] = \\sum_{i=1}^n \\log \\mathcal{N}(z_i | 0, 1) - \\sum_{i=1}^n \\alpha_i\\] <p>This shows that the autoregressive formulation (using conditional Gaussians) is equivalent to the flow formulation (using change of variables with a standard normal prior and the Jacobian determinant term \\(-\\sum_{i=1}^n \\alpha_i\\)).</p> <p>Key insight: The \\(\\alpha_i\\) terms serve dual purposes - they parameterize the conditional standard deviations in the autoregressive view, and they contribute to the Jacobian determinant in the flow view.</p> <p>What are \\(\\mu_1\\) and \\(\\alpha_1\\) in MAF?</p> <p>In MAF, for the first dimension (\\(i=1\\)):</p> <ul> <li> <p>\\(\\mu_1\\): This is the mean of the first conditional distribution \\(p(x_1)\\). Since \\(x_1\\) has no previous dimensions to condition on (\\(\\mathbf{x}_{&lt;1}\\) is empty), \\(\\mu_1\\) is typically a learned constant parameter or computed from a bias term in the neural network.</p> </li> <li> <p>\\(\\alpha_1\\): This is the log standard deviation of the first conditional distribution \\(p(x_1)\\). The actual standard deviation is \\(\\exp(\\alpha_1)\\), and \\(\\alpha_1\\) is also typically a learned constant parameter.</p> </li> </ul> <p>This makes sense because the first dimension has no autoregressive dependencies - it's the starting point of the autoregressive chain.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#made-blocks","title":"MADE Blocks","text":"<p>MADE (Masked Autoencoder for Distribution Estimation) is a key architectural component that enables efficient autoregressive modeling. MADE uses a special masking scheme to ensure that the autoregressive property is preserved while allowing for efficient parallel computation of all conditional parameters.</p> <p>How MADE Works:</p> <ol> <li> <p>Masking Scheme: Each layer in the neural network has a mask that ensures each output unit only depends on a subset of input units, maintaining the autoregressive ordering.</p> </li> <li> <p>Autoregressive Property: For dimension \\(i\\), the network can only access inputs \\(x_j\\) where \\(j &lt; i\\), ensuring that \\(p(x_i | \\mathbf{x}_{&lt;i})\\) only depends on previous dimensions.</p> </li> <li> <p>Parallel Parameter Computation: Despite the autoregressive constraints, MADE can compute all \\(\\mu_i\\) and \\(\\alpha_i\\) parameters in parallel during training, making it much more efficient than sequential autoregressive models.</p> </li> </ol> <p>Mathematical Implementation:</p> <p>The masking is implemented by multiplying the weight matrices with binary masks:</p> \\[W_{masked} = W \\odot M\\] <p>where \\(M\\) is a binary mask matrix that enforces the autoregressive dependencies. The mask ensures that: - Output \\(i\\) can only depend on inputs \\(j &lt; i\\) - This creates a lower triangular dependency structure</p> <p>Connection to MAF: MAF uses MADE blocks as its core building blocks, allowing it to efficiently compute all the conditional parameters \\(\\mu_i\\) and \\(\\alpha_i\\) while maintaining the autoregressive structure required for the flow transformation.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#detailed-maf-implementation-analysis","title":"Detailed MAF Implementation Analysis","text":"<p>Let's analyze a complete MAF implementation that demonstrates the concepts discussed above:</p> <p>Core Components:</p> <ol> <li>MaskedLinear: Implements the masking mechanism for autoregressive dependencies</li> <li>PermuteLayer: Reorders dimensions between flow layers</li> <li>MADE: Single MADE block with forward and inverse transformations</li> <li>MAF: Complete model with multiple MADE blocks</li> </ol> <p>1. MaskedLinear Layer:</p> <pre><code>class MaskedLinear(nn.Linear):\n    def __init__(self, input_size, output_size, mask):\n        super().__init__(input_size, output_size)\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, x):\n        return F.linear(x, self.mask * self.weight, self.bias)\n</code></pre> <p>Key Features: - Masking: The mask is a binary matrix that enforces autoregressive dependencies - Element-wise Multiplication: <code>self.mask * self.weight</code> zeros out forbidden connections - Autoregressive Property: Ensures output \\(i\\) only depends on inputs \\(j &lt; i\\)</p> <p>2. PermuteLayer:</p> <pre><code>class PermuteLayer(nn.Module):\n    def __init__(self, num_inputs):\n        super().__init__()\n        self.perm = np.array(np.arange(0, num_inputs)[::-1])\n\n    def forward(self, inputs):\n        return inputs[:, self.perm], torch.zeros(inputs.size(0), 1, device=inputs.device)\n\n    def inverse(self, inputs):\n        return inputs[:, self.perm], torch.zeros(inputs.size(0), 1, device=inputs.device)\n</code></pre> <p>Purpose: - Dimension Reordering: Reverses the order of dimensions between flow layers - Expressiveness: Allows different autoregressive orderings across layers - Jacobian: Since it's just a permutation, the Jacobian determinant is 1 (log_det = 0)</p> <p>3. MADE Block Implementation:</p> <p>Forward Method (z \u2192 x): <pre><code>def forward(self, z):\n    x = torch.zeros_like(z)\n    log_det = None\n    for i in range(self.input_size):\n        out = self.net(x)  # MADE network with masking\n        mean, alpha = out.chunk(2, dim=1)  # Split into mean and log_std\n        x[:, i] = mean[:, i] + z[:, i] * torch.exp(alpha[:, i])  # Transform\n        if log_det is None:\n            log_det = alpha[:, i].unsqueeze(1)\n        else:\n            log_det = torch.cat((log_det, alpha[:, i].unsqueeze(1)), dim=1)\n    log_det = -torch.sum(log_det, dim=1)  # Negative sum for change of variables\n    return x, log_det\n</code></pre></p> <p>Key Implementation Details: - Sequential Processing: Each dimension is processed one by one - Autoregressive Access: The MADE network can only access previously computed \\(x\\) values - Transformation: \\(x_i = \\mu_i + z_i \\cdot \\exp(\\alpha_i)\\) - Log Determinant: Accumulates \\(\\alpha_i\\) values and takes negative sum</p> <p>Inverse Method (x \u2192 z): <pre><code>def inverse(self, x):\n    out = self.net(x)  # MADE network with masking\n    mean, alpha = out.chunk(2, dim=1)  # Split into mean and log_std\n    z = (x - mean) * torch.exp(-alpha)  # Inverse transform\n    log_det = -torch.sum(alpha, dim=1)  # Negative sum for change of variables\n    return z, log_det\n</code></pre></p> <p>Key Implementation Details: - Parallel Processing: All dimensions can be processed simultaneously - Autoregressive Masking: The masking ensures proper dependencies - Inverse Transformation: \\(z_i = (x_i - \\mu_i) / \\exp(\\alpha_i)\\) - Log Determinant: Same formula as forward, but computed in parallel</p> <p>4. Complete MAF Model:</p> <p>Architecture: <pre><code>def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n    nf_blocks = []\n    for i in range(self.n_flows):\n        nf_blocks.append(MADE(self.input_size, hidden_size, n_hidden))\n        nf_blocks.append(PermuteLayer(self.input_size))\n    self.nf = nn.Sequential(*nf_blocks)\n</code></pre></p> <p>Structure: <pre><code>Input \u2192 MADE\u2081 \u2192 Permute\u2081 \u2192 MADE\u2082 \u2192 Permute\u2082 \u2192 ... \u2192 MADE\u2096 \u2192 Permute\u2096 \u2192 Output\n</code></pre></p> <p>Log Probability Computation: <pre><code>def log_probs(self, x):\n    log_det_list = []\n    for flow in self.nf:\n        x, log_det = flow.inverse(x)  # Transform x \u2192 z\n        log_det_list.append(log_det)\n\n    sum_log_det = torch.stack(log_det_list, dim=1).sum(dim=1)\n    z = x  # Final z after all transformations\n    p_z = self.base_dist.log_prob(z).sum(-1)  # Prior log probability\n    log_prob = (p_z + sum_log_det).mean()  # Change of variables formula\n    return log_prob\n</code></pre></p> <p>Sampling Process: <pre><code>def sample(self, device, n):\n    x_sample = torch.randn(n, self.input_size).to(device)  # Sample from prior\n    for flow in self.nf[::-1]:  # Reverse order for sampling\n        x_sample, log_det = flow.forward(x_sample)  # Transform z \u2192 x\n    return x_sample.cpu().data.numpy()\n</code></pre></p> <p>Understanding the Flow Methods:</p> <ol> <li> <p>During Training (likelihood computation): <pre><code># We have x, want to compute log p(x)\nfor flow in self.nf:  # Forward order\n    x, log_det = flow.inverse(x)  # x \u2192 z (inverse of this flow)\n</code></pre></p> </li> <li> <p>During Sampling: <pre><code># We have z, want to get x\nfor flow in self.nf[::-1]:  # Reverse order\n    x_sample, log_det = flow.forward(x_sample)  # z \u2192 x (forward of this flow)\n</code></pre></p> </li> </ol> <p>In other words:</p> <ul> <li>Each flow's <code>forward()</code> method: Transforms \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\) for that specific flow</li> <li>Each flow's <code>inverse()</code> method: Transforms \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\) for that specific flow</li> <li>During training: We use <code>inverse()</code> to go from data space to latent space</li> <li>During sampling: We use <code>forward()</code> to go from latent space to data space</li> </ul> <p>Mathematical Perspective: Let \\(f_i\\) denote the forward transformation of the \\(i\\)-th flow (from \\(\\mathbf{z}\\) to \\(\\mathbf{x}\\)), and \\(f_i^{-1}\\) denote its inverse transformation (from \\(\\mathbf{x}\\) to \\(\\mathbf{z}\\)).</p> <ul> <li>Training: \\(f_k^{-1} \\circ f_{k-1}^{-1} \\circ \\cdots \\circ f_1^{-1}(\\mathbf{x}) = \\mathbf{z}\\) (using <code>inverse()</code> methods)</li> <li>Sampling: \\(f_1 \\circ f_2 \\circ \\cdots \\circ f_k(\\mathbf{z}) = \\mathbf{x}\\) (using <code>forward()</code> methods)</li> </ul> <p>What is \\(k\\)?</p> <p>The parameter \\(k\\) represents the total number of flow layers in the MAF model. In the implementation, this corresponds to <code>n_flows</code> in the MAF constructor.</p> <p>In the MAF Architecture: <pre><code>def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n    # n_flows = k (total number of flow layers)\n    for i in range(self.n_flows):  # i goes from 0 to k-1\n        nf_blocks.append(MADE(self.input_size, hidden_size, n_hidden))\n        nf_blocks.append(PermuteLayer(self.input_size))\n</code></pre></p> <p>Flow Composition Structure: <pre><code>Input \u2192 MADE\u2081 \u2192 Permute\u2081 \u2192 MADE\u2082 \u2192 Permute\u2082 \u2192 ... \u2192 MADE\u2096 \u2192 Permute\u2096 \u2192 Output\n</code></pre></p> <p>Where: - \\(f_1\\): First MADE block (MADE\u2081) - \\(f_2\\): Second MADE block (MADE\u2082) - ... - \\(f_k\\): Last MADE block (MADE\u2096)</p> <p>Example with \\(k = 3\\): - Training: \\(f_3^{-1} \\circ f_2^{-1} \\circ f_1^{-1}(\\mathbf{x}) = \\mathbf{z}\\) - Sampling: \\(f_1 \\circ f_2 \\circ f_3(\\mathbf{z}) = \\mathbf{x}\\)</p> <p>Key Insight: The <code>forward()</code> method of each flow is designed to be the inverse transformation for the overall model's training direction. This is why we use <code>forward()</code> during sampling in reverse order.</p> <p>This implementation demonstrates how the theoretical concepts of MAF translate into practical code, showing the interplay between autoregressive structure, masking, and flow transformations.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#inverse-autoregressive-flow-iaf","title":"Inverse Autoregressive Flow (IAF)","text":"<p>To address the sampling problem (sequential) in MAF, the Inverse Autoregressive Flow (IAF) simply inverts the generating process. In this case, the sampling (generation), is still parallelized. However, computing the likelihood of new data points is slow.</p> <p>Forward mapping from \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\) (parallel):</p> <ol> <li> <p>Sample \\(z_i \\sim \\mathcal{N}(0,1)\\) for \\(i = 1, \\ldots, n\\)</p> </li> <li> <p>Compute all \\(\\mu_i, \\alpha_i\\) (can be done in parallel)</p> </li> <li> <p>Let \\(x_1 = \\exp(\\alpha_1)z_1 + \\mu_1\\)</p> </li> <li> <p>Let \\(x_2 = \\exp(\\alpha_2)z_2 + \\mu_2\\)</p> </li> <li> <p>\\(\\ldots\\)</p> </li> </ol> <p>Inverse mapping from \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\) (sequential):</p> <ol> <li> <p>Let \\(z_1 = (x_1 - \\mu_1)/\\exp(\\alpha_1)\\)</p> </li> <li> <p>Compute \\(\\mu_2(z_1), \\alpha_2(z_1)\\)</p> </li> <li> <p>Let \\(z_2 = (x_2 - \\mu_2)/\\exp(\\alpha_2)\\)</p> </li> <li> <p>Compute \\(\\mu_3(z_1,z_2), \\alpha_3(z_1,z_2)\\)</p> </li> <li> <p>\\(\\ldots\\)</p> </li> </ol> <p>Key insight: Fast to sample from, slow to evaluate likelihoods of data points (train).</p> <p>Efficient Likelihood for Generated Points: However, for generated points the likelihood can be computed efficiently (since the noise are already obtained). When we generate samples using IAF, we start with known noise values \\(\\mathbf{z}\\) and transform them to get \\(\\mathbf{x}\\). Since we already have the noise values, we don't need to perform the expensive sequential inverse mapping to recover them. We can directly compute the likelihood using the change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) - \\sum_{i=1}^n \\alpha_i\\] <p>where we already know all the \\(\\alpha_i\\) values from the forward pass. This is much faster than the \\(O(n)\\) sequential computation required for arbitrary data points.</p> <p>Derivation of the Change of Variables Formula for IAF:</p> <p>Let's derive how we get this formula. Starting with the general change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) + \\log \\left|\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>For IAF, the forward transformation is:</p> \\[x_i = \\exp(\\alpha_i)z_i + \\mu_i\\] <p>The inverse transformation is:</p> \\[z_i = \\frac{x_i - \\mu_i}{\\exp(\\alpha_i)}\\] <p>The Jacobian matrix \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\) is diagonal because each \\(z_i\\) only depends on \\(x_i\\):</p> \\[\\frac{\\partial z_i}{\\partial x_j} = \\begin{cases}  \\frac{1}{\\exp(\\alpha_i)} &amp; \\text{if } i = j \\\\ 0 &amp; \\text{if } i \\neq j \\end{cases}\\] <p>Therefore, the determinant is the product of the diagonal elements:</p> \\[\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right) = \\prod_{i=1}^n \\frac{1}{\\exp(\\alpha_i)} = \\exp\\left(-\\sum_{i=1}^n \\alpha_i\\right)\\] <p>Taking the absolute value and logarithm:</p> \\[\\log \\left|\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)\\right| = \\log \\exp\\left(-\\sum_{i=1}^n \\alpha_i\\right) = -\\sum_{i=1}^n \\alpha_i\\] <p>Substituting back into the change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) - \\sum_{i=1}^n \\alpha_i\\] <p>This derivation shows why the likelihood computation is efficient for generated samples - we already have all the \\(\\alpha_i\\) values from the forward pass, so we just need to sum them up.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/","title":"Variational autoencoders","text":""},{"location":"AI/deep_generative_models/variational_autoencoders/#representation","title":"Representation","text":"<p>Consider a directed, latent variable model as shown below.</p> <p></p> <p>In the model above, \\(z\\) and \\(x\\) denote the latent and observed variables respectively. The joint distribution expressed by this model is given as</p> \\[p(x,z) = p(x|z)p(z).\\] <p>From a generative modeling perspective, this model describes a generative process for the observed data \\(x\\) using the following procedure:</p> \\[z \\sim p(z)\\] \\[x \\sim p(x|z)\\] <p>If one adopts the belief that the latent variables \\(z\\) somehow encode semantically meaningful information about \\(x\\), it is natural to view this generative process as first generating the \"high-level\" semantic information about \\(x\\) first before fully generating \\(x\\).</p> <p>We now consider a family of distributions \\(\\mathcal{Z}\\) where \\(p(z) \\in \\mathcal{Z}\\) describes a probability distribution over \\(z\\). Next, consider a family of conditional distributions \\(\\mathcal{X|Z}\\) where \\(p(x|z) \\in \\mathcal{X|Z}\\) describes a conditional probability distribution over \\(x\\) given \\(z\\). Then our hypothesis class of generative models is the set of all possible combinations</p> \\[\\mathcal{X,Z} = \\{p(x,z) \\mid p(z) \\in \\mathcal{Z}, p(x|z) \\in \\mathcal{X|Z}\\}.\\] <p>Given a dataset \\(\\mathcal{D} = \\{x^{(1)}, \\ldots, x^{(n)}\\}\\), we are interested in the following learning and inference tasks:</p> <ol> <li>Selecting \\(p \\in \\mathcal{X,Z}\\) that \"best\" fits \\(\\mathcal{D}\\).</li> <li>Given a sample \\(x\\) and a model \\(p \\in \\mathcal{X,Z}\\), what is the posterior distribution over the latent variables \\(z\\)?</li> </ol> <p>The posterior distribution \\(p(z|x)\\) represents our updated beliefs about the latent variables \\(z\\) after observing the data \\(x\\). In other words, it tells us what values of \\(z\\) are most likely to have generated the observed \\(x\\). This is particularly useful for tasks like feature extraction, where we want to understand what latent factors might have generated our observed data.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#learning-directed-latent-variable-models","title":"Learning Directed Latent Variable Models","text":"<p>One way to measure how closely \\(p(x,z)\\) fits the observed dataset \\(\\mathcal{D}\\) is to measure the Kullback-Leibler (KL) divergence between the data distribution (which we denote as \\(p_{data}(x)\\)) and the model's marginal distribution \\(p(x) = \\int p(x,z)dz\\). The distribution that \"best\" fits the data is thus obtained by minimizing the KL divergence.</p> \\[\\min_{p \\in \\mathcal{X,Z}} D_{KL}(p_{data}(x) \\| p(x)).\\] <p>As we have seen previously, optimizing an empirical estimate of the KL divergence is equivalent to maximizing the marginal log-likelihood \\(\\log p(x)\\) over \\(\\mathcal{D}\\):</p> \\[\\max_{p \\in \\mathcal{X,Z}} \\sum_{x \\in \\mathcal{D}} \\log p(x) = \\max_{p \\in \\mathcal{X,Z}} \\sum_{x \\in \\mathcal{D}} \\log \\int p(x,z)dz.\\] <p>However, it turns out this problem is generally intractable for high-dimensional \\(z\\) as it involves an integration (or sums in the case \\(z\\) is discrete) over all the possible latent sources of variation \\(z\\). This intractability arises from several challenges:</p> <ol> <li> <p>Computational Complexity: The integral \\(\\int p(x,z)dz\\) requires evaluating the joint distribution \\(p(x,z)\\) for all possible values of \\(z\\). In high-dimensional spaces, this becomes computationally prohibitive as the number of points to evaluate grows exponentially with the dimension of \\(z\\).</p> </li> <li> <p>Numerical Integration: Even if we could evaluate the integrand at all points, computing the integral numerically becomes increasingly difficult as the dimension of \\(z\\) grows. Traditional numerical integration methods like quadrature become impractical in high dimensions.</p> </li> <li> <p>Posterior Inference: The intractability of the marginal likelihood also makes it difficult to compute the posterior distribution \\(p(z|x)\\), which is crucial for tasks like feature extraction and data generation.</p> </li> </ol> <p>This intractability motivates the need for approximate inference methods, such as variational inference. One option is to estimate the objective via Monte Carlo. For any given datapoint \\(x\\), we can obtain the following estimate for its marginal log-likelihood:</p> \\[\\log p(x) \\approx \\log \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>This Monte Carlo estimate is derived as follows:</p> <p>First, recall that the marginal likelihood \\(p(x)\\) can be written as an expectation:</p> \\[p(x) = \\int p(x|z)p(z)dz = \\mathbb{E}_{z \\sim p(z)}[p(x|z)]\\] <p>The Monte Carlo method approximates this expectation by drawing \\(k\\) samples from \\(p(z)\\) and computing their average:</p> \\[\\mathbb{E}_{z \\sim p(z)}[p(x|z)] \\approx \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>Taking the logarithm of both sides gives us our final estimate:</p> \\[\\log p(x) \\approx \\log \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>This approximation becomes more accurate as \\(k\\) increases, but at the cost of more computational resources. The key insight is that we're using random sampling to approximate the intractable integral, trading exact computation for statistical estimation.</p> <p>Rather than maximizing the log-likelihood directly, an alternate is to instead construct a lower bound that is more amenable to optimization. To do so, we note that evaluating the marginal likelihood \\(p(x)\\) is at least as difficult as as evaluating the posterior \\(p(z|x)\\) for any latent vector \\(z\\) since by definition \\(p(z|x) = p(x,z)/p(x)\\).</p> <p>Next, we introduce a variational family \\(\\mathcal{Q}\\) of distributions that approximate the true, but intractable posterior \\(p(z|x)\\). Further henceforth, we will assume a parameteric setting where any distribution in the model family \\(\\mathcal{X,Z}\\) is specified via a set of parameters \\(\\theta \\in \\Theta\\) and distributions in the variational family \\(\\mathcal{Q}\\) are specified via a set of parameters \\(\\lambda \\in \\Lambda\\).</p> <p>Given \\(\\mathcal{X,Z}\\) and \\(\\mathcal{Q}\\), we note that the following relationships hold true for any \\(x\\) and all variational distributions \\(q_\\lambda(z) \\in \\mathcal{Q}\\):</p> \\[\\log p_\\theta(x) = \\log \\int p_\\theta(x,z)dz = \\log \\int \\frac{q_\\lambda(z)}{q_\\lambda(z)}p_\\theta(x,z)dz \\geq \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] := \\text{ELBO}(x;\\theta,\\lambda)\\] <p>where we have used Jensen's inequality in the final step. The key insight here is that since the logarithm function is concave, Jensen's inequality tells us that for any random variable \\(X\\) and concave function \\(f\\), we have \\(\\mathbb{E}[f(X)] \\leq f(\\mathbb{E}[X])\\). In our case:</p> <p>We first multiply and divide by \\(q_\\lambda(z)\\) inside the integral to get:</p> \\[\\log \\int \\frac{q_\\lambda(z)}{q_\\lambda(z)}p_\\theta(x,z)dz = \\log \\int q_\\lambda(z)\\frac{p_\\theta(x,z)}{q_\\lambda(z)}dz\\] <p>The integral \\(\\int q_\\lambda(z)\\frac{p_\\theta(x,z)}{q_\\lambda(z)}dz\\) can be seen as an expectation \\(\\mathbb{E}_{q_\\lambda(z)}\\left[\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\)</p> <p>Since \\(\\log\\) is a concave function, Jensen's inequality gives us:</p> \\[\\log \\mathbb{E}_{q_\\lambda(z)}\\left[\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] \\geq \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\] <p>This inequality is what allows us to obtain a lower bound on the log-likelihood, which we call the Evidence Lower BOund (ELBO). The ELBO admits a tractable unbiased Monte Carlo estimator</p> \\[\\frac{1}{k}\\sum_{i=1}^k \\log\\frac{p_\\theta(x,z^{(i)})}{q_\\lambda(z^{(i)})}, \\text{ where } z^{(i)} \\sim q_\\lambda(z),\\] <p>so long as it is easy to sample from and evaluate densities for \\(q_\\lambda(z)\\).</p> <p>In summary, we can learn a latent variable model by maximizing the ELBO with respect to both the model parameters \\(\\theta\\) and the variational parameters \\(\\lambda\\) for any given datapoint \\(x\\):</p> \\[\\max_\\theta \\sum_{x \\in \\mathcal{D}} \\max_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right].\\] <p>This optimization objective can be broken down into two parts:</p> <ol> <li>Inner Optimization: For each datapoint \\(x\\), we find the best variational parameters \\(\\lambda\\) that make \\(q_\\lambda(z)\\) as close as possible to the true posterior \\(p(z|x)\\). This is done by maximizing the ELBO with respect to \\(\\lambda\\). </li> </ol> <p>Why do we need \\(q_\\lambda(z)\\) to approximate \\(p(z|x)\\)? Since \\(p(x) = p(x,z)/p(z|x)\\), as \\(q_\\lambda(z)\\) tends to \\(p(z|x)\\), the ratio \\(p(x,z)/q_\\lambda(z)\\) tends to \\(p(x)\\). This means that by making our variational approximation closer to the true posterior, we get a better estimate of the marginal likelihood \\(p(x)\\).</p> <ol> <li>Outer Optimization: Across all datapoints in the dataset \\(\\mathcal{D}\\), we find the best model parameters \\(\\theta\\) that maximize the average ELBO. This improves the generative model's ability to explain the data.</li> </ol> <p>The outer sum \\(\\sum_{x \\in \\mathcal{D}}\\) is necessary because we want to learn a model that works well for all datapoints in our dataset, not just a single example. This is equivalent to maximizing the average ELBO across all datapoints.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#black-box-variational-inference","title":"Black-Box Variational Inference","text":"<p>We shall focus on first-order stochastic gradient methods for optimizing the ELBO. This inspires Black-Box Variational Inference (BBVI), a general-purpose Expectation-Maximization-like algorithm for variational learning of latent variable models, where, for each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), the following two steps are performed.</p> <p>Step 1</p> <p>We first do per-sample optimization of \\(q\\) by iteratively applying the update</p> \\[\\lambda^{(i)} \\leftarrow \\lambda^{(i)} + \\tilde{\\nabla}_\\lambda \\text{ELBO}(x^{(i)}; \\theta, \\lambda^{(i)}),\\] <p>where \\(\\text{ELBO}(x; \\theta, \\lambda) = \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\), and \\(\\tilde{\\nabla}_\\lambda\\) denotes an unbiased estimate of the ELBO gradient. This step seeks to approximate the log-likelihood \\(\\log p_\\theta(x^{(i)})\\).</p> <p>Step 2</p> <p>We then perform a single update step based on the mini-batch</p> \\[\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_i \\text{ELBO}(x^{(i)}; \\theta, \\lambda^{(i)}),\\] <p>which corresponds to the step that hopefully moves \\(p_\\theta\\) closer to \\(p_{data}\\).</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#gradient-estimation","title":"Gradient Estimation","text":"<p>The gradients \\(\\nabla_\\lambda \\text{ELBO}\\) and \\(\\nabla_\\theta \\text{ELBO}\\) can be estimated via Monte Carlo sampling. While it is straightforward to construct an unbiased estimate of \\(\\nabla_\\theta \\text{ELBO}\\) by simply pushing \\(\\nabla_\\theta\\) through the expectation operator, the same cannot be said for \\(\\nabla_\\lambda\\). Instead, we see that</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\mathbb{E}_{q_\\lambda(z)}\\left[\\left(\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z)\\right].\\] <p>This equality follows from the log-derivative trick (also commonly referred to as the REINFORCE trick). To derive this, we start with the gradient of the expectation:</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\nabla_\\lambda \\int q_\\lambda(z) \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>Using the product rule and chain rule:</p> \\[= \\int \\nabla_\\lambda q_\\lambda(z) \\cdot \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} + q_\\lambda(z) \\cdot \\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>The second term vanishes because: \\(\\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} = \\nabla_\\lambda [\\log p_\\theta(x,z) - \\log q_\\lambda(z)]\\). Since \\(p_\\theta(x,z)\\) doesn't depend on \\(\\lambda\\), \\(\\nabla_\\lambda \\log p_\\theta(x,z) = 0\\). Therefore, \\(\\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} = -\\nabla_\\lambda \\log q_\\lambda(z)\\).  When we multiply by \\(q_\\lambda(z)\\) and integrate, we get:</p> \\[\\int q_\\lambda(z) \\cdot (-\\nabla_\\lambda \\log q_\\lambda(z)) dz = -\\int \\nabla_\\lambda q_\\lambda(z) dz = -\\nabla_\\lambda \\int q_\\lambda(z) dz = -\\nabla_\\lambda 1 = 0\\] <p>where we used the fact that \\(\\int q_\\lambda(z) dz = 1\\) for any valid probability distribution.</p> <p>For the first term, we use the identity \\(\\nabla_\\lambda q_\\lambda(z) = q_\\lambda(z) \\nabla_\\lambda \\log q_\\lambda(z)\\):</p> \\[= \\int q_\\lambda(z) \\nabla_\\lambda \\log q_\\lambda(z) \\cdot \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>This can be rewritten as an expectation:</p> \\[= \\mathbb{E}_{q_\\lambda(z)}\\left[\\left(\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z)\\right]\\] <p>The gradient estimator \\(\\tilde{\\nabla}_\\lambda \\text{ELBO}\\) is thus</p> \\[\\frac{1}{k}\\sum_{i=1}^k \\left[\\left(\\log\\frac{p_\\theta(x,z^{(i)})}{q_\\lambda(z^{(i)})}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z^{(i)})\\right], \\text{ where } z^{(i)} \\sim q_\\lambda(z).\\] <p>However, it is often noted that this estimator suffers from high variance. One of the key contributions of the variational autoencoder paper is the reparameterization trick, which introduces a fixed, auxiliary distribution \\(p(\\epsilon)\\) and a differentiable function \\(T(\\epsilon; \\lambda)\\) such that the procedure</p> \\[\\epsilon \\sim p(\\epsilon)\\] \\[z \\leftarrow T(\\epsilon; \\lambda),\\] <p>is equivalent to sampling from \\(q_\\lambda(z)\\). This two-step procedure works as follows:</p> <ol> <li>First, we sample \\(\\epsilon\\) from a fixed distribution \\(p(\\epsilon)\\) that doesn't depend on \\(\\lambda\\) (e.g., standard normal)</li> <li>Then, we transform this sample using a deterministic function \\(T(\\epsilon; \\lambda)\\) that depends on \\(\\lambda\\)</li> </ol> <p>The key insight is that if we choose \\(T\\) appropriately, the distribution of \\(z = T(\\epsilon; \\lambda)\\) will be exactly \\(q_\\lambda(z)\\). For example, if \\(q_\\lambda(z)\\) is a normal distribution with mean \\(\\mu_\\lambda\\) and standard deviation \\(\\sigma_\\lambda\\), we can use:</p> <p>\\(p(\\epsilon) = \\mathcal{N}(0, 1)\\)</p> <p>\\(T(\\epsilon; \\lambda) = \\mu_\\lambda + \\sigma_\\lambda \\cdot \\epsilon\\)</p> <p>By the Law of the Unconscious Statistician, we can see that</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\mathbb{E}_{p(\\epsilon)}\\left[\\nabla_\\lambda \\log\\frac{p_\\theta(x,T(\\epsilon; \\lambda))}{q_\\lambda(T(\\epsilon; \\lambda))}\\right].\\] <p>In contrast to the REINFORCE trick, the reparameterization trick is often noted empirically to have lower variance and thus results in more stable training.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#parameterizing-distributions-via-deep-neural-networks","title":"Parameterizing Distributions via Deep Neural Networks","text":"<p>So far, we have described \\(p_\\theta(x,z)\\) and \\(q_\\lambda(z)\\) in the abstract. To instantiate these objects, we consider choices of parametric distributions for \\(p_\\theta(z)\\), \\(p_\\theta(x|z)\\), and \\(q_\\lambda(z)\\). A popular choice for \\(p_\\theta(z)\\) is the unit Gaussian</p> \\[p_\\theta(z) = \\mathcal{N}(z|0,I),\\] <p>in which case \\(\\theta\\) is simply the empty set since the prior is a fixed distribution.</p> <p>In the case where \\(p_\\theta(x|z)\\) is a Gaussian distribution, we can thus represent it as</p> \\[p_\\theta(x|z) = \\mathcal{N}(x|\\mu_\\theta(z), \\Sigma_\\theta(z)),\\] <p>where \\(\\mu_\\theta(z)\\) and \\(\\Sigma_\\theta(z)\\) are neural networks that specify the mean and covariance matrix for the Gaussian distribution over \\(x\\) when conditioned on \\(z\\).</p> <p>Finally, the variational family for the proposal distribution \\(q_\\lambda(z)\\) needs to be chosen judiciously so that the reparameterization trick is possible. Many continuous distributions in the location-scale family can be reparameterized. In practice, a popular choice is again the Gaussian distribution, where</p> \\[\\begin{align*} \\lambda &amp;= (\\mu, \\Sigma) \\\\ q_\\lambda(z) &amp;= \\mathcal{N}(z|\\mu, \\Sigma) \\\\ p(\\varepsilon) &amp;= \\mathcal{N}(z|0,I) \\\\ T(\\varepsilon; \\lambda) &amp;= \\mu + \\Sigma^{1/2}\\varepsilon, \\end{align*}\\] <p>where \\(\\Sigma^{1/2}\\) is the Cholesky decomposition of \\(\\Sigma\\). For simplicity, practitioners often restrict \\(\\Sigma\\) to be a diagonal matrix (which restricts the distribution family to that of factorized Gaussians).</p> <p>The reparameterization trick consists of four key steps:</p> <ol> <li> <p>Parameter Definition: We define the variational parameters \\(\\lambda\\) as a tuple containing the mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\) of our Gaussian distribution. These parameters will be learned during training.</p> </li> <li> <p>Variational Distribution: We specify that our variational distribution \\(q_\\lambda(z)\\) is a Gaussian distribution parameterized by \\(\\mu\\) and \\(\\Sigma\\). This is the distribution we ideally want to sample from.</p> </li> <li> <p>Auxiliary Distribution: Instead of sampling directly from \\(q_\\lambda(z)\\), we introduce a fixed auxiliary distribution \\(p(\\varepsilon)\\) which is a standard normal distribution (mean 0, identity covariance). This distribution doesn't depend on our parameters \\(\\lambda\\).</p> </li> <li> <p>Transformation Function: We define a deterministic function \\(T(\\varepsilon; \\lambda)\\) that transforms samples from the auxiliary distribution into samples from our variational distribution. The transformation is given by \\(\\mu + \\Sigma^{1/2}\\varepsilon\\), where \\(\\Sigma^{1/2}\\) is the Cholesky decomposition of \\(\\Sigma\\).</p> </li> </ol> <p>The key insight is that instead of sampling directly from \\(q_\\lambda(z)\\), we can: 1. Sample \\(\\varepsilon\\) from the standard normal distribution \\(p(\\varepsilon)\\) 2. Transform it using \\(T(\\varepsilon; \\lambda)\\) to make it seem like we're getting a sample from \\(q_\\lambda(z)\\)</p> <p>This trick is crucial because it allows us to compute gradients with respect to \\(\\lambda\\) through the sampling process. Since the transformation \\(T\\) is differentiable, we can backpropagate through it to update the parameters \\(\\lambda\\) during training. This is why the reparameterization trick often leads to lower variance in gradient estimates compared to the REINFORCE trick.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#amortized-variational-inference","title":"Amortized Variational Inference","text":"<p>A noticeable limitation of black-box variational inference is that Step 1 executes an optimization subroutine that is computationally expensive. Recall that the goal of Step 1 is to find</p> \\[\\lambda^* = \\arg\\max_{\\lambda \\in \\Lambda} \\text{ELBO}(x; \\theta, \\lambda).\\] <p>For a given choice of \\(\\theta\\), there is a well-defined mapping from \\(x \\mapsto \\lambda^*\\). A key realization is that this mapping can be learned. In particular, one can train an encoding function (parameterized by \\(\\phi\\)) \\(f_\\phi: \\mathcal{X} \\to \\Lambda\\) (where \\(\\Lambda\\) is the space of \\(\\lambda\\) parameters) on the following objective</p> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, f_\\phi(x)).\\] <p>It is worth noting at this point that \\(f_\\phi(x)\\) can be interpreted as defining the conditional distribution \\(q_\\phi(z|x)\\). With a slight abuse of notation, we define</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right],\\] <p>and rewrite the optimization problem as</p> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, \\phi).\\] <p>It is also worth noting that optimizing \\(\\phi\\) over the entire dataset as a subroutine every time we sample a new mini-batch is clearly not reasonable. However, if we believe that \\(f_\\phi\\) is capable of quickly adapting to a close-enough approximation of \\(\\lambda^*\\) given the current choice of \\(\\theta\\), then we can interleave the optimization of \\(\\phi\\) and \\(\\theta\\). This yields the following procedure, where for each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), we perform the following two updates jointly:</p> \\[\\begin{align*} \\phi &amp;\\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\\\ \\theta &amp;\\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi), \\end{align*}\\] <p>rather than running BBVI's Step 1 as a subroutine. By leveraging the learnability of \\(x \\mapsto \\lambda^*\\), this optimization procedure amortizes the cost of variational inference. If one further chooses to define \\(f_\\phi\\) as a neural network, the result is the variational autoencoder.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#steps-of-amortized-variational-inference","title":"Steps of Amortized Variational Inference","text":"<p>Let's break down the amortized variational inference procedure in detail:</p> <ol> <li>Initial Setup:</li> <li>We have a dataset \\(\\mathcal{D} = \\{x^{(1)}, \\ldots, x^{(n)}\\}\\)</li> <li>We have a generative model \\(p_\\theta(x,z)\\) with parameters \\(\\theta\\)</li> <li> <p>We want to learn both the model parameters \\(\\theta\\) and the variational parameters \\(\\lambda\\) for each datapoint</p> </li> <li> <p>Traditional BBVI Approach:</p> </li> <li>For each datapoint \\(x\\), we would need to run an optimization to find:</li> </ol> \\[\\lambda^* = \\arg\\max_{\\lambda \\in \\Lambda} \\text{ELBO}(x; \\theta, \\lambda)\\] <ul> <li> <p>This is computationally expensive as it requires running an optimization subroutine for each datapoint</p> </li> <li> <p>Key Insight - Learnable Mapping:</p> </li> <li>Instead of optimizing \\(\\lambda\\) separately for each \\(x\\), we realize that there's a mapping from \\(x\\) to \\(\\lambda^*\\)</li> <li>This mapping can be learned using a function \\(f_\\phi: \\mathcal{X} \\to \\Lambda\\) parameterized by \\(\\phi\\)</li> <li> <p>The function \\(f_\\phi\\) takes a datapoint \\(x\\) and outputs the variational parameters \\(\\lambda\\)</p> </li> <li> <p>Training the Encoder:</p> </li> <li>We train \\(f_\\phi\\) to maximize the ELBO across all datapoints:</li> </ul> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, f_\\phi(x))\\] <ul> <li> <p>This is equivalent to learning a conditional distribution \\(q_\\phi(z|x)\\)</p> </li> <li> <p>Joint Optimization:</p> </li> <li>Instead of running BBVI's Step 1 as a subroutine, we interleave the optimization of \\(\\phi\\) and \\(\\theta\\)</li> <li>For each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), we perform two updates:</li> </ul> \\[\\begin{align*} \\phi &amp;\\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\\\ \\theta &amp;\\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\end{align*}\\] <ol> <li>Practical Implementation:</li> <li>When \\(f_\\phi\\) is implemented as a neural network, we get a variational autoencoder</li> <li>The encoder network \\(f_\\phi\\) maps inputs \\(x\\) to variational parameters</li> <li>The decoder network maps latent variables \\(z\\) to reconstructed inputs</li> <li>Both networks are trained end-to-end using the ELBO objective</li> </ol> <p>In practice, the encoder neural network \\(f_\\phi\\) outputs the parameters of a diagonal Gaussian distribution:</p> \\[q_\\phi(z|x) = \\mathcal{N}(z|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))\\] <p>where \\(\\mu_\\phi(x)\\) and \\(\\sigma^2_\\phi(x)\\) are the mean and variance vectors output by the encoder network. To sample from this distribution during training, we use the reparameterization trick:</p> \\[z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,I)\\] <p>where \\(\\odot\\) denotes element-wise multiplication. This allows us to backpropagate through the sampling process and train the encoder network end-to-end.</p> <p>The key advantage of this approach is that it amortizes the cost of variational inference by learning a single function \\(f_\\phi\\) that can quickly approximate the optimal variational parameters for any input \\(x\\), rather than running a separate optimization for each datapoint.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#decomposition-of-the-negative-elbo","title":"Decomposition of the Negative ELBO","text":"<p>Starting with the definition of the ELBO:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right]\\] <p>We can expand the joint distribution \\(p_\\theta(x,z)\\) using the chain rule of probability:</p> \\[p_\\theta(x,z) = p_\\theta(x|z)p_\\theta(z)\\] <p>Substituting this into the ELBO:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x|z)p_\\theta(z)}{q_\\phi(z|x)}\\right]\\] <p>Using the properties of logarithms, we can split this into three terms:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)]\\] <p>The second and third terms can be combined to form the KL divergence between \\(q_\\phi(z|x)\\) and \\(p_\\theta(z)\\):</p> \\[\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)] = -\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{q_\\phi(z|x)}{p_\\theta(z)}\\right] = -D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\] <p>Therefore, the ELBO can be written as:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\] <p>It is insightful to note that the negative ELBO can be decomposed into two terms:</p> \\[-\\text{ELBO}(x; \\theta, \\phi) = \\underbrace{-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{Reconstruction Loss}} + \\underbrace{D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))}_{\\text{KL Divergence}}\\] <p>This decomposition reveals two key components of the training objective:</p> <ol> <li>Reconstruction Loss: \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\)</li> <li>This term measures how well the model can reconstruct the input \\(x\\) from its latent representation \\(z\\)</li> <li>It encourages the encoder to produce latent codes that preserve the essential information about the input</li> <li> <p>In practice, this is often implemented as the mean squared error or binary cross-entropy between the input and its reconstruction</p> </li> <li> <p>KL Divergence: \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\)</p> </li> <li>This term measures how far the approximate posterior \\(q_\\phi(z|x)\\) is from the prior \\(p_\\theta(z)\\)</li> <li>It encourages the latent space to follow the prior distribution (typically a standard normal distribution)</li> </ol>"},{"location":"AI/deep_generative_models/variational_autoencoders/#practical-implementation-of-elbo-computation","title":"Practical Implementation of ELBO Computation","text":"<p>Let's look at how the ELBO is actually computed in practice. Here's a detailed implementation with explanations:</p> <p>We implement the (rec+kl) decomposed form for practicality and clarity because:</p> <ul> <li>KL has a closed form (for two Gaussians \\(q_\\phi(z|x) \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and \\(p(z) \\sim \\mathcal{N}(0,I)\\), the KL term can be computed analytically). A closed form means we can compute the exact value using a finite number of standard operations (addition, multiplication, logarithms, etc.) without needing numerical integration or approximation. This closed form is derived as follows:</li> </ul> <p>For two multivariate Gaussians \\(q_\\phi(z|x) = \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(z) = \\mathcal{N}(0,I)\\), the KL divergence is:</p> \\[D_{KL}(q_\\phi(z|x) \\| p(z)) = \\frac{1}{2}\\left[\\text{tr}(\\Sigma) + \\mu^T\\mu - d - \\log|\\Sigma|\\right]\\] <p>where \\(\\text{tr}(\\Sigma)\\) is the trace of the covariance matrix \\(\\Sigma\\) (the sum of its diagonal elements), \\(\\mu^T\\mu\\) is the squared L2 norm of the mean vector, \\(d\\) is the dimension of the latent space, and \\(|\\Sigma|\\) is the determinant of \\(\\Sigma\\). For diagonal covariance matrices \\(\\Sigma = \\text{diag}(\\sigma^2)\\), this simplifies to:</p> \\[D_{KL}(q_\\phi(z|x) \\| p(z)) = \\frac{1}{2}\\sum_{i=1}^d (\\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1)\\] <p>This analytical solution is not only computationally efficient but also provides exact gradients, unlike Monte Carlo estimates which would require sampling.</p> <ul> <li> <p>The analytical KL avoids noisy gradients that arise from computing KL via sampling so the decomposition makes training more stable. When using Monte Carlo estimation, the gradients can have high variance due to the randomness in sampling. The analytical form provides deterministic gradients, which leads to more stable optimization. This is particularly important because the KL term acts as a regularizer, and having stable gradients for this term helps prevent the model from either collapsing to a degenerate solution (where the KL term becomes too small) or failing to learn meaningful representations (where the KL term dominates).</p> </li> <li> <p>The decomposed form allows you to monitor reconstruction loss and KL separately which is very helpful in debugging and understanding model behavior</p> </li> </ul> <pre><code>def negative_elbo_bound(self, x):\n    \"\"\"\n    Computes the Evidence Lower Bound, KL and, Reconstruction costs\n\n    Args:\n        x: tensor: (batch, dim): Observations\n\n    Returns:\n        nelbo: tensor: (): Negative evidence lower bound\n        kl: tensor: (): ELBO KL divergence to prior\n        rec: tensor: (): ELBO Reconstruction term\n    \"\"\"\n    # Step 1: Get the parameters of the approximate posterior q_phi(z|x)\n    q_phi_z_given_x_m, q_phi_z_given_x_v = self.enc(x)\n\n    # Step 2: Compute the KL divergence term\n    # This computes D_KL(q_phi(z|x) || p_theta(z))\n    kl = ut.kl_normal(q_phi_z_given_x_m, q_phi_z_given_x_v,\n                      self.z_prior_m, self.z_prior_v)\n\n    # Step 3: Take m samples from the approximate posterior using reparameterization\n    # This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)\n    z_samples = ut.sample_gaussian(\n        q_phi_z_given_x_m.expand(x.shape[0], self.z_dim),\n        q_phi_z_given_x_v.expand(x.shape[0], self.z_dim))\n\n    # Step 4: Get the decoder outputs (logits)\n    # These parameterize the Bernoulli distributions for reconstruction\n    f_theta_of_z = self.dec(z_samples)\n\n    # Step 5: Compute the reconstruction term\n    # This computes -E_q[log p_theta(x|z)] using binary cross-entropy\n    rec = -ut.log_bernoulli_with_logits(x, f_theta_of_z)\n\n    # Step 6: Combine terms to get the negative ELBO\n    nelbo = kl + rec\n\n    # Step 7: Average over the batch\n    nelbo_avg = torch.mean(nelbo)\n    kl_avg = torch.mean(kl)\n    rec_avg = torch.mean(rec)\n\n    return nelbo_avg, kl_avg, rec_avg\n</code></pre> <p>Let's break down each step:</p> <ol> <li>Encoder Output: </li> <li>The encoder network takes input \\(x\\) and outputs the parameters of the approximate posterior \\(q_\\phi(z|x)\\)</li> <li> <p>These parameters are the mean (\\(\\mu_\\phi(x)\\)) and variance (\\(\\sigma^2_\\phi(x)\\)) of a diagonal Gaussian</p> </li> <li> <p>KL Divergence:</p> </li> <li>Computes \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\)</li> <li>For diagonal Gaussians, this has a closed-form solution</li> <li> <p>The prior \\(p_\\theta(z)\\) is typically a standard normal distribution</p> </li> <li> <p>Sampling:</p> </li> <li>Uses the reparameterization trick to sample from \\(q_\\phi(z|x)\\)</li> <li>Implements \\(z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon\\) where \\(\\varepsilon \\sim \\mathcal{N}(0,I)\\)</li> <li> <p>The samples are used to estimate the reconstruction term</p> </li> <li> <p>Decoder Output:</p> </li> <li>The decoder network takes the sampled \\(z\\) and outputs logits</li> <li> <p>These logits parameterize Bernoulli distributions for each element of \\(x\\)</p> </li> <li> <p>Reconstruction Term:</p> </li> <li>Computes \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\)</li> <li>Uses binary cross-entropy loss which takes logits directly</li> <li> <p>The sigmoid function is incorporated into the loss computation</p> </li> <li> <p>Final ELBO:</p> </li> <li>Combines the KL divergence and reconstruction terms</li> <li> <p>The negative ELBO is what we minimize during training</p> </li> <li> <p>Batch Averaging:</p> </li> <li>Averages the losses over the batch</li> <li>This gives us the final training objective</li> </ol> <p>This implementation shows how the theoretical ELBO decomposition we discussed earlier is actually computed in practice, with all the necessary components for training a VAE on binary data.</p> <p>Note on Sampling from \\(q_\\phi(z|x)\\): The sampling step in the implementation is crucial for two reasons:</p> <p>Monte Carlo Estimation: The reconstruction term \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\) involves an expectation over \\(q_\\phi(z|x)\\). We estimate this expectation using Monte Carlo sampling:</p> \\[-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] \\approx -\\frac{1}{K}\\sum_{k=1}^K \\log p_\\theta(x|z^{(k)})\\] <p>where \\(z^{(k)} \\sim q_\\phi(z|x)\\). In practice, we often use \\(K=1\\) (a single sample) as it works well and is computationally efficient.</p> <p>Gradient Estimation: We need to compute gradients of this expectation with respect to both \\(\\phi\\) (encoder parameters) and \\(\\theta\\) (decoder parameters). The reparameterization trick allows us to: - Sample from a fixed distribution \\(p(\\varepsilon)\\) that doesn't depend on \\(\\phi\\) - Transform these samples using a deterministic function that depends on \\(\\phi\\) - Backpropagate through this transformation to compute gradients - This results in lower variance gradient estimates compared to the REINFORCE trick</p> <p>The sampling step is therefore essential for both estimating the ELBO and computing its gradients during training.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#-vae","title":"\u03b2-VAE","text":"<p>A popular variation of the normal VAE is called the \u03b2-VAE. The \u03b2-VAE optimizes the following objective:</p> \\[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\beta D_{KL}(q_\\phi(z|x) || p(z)) \\] <p>Here, \u03b2 is a positive real number. From a training objective, we want to decrease the negative of ELBO, also called NELBO:</p> \\[ \\text{NELBO} = \\text{Reconstruction Loss} + \\beta D_{KL}(q_\\phi(z|x) \\| p(z)) \\] <p>We see that the second term acts as a regularization term. \u03b2 can be thought of as a hyperparameter that adjusts how much we want to regularize. Greater the \u03b2, more is the training optimized to reduce KL divergence, and a higher possibility of overfitting (and also more the \\(q_\\phi(z|x)\\) closely approximates \\(p(z)\\)). Lesser the \u03b2, optimization is geared towards increasing the KL divergence, leading to a more general model. When \u03b2 is set to 1 however, we get the standard VAE.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#importance-weighted-autoencoder-iwae","title":"Importance Weighted Autoencoder (IWAE)","text":"<p>While the ELBO serves as a lower bound to the true marginal log-likelihood, it may be loose if the variational posterior \\(q_\\phi(z|x)\\) is a poor approximation to the true posterior \\(p_\\theta(z|x)\\). The key idea behind IWAE is to use \\(m &gt; 1\\) samples from the approximate posterior \\(q_\\phi(z|x)\\) to obtain the following IWAE bound:</p> \\[ \\mathcal{L}_m(x; \\theta,\\phi) = \\mathbb{E}_{z^{(1)},...,z^{(m)} \\text{ i.i.d.} \\sim q_\\phi(z|x)} \\log \\frac{1}{m}\\sum_{i=1}^m \\frac{p_\\theta(x,z^{(i)})}{q_\\phi(z^{(i)}|x)} \\] <p>Notice that for the special case of \\(m=1\\), the IWAE objective \\(\\mathcal{L}_m\\) reduces to the standard ELBO \\(\\mathcal{L}_1 = \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\).</p> <p>As a pseudocode, the main modification to the standard VAE would be:</p> <pre><code># Step 3: Take m samples from the approximate posterior using reparameterization\n# This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)\nz_samples = ut.sample_gaussian(\n    q_phi_z_given_x_m.expand(x.shape[0], self.z_dim),\n    q_phi_z_given_x_v.expand(x.shape[0], self.z_dim))\n</code></pre>"},{"location":"AI/deep_generative_models/variational_autoencoders/#gaussian-mixture-vae-gmvae","title":"Gaussian Mixture VAE (GMVAE)","text":"<p>The VAE's prior distribution was a parameter-free isotropic Gaussian \\(p_\\theta(z) = \\mathcal{N}(z|0,I)\\). While this original setup works well, there are settings in which we desire more expressivity to better model our data. Let's look at GMVAE, which has a mixture of Gaussians as the prior distribution.</p> \\[p_\\theta(z) = \\sum_{i=1}^k \\frac{1}{k}\\mathcal{N}(z|\\mu_i, \\text{diag}(\\sigma^2_i))\\] <p>where \\(i \\in \\{1, ..., k\\}\\) denotes the \\(i\\)th cluster index. For notational simplicity, we shall subsume our mixture of Gaussian parameters \\(\\{\\mu_i, \\sigma_i\\}_{i=1}^k\\) into our generative model parameters \\(\\theta\\). For simplicity, we have also assumed fixed uniform weights \\(1/k\\) over the possible different clusters.</p> <p>Apart from the prior, the GMVAE shares an identical setup as the VAE:</p> \\[q_\\phi(z|x) = \\mathcal{N}(z|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))\\] \\[p_\\theta(x|z) = \\text{Bern}(x|f_\\theta(z))\\] <p>Although the ELBO for the GMVAE: \\(\\mathbb{E}_{q_\\phi(z)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\) is identical to that of the VAE, we note that the KL term \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\) cannot be computed analytically between a Gaussian distribution \\(q_\\phi(z|x)\\) and a mixture of Gaussians \\(p_\\theta(z)\\). However, we can obtain its unbiased estimator via Monte Carlo sampling:</p> \\[D_{KL}(q_\\phi(z|x) \\| p_\\theta(z)) \\approx \\log q_\\phi(z^{(1)}|x) - \\log p_\\theta(z^{(1)})\\] \\[= \\underbrace{\\log\\mathcal{N}(z^{(1)}|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))}_{\\text{log normal}} - \\underbrace{\\log\\sum_{i=1}^k \\frac{1}{k}\\mathcal{N}(z^{(1)}|\\mu_i, \\text{diag}(\\sigma^2_i))}_{\\text{log normal mixture}}\\] <p>where \\(z^{(1)} \\sim q_\\phi(z|x)\\) denotes a single sample.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#the-semi-supervised-vae-ssvae","title":"The Semi-Supervised VAE (SSVAE)","text":"<p>The Semi-Supervised VAE (SSVAE) extends the standard VAE to handle both labeled and unlabeled data. In a semi-supervised setting, we have a dataset \\(\\mathcal{D}\\) that consists of: - Labeled data: \\(\\mathcal{D}_l = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N_l}\\) - Unlabeled data: \\(\\mathcal{D}_u = \\{x^{(i)}\\}_{i=1}^{N_u}\\)</p> <p>where \\(y^{(i)}\\) represents the class label for the \\(i\\)-th labeled example. The SSVAE introduces an additional latent variable \\(y\\) to model the class labels, and the joint distribution is factorized as:</p> \\[ p_\\theta(x, y, z) = p_\\theta(x|y,z)p_\\theta(y|z)p_\\theta(z) \\] <p>This factorization is derived from the chain rule of probability. We first factorize \\(p_\\theta(x, y, z)\\) as \\(p_\\theta(x|y,z)p_\\theta(y,z)\\), and then further factorize \\(p_\\theta(y,z)\\) as \\(p_\\theta(y|z)p_\\theta(z)\\). This reflects the generative process where: 1. First, we sample \\(z\\) from the prior \\(p_\\theta(z)\\) 2. Then, we sample \\(y\\) conditioned on \\(z\\) from \\(p_\\theta(y|z)\\) 3. Finally, we generate \\(x\\) conditioned on both \\(y\\) and \\(z\\) from \\(p_\\theta(x|y,z)\\)</p> <p>The approximate posterior for labeled data is:</p> \\[ q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y|x) \\] <p>This factorization is derived from the chain rule of probability for the approximate posterior. The chain rule states that for any random variables \\(A\\), \\(B\\), and \\(C\\), we can write:</p> \\[p(A,B|C) = p(A|B,C)p(B|C)\\] <p>This equation is derived from the definition of conditional probability. Let's break it down step by step:</p> <ol> <li>First, recall that conditional probability is defined as:</li> </ol> \\[p(A|B) = \\frac{p(A,B)}{p(B)}\\] <ol> <li>For our case with three variables, we can write:</li> </ol> \\[p(A,B|C) = \\frac{p(A,B,C)}{p(C)}\\] <ol> <li>We can also write:</li> </ol> \\[p(A|B,C) = \\frac{p(A,B,C)}{p(B,C)}\\] <p>and</p> \\[p(B|C) = \\frac{p(B,C)}{p(C)}\\] <ol> <li>Multiplying these last two equations:</li> </ol> \\[p(A|B,C)p(B|C) = \\frac{p(A,B,C)}{p(B,C)} \\cdot \\frac{p(B,C)}{p(C)} = \\frac{p(A,B,C)}{p(C)} = p(A,B|C)\\] <p>Therefore, we have proven that:</p> \\[p(A,B|C) = p(A|B,C)p(B|C)\\] <p>In our case, we can identify: - \\(A\\) as \\(z\\) (the latent code) - \\(B\\) as \\(y\\) (the label) - \\(C\\) as \\(x\\) (the observed data)</p> <p>Therefore, applying the chain rule:</p> \\[q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y|x)\\] <p>This means: 1. First, we predict the label \\(y\\) from \\(x\\) using \\(q_\\phi(y|x)\\) 2. Then, we infer the latent code \\(z\\) using both \\(x\\) and the predicted \\(y\\) through \\(q_\\phi(z|x,y)\\)</p> <p>and for unlabeled data:</p> \\[ q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y) \\] <p>For unlabeled data, since we don't know the true label \\(y\\), we use a prior distribution \\(q_\\phi(y)\\) (typically a uniform distribution over classes) instead of \\(q_\\phi(y|x)\\). The factorization reflects that: 1. We sample a label \\(y\\) from the prior \\(q_\\phi(y)\\) 2. Then, we infer the latent code \\(z\\) using both \\(x\\) and the sampled \\(y\\) through \\(q_\\phi(z|x,y)\\)</p> <p>The training objective for SSVAE combines: 1. The ELBO for labeled data 2. The ELBO for unlabeled data 3. A classification loss for labeled data</p> <p>This allows the model to learn both the data distribution and the class labels in a semi-supervised manner.</p>"},{"location":"math/probability/probability_and_counting/","title":"Probability and counting","text":"<p>Mathematics is the logic of certainty; probability is the logic of uncertainty.</p>"},{"location":"math/probability/probability_and_counting/#sample-spaces","title":"Sample spaces","text":"<p>The mathematical framework for probability is built around sets. Imagine that an experiment is performed, resulting in one out of a set of possible outcomes. Before the experiment is performed, it is unknown which outcome will be the result; after, the result \"crystallizes\" into the actual outcome. The sample space S of an experiment is the set of all possible outcomes of the experiment. An event A is a subset of the sample space S, and we say that A occurred if the actual outcome is in A.  When the sample space is finite, we can visualize it as Pebble World (figure above). Each pebble represents an outcome, and an event is a set of pebbles. Performing the experiment amounts to randomly selecting one pebble. If all the pebbles are of the same mass, all the pebbles are equally likely to be chosen.</p> <p>Set theory is very useful in probability, since it provides a rich language for express_ing and working with events. Set operations, especially unions, intersections, and complements, make it easy to build new events in terms of already defined events.</p> <p>Example:  A coin is flipped 10 times. Writing Heads as H and Tails as T, a possible outcome is: HHHTHHTTHT. The sample space is the set of all possible strings of length 10 consisting of H's and T's. We can (and will) encode H as <code>1</code> and T as <code>0</code>, so that an outcome is a sequence: $$ (s_1, s_2, \\dots, s_{10}) \\quad \\text{with} \\quad s_j \\in {0, 1} $$ The sample space is the set of all such sequences.</p> <p>Some Events:</p> <ol> <li>Event A_1: the first flip is Heads. As a set: $$ A_1 =  (1, s_2, \\dots, s_{10}) \\; \\mid \\; s_j \\in {0,1} \\; \\text{and } 2 \\leq j \\leq 10  $$ This is a subset of the sample space, so it is indeed an event. Saying that A_1 occurs is equivalent to saying that the first flip is Heads. Similarly, let A_j be the event that the j-th flip is Heads, for: $$ j = 2, 3, \\dots, 10 $$</li> <li>Event B: at least one flip was Heads. As a set: $$ B = \\bigcup_{j=1}^{10} A_j $$</li> <li>Event C: all the flips were Heads. As a set: $$ C = \\bigcap_{j=1}^{10} A_j $$</li> <li>Event D: there were at least two consecutive Heads. As a set: $$ D = \\bigcup_{j=1}^{9} \\left( A_j \\cap A_{j+1} \\right) $$ </li> </ol>"},{"location":"math/probability/probability_and_counting/#naive-definition-of-probability","title":"Naive definition of probability","text":"<p>(Naive definition of probability). Let \\( A \\) be an event for an experiment with a finite sample space \\( S \\). The naive probability of \\( A \\) is</p> \\[ P_{\\text{naive}}(A) = \\frac{|A|}{|S|} = \\frac{\\text{number of outcomes favorable to } A}{\\text{total number of outcomes in } S} \\] <p>The naive definition is very restrictive in that it requires S to be finite, with equal mass for each pebble. It has often been misapplied by people who assume equally likely outcomes without justification and make arguments to the e\ufb00ect of \u201ceither it will happen or it won\u2019t, and we don\u2019t know which, so it\u2019s 50-50\u201d.</p>"}]}