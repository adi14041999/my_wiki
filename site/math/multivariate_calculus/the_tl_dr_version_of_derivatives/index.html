
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/math/multivariate_calculus/the_tl_dr_version_of_derivatives/">
      
      
        <link rel="prev" href="../../probability/markov_chains/">
      
      
        <link rel="next" href="../multivariable_functions_level_sets_and_contour_plots/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>The TL;DR version of Derivatives - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-tldr-version-of-derivatives" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              The TL;DR version of Derivatives
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../ai/deep_learning_for_computer_vision/introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-tabs__link">
          
  
  
  Productivity

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Deep Learning for Computer Vision
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning for Computer Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/image_classification_with_linear_classifiers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Classification with Linear Classifiers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Regularization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/backpropagation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backpropagation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/backpropagation_for_a_linear_layer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backpropagation for a Linear Layer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Setting up the Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/neural_networks_setting_up_the_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Setting up the Data
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/neural_networks_learning_and_evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks- Learning and Evaluation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/putting_it_together_minimal_neural_network_case_study/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Putting it together- Minimal Neural Network Case Study
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/convolutional_neural_networks_architectures_convolution_pooling_layers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolutional Neural Networks- Architectures, Convolution / Pooling Layers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/a_review_of_rnns_and_transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    A review of RNNs and Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/self_supervised_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Self-Supervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_learning_for_computer_vision/a_review_of_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    A review of Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Natural Language Processing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Natural Language Processing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/representing_words/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Representing words
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/svd_based_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SVD based methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/distributional_semantics_and_word2vec/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributional semantics and Word2vec
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/language_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/recurrent_neural_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recurrent Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/gated_recurrent_units/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gated Recurrent Units (GRUs)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/long_short_term_memory_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Long-Short Term Memory (LSTM) Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/seq2seq_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Seq2Seq Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/attention_mechanism/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention Mechanism
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/how_large_language_models_work_a_visual_intro_to_transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    How large language models work, a visual intro to transformers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/attention_in_transformers_visually_explained/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention in transformers, visually explained
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/the_basic_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    The basic Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/decoder_only_transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Decoder-only Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/differences_between_the_basic_transformer_and_the_decoder_only_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Differences between the basic Transformer and the Decoder-Only Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/natural_language_processing/self_attention_and_transformers_a_mathematical_approach/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Self-Attention and Transformers, a mathematical approach
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/score_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/score_based_generative_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Generative Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/evaluating_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluating Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/diffusion_models_from_an_image_generation_perspective/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Diffusion Models from an image generation perspective
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/score_based_diffusion_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../linear_algebra/vectors_vector_addition_and_scalar_multiplication/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors, vector addition, and scalar multiplication
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector geometry in Rn and correlation coefficients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../linear_algebra/planes_in_r3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Planes in R3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../linear_algebra/span_subspaces_and_dimension/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Span, subspaces, and dimension
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../linear_algebra/basis_and_orthogonality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basis and orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../linear_algebra/projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Applications of projections in Rn- orthogonal bases of planes and linear regression
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/story_proofs_and_axioms_of_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Story Proofs and Axioms of Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/conditional_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/some_famous_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Some famous problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/indicator_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Indicator Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/poisson_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Poisson Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/continuous_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Continuous Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/exponential_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exponential Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/joint_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Joint Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/independence_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Independence of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/conditional_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/multinomial_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multinomial Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/covariance_and_correlation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Covariance and Correlation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/transformations_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformations of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/convolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/beta_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Beta Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/conditional_expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/jensens_inequality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jensen's inequality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/central_limit_theorem/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Central Limit Theorem
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/multivariate_normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multivariate Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/markov_chains/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Markov Chains
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Multivariate Calculus
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Multivariate Calculus
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    The TL;DR version of Derivatives
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    The TL;DR version of Derivatives
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#scalar-case" class="md-nav__link">
    <span class="md-ellipsis">
      Scalar case
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chain-rule" class="md-nav__link">
    <span class="md-ellipsis">
      Chain rule
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-vector-in-scalar-out" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient: Vector in, scalar out
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jacobian-vector-in-vector-out" class="md-nav__link">
    <span class="md-ellipsis">
      Jacobian: Vector in, Vector out
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalized-jacobian-tensor-in-tensor-out" class="md-nav__link">
    <span class="md-ellipsis">
      Generalized Jacobian: Tensor in, Tensor out
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#some-tips-and-tricks" class="md-nav__link">
    <span class="md-ellipsis">
      Some tips and tricks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Some tips and tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simplify-simplify-simplify" class="md-nav__link">
    <span class="md-ellipsis">
      Simplify, simplify, simplify
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Simplify, simplify, simplify">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#expanding-notation-into-explicit-sums-and-equations-for-each-component" class="md-nav__link">
    <span class="md-ellipsis">
      Expanding notation into explicit sums and equations for each component
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#completing-the-derivative-the-jacobian-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Completing the derivative: the Jacobian matrix
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#row-vectors-instead-of-column-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      Row vectors instead of column vectors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dealing-with-more-than-two-dimensions" class="md-nav__link">
    <span class="md-ellipsis">
      Dealing with more than two dimensions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-data-points" class="md-nav__link">
    <span class="md-ellipsis">
      Multiple data points
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../multivariable_functions_level_sets_and_contour_plots/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multivariable functions, level sets, and contour plots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Productivity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Productivity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    How to Build Your Career in AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            How to Build Your Career in AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Three Steps to Career Growth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_1_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 1- Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_2_projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 2- Projects
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_3_job/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 3- Job
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/make_every_day_count/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Make Every Day Count
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="the-tldr-version-of-derivatives">The TL;DR version of Derivatives</h1>
<h2 id="scalar-case">Scalar case</h2>
<p>You are probably familiar with the concept of a derivative in the scalar case.</p>
<p>Given a function <span class="arithmatex">\(f: \mathbb{R} \to \mathbb{R}\)</span>, the derivative of <span class="arithmatex">\(f\)</span> at a point <span class="arithmatex">\(x \in \mathbb{R}\)</span> is defined as:</p>
<div class="arithmatex">\[f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}\]</div>
<p>Derivatives are a way to measure change. In the scalar case, the derivative of the function <span class="arithmatex">\(f\)</span> at the point <span class="arithmatex">\(x\)</span> tells us how much the function <span class="arithmatex">\(f\)</span> changes as the input <span class="arithmatex">\(x\)</span> changes by a small amount <span class="arithmatex">\(\varepsilon\)</span>:</p>
<div class="arithmatex">\[f(x + \varepsilon) \approx f(x) + \varepsilon f'(x)\]</div>
<p>For ease of notation we will commonly assign a name to the output of <span class="arithmatex">\(f\)</span>, say <span class="arithmatex">\(y = f(x)\)</span>, and write <span class="arithmatex">\(\frac{\partial y}{\partial x}\)</span> for the derivative of <span class="arithmatex">\(y\)</span> with respect to <span class="arithmatex">\(x\)</span>. This notation emphasizes that <span class="arithmatex">\(\frac{\partial y}{\partial x}\)</span> is the rate of change between the variables <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span>; concretely if <span class="arithmatex">\(x\)</span> were to change by <span class="arithmatex">\(\varepsilon\)</span> then <span class="arithmatex">\(y\)</span> will change by approximately <span class="arithmatex">\(\varepsilon \frac{\partial y}{\partial x}\)</span>.</p>
<p>We can write this relationship as</p>
<div class="arithmatex">\[x \to x + \Delta x \implies y \to y + \frac{\partial y}{\partial x} \Delta x\]</div>
<p>You should read this as saying "changing <span class="arithmatex">\(x\)</span> to <span class="arithmatex">\(x + \Delta x\)</span> implies that <span class="arithmatex">\(y\)</span> will change to approximately <span class="arithmatex">\(y + \Delta x \frac{\partial y}{\partial x}\)</span>". This notation is nonstandard, but it emphasizes the relationship between changes in <span class="arithmatex">\(x\)</span> and changes in <span class="arithmatex">\(y\)</span>.</p>
<h2 id="chain-rule">Chain rule</h2>
<p>The chain rule tells us how to compute the derivative of the composition of functions. In the scalar case suppose that <span class="arithmatex">\(f, g: \mathbb{R} \to \mathbb{R}\)</span> and <span class="arithmatex">\(y = f(x)\)</span>, <span class="arithmatex">\(z = g(y)\)</span>; then we can also write <span class="arithmatex">\(z = (g \circ f)(x)\)</span>, or draw the following computational graph:</p>
<div class="arithmatex">\[x \xrightarrow{f} y \xrightarrow{g} z\]</div>
<p>The (scalar) chain rule tells us that</p>
<div class="arithmatex">\[\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}\]</div>
<p>This equation makes intuitive sense. The derivatives <span class="arithmatex">\(\frac{\partial z}{\partial y}\)</span> and <span class="arithmatex">\(\frac{\partial y}{\partial x}\)</span> give:</p>
<div class="arithmatex">\[x \to x + \Delta x \implies y \to y + \frac{\partial y}{\partial x} \Delta x\]</div>
<div class="arithmatex">\[y \to y + \Delta y \implies z \to z + \frac{\partial z}{\partial y} \Delta y\]</div>
<p>Combining these two rules lets us compute the effect of <span class="arithmatex">\(x\)</span> on <span class="arithmatex">\(z\)</span>: if <span class="arithmatex">\(x\)</span> changes by <span class="arithmatex">\(\Delta x\)</span> then <span class="arithmatex">\(y\)</span> will change by <span class="arithmatex">\(\frac{\partial y}{\partial x}\Delta x\)</span>, so we have <span class="arithmatex">\(\Delta y = \frac{\partial y}{\partial x}\Delta x\)</span>. If <span class="arithmatex">\(y\)</span> changes by <span class="arithmatex">\(\Delta y\)</span> then <span class="arithmatex">\(z\)</span> will change by <span class="arithmatex">\(\frac{\partial z}{\partial y}\Delta y = \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}\Delta x\)</span> which is exactly what the chain rule tells us.</p>
<h2 id="gradient-vector-in-scalar-out">Gradient: Vector in, scalar out</h2>
<p>This same intuition carries over into the vector case. Now suppose that <span class="arithmatex">\(f: \mathbb{R}^N \to \mathbb{R}\)</span> takes a vector as input and produces a scalar. The derivative of <span class="arithmatex">\(f\)</span> at the point <span class="arithmatex">\(\mathbf{x} \in \mathbb{R}^N\)</span> is now called the gradient, and it is defined as:</p>
<div class="arithmatex">\[\nabla_{\mathbf{x}} f(\mathbf{x}) = \lim_{\mathbf{h} \to \mathbf{0}} \frac{f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x})}{\|\mathbf{h}\|}\]</div>
<p>Now the gradient <span class="arithmatex">\(\nabla_{\mathbf{x}} f(\mathbf{x}) \in \mathbb{R}^N\)</span> is a vector, with the same intuition as the scalar case. If we set <span class="arithmatex">\(y = f(\mathbf{x})\)</span> then we have the relationship</p>
<div class="arithmatex">\[\mathbf{x} \to \mathbf{x} + \Delta \mathbf{x} \implies y \to y + \frac{\partial y}{\partial \mathbf{x}} \cdot \Delta \mathbf{x}\]</div>
<p>The formula changes a bit from the scalar case to account for the fact that <span class="arithmatex">\(\mathbf{x}\)</span>, <span class="arithmatex">\(\Delta \mathbf{x}\)</span>, and <span class="arithmatex">\(\frac{\partial y}{\partial \mathbf{x}}\)</span> are now vectors in <span class="arithmatex">\(\mathbb{R}^N\)</span> while <span class="arithmatex">\(y\)</span> is a scalar. In particular when multiplying <span class="arithmatex">\(\frac{\partial y}{\partial \mathbf{x}}\)</span> by <span class="arithmatex">\(\Delta \mathbf{x}\)</span> we use the dot product, which combines two vectors to give a scalar.</p>
<p>One nice outcome of this formula is that it gives meaning to the individual elements of the gradient <span class="arithmatex">\(\frac{\partial y}{\partial \mathbf{x}}\)</span>. Suppose that <span class="arithmatex">\(\Delta \mathbf{x}\)</span> is the <span class="arithmatex">\(i\)</span>th basis vector <span class="arithmatex">\(\mathbf{e}_i\)</span>, so that the <span class="arithmatex">\(i\)</span>th coordinate of <span class="arithmatex">\(\Delta \mathbf{x}\)</span> is 1 and all other coordinates of <span class="arithmatex">\(\Delta \mathbf{x}\)</span> are 0. </p>
<p>Then the dot product <span class="arithmatex">\(\frac{\partial y}{\partial \mathbf{x}} \cdot \Delta \mathbf{x}\)</span> here is simply the <span class="arithmatex">\(i\)</span>th coordinate of <span class="arithmatex">\(\frac{\partial y}{\partial \mathbf{x}}\)</span>. Thus the <span class="arithmatex">\(i\)</span>th coordinate of <span class="arithmatex">\(\frac{\partial y }{\partial \mathbf{x}}\)</span> times <span class="arithmatex">\(\varepsilon\)</span> tells us the amount by which <span class="arithmatex">\(y\)</span> will change if we move <span class="arithmatex">\(\mathbf{x}\)</span> by a small amount <span class="arithmatex">\(\varepsilon\)</span> along the <span class="arithmatex">\(i\)</span>th coordinate axis (meaning, if we change only the <span class="arithmatex">\(i\)</span>th component of <span class="arithmatex">\(\mathbf{x}\)</span> by <span class="arithmatex">\(\varepsilon\)</span>, where <span class="arithmatex">\(\varepsilon\)</span> is the <span class="arithmatex">\(i\)</span>th component of <span class="arithmatex">\(\Delta \mathbf{x}\)</span>).</p>
<p>This means that we can also view the gradient <span class="arithmatex">\(\frac{\partial y}{\partial \mathbf{x}}\)</span> as a vector of partial derivatives:</p>
<div class="arithmatex">\[\frac{\partial y}{\partial \mathbf{x}} = \left(\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \ldots, \frac{\partial y}{\partial x_N}\right)\]</div>
<p>where <span class="arithmatex">\(x_i\)</span> is the <span class="arithmatex">\(i\)</span>th coordinate of the vector <span class="arithmatex">\(\mathbf{x}\)</span>, which is a scalar, so each partial derivative <span class="arithmatex">\(\frac{\partial y}{\partial x_i}\)</span> is also a scalar.</p>
<h2 id="jacobian-vector-in-vector-out">Jacobian: Vector in, Vector out</h2>
<p>Now suppose that <span class="arithmatex">\(f: \mathbb{R}^N \to \mathbb{R}^M\)</span> takes a vector as input and produces a vector as output. Then the derivative of <span class="arithmatex">\(f\)</span> at a point <span class="arithmatex">\(\mathbf{x}\)</span>, also called the Jacobian, is the <span class="arithmatex">\(M \times N\)</span> matrix of partial derivatives. If we again set <span class="arithmatex">\(\mathbf{y} = f(\mathbf{x})\)</span> then we can write:</p>
<div class="arithmatex">\[\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{pmatrix}
\frac{\partial y_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_1}{\partial x_N} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_M}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_M}{\partial x_N}
\end{pmatrix}\]</div>
<p>The Jacobian tells us the relationship between each element of <span class="arithmatex">\(\mathbf{x}\)</span> and each element of <span class="arithmatex">\(\mathbf{y}\)</span>: the <span class="arithmatex">\((i, j)\)</span>-th element of <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is equal to <span class="arithmatex">\(\frac{\partial y_i}{\partial x_j}\)</span>, so it tells us the amount by which <span class="arithmatex">\(y_i\)</span> will change if <span class="arithmatex">\(x_j\)</span> is changed by a small amount.</p>
<p>Just as in the previous cases, the Jacobian tells us the relationship between changes in the input and changes in the output:</p>
<div class="arithmatex">\[\mathbf{x} \to \mathbf{x} + \Delta \mathbf{x} \implies \mathbf{y} \to \mathbf{y} + \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x}\]</div>
<p>Here <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is a <span class="arithmatex">\(M \times N\)</span> matrix and <span class="arithmatex">\(\Delta \mathbf{x}\)</span> is an <span class="arithmatex">\(N\)</span>-dimensional vector, so the product <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x}\)</span> is a matrix-vector multiplication resulting in an <span class="arithmatex">\(M\)</span>-dimensional vector.</p>
<p>It's worth noting that each row of the Jacobian matrix <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is actually a gradient! Specifically, the <span class="arithmatex">\(i\)</span>th row of <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is the gradient of the scalar function <span class="arithmatex">\(y_i\)</span> with respect to <span class="arithmatex">\(\mathbf{x}\)</span>:</p>
<div class="arithmatex">\[\text{Row } i \text{ of } \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \nabla_{\mathbf{x}} y_i = \left(\frac{\partial y_i}{\partial x_1}, \frac{\partial y_i}{\partial x_2}, \ldots, \frac{\partial y_i}{\partial x_N}\right)\]</div>
<p>This makes sense because <span class="arithmatex">\(y_i\)</span> is a scalar function of the vector <span class="arithmatex">\(\mathbf{x}\)</span>, so its gradient is an <span class="arithmatex">\(N\)</span>-dimensional vector. When we stack all <span class="arithmatex">\(M\)</span> of these gradient vectors as rows, we get the <span class="arithmatex">\(M \times N\)</span> Jacobian matrix.</p>
<p>This insight also explains why the matrix-vector multiplication <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x}\)</span> works the way it does. Since each row of the Jacobian is a gradient, the multiplication is equivalent to computing the dot product of each gradient with <span class="arithmatex">\(\Delta \mathbf{x}\)</span>:</p>
<div class="arithmatex">\[\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x} = \begin{pmatrix}
\nabla_{\mathbf{x}} y_1 \cdot \Delta \mathbf{x} \\
\nabla_{\mathbf{x}} y_2 \cdot \Delta \mathbf{x} \\
\vdots \\
\nabla_{\mathbf{x}} y_M \cdot \Delta \mathbf{x}
\end{pmatrix}\]</div>
<p>In other words, the Jacobian-vector product is just a stack of gradient-vector products! Each component of the result tells us how much the corresponding output component <span class="arithmatex">\(y_i\)</span> changes when we move <span class="arithmatex">\(\mathbf{x}\)</span> by <span class="arithmatex">\(\Delta \mathbf{x}\)</span>.</p>
<p>For example, if <span class="arithmatex">\(M = 2\)</span> and <span class="arithmatex">\(N = 3\)</span>, we might have:</p>
<div class="arithmatex">\[\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{pmatrix} 2 &amp; 1 &amp; 0 \\ 0 &amp; 3 &amp; 1 \end{pmatrix}, \quad \Delta \mathbf{x} = \begin{pmatrix} 0.1 \\ 0.2 \\ 0.3 \end{pmatrix}\]</div>
<p>Then:</p>
<div class="arithmatex">\[\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x} = \begin{pmatrix} 2 &amp; 1 &amp; 0 \\ 0 &amp; 3 &amp; 1 \end{pmatrix} \begin{pmatrix} 0.1 \\ 0.2 \\ 0.3 \end{pmatrix} = \begin{pmatrix} 2(0.1) + 1(0.2) + 0(0.3) \\ 0(0.1) + 3(0.2) + 1(0.3) \end{pmatrix} = \begin{pmatrix} 0.4 \\ 0.9 \end{pmatrix}\]</div>
<p>The chain rule can be extended to the vector case using Jacobian matrices. Suppose that <span class="arithmatex">\(f: \mathbb{R}^N \to \mathbb{R}^M\)</span> and <span class="arithmatex">\(g: \mathbb{R}^M \to \mathbb{R}^K\)</span>. Let <span class="arithmatex">\(\mathbf{x} \in \mathbb{R}^N\)</span>, <span class="arithmatex">\(\mathbf{y} \in \mathbb{R}^M\)</span>, and <span class="arithmatex">\(\mathbf{z} \in \mathbb{R}^K\)</span> with <span class="arithmatex">\(\mathbf{y} = f(\mathbf{x})\)</span> and <span class="arithmatex">\(\mathbf{z} = g(\mathbf{y})\)</span>, so we have the same computational graph as the scalar case:</p>
<div class="arithmatex">\[\mathbf{x} \xrightarrow{f} \mathbf{y} \xrightarrow{g} \mathbf{z}\]</div>
<p>The chain rule also has the same form as the scalar case:</p>
<div class="arithmatex">\[\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}\]</div>
<p>However now each of these terms is a matrix: <span class="arithmatex">\(\frac{\partial \mathbf{z}}{\partial \mathbf{y}}\)</span> is a <span class="arithmatex">\(K \times M\)</span> matrix, <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is a <span class="arithmatex">\(M \times N\)</span> matrix, and <span class="arithmatex">\(\frac{\partial \mathbf{z}}{\partial \mathbf{x}}\)</span> is a <span class="arithmatex">\(K \times N\)</span> matrix; the multiplication of <span class="arithmatex">\(\frac{\partial \mathbf{z}}{\partial \mathbf{y}}\)</span> and <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is matrix multiplication.</p>
<h2 id="generalized-jacobian-tensor-in-tensor-out">Generalized Jacobian: Tensor in, Tensor out</h2>
<p>Just as a vector is a one-dimensional list of numbers and a matrix is a two-dimensional grid of numbers, a tensor is a <span class="arithmatex">\(D\)</span>-dimensional grid of numbers. Many operations in deep learning accept tensors as inputs and produce tensors as outputs. For example an image is usually represented as a three-dimensional grid of numbers, where the three dimensions correspond to the height, width, and color channels (red, green, blue) of the image. We must therefore develop a derivative that is compatible with functions operating on general tensors.</p>
<p>Suppose now that <span class="arithmatex">\(f: \mathbb{R}^{N_1 \times \cdots \times N_{D_x}} \to \mathbb{R}^{M_1 \times \cdots \times M_{D_y}}\)</span>. Then the input to <span class="arithmatex">\(f\)</span> is a <span class="arithmatex">\(D_x\)</span>-dimensional tensor of shape <span class="arithmatex">\(N_1 \times \cdots \times N_{D_x}\)</span>, and the output of <span class="arithmatex">\(f\)</span> is a <span class="arithmatex">\(D_y\)</span>-dimensional tensor of shape <span class="arithmatex">\(M_1 \times \cdots \times M_{D_y}\)</span>. If <span class="arithmatex">\(\mathbf{y} = f(\mathbf{x})\)</span> then the derivative <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is a generalized Jacobian, which is an object with shape</p>
<div class="arithmatex">\[(M_1 \times \cdots \times M_{D_y}) \times (N_1 \times \cdots \times N_{D_x})\]</div>
<p>Note that we have separated the dimensions of <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> into two groups: the first group matches the dimensions of <span class="arithmatex">\(\mathbf{y}\)</span> and the second group matches the dimensions of <span class="arithmatex">\(\mathbf{x}\)</span>. With this grouping, we can think of the generalized Jacobian as generalization of a matrix, where each "row" has the same shape as <span class="arithmatex">\(\mathbf{y}\)</span> and each "column" has the same shape as <span class="arithmatex">\(\mathbf{x}\)</span>.</p>
<p>Now if we let <span class="arithmatex">\(\mathbf{i} \in \mathbb{Z}^{D_y}\)</span> and <span class="arithmatex">\(\mathbf{j} \in \mathbb{Z}^{D_x}\)</span> be vectors of integer indices, then we can write</p>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{\mathbf{i},\mathbf{j}} = \frac{\partial y_{\mathbf{i}}}{\partial x_{\mathbf{j}}}\]</div>
<p>For example, if <span class="arithmatex">\(\mathbf{y}\)</span> is a <span class="arithmatex">\(2 \times 3\)</span> tensor and <span class="arithmatex">\(\mathbf{x}\)</span> is a <span class="arithmatex">\(4 \times 2\)</span> tensor, then <span class="arithmatex">\(\mathbf{i} = (i_1, i_2)\)</span> where <span class="arithmatex">\(i_1 \in \{0,1\}\)</span> and <span class="arithmatex">\(i_2 \in \{0,1,2\}\)</span>, and <span class="arithmatex">\(\mathbf{j} = (j_1, j_2)\)</span> where <span class="arithmatex">\(j_1 \in \{0,1,2,3\}\)</span> and <span class="arithmatex">\(j_2 \in \{0,1\}\)</span>. So we might have:</p>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,2),(3,0)} = \frac{\partial y_{(1,2)}}{\partial x_{(3,0)}}\]</div>
<p>This tells us how the element at position <span class="arithmatex">\((1,2)\)</span> in <span class="arithmatex">\(\mathbf{y}\)</span> changes with respect to the element at position <span class="arithmatex">\((3,0)\)</span> in <span class="arithmatex">\(\mathbf{x}\)</span>.</p>
<p>In this equation note that <span class="arithmatex">\(y_{\mathbf{i}}\)</span> and <span class="arithmatex">\(x_{\mathbf{j}}\)</span> are scalars, so the derivative <span class="arithmatex">\(\frac{\partial y_{\mathbf{i}}}{\partial x_{\mathbf{j}}}\)</span> is also a scalar. Using this notation we see that like the standard Jacobian, the generalized Jacobian tells us the relative rates of change between all elements of <span class="arithmatex">\(\mathbf{x}\)</span> and all elements of <span class="arithmatex">\(\mathbf{y}\)</span>.</p>
<p>The generalized Jacobian gives the same relationship between inputs and outputs as before:</p>
<div class="arithmatex">\[\mathbf{x} \to \mathbf{x} + \Delta \mathbf{x} \implies \mathbf{y} \to \mathbf{y} + \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x}\]</div>
<p>The difference is that now <span class="arithmatex">\(\Delta \mathbf{x}\)</span> is a tensor of shape <span class="arithmatex">\(N_1 \times \cdots \times N_{D_x}\)</span> and <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is a generalized matrix of shape <span class="arithmatex">\((M_1 \times \cdots \times M_{D_y}) \times (N_1 \times \cdots \times N_{D_x})\)</span>. The product <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x}\)</span> is therefore a generalized matrix-vector multiply, which results in a tensor of shape <span class="arithmatex">\(M_1 \times \cdots \times M_{D_y}\)</span>.</p>
<p>The generalized matrix-vector multiply follows the same algebraic rules as a traditional matrix-vector multiply:</p>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x}\right)_{\mathbf{i}} = \sum_{\mathbf{j}} \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{\mathbf{i},\mathbf{j}} (\Delta \mathbf{x})_{\mathbf{j}} = \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{\mathbf{i},:} \cdot \Delta \mathbf{x}\]</div>
<p>For example, continuing with our <span class="arithmatex">\(2 \times 3\)</span> output tensor <span class="arithmatex">\(\mathbf{y}\)</span> and <span class="arithmatex">\(4 \times 2\)</span> input tensor <span class="arithmatex">\(\mathbf{x}\)</span>, the generalized Jacobian <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> has shape <span class="arithmatex">\((2 \times 3) \times (4 \times 2)\)</span>. </p>
<p>Let's say we have specific values for the Jacobian entries at position <span class="arithmatex">\((1,1)\)</span>:</p>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(0,0)} = 2, \quad \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(0,1)} = 3\]</div>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(1,0)} = 1, \quad \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(1,1)} = 4\]</div>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(2,0)} = 5, \quad \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(2,1)} = 2\]</div>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(3,0)} = 3, \quad \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(3,1)} = 1\]</div>
<p>And suppose:</p>
<div class="arithmatex">\[\Delta \mathbf{x} = \begin{pmatrix} 0.1 &amp; 0.2 \\ 0.3 &amp; 0.4 \\ 0.5 &amp; 0.6 \\ 0.7 &amp; 0.8 \end{pmatrix}\]</div>
<p>Let's use the general formula.</p>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x}\right)_{(1,1)} = \sum_{\mathbf{j}} \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),\mathbf{j}} (\Delta \mathbf{x})_{\mathbf{j}}\]</div>
<p>Here, <span class="arithmatex">\(\mathbf{j}\)</span> iterates over all input positions <span class="arithmatex">\((j_1, j_2)\)</span> where <span class="arithmatex">\(j_1 \in \{0,1,2,3\}\)</span> and <span class="arithmatex">\(j_2 \in \{0,1\}\)</span>:</p>
<ul>
<li>
<p><span class="arithmatex">\(\mathbf{j} = (0,0)\)</span>: <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(0,0)} (\Delta \mathbf{x})_{(0,0)} = 2 \times 0.1 = 0.2\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\mathbf{j} = (0,1)\)</span>: <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(0,1)} (\Delta \mathbf{x})_{(0,1)} = 3 \times 0.2 = 0.6\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\mathbf{j} = (1,0)\)</span>: <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(1,0)} (\Delta \mathbf{x})_{(1,0)} = 1 \times 0.3 = 0.3\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\mathbf{j} = (1,1)\)</span>: <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(1,1)} (\Delta \mathbf{x})_{(1,1)} = 4 \times 0.4 = 1.6\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\mathbf{j} = (2,0)\)</span>: <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(2,0)} (\Delta \mathbf{x})_{(2,0)} = 5 \times 0.5 = 2.5\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\mathbf{j} = (2,1)\)</span>: <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(2,1)} (\Delta \mathbf{x})_{(2,1)} = 2 \times 0.6 = 1.2\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\mathbf{j} = (3,0)\)</span>: <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(3,0)} (\Delta \mathbf{x})_{(3,0)} = 3 \times 0.7 = 2.1\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\mathbf{j} = (3,1)\)</span>: <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{(1,1),(3,1)} (\Delta \mathbf{x})_{(3,1)} = 1 \times 0.8 = 0.8\)</span></p>
</li>
</ul>
<p>Summing all these terms gives us <span class="arithmatex">\(0.2 + 0.6 + 0.3 + 1.6 + 2.5 + 1.2 + 2.1 + 0.8 = 9.3\)</span>.</p>
<p>Thus, </p>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \Delta \mathbf{x}\right)_{(1,1)} =  9.3\]</div>
<p>The only difference is that the indices <span class="arithmatex">\(\mathbf{i}\)</span> and <span class="arithmatex">\(\mathbf{j}\)</span> are not scalars; instead they are vectors of indices. In the equation above the term <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{\mathbf{i},:}\)</span> is the <span class="arithmatex">\(\mathbf{i}\)</span>th "row" of the generalized matrix <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span>, which is a tensor with the same shape as <span class="arithmatex">\(\mathbf{x}\)</span>. We have also used the convention that the dot product between two tensors of the same shape is an elementwise product followed by a sum, identical to the dot product between vectors.</p>
<p>The chain rule also looks the same in the case of tensor-valued functions. Suppose that <span class="arithmatex">\(\mathbf{y} = f(\mathbf{x})\)</span> and <span class="arithmatex">\(\mathbf{z} = g(\mathbf{y})\)</span>, where <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span> have the same shapes as above and <span class="arithmatex">\(\mathbf{z}\)</span> has shape <span class="arithmatex">\(K_1 \times \cdots \times K_{D_z}\)</span>. Now the chain rule looks the same as before:</p>
<div class="arithmatex">\[\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}\]</div>
<p>The difference is that now <span class="arithmatex">\(\frac{\partial \mathbf{z}}{\partial \mathbf{y}}\)</span> is a generalized matrix of shape <span class="arithmatex">\((K_1 \times \cdots \times K_{D_z}) \times (M_1 \times \cdots \times M_{D_y})\)</span>, and <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is a generalized matrix of shape <span class="arithmatex">\((M_1 \times \cdots \times M_{D_y}) \times (N_1 \times \cdots \times N_{D_x})\)</span>; the product <span class="arithmatex">\(\frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> is a generalized matrix-matrix multiply, resulting in an object of shape <span class="arithmatex">\((K_1 \times \cdots \times K_{D_z}) \times (N_1 \times \cdots \times N_{D_x})\)</span>. Like the generalized matrix-vector multiply defined above, the generalized matrix-matrix multiply follows the same algebraic rules as the traditional matrix-matrix multiply:</p>
<div class="arithmatex">\[\left(\frac{\partial \mathbf{z}}{\partial \mathbf{x}}\right)_{\mathbf{i},\mathbf{j}} = \sum_{\mathbf{k}} \left(\frac{\partial \mathbf{z}}{\partial \mathbf{y}}\right)_{\mathbf{i},\mathbf{k}} \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{\mathbf{k},\mathbf{j}} = \left(\frac{\partial \mathbf{z}}{\partial \mathbf{y}}\right)_{\mathbf{i},:} \cdot \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{:,\mathbf{j}}\]</div>
<p>In this equation the indices <span class="arithmatex">\(\mathbf{i}\)</span>, <span class="arithmatex">\(\mathbf{j}\)</span>, <span class="arithmatex">\(\mathbf{k}\)</span> are vectors of indices, and the terms <span class="arithmatex">\(\left(\frac{\partial \mathbf{z}}{\partial \mathbf{y}}\right)_{\mathbf{i},:}\)</span> and <span class="arithmatex">\(\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)_{:,\mathbf{j}}\)</span> are the <span class="arithmatex">\(\mathbf{i}\)</span>th "row" of <span class="arithmatex">\(\frac{\partial \mathbf{z}}{\partial \mathbf{y}}\)</span> and the <span class="arithmatex">\(\mathbf{j}\)</span>th "column" of <span class="arithmatex">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> respectively.</p>
<h2 id="some-tips-and-tricks">Some tips and tricks</h2>
<h3 id="simplify-simplify-simplify">Simplify, simplify, simplify</h3>
<p>Much of the confusion in taking derivatives involving arrays stems from trying to do too many things at once. These "things" include taking derivatives of multiple components simultaneously, taking derivatives in the presence of summation notation, and applying the chain rule.</p>
<h4 id="expanding-notation-into-explicit-sums-and-equations-for-each-component">Expanding notation into explicit sums and equations for each component</h4>
<p>In order to simplify a given calculation, it is often useful to write out the explicit formula for a single scalar element of the output in terms of nothing but scalar variables. Once one has an explicit formula for a single scalar element of the output in terms of other scalar values, then one can use the calculus that you used as a beginner, which is much easier than trying to do matrix math, summations, and derivatives all at the same time.</p>
<p><strong>Example:</strong> Suppose we have a column vector <span class="arithmatex">\(\mathbf{y}\)</span> of length <span class="arithmatex">\(C\)</span> that is calculated by forming the product of a matrix <span class="arithmatex">\(W\)</span> that is <span class="arithmatex">\(C\)</span> rows by <span class="arithmatex">\(D\)</span> columns with a column vector <span class="arithmatex">\(\mathbf{x}\)</span> of length <span class="arithmatex">\(D\)</span>:</p>
<div class="arithmatex">\[\mathbf{y} = W\mathbf{x} \quad\]</div>
<p>Suppose we are interested in the derivative of <span class="arithmatex">\(\mathbf{y}\)</span> with respect to <span class="arithmatex">\(\mathbf{x}\)</span>. A full characterization of this derivative requires the (partial) derivatives of each component of <span class="arithmatex">\(\mathbf{y}\)</span> with respect to each component of <span class="arithmatex">\(\mathbf{x}\)</span>, which in this case will contain <span class="arithmatex">\(C \times D\)</span> values since there are <span class="arithmatex">\(C\)</span> components in <span class="arithmatex">\(\mathbf{y}\)</span> and <span class="arithmatex">\(D\)</span> components of <span class="arithmatex">\(\mathbf{x}\)</span>.</p>
<p>Let's start by computing one of these, say, the 3rd component of <span class="arithmatex">\(\mathbf{y}\)</span> with respect to the 7th component of <span class="arithmatex">\(\mathbf{x}\)</span>. That is, we want to compute</p>
<div class="arithmatex">\[\frac{\partial y_3}{\partial x_7}\]</div>
<p>which is just the derivative of one scalar with respect to another.</p>
<p>The first thing to do is to write down the formula for computing <span class="arithmatex">\(y_3\)</span> so we can take its derivative. From the definition of matrix-vector multiplication, the value <span class="arithmatex">\(y_3\)</span> is computed by taking the dot product between the 3rd row of <span class="arithmatex">\(W\)</span> and the vector <span class="arithmatex">\(\mathbf{x}\)</span>:</p>
<div class="arithmatex">\[y_3 = \sum_{j=1}^{D} W_{3,j} x_j \quad\]</div>
<p>At this point, we have reduced the original matrix equation to a scalar equation. This makes it much easier to compute the desired derivatives.</p>
<h4 id="completing-the-derivative-the-jacobian-matrix">Completing the derivative: the Jacobian matrix</h4>
<p>Recall that our original goal was to compute the derivatives of each component of <span class="arithmatex">\(\mathbf{y}\)</span> with respect to each component of <span class="arithmatex">\(\mathbf{x}\)</span>, and we noted that there would be <span class="arithmatex">\(C \times D\)</span> of these. They can be written out as a matrix in the following form:</p>
<div class="arithmatex">\[\begin{pmatrix}
\frac{\partial y_1}{\partial x_1} &amp; \frac{\partial y_1}{\partial x_2} &amp; \frac{\partial y_1}{\partial x_3} &amp; \cdots &amp; \frac{\partial y_1}{\partial x_D} \\
\frac{\partial y_2}{\partial x_1} &amp; \frac{\partial y_2}{\partial x_2} &amp; \frac{\partial y_2}{\partial x_3} &amp; \cdots &amp; \frac{\partial y_2}{\partial x_D} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_C}{\partial x_1} &amp; \frac{\partial y_C}{\partial x_2} &amp; \frac{\partial y_C}{\partial x_3} &amp; \cdots &amp; \frac{\partial y_C}{\partial x_D}
\end{pmatrix}\]</div>
<p>In this particular case, this is called the Jacobian matrix, but this terminology is not too important for our purposes.</p>
<p>Notice that for the equation <span class="arithmatex">\(\mathbf{y} = W\mathbf{x}\)</span>, the partial of <span class="arithmatex">\(y_3\)</span> with respect to <span class="arithmatex">\(x_7\)</span> was simply given by <span class="arithmatex">\(W_{3,7}\)</span>. If you go through the same process for other components, you will find that, for all <span class="arithmatex">\(i\)</span> and <span class="arithmatex">\(j\)</span>,</p>
<div class="arithmatex">\[\frac{\partial y_i}{\partial x_j} = W_{i,j}\]</div>
<p>This means that the matrix of partial derivatives is</p>
<div class="arithmatex">\[\begin{pmatrix}
\frac{\partial y_1}{\partial x_1} &amp; \frac{\partial y_1}{\partial x_2} &amp; \frac{\partial y_1}{\partial x_3} &amp; \cdots &amp; \frac{\partial y_1}{\partial x_D} \\
\frac{\partial y_2}{\partial x_1} &amp; \frac{\partial y_2}{\partial x_2} &amp; \frac{\partial y_2}{\partial x_3} &amp; \cdots &amp; \frac{\partial y_2}{\partial x_D} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_C}{\partial x_1} &amp; \frac{\partial y_C}{\partial x_2} &amp; \frac{\partial y_C}{\partial x_3} &amp; \cdots &amp; \frac{\partial y_C}{\partial x_D}
\end{pmatrix} = \begin{pmatrix}
W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; \cdots &amp; W_{1,D} \\
W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; \cdots &amp; W_{2,D} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
W_{C,1} &amp; W_{C,2} &amp; W_{C,3} &amp; \cdots &amp; W_{C,D}
\end{pmatrix}\]</div>
<p>This, of course, is just <span class="arithmatex">\(W\)</span> itself.</p>
<p>Thus, after all this work, we have concluded that for <span class="arithmatex">\(\mathbf{y} = W\mathbf{x}\)</span>, we have</p>
<div class="arithmatex">\[\frac{d\mathbf{y}}{d\mathbf{x}} = W\]</div>
<h3 id="row-vectors-instead-of-column-vectors">Row vectors instead of column vectors</h3>
<p>It is important in working with different neural networks packages to pay close attention to the arrangement of weight matrices, data matrices, and so on. For example, if a data matrix <span class="arithmatex">\(X\)</span> contains many different vectors, each of which represents an input, is each data vector a row or column of the data matrix <span class="arithmatex">\(X\)</span>?</p>
<p>In the previous example, we worked with a vector <span class="arithmatex">\(\mathbf{x}\)</span> that was a column vector. However, you should also be able to use the same basic ideas when <span class="arithmatex">\(\mathbf{x}\)</span> is a row vector.</p>
<p>Let <span class="arithmatex">\(\mathbf{y}\)</span> be a row vector with <span class="arithmatex">\(C\)</span> components computed by taking the product of another row vector <span class="arithmatex">\(\mathbf{x}\)</span> with <span class="arithmatex">\(D\)</span> components and a matrix <span class="arithmatex">\(W\)</span> that is <span class="arithmatex">\(D\)</span> rows by <span class="arithmatex">\(C\)</span> columns:</p>
<div class="arithmatex">\[\mathbf{y} = \mathbf{x}W\]</div>
<p>Importantly, despite the fact that <span class="arithmatex">\(\mathbf{y}\)</span> and <span class="arithmatex">\(\mathbf{x}\)</span> have the same number of components as before, the shape of <span class="arithmatex">\(W\)</span> is the transpose of the shape that we used before for <span class="arithmatex">\(W\)</span>.</p>
<p>In this case, you will see, by writing</p>
<div class="arithmatex">\[y_3 = \sum_{j=1}^{D} x_j W_{j,3}\]</div>
<p>that</p>
<div class="arithmatex">\[\frac{\partial y_3}{\partial x_7} = W_{7,3}\]</div>
<p>Notice that the indexing into <span class="arithmatex">\(W\)</span> is the opposite from what it was in the first example.</p>
<p>However, when we assemble the full Jacobian matrix, we can still see that in this case as well,</p>
<div class="arithmatex">\[\frac{d\mathbf{y}}{d\mathbf{x}} = W\]</div>
<h3 id="dealing-with-more-than-two-dimensions">Dealing with more than two dimensions</h3>
<p>Let's consider another closely related problem, that of computing <span class="arithmatex">\(\frac{d\mathbf{y}}{dW}\)</span>:</p>
<p>In this case, <span class="arithmatex">\(\mathbf{y}\)</span> varies along one coordinate while <span class="arithmatex">\(W\)</span> varies along two coordinates. Thus, the entire derivative is most naturally contained in a three-dimensional array.</p>
<p>Let's again compute a scalar derivative between one component of <span class="arithmatex">\(\mathbf{y}\)</span>, say <span class="arithmatex">\(y_3\)</span> and one component of <span class="arithmatex">\(W\)</span>, say <span class="arithmatex">\(W_{7,8}\)</span>. Let's start with the same basic setup in which we write down an equation for <span class="arithmatex">\(y_3\)</span> in terms of other scalar components. Now we would like an equation that expresses <span class="arithmatex">\(y_3\)</span> in terms of scalar values, and shows the role that <span class="arithmatex">\(W_{7,8}\)</span> plays in its computation.</p>
<p>However, what we see is that <span class="arithmatex">\(W_{7,8}\)</span> plays no role in the computation of <span class="arithmatex">\(y_3\)</span>, since</p>
<div class="arithmatex">\[y_3 = x_1 W_{1,3} + x_2 W_{2,3} + \cdots + x_D W_{D,3} \quad\]</div>
<p>In other words,</p>
<div class="arithmatex">\[\frac{\partial y_3}{\partial W_{7,8}} = 0\]</div>
<p>However, the partials of <span class="arithmatex">\(y_3\)</span> with respect to elements of the 3rd column of <span class="arithmatex">\(W\)</span> will certainly be non-zero. For example, the derivative of <span class="arithmatex">\(y_3\)</span> with respect to <span class="arithmatex">\(W_{2,3}\)</span> is given by</p>
<div class="arithmatex">\[\frac{\partial y_3}{\partial W_{2,3}} = x_2 \quad\]</div>
<p>as can be easily seen by examining the equation for <span class="arithmatex">\(y_3\)</span>.</p>
<p>In general, when the index of the <span class="arithmatex">\(\mathbf{y}\)</span> component is equal to the second index of <span class="arithmatex">\(W\)</span>, the derivative will be non-zero, but will be zero otherwise. We can write:</p>
<div class="arithmatex">\[\frac{\partial y_j}{\partial W_{i,j}} = x_i\]</div>
<p>but the other elements of the 3-d array will be 0. If we let <span class="arithmatex">\(F\)</span> represent the 3d array representing the derivative of <span class="arithmatex">\(\mathbf{y}\)</span> with respect to <span class="arithmatex">\(W\)</span>, where</p>
<div class="arithmatex">\[F_{i,j,k} = \frac{\partial y_i}{\partial W_{j,k}}\]</div>
<p>then</p>
<div class="arithmatex">\[F_{i,j,i} = x_j\]</div>
<p>but all other entries of <span class="arithmatex">\(F\)</span> are zero.</p>
<p>Finally, if we define a new two-dimensional array <span class="arithmatex">\(G\)</span> as</p>
<div class="arithmatex">\[G_{i,j} = F_{i,j,i}\]</div>
<p>we can see that all of the information we need about <span class="arithmatex">\(F\)</span> can be stored in <span class="arithmatex">\(G\)</span>, and that the non-trivial portion of <span class="arithmatex">\(F\)</span> is really two-dimensional, not three-dimensional.</p>
<p>Representing the important part of derivative arrays in a compact way is critical to efficient implementations of neural networks.</p>
<h3 id="multiple-data-points">Multiple data points</h3>
<p>Let's assume that each individual <span class="arithmatex">\(\mathbf{x}\)</span> is a row vector of length <span class="arithmatex">\(D\)</span>, and that <span class="arithmatex">\(X\)</span> is a two-dimensional array with <span class="arithmatex">\(N\)</span> rows and <span class="arithmatex">\(D\)</span> columns. <span class="arithmatex">\(W\)</span> will be a matrix with <span class="arithmatex">\(D\)</span> rows and <span class="arithmatex">\(C\)</span> columns. <span class="arithmatex">\(Y\)</span>, given by</p>
<div class="arithmatex">\[Y = XW\]</div>
<p>will also be a matrix, with <span class="arithmatex">\(N\)</span> rows and <span class="arithmatex">\(C\)</span> columns. Thus, each row of <span class="arithmatex">\(Y\)</span> will give a row vector associated with the corresponding row of the input <span class="arithmatex">\(X\)</span>.</p>
<p>Sticking to our technique of writing down an expression for a given component of the output, we have</p>
<div class="arithmatex">\[Y_{i,j} = \sum_{k=1}^{D} X_{i,k} W_{k,j}\]</div>
<p>We can see immediately from this equation that among the derivatives</p>
<div class="arithmatex">\[\frac{\partial Y_{a,b}}{\partial X_{c,d}}\]</div>
<p>they are all zero unless <span class="arithmatex">\(a = c\)</span>. That is, since each component of <span class="arithmatex">\(Y\)</span> is computed using only the corresponding row of <span class="arithmatex">\(X\)</span>, derivatives of components between different rows of <span class="arithmatex">\(Y\)</span> and <span class="arithmatex">\(X\)</span> are all zero.</p>
<p>Furthermore, we can see that</p>
<div class="arithmatex">\[\frac{\partial Y_{i,j}}{\partial X_{i,k}} = W_{k,j}\]</div>
<p>doesn't depend at all upon which row of <span class="arithmatex">\(Y\)</span> and <span class="arithmatex">\(X\)</span> we are comparing.</p>
<p>In fact, the matrix <span class="arithmatex">\(W\)</span> holds all of these partials as it is we just have to remember to index into it according to the equation above to obtain the specific partial derivative that we want.</p>
<p>If we let <span class="arithmatex">\(Y_{i,:}\)</span> be the <span class="arithmatex">\(i\)</span>th row of <span class="arithmatex">\(Y\)</span> and let <span class="arithmatex">\(X_{i,:}\)</span> be the <span class="arithmatex">\(i\)</span>th row of <span class="arithmatex">\(X\)</span>, then we see that</p>
<div class="arithmatex">\[\frac{\partial Y_{i,:}}{\partial X_{i,:}} = W\]</div>
<p>which is a simple generalization of our previous result.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>