
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A personal wiki for notes, ideas, and projects.">
      
      
      
        <link rel="canonical" href="https://adi14041999.github.io/my_wiki/math/linear_algebra/applications_of_projections_in_rn_orthogonal_bases_of_planes_and_linear_regression/">
      
      
        <link rel="prev" href="../projections/">
      
      
        <link rel="next" href="../../probability/probability_and_counting/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Applications of projections in Rn- orthogonal bases of planes and linear regression - My Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#applications-of-projections-in-rn-orthogonal-bases-of-planes-and-linear-regression" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="My Wiki" class="md-header__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Applications of projections in Rn- orthogonal bases of planes and linear regression
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../ai/deep_generative_models/introduction/" class="md-tabs__link">
          
  
  
  AI

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../vectors_vector_addition_and_scalar_multiplication/" class="md-tabs__link">
          
  
  
  Math

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-tabs__link">
          
  
  
  Productivity

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="My Wiki" class="md-nav__button md-logo" aria-label="My Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Deep Generative Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Deep Generative Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/autoregressive_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoregressive Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/variational_autoencoders/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/normalizing_flow_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalizing flow models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/recap_at_this_point/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Recap at this point
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/generative_adversarial_networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/energy_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Energy Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/score_based_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/score_based_generative_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Generative Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/evaluating_generative_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluating Generative Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ai/deep_generative_models/score_based_diffusion_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Score Based Diffusion Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Math
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vectors_vector_addition_and_scalar_multiplication/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors, vector addition, and scalar multiplication
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vector_geometry_in_mathbb_r_n_and_correlation_coefficients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector geometry in Rn and correlation coefficients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../planes_in_r3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Planes in R3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../span_subspaces_and_dimension/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Span, subspaces, and dimension
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../basis_and_orthogonality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basis and orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Applications of projections in Rn- orthogonal bases of planes and linear regression
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Applications of projections in Rn- orthogonal bases of planes and linear regression
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#finding-an-orthogonal-basis-special-case" class="md-nav__link">
    <span class="md-ellipsis">
      Finding an orthogonal basis: special case
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fitting-a-function-to-data" class="md-nav__link">
    <span class="md-ellipsis">
      Fitting a function to data
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#correlation-coefficient-and-quality-of-fit" class="md-nav__link">
    <span class="md-ellipsis">
      Correlation Coefficient and quality of fit
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonal-basis-formula-and-relation-of-correlation-coefficient-to-best-fit-lines" class="md-nav__link">
    <span class="md-ellipsis">
      Orthogonal basis formula and relation of correlation coefficient to best fit lines
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/probability_and_counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability and Counting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/story_proofs_and_axioms_of_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Story Proofs and Axioms of Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/conditional_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/some_famous_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Some famous problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/expectation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Expectation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/indicator_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Indicator Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/poisson_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Poisson Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/continuous_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Continuous Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/normal_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normal Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/exponential_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exponential Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/joint_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Joint Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/independence_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Independence of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/conditional_distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conditional Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/multinomial_distribution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multinomial Distribution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/covariance_and_correlation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Covariance and Correlation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/transformations_of_random_variables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformations of Random Variables
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Productivity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Productivity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    How to Build Your Career in AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            How to Build Your Career in AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/three_steps_to_career_growth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Three Steps to Career Growth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_1_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 1- Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_2_projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 2- Projects
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../productivity/how_to_build_your_career_in_ai/phase_3_job/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Phase 3- Job
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="applications-of-projections-in-rn-orthogonal-bases-of-planes-and-linear-regression">Applications of Projections in ℝⁿ: Orthogonal Bases of Planes and Linear Regression</h1>
<p>Linear regression refers to the problem of finding a function <span class="arithmatex">\(f(x) = mx + b\)</span> which best fits a collection of given data points <span class="arithmatex">\((x_i, y_i)\)</span>.</p>
<h2 id="finding-an-orthogonal-basis-special-case">Finding an orthogonal basis: special case</h2>
<p><strong>Theorem</strong>: Suppose <span class="arithmatex">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\)</span> are nonzero, and not scalar multiples of each other. The vectors <span class="arithmatex">\(\mathbf{y}\)</span> and <span class="arithmatex">\(\mathbf{x}' = \mathbf{x} - \text{Proj}_{\mathbf{y}} \mathbf{x}\)</span> constitute an orthogonal basis of <span class="arithmatex">\(\text{span}(\mathbf{x}, \mathbf{y})\)</span>. In particular, <span class="arithmatex">\(\text{span}(\mathbf{x}, \mathbf{y})\)</span> is 2-dimensional.</p>
<p>The setup is symmetric in <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span>, so <span class="arithmatex">\(\{\mathbf{x}, \mathbf{y}' = \mathbf{y} - \text{Proj}_{\mathbf{x}} \mathbf{y}\}\)</span> is also an orthogonal basis of <span class="arithmatex">\(\text{span}(\mathbf{x}, \mathbf{y})\)</span>.</p>
<p><img alt="2-D span" src="../symm_proj.png" /></p>
<p><strong>Note</strong>: This is similar to the situation of projection of <span class="arithmatex">\(\mathbf{x}\)</span> onto a linear subspace <span class="arithmatex">\(V\)</span>. The displacement vector between the projection and <span class="arithmatex">\(\mathbf{x}\)</span> is perpendicular to everything in <span class="arithmatex">\(V\)</span>. In our case, when we project <span class="arithmatex">\(\mathbf{x}\)</span> onto the span of <span class="arithmatex">\(\mathbf{y}\)</span>, the resulting vector <span class="arithmatex">\(\mathbf{x}' = \mathbf{x} - \text{Proj}_{\mathbf{y}} \mathbf{x}\)</span> is orthogonal to <span class="arithmatex">\(\mathbf{y}\)</span>, which means it's perpendicular to everything in the span of <span class="arithmatex">\(\mathbf{y}\)</span>. This is why <span class="arithmatex">\(\mathbf{y}\)</span> and <span class="arithmatex">\(\mathbf{x}'\)</span> form an orthogonal basis - they are perpendicular to each other and together span the same 2-dimensional space as the original vectors <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span>.</p>
<p><strong>Example</strong>: Consider the plane <span class="arithmatex">\(V\)</span> in <span class="arithmatex">\(\mathbb{R}^3\)</span> through <span class="arithmatex">\(0\)</span> spanned by the vectors</p>
<div class="arithmatex">\[\mathbf{v} = \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} 0 \\ 3 \\ 4 \end{bmatrix}\]</div>
<p>Imagine that this plane is a metal sheet on which an electric charge is uniformly distributed. An iron particle placed at the point <span class="arithmatex">\(\mathbf{p} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}\)</span> would then be attracted to the metal sheet, and by the symmetry of the situation this particle would move straight towards the point on the plane closest to the initial position of the particle. What is that point?</p>
<p>In other words, we seek to compute the projection <span class="arithmatex">\(\text{Proj}_V(\mathbf{p})\)</span>. To compute this, we first seek an orthogonal basis for the plane <span class="arithmatex">\(V\)</span>. By the theorem above, such an orthogonal basis is given by <span class="arithmatex">\(\mathbf{w}\)</span> and <span class="arithmatex">\(\mathbf{v}' = \mathbf{v} - \text{Proj}_{\mathbf{w}}(\mathbf{v})\)</span>. We first compute <span class="arithmatex">\(\text{Proj}_{\mathbf{w}}(\mathbf{v})\)</span>. This is given by</p>
<div class="arithmatex">\[\text{Proj}_{\mathbf{w}}(\mathbf{v}) = \frac{\mathbf{v} \cdot \mathbf{w}}{\mathbf{w} \cdot \mathbf{w}} \mathbf{w} = \frac{3}{25} \begin{bmatrix} 0 \\ 3 \\ 4 \end{bmatrix} = \begin{bmatrix} 0 \\ \frac{9}{25} \\ \frac{12}{25} \end{bmatrix}\]</div>
<p>Thus <span class="arithmatex">\(\mathbf{v}' = \mathbf{v} - \begin{bmatrix} 0 \\ \frac{9}{25} \\ \frac{12}{25} \end{bmatrix} = \begin{bmatrix} 2 \\ \frac{16}{25} \\ -\frac{12}{25} \end{bmatrix}\)</span>. As a safety check, <span class="arithmatex">\(\mathbf{w}\)</span> and <span class="arithmatex">\(\mathbf{v}'\)</span> are indeed orthogonal.</p>
<p>The vector <span class="arithmatex">\(\mathbf{v}'\)</span> is a bit ugly due to the fractions, and for the purposes of having an orthogonal basis it is harmless to replace it with a nonzero scalar multiple, such as</p>
<div class="arithmatex">\[\mathbf{v}'' = 25\mathbf{v}' = \begin{bmatrix} 50 \\ 16 \\ -12 \end{bmatrix}\]</div>
<p>Since <span class="arithmatex">\(\{\mathbf{w}, \mathbf{v}''\}\)</span> is an orthogonal basis of the plane <span class="arithmatex">\(V\)</span>, we have</p>
<div class="arithmatex">\[\text{Proj}_V(\mathbf{p}) = \text{Proj}_V \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \text{Proj}_{\mathbf{w}} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} + \text{Proj}_{\mathbf{v}''} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \text{Proj}_{\mathbf{w}}(\mathbf{p}) + \text{Proj}_{\mathbf{v}''}(\mathbf{p})\]</div>
<p>To compute these projections, we first work out some relevant dot products:</p>
<div class="arithmatex">\[\mathbf{w} \cdot \mathbf{w} = 25, \quad \mathbf{v}'' \cdot \mathbf{v}'' = 2900, \quad \mathbf{p} \cdot \mathbf{w} = 7, \quad \mathbf{p} \cdot \mathbf{v}'' = 54\]</div>
<p>Hence</p>
<div class="arithmatex">\[\text{Proj}_{\mathbf{w}}(\mathbf{p}) = \frac{\mathbf{p} \cdot \mathbf{w}}{\mathbf{w} \cdot \mathbf{w}} \mathbf{w} = \frac{7}{25} \begin{bmatrix} 0 \\ 3 \\ 4 \end{bmatrix} = \begin{bmatrix} 0 \\ \frac{21}{25} \\ \frac{28}{25} \end{bmatrix}\]</div>
<div class="arithmatex">\[\text{Proj}_{\mathbf{v}''}(\mathbf{p}) = \frac{\mathbf{p} \cdot \mathbf{v}''}{\mathbf{v}'' \cdot \mathbf{v}''} \mathbf{v}'' = \frac{54}{2900} \begin{bmatrix} 50 \\ 16 \\ -12 \end{bmatrix} = \begin{bmatrix} \frac{27}{29} \\ \frac{216}{725} \\ -\frac{162}{725} \end{bmatrix}\]</div>
<p>Thus, the place on the metal sheet that the particle ends up at is</p>
<div class="arithmatex">\[\text{Proj}_V \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \text{Proj}_{\mathbf{w}} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} + \text{Proj}_{\mathbf{v}''} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ \frac{21}{25} \\ \frac{28}{25} \end{bmatrix} + \begin{bmatrix} \frac{27}{29} \\ \frac{216}{725} \\ -\frac{162}{725} \end{bmatrix} = \begin{bmatrix} \frac{27}{29} \\ \frac{33}{29} \\ \frac{26}{29} \end{bmatrix} \approx \begin{bmatrix} 0.931 \\ 1.138 \\ 0.897 \end{bmatrix}\]</div>
<p><strong>Example</strong>: Let <span class="arithmatex">\(\mathbf{w}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}\)</span> and <span class="arithmatex">\(\mathbf{w}_2 = \begin{bmatrix} 1 \\ -3 \\ 1 \\ 1 \end{bmatrix}\)</span>. Define <span class="arithmatex">\(U\)</span> to be the collection of all 4-vectors <span class="arithmatex">\(\mathbf{u}\)</span> that are orthogonal to both <span class="arithmatex">\(\mathbf{w}_1\)</span> and <span class="arithmatex">\(\mathbf{w}_2\)</span>. Show that <span class="arithmatex">\(U\)</span> is a linear subspace of <span class="arithmatex">\(\mathbb{R}^4\)</span> by writing it as a span of finitely many vectors. Explain why <span class="arithmatex">\(\dim(U) = 2\)</span>.</p>
<p><strong>Solution</strong>:</p>
<p>First, let's understand what <span class="arithmatex">\(U\)</span> represents. A vector <span class="arithmatex">\(\mathbf{u} = \begin{bmatrix} u_1 \\ u_2 \\ u_3 \\ u_4 \end{bmatrix}\)</span> belongs to <span class="arithmatex">\(U\)</span> if and only if:</p>
<div class="arithmatex">\[\mathbf{u} \cdot \mathbf{w}_1 = 0 \quad \text{and} \quad \mathbf{u} \cdot \mathbf{w}_2 = 0\]</div>
<p>This gives us the system of equations:</p>
<div class="arithmatex">\[u_1 + u_2 + u_3 + u_4 = 0\]</div>
<div class="arithmatex">\[u_1 - 3u_2 + u_3 + u_4 = 0\]</div>
<p>Subtracting the second equation from the first:</p>
<div class="arithmatex">\[(u_1 + u_2 + u_3 + u_4) - (u_1 - 3u_2 + u_3 + u_4) = 0 - 0\]</div>
<div class="arithmatex">\[4u_2 = 0\]</div>
<div class="arithmatex">\[u_2 = 0\]</div>
<p>Substituting <span class="arithmatex">\(u_2 = 0\)</span> back into the first equation:</p>
<div class="arithmatex">\[u_1 + 0 + u_3 + u_4 = 0\]</div>
<div class="arithmatex">\[u_1 + u_3 + u_4 = 0\]</div>
<p>This means <span class="arithmatex">\(u_1 = -u_3 - u_4\)</span>. So any vector in <span class="arithmatex">\(U\)</span> must have the form:</p>
<div class="arithmatex">\[\mathbf{u} = \begin{bmatrix} -u_3 - u_4 \\ 0 \\ u_3 \\ u_4 \end{bmatrix} = u_3 \begin{bmatrix} -1 \\ 0 \\ 1 \\ 0 \end{bmatrix} + u_4 \begin{bmatrix} -1 \\ 0 \\ 0 \\ 1 \end{bmatrix}\]</div>
<p>Let's define:</p>
<div class="arithmatex">\[\mathbf{v}_1 = \begin{bmatrix} -1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \quad \mathbf{v}_2 = \begin{bmatrix} -1 \\ 0 \\ 0 \\ 1 \end{bmatrix}\]</div>
<p>Then <span class="arithmatex">\(U = \text{span}(\mathbf{v}_1, \mathbf{v}_2)\)</span>, which shows that <span class="arithmatex">\(U\)</span> is indeed a linear subspace of <span class="arithmatex">\(\mathbb{R}^4\)</span>.</p>
<p>The dimension of <span class="arithmatex">\(U\)</span> is 2 because we found that <span class="arithmatex">\(U\)</span> is spanned by two vectors: <span class="arithmatex">\(\mathbf{v}_1\)</span> and <span class="arithmatex">\(\mathbf{v}_2\)</span>. These vectors are linearly independent (neither is a scalar multiple of the other). Therefore, <span class="arithmatex">\(\{\mathbf{v}_1, \mathbf{v}_2\}\)</span> is a basis for <span class="arithmatex">\(U\)</span>. Since the basis has 2 elements, <span class="arithmatex">\(\dim(U) = 2\)</span>.</p>
<h2 id="fitting-a-function-to-data">Fitting a function to data</h2>
<p>What does "best fit" mean? Informally, we want <span class="arithmatex">\(f(x_i)\)</span> to be as close as possible to <span class="arithmatex">\(y_i\)</span> for all <span class="arithmatex">\(i\)</span>. The error</p>
<div class="arithmatex">\[\text{error}_i = y_i - (mx_i + b)\]</div>
<p>measures in absolute value how close the line <span class="arithmatex">\(y = mx + b\)</span> is vertically to <span class="arithmatex">\((x_i, y_i)\)</span>.</p>
<p><img alt="Best fit" src="../best_fit.png" /></p>
<p>Suppose the line is given by the equation <span class="arithmatex">\(y = mx + b\)</span>. Suppose the <span class="arithmatex">\(i\)</span>th data point is denoted <span class="arithmatex">\((x_i, y_i)\)</span>. The <span class="arithmatex">\(i\)</span>th error is given by <span class="arithmatex">\(\text{error}_i = e_i = y_i - (mx_i + b)\)</span>. These errors are shown as blue line segments in the figure.</p>
<p>To be a "good fit" means to choose <span class="arithmatex">\((m, b)\)</span> so that the errors are collectively small. There are many ways to specify what "collectively small" means. The meaning in the least squares method is this: choose <span class="arithmatex">\((m, b)\)</span> to minimize the sum of the squares of the errors; i.e., choose <span class="arithmatex">\((m, b)\)</span> to minimize</p>
<div class="arithmatex">\[\sum_{i=1}^n (y_i - (mx_i + b))^2\]</div>
<p>Why use the sum of squares of the errors? The errors themselves might be positive and might be negative; we want to penalize a large negative error as well as a large positive error, so squaring errors removes the sign.</p>
<p>But sometimes other ways to define the "total error" are indeed more appropriate, such as summing the absolute values of the errors (used in computational statistics, geophysics, and the important signal processing algorithm called "compressed sensing"). The absolute value function is inconvenient for our purposes; e.g., from a calculus viewpoint, <span class="arithmatex">\(|x|\)</span> has the defect relative to <span class="arithmatex">\(x^2\)</span> that it is not differentiable at <span class="arithmatex">\(x = 0\)</span>. Always remember that we choose how to define "total error" for any particular application, and experience determines the appropriateness of that choice; mathematics is a creation of the human mind.</p>
<p>Put the data of all <span class="arithmatex">\(x\)</span>-values into a single <span class="arithmatex">\(n\)</span>-vector, and the data of all <span class="arithmatex">\(y\)</span>-values into a single <span class="arithmatex">\(n\)</span>-vector:</p>
<div class="arithmatex">\[X = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \quad Y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}\]</div>
<p>Also, define <span class="arithmatex">\(\mathbf{1} = \begin{bmatrix} 1 \\ 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \in \mathbb{R}^n\)</span> to be the vector with all entries equal to 1 (analogous to <span class="arithmatex">\(\mathbf{0} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \in \mathbb{R}^n\)</span>), so</p>
<div class="arithmatex">\[mX + b\mathbf{1} = \begin{bmatrix} mx_1 \\ mx_2 \\ \vdots \\ mx_n \end{bmatrix} + \begin{bmatrix} b \\ b \\ \vdots \\ b \end{bmatrix} = \begin{bmatrix} mx_1 + b \\ mx_2 + b \\ \vdots \\ mx_n + b \end{bmatrix}\]</div>
<p>and hence</p>
<div class="arithmatex">\[Y - (mX + b\mathbf{1}) = \begin{bmatrix} y_1 - (mx_1 + b) \\ y_2 - (mx_2 + b) \\ \vdots \\ y_n - (mx_n + b) \end{bmatrix} = \text{"vector of errors"}\]</div>
<p>Thus, since <span class="arithmatex">\(\sum_{i=1}^n v_i^2 = \|\mathbf{v}\|^2\)</span> for any <span class="arithmatex">\(\mathbf{v} \in \mathbb{R}^n\)</span> (by definition of <span class="arithmatex">\(\|\mathbf{v}\|\)</span>!), the sum of the squares of the errors is</p>
<div class="arithmatex">\[\sum_{i=1}^n (y_i - (mx_i + b))^2 = \|Y - (mX + b\mathbf{1})\|^2\]</div>
<p>So we seek <span class="arithmatex">\(m\)</span> and <span class="arithmatex">\(b\)</span> that minimizes the squared length of <span class="arithmatex">\(Y - (mX + b\mathbf{1})\)</span>, which is the same as minimizing the length of that difference.</p>
<p>The length <span class="arithmatex">\(\|Y - (mX + b\mathbf{1})\|\)</span> is the distance from <span class="arithmatex">\(Y\)</span> to <span class="arithmatex">\(mX + b\mathbf{1}\)</span> since "distance" between any <span class="arithmatex">\(n\)</span>-vectors <span class="arithmatex">\(\mathbf{v}\)</span> and <span class="arithmatex">\(\mathbf{w}\)</span> is <span class="arithmatex">\(\|\mathbf{v} - \mathbf{w}\|\)</span> by definition. As <span class="arithmatex">\(m\)</span> and <span class="arithmatex">\(b\)</span> vary, the vectors of the form <span class="arithmatex">\(mX + b\mathbf{1}\)</span> are exactly the vectors in <span class="arithmatex">\(\text{span}(X, \mathbf{1})\)</span>, due to the definition of "span". Hence, the least-squares minimization problem for <span class="arithmatex">\(n\)</span> data points is equivalent to the following geometric problem:</p>
<p><strong>find the vector in <span class="arithmatex">\(\text{span}(X, \mathbf{1})\)</span> that is closest to the vector <span class="arithmatex">\(Y \in \mathbb{R}^n\)</span>.</strong></p>
<p>Our task is now an instance of finding the point of a linear subspace of <span class="arithmatex">\(\mathbb{R}^n\)</span> closest to a given <span class="arithmatex">\(n\)</span>-vector.</p>
<p>The vectors <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(\mathbf{1}\)</span> are not scalar multiples of each other because the hypothesis that the <span class="arithmatex">\(n\)</span> data points do not lie in a common vertical line (i.e., the <span class="arithmatex">\(x_i\)</span>'s are not all equal to each other) says that <span class="arithmatex">\(X\)</span> is not a scalar multiple of the nonzero vector <span class="arithmatex">\(\mathbf{1}\)</span>.</p>
<p>By using the Theorem above, an orthogonal basis of <span class="arithmatex">\(\text{span}(X, \mathbf{1})\)</span> is given by <span class="arithmatex">\(\mathbf{1}\)</span> and <span class="arithmatex">\(\hat{X} = X - \text{Proj}_{\mathbf{1}}X\)</span> with</p>
<div class="arithmatex">\[\text{Proj}_{\mathbf{1}}(X) = \frac{X \cdot \mathbf{1}}{\mathbf{1} \cdot \mathbf{1}} \mathbf{1} = \frac{\sum_{i=1}^n x_i \cdot 1}{\sum_{i=1}^n 1 \cdot 1} \mathbf{1} = \frac{\sum_{i=1}^n x_i}{n} \mathbf{1} = \bar{x} \mathbf{1} = \begin{bmatrix} \bar{x} \\ \bar{x} \\ \vdots \\ \bar{x} \end{bmatrix}\]</div>
<p>equal to the <span class="arithmatex">\(n\)</span>-vector each of whose entries is equal to the average <span class="arithmatex">\(\bar{x}\)</span> of the <span class="arithmatex">\(x_i\)</span>'s. Hence,</p>
<div class="arithmatex">\[\hat{X} = X - \text{Proj}_{\mathbf{1}}(X) = \begin{bmatrix} x_1 - \bar{x} \\ x_2 - \bar{x} \\ \vdots \\ x_n - \bar{x} \end{bmatrix}\]</div>
<p>is obtained from <span class="arithmatex">\(X\)</span> by subtracting the average <span class="arithmatex">\(\bar{x}\)</span> from all entries.</p>
<p>By applying to this span the formula for the nearest point on a linear subspace in terms of an orthogonal basis, we obtain that the closest vector to <span class="arithmatex">\(Y\)</span> in <span class="arithmatex">\(\text{span}(X, \mathbf{1})\)</span> is</p>
<div class="arithmatex">\[\frac{Y \cdot \hat{X}}{\hat{X} \cdot \hat{X}} \hat{X} + \frac{Y \cdot \mathbf{1}}{\mathbf{1} \cdot \mathbf{1}} \mathbf{1} = \frac{Y \cdot \hat{X}}{\hat{X} \cdot \hat{X}} \hat{X} + \bar{y} \mathbf{1}\]</div>
<p>where <span class="arithmatex">\(\bar{y} = (1/n) \sum_{i=1}^n y_i\)</span> is the average of the <span class="arithmatex">\(y_i\)</span>'s.</p>
<p>In the expression <span class="arithmatex">\(\frac{Y \cdot \hat{X}}{\hat{X} \cdot \hat{X}} \hat{X} + \bar{y} \mathbf{1}\)</span> on the right side, we can expand <span class="arithmatex">\(\hat{X}\)</span> in terms of <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(\mathbf{1}\)</span> using the definition of <span class="arithmatex">\(\text{Proj}_{\mathbf{1}}(X)\)</span> and collect terms to rewrite this as a linear combination <span class="arithmatex">\(mX + b\mathbf{1}\)</span> of <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(\mathbf{1}\)</span>. Those coefficients <span class="arithmatex">\(m\)</span> and <span class="arithmatex">\(b\)</span> are exactly the desired "<span class="arithmatex">\(m\)</span>" and "<span class="arithmatex">\(b\)</span>" for the best-fit line!</p>
<h2 id="correlation-coefficient-and-quality-of-fit">Correlation Coefficient and quality of fit</h2>
<p>Let the best-fit line be <span class="arithmatex">\(y = mx + b\)</span>, and let <span class="arithmatex">\(r\)</span> be the correlation coefficient for the recentered data <span class="arithmatex">\((x_i - \bar{x}, y_i - \bar{y})\)</span> (whose coordinates average to 0) with associated <span class="arithmatex">\(n\)</span>-vectors <span class="arithmatex">\(\hat{X}\)</span> and <span class="arithmatex">\(\hat{Y}\)</span>. Then the role of nearness of <span class="arithmatex">\(r^2\)</span> to 1 (or equivalently of nearness of <span class="arithmatex">\(1 - r^2\)</span> to 0) as a measure of quality of fit is expressed by the following identity:</p>
<div class="arithmatex">\[\|Y - (mX + b\mathbf{1})\|^2 = \|\hat{Y}\|^2 (1 - r^2)\]</div>
<p>This equation will be proven later.</p>
<p>where <span class="arithmatex">\(\hat{Y}\)</span> is the "recentered" version of <span class="arithmatex">\(Y\)</span> (subtracting <span class="arithmatex">\(\bar{y}\)</span> from all <span class="arithmatex">\(y_i\)</span>'s).</p>
<p>To explain the meaning of the above equation, expand out the left side (and use that <span class="arithmatex">\(t^2 = |t|^2\)</span> for any <span class="arithmatex">\(t\)</span>) to get</p>
<div class="arithmatex">\[|y_1 - (mx_1 + b)|^2 + |y_2 - (mx_2 + b)|^2 + \cdots + |y_n - (mx_n + b)|^2\]</div>
<p>The number <span class="arithmatex">\(|y_i - (mx_i + b)|\)</span> is the vertical distance between the data point <span class="arithmatex">\((x_i, y_i)\)</span> and the best fit line <span class="arithmatex">\(y = mx + b\)</span>. When <span class="arithmatex">\(r^2 \approx 1\)</span>, the equation therefore says that these vertical distances are "collectively small": the sum of their squares is tiny since <span class="arithmatex">\(1 - r^2\)</span> on the right side of the equation is small, so the data points are all close to the best fit line. When <span class="arithmatex">\(r^2 \approx 0\)</span> then (at least informally) the opposite happens since the right side is approximately <span class="arithmatex">\(\|\hat{Y}\|^2\)</span>, which is typically quite far from 0 (even though the average of the entries in <span class="arithmatex">\(\hat{Y}\)</span> is 0 by design).</p>
<p><strong>Example:</strong> Sometimes a quantity <span class="arithmatex">\(y\)</span> of interest is expected to be (approximately "linearly") related to a pair of quantities <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(v\)</span> rather than just a single quantity <span class="arithmatex">\(x\)</span>. In such cases, as a variant on linear regression, we seek three constants <span class="arithmatex">\(a, b, c\)</span> for which</p>
<div class="arithmatex">\[y \approx a + bx + cv\]</div>
<p>as measured by data.</p>
<p>Suppose we make <span class="arithmatex">\(n\)</span> measurements of <span class="arithmatex">\(x, v, y\)</span>, yielding data points <span class="arithmatex">\((x_i, v_i, y_i)\)</span>. Let <span class="arithmatex">\(X, V, Y \in \mathbb{R}^n\)</span> be the corresponding <span class="arithmatex">\(n\)</span>-vectors for the <span class="arithmatex">\(n\)</span> measurements of each of <span class="arithmatex">\(x, v, y\)</span>. Assume <span class="arithmatex">\(W = \text{span}(\mathbf{1}, X, V)\)</span> is 3-dimensional (a reasonable assumption when neither <span class="arithmatex">\(x\)</span> nor <span class="arithmatex">\(v\)</span> determines the other).</p>
<p><strong>(a)</strong> Explain in words how the vector <span class="arithmatex">\(\text{Proj}_W(Y) \in W\)</span> encodes a "least squares" choice of <span class="arithmatex">\((a, b, c)\)</span> in terms of the data.</p>
<p><strong>(b)</strong> What is the practical difficulty in using the equation of finding a projection of a vector to compute <span class="arithmatex">\(\text{Proj}_W(Y)\)</span>, whereas we had no difficulty in computing the analogous such projection for linear regression?</p>
<p><strong>Solution:</strong></p>
<p><strong>(a) Least Squares Interpretation</strong></p>
<p>The vector <span class="arithmatex">\(\text{Proj}_W(Y) \in W\)</span> represents the closest point in the subspace <span class="arithmatex">\(W = \text{span}(\mathbf{1}, X, V)\)</span> to the data vector <span class="arithmatex">\(Y\)</span>. Since <span class="arithmatex">\(W\)</span> is 3-dimensional, any vector in <span class="arithmatex">\(W\)</span> can be written as a linear combination:</p>
<div class="arithmatex">\[\text{Proj}_W(Y) = a\mathbf{1} + bX + cV\]</div>
<p>for some constants <span class="arithmatex">\(a, b, c \in \mathbb{R}\)</span>.</p>
<p>This projection minimizes the distance <span class="arithmatex">\(\|Y - (a\mathbf{1} + bX + cV)\|\)</span>, which is equivalent to minimizing the sum of squared errors:</p>
<div class="arithmatex">\[\sum_{i=1}^n (y_i - (a + bx_i + cv_i))^2\]</div>
<p>Therefore, the coefficients <span class="arithmatex">\((a, b, c)\)</span> in the expression <span class="arithmatex">\(\text{Proj}_W(Y) = a\mathbf{1} + bX + cV\)</span> represent the least squares solution to the multiple linear regression problem <span class="arithmatex">\(y \approx a + bx + cv\)</span>.</p>
<p><strong>(b) Practical Difficulty</strong></p>
<p>The practical difficulty in computing <span class="arithmatex">\(\text{Proj}_W(Y)\)</span> for multiple linear regression compared to simple linear regression is the <strong>dimensionality of the subspace</strong>.</p>
<p><strong>Simple Linear Regression (2D subspace)</strong>:</p>
<ul>
<li>
<p>We had <span class="arithmatex">\(W = \text{span}(\mathbf{1}, X)\)</span>, a 2-dimensional subspace</p>
</li>
<li>
<p>We could easily construct an orthogonal basis using the Gram-Schmidt process</p>
</li>
<li>
<p>The projection formula was straightforward: <span class="arithmatex">\(\text{Proj}_W(Y) = \text{Proj}_{\mathbf{1}}(Y) + \text{Proj}_{\hat{X}}(Y)\)</span></p>
</li>
<li>
<p>We could compute this step-by-step with simple projections</p>
</li>
</ul>
<p><strong>Multiple Linear Regression (3D subspace)</strong>:</p>
<ul>
<li>
<p>We now have <span class="arithmatex">\(W = \text{span}(\mathbf{1}, X, V)\)</span>, a 3-dimensional subspace</p>
</li>
<li>
<p>Constructing an orthogonal basis becomes more complex</p>
</li>
<li>
<p>The Gram-Schmidt process requires more steps and can lead to numerical instability</p>
</li>
<li>
<p>The projection formula involves more terms and becomes computationally intensive</p>
</li>
</ul>
<p><strong>Specific Challenges</strong>:</p>
<ol>
<li>
<p><strong>Orthogonal Basis Construction</strong>: We need to find three mutually orthogonal vectors spanning <span class="arithmatex">\(W\)</span>, which requires applying Gram-Schmidt to three vectors instead of two.</p>
</li>
<li>
<p><strong>Numerical Stability</strong>: As the dimension increases, small errors in computations can accumulate, leading to less accurate results.</p>
</li>
<li>
<p><strong>Computational Complexity</strong>: The projection involves more dot products and vector operations, making it computationally expensive for large datasets.</p>
</li>
<li>
<p><strong>Matrix Methods</strong>: For higher dimensions, it becomes more practical to use matrix methods (like QR decomposition or solving the normal equations) rather than geometric projection formulas.</p>
</li>
</ol>
<p>This is why in practice, multiple linear regression is typically solved using matrix algebra and computational algorithms rather than the geometric projection approach, even though the geometric interpretation remains valid and insightful.</p>
<p><strong>Example</strong>: The vectors <span class="arithmatex">\(\mathbf{v} = \begin{bmatrix} 2 \\ -1 \\ -1 \\ 1 \end{bmatrix}\)</span> and <span class="arithmatex">\(\mathbf{w} = \begin{bmatrix} 11 \\ 5 \\ -10 \\ 1 \end{bmatrix}\)</span> span a plane <span class="arithmatex">\(P\)</span> through the origin in <span class="arithmatex">\(\mathbb{R}^4\)</span>. Let</p>
<div class="arithmatex">\[L = \left\{\begin{bmatrix} 4-t \\ 4+4t \\ 4-t \\ -7-2t \end{bmatrix} : t \in \mathbb{R}\right\}\]</div>
<p>be a line in <span class="arithmatex">\(\mathbb{R}^4\)</span>.</p>
<p><strong>(a)</strong> Consider the displacement vector <span class="arithmatex">\(\mathbf{x}\)</span> between any two different points of <span class="arithmatex">\(L\)</span> (all such displacements are scalar multiples of each other since <span class="arithmatex">\(L\)</span> is a line). Show that <span class="arithmatex">\(\mathbf{x}\)</span> belongs to <span class="arithmatex">\(P\)</span>; this is described in words by saying <span class="arithmatex">\(L\)</span> is <strong>parallel</strong> to <span class="arithmatex">\(P\)</span>.</p>
<p><strong>Solution:</strong></p>
<p>Let's take two different points on the line <span class="arithmatex">\(L\)</span> by choosing two different values of <span class="arithmatex">\(t\)</span>. Let's use <span class="arithmatex">\(t = 0\)</span> and <span class="arithmatex">\(t = 1\)</span>:</p>
<ul>
<li>
<p>Point 1 (when <span class="arithmatex">\(t = 0\)</span>): <span class="arithmatex">\(\begin{bmatrix} 4 \\ 4 \\ 4 \\ -7 \end{bmatrix}\)</span></p>
</li>
<li>
<p>Point 2 (when <span class="arithmatex">\(t = 1\)</span>): <span class="arithmatex">\(\begin{bmatrix} 3 \\ 8 \\ 3 \\ -9 \end{bmatrix}\)</span></p>
</li>
</ul>
<p>The displacement vector between these two points is:</p>
<div class="arithmatex">\[\mathbf{x} = \begin{bmatrix} 3 \\ 8 \\ 3 \\ -9 \end{bmatrix} - \begin{bmatrix} 4 \\ 4 \\ 4 \\ -7 \end{bmatrix} = \begin{bmatrix} -1 \\ 4 \\ -1 \\ -2 \end{bmatrix}\]</div>
<p>We need to find scalars <span class="arithmatex">\(a, b \in \mathbb{R}\)</span> such that:</p>
<div class="arithmatex">\[\mathbf{x} = a\mathbf{v} + b\mathbf{w}\]</div>
<p>This gives us the system of equations:</p>
<div class="arithmatex">\[\begin{bmatrix} -1 \\ 4 \\ -1 \\ -2 \end{bmatrix} = a\begin{bmatrix} 2 \\ -1 \\ -1 \\ 1 \end{bmatrix} + b\begin{bmatrix} 11 \\ 5 \\ -10 \\ 1 \end{bmatrix}\]</div>
<p>Which expands to:</p>
<div class="arithmatex">\[-1 = 2a + 11b \quad \text{(1)}\]</div>
<div class="arithmatex">\[4 = -a + 5b \quad \text{(2)}\]</div>
<div class="arithmatex">\[-1 = -a - 10b \quad \text{(3)}\]</div>
<div class="arithmatex">\[-2 = a + b \quad \text{(4)}\]</div>
<p>Let's solve equations (1) and (2) first:</p>
<p>From equation (2): <span class="arithmatex">\(4 = -a + 5b\)</span>, so <span class="arithmatex">\(a = 5b - 4\)</span></p>
<p>Substitute into equation (1):</p>
<div class="arithmatex">\[-1 = 2(5b - 4) + 11b = 10b - 8 + 11b = 21b - 8\]</div>
<div class="arithmatex">\[21b = 7\]</div>
<div class="arithmatex">\[b = \frac{1}{3}\]</div>
<p>Now substitute <span class="arithmatex">\(b = \frac{1}{3}\)</span> back to find <span class="arithmatex">\(a\)</span>:</p>
<div class="arithmatex">\[a = 5\left(\frac{1}{3}\right) - 4 = \frac{5}{3} - 4 = \frac{5}{3} - \frac{12}{3} = -\frac{7}{3}\]</div>
<p>Let's check if <span class="arithmatex">\(a = -\frac{7}{3}\)</span> and <span class="arithmatex">\(b = \frac{1}{3}\)</span> satisfy all four equations:</p>
<p>Equation (1): <span class="arithmatex">\(2a + 11b = 2\left(-\frac{7}{3}\right) + 11\left(\frac{1}{3}\right) = -\frac{14}{3} + \frac{11}{3} = -\frac{3}{3} = -1\)</span> ✓</p>
<p>Equation (2): <span class="arithmatex">\(-a + 5b = -\left(-\frac{7}{3}\right) + 5\left(\frac{1}{3}\right) = \frac{7}{3} + \frac{5}{3} = \frac{12}{3} = 4\)</span> ✓</p>
<p>Equation (3): <span class="arithmatex">\(-a - 10b = -\left(-\frac{7}{3}\right) - 10\left(\frac{1}{3}\right) = \frac{7}{3} - \frac{10}{3} = -\frac{3}{3} = -1\)</span> ✓</p>
<p>Equation (4): <span class="arithmatex">\(a + b = -\frac{7}{3} + \frac{1}{3} = -\frac{6}{3} = -2\)</span> ✓</p>
<p>Since we found scalars <span class="arithmatex">\(a = -\frac{7}{3}\)</span> and <span class="arithmatex">\(b = \frac{1}{3}\)</span> such that:</p>
<div class="arithmatex">\[\mathbf{x} = -\frac{7}{3}\mathbf{v} + \frac{1}{3}\mathbf{w}\]</div>
<p>This proves that the displacement vector <span class="arithmatex">\(\mathbf{x} = \begin{bmatrix} -1 \\ 4 \\ -1 \\ -2 \end{bmatrix}\)</span> belongs to the plane <span class="arithmatex">\(P = \text{span}(\mathbf{v}, \mathbf{w})\)</span>.</p>
<p>This means that the line <span class="arithmatex">\(L\)</span> is parallel to the plane <span class="arithmatex">\(P\)</span>. In <span class="arithmatex">\(\mathbb{R}^4\)</span>, just as in <span class="arithmatex">\(\mathbb{R}^3\)</span>, a line is parallel to a plane if the direction vector of the line (which is a scalar multiple of any displacement vector between two points on the line) lies in the plane.</p>
<p><strong>(b)</strong> Whenever one has a linear subspace <span class="arithmatex">\(V\)</span> of <span class="arithmatex">\(\mathbb{R}^n\)</span> and a line <span class="arithmatex">\(\ell\)</span> in <span class="arithmatex">\(\mathbb{R}^n\)</span> (possibly not through the origin) that is parallel to <span class="arithmatex">\(V\)</span>, it is a fact (not difficult to show, but you may take it on faith) that all points in <span class="arithmatex">\(\ell\)</span> have the same distance to <span class="arithmatex">\(V\)</span>. That is, for every point <span class="arithmatex">\(\mathbf{y} \in \ell\)</span> and the point <span class="arithmatex">\(\mathbf{y}' \in V\)</span> nearest to <span class="arithmatex">\(\mathbf{y}\)</span>, the distance <span class="arithmatex">\(\|\mathbf{y} - \mathbf{y}'\|\)</span> is the same regardless of which <span class="arithmatex">\(\mathbf{y}\)</span> on <span class="arithmatex">\(\ell\)</span> we consider. Taking <span class="arithmatex">\(V\)</span> and <span class="arithmatex">\(\ell\)</span> to be <span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(L\)</span> above, compute the common distance <span class="arithmatex">\(\|\mathbf{y} - \mathbf{y}'\|\)</span> (since it is independent of <span class="arithmatex">\(\mathbf{y}\)</span>, you may pick whatever you consider to be the most convenient point <span class="arithmatex">\(\mathbf{y}\)</span> in <span class="arithmatex">\(L\)</span> to do the calculation).</p>
<p><strong>Solution:</strong></p>
<p>Since all points on the line <span class="arithmatex">\(L\)</span> have the same distance to the plane <span class="arithmatex">\(P\)</span>, we can choose the most convenient point. Let's use the point when <span class="arithmatex">\(t = 0\)</span>: <span class="arithmatex">\(\mathbf{y} = \begin{bmatrix} 4 \\ 4 \\ 4 \\ -7 \end{bmatrix}\)</span>.</p>
<p>The distance from <span class="arithmatex">\(\mathbf{y}\)</span> to the plane <span class="arithmatex">\(P\)</span> is the distance from <span class="arithmatex">\(\mathbf{y}\)</span> to its projection onto <span class="arithmatex">\(P\)</span>. To find this projection, we need an orthogonal basis for <span class="arithmatex">\(P\)</span>.</p>
<p><strong>Step 1: Find an orthogonal basis for <span class="arithmatex">\(P\)</span></strong></p>
<p>Using the theorem from earlier, we can construct an orthogonal basis for <span class="arithmatex">\(P = \text{span}(\mathbf{v}, \mathbf{w})\)</span>:</p>
<p>Let <span class="arithmatex">\(\mathbf{v}_1 = \mathbf{v} = \begin{bmatrix} 2 \\ -1 \\ -1 \\ 1 \end{bmatrix}\)</span></p>
<p>Then <span class="arithmatex">\(\mathbf{v}_2 = \mathbf{w} - \text{Proj}_{\mathbf{v}_1}(\mathbf{w})\)</span></p>
<p>First, compute <span class="arithmatex">\(\text{Proj}_{\mathbf{v}_1}(\mathbf{w})\)</span>:</p>
<div class="arithmatex">\[\text{Proj}_{\mathbf{v}_1}(\mathbf{w}) = \frac{\mathbf{w} \cdot \mathbf{v}_1}{\mathbf{v}_1 \cdot \mathbf{v}_1} \mathbf{v}_1\]</div>
<div class="arithmatex">\[\mathbf{w} \cdot \mathbf{v}_1 = 11(2) + 5(-1) + (-10)(-1) + 1(1) = 22 - 5 + 10 + 1 = 28\]</div>
<div class="arithmatex">\[\mathbf{v}_1 \cdot \mathbf{v}_1 = 2^2 + (-1)^2 + (-1)^2 + 1^2 = 4 + 1 + 1 + 1 = 7\]</div>
<p>So:</p>
<div class="arithmatex">\[\text{Proj}_{\mathbf{v}_1}(\mathbf{w}) = \frac{28}{7} \begin{bmatrix} 2 \\ -1 \\ -1 \\ 1 \end{bmatrix} = 4 \begin{bmatrix} 2 \\ -1 \\ -1 \\ 1 \end{bmatrix} = \begin{bmatrix} 8 \\ -4 \\ -4 \\ 4 \end{bmatrix}\]</div>
<p>Therefore:</p>
<div class="arithmatex">\[\mathbf{v}_2 = \mathbf{w} - \text{Proj}_{\mathbf{v}_1}(\mathbf{w}) = \begin{bmatrix} 11 \\ 5 \\ -10 \\ 1 \end{bmatrix} - \begin{bmatrix} 8 \\ -4 \\ -4 \\ 4 \end{bmatrix} = \begin{bmatrix} 3 \\ 9 \\ -6 \\ -3 \end{bmatrix}\]</div>
<p><strong>Step 2: Compute the projection of <span class="arithmatex">\(\mathbf{y}\)</span> onto <span class="arithmatex">\(P\)</span></strong></p>
<p>Using the orthogonal basis <span class="arithmatex">\(\{\mathbf{v}_1, \mathbf{v}_2\}\)</span>, the projection is:</p>
<div class="arithmatex">\[\text{Proj}_P(\mathbf{y}) = \frac{\mathbf{y} \cdot \mathbf{v}_1}{\mathbf{v}_1 \cdot \mathbf{v}_1} \mathbf{v}_1 + \frac{\mathbf{y} \cdot \mathbf{v}_2}{\mathbf{v}_2 \cdot \mathbf{v}_2} \mathbf{v}_2\]</div>
<p>Compute the dot products:</p>
<div class="arithmatex">\[\mathbf{y} \cdot \mathbf{v}_1 = 4(2) + 4(-1) + 4(-1) + (-7)(1) = 8 - 4 - 4 - 7 = -7\]</div>
<div class="arithmatex">\[\mathbf{y} \cdot \mathbf{v}_2 = 4(3) + 4(9) + 4(-6) + (-7)(-3) = 12 + 36 - 24 + 21 = 45\]</div>
<div class="arithmatex">\[\mathbf{v}_2 \cdot \mathbf{v}_2 = 3^2 + 9^2 + (-6)^2 + (-3)^2 = 9 + 81 + 36 + 9 = 135\]</div>
<p>So:</p>
<div class="arithmatex">\[\text{Proj}_P(\mathbf{y}) = \frac{-7}{7} \begin{bmatrix} 2 \\ -1 \\ -1 \\ 1 \end{bmatrix} + \frac{45}{135} \begin{bmatrix} 3 \\ 9 \\ -6 \\ -3 \end{bmatrix}\]</div>
<div class="arithmatex">\[= -1 \begin{bmatrix} 2 \\ -1 \\ -1 \\ 1 \end{bmatrix} + \frac{1}{3} \begin{bmatrix} 3 \\ 9 \\ -6 \\ -3 \end{bmatrix}\]</div>
<div class="arithmatex">\[= \begin{bmatrix} -2 \\ 1 \\ 1 \\ -1 \end{bmatrix} + \begin{bmatrix} 1 \\ 3 \\ -2 \\ -1 \end{bmatrix}\]</div>
<div class="arithmatex">\[= \begin{bmatrix} -1 \\ 4 \\ -1 \\ -2 \end{bmatrix}\]</div>
<p><strong>Step 3: Compute the distance</strong></p>
<p>The distance from <span class="arithmatex">\(\mathbf{y}\)</span> to the plane <span class="arithmatex">\(P\)</span> is:</p>
<div class="arithmatex">\[\|\mathbf{y} - \text{Proj}_P(\mathbf{y})\| = \left\|\begin{bmatrix} 4 \\ 4 \\ 4 \\ -7 \end{bmatrix} - \begin{bmatrix} -1 \\ 4 \\ -1 \\ -2 \end{bmatrix}\right\| = \left\|\begin{bmatrix} 5 \\ 0 \\ 5 \\ -5 \end{bmatrix}\right\|\]</div>
<div class="arithmatex">\[\|\mathbf{y} - \text{Proj}_P(\mathbf{y})\| = \sqrt{5^2 + 0^2 + 5^2 + (-5)^2} = \sqrt{25 + 0 + 25 + 25} = \sqrt{75} = \sqrt{25 \times 3} = 5\sqrt{3}\]</div>
<p><strong>Answer:</strong> The common distance from any point on the line <span class="arithmatex">\(L\)</span> to the plane <span class="arithmatex">\(P\)</span> is <span class="arithmatex">\(\sqrt{75}\)</span>.</p>
<h2 id="orthogonal-basis-formula-and-relation-of-correlation-coefficient-to-best-fit-lines">Orthogonal basis formula and relation of correlation coefficient to best fit lines</h2>
<p>In this section we prove some results discussed earlier.</p>
<p><strong>Theorem</strong>: Suppose <span class="arithmatex">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\)</span> are nonzero, and not scalar multiples of each other. The vectors <span class="arithmatex">\(\mathbf{y}\)</span> and <span class="arithmatex">\(\mathbf{x}' = \mathbf{x} - \text{Proj}_{\mathbf{y}} \mathbf{x}\)</span> constitute an orthogonal basis of <span class="arithmatex">\(\text{span}(\mathbf{x}, \mathbf{y})\)</span>. In particular, <span class="arithmatex">\(\text{span}(\mathbf{x}, \mathbf{y})\)</span> is 2-dimensional.</p>
<p>The setup is symmetric in <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span>, so <span class="arithmatex">\(\{\mathbf{x}, \mathbf{y}' = \mathbf{y} - \text{Proj}_{\mathbf{x}} \mathbf{y}\}\)</span> is also an orthogonal basis of <span class="arithmatex">\(\text{span}(\mathbf{x}, \mathbf{y})\)</span>.</p>
<p><strong>Proof</strong>: Write <span class="arithmatex">\(\mathbf{x}' = \mathbf{x} - \text{Proj}_{\mathbf{y}} \mathbf{x}\)</span>.</p>
<div class="arithmatex">\[\mathbf{x}' \cdot \mathbf{y} = \left(\mathbf{x} - \frac{\mathbf{x} \cdot \mathbf{y}}{\mathbf{y} \cdot \mathbf{y}}\mathbf{y}\right) \cdot \mathbf{y} = \mathbf{x} \cdot \mathbf{y} - \frac{\mathbf{x} \cdot \mathbf{y}}{\mathbf{y} \cdot \mathbf{y}}\mathbf{y} \cdot \mathbf{y} = \mathbf{x} \cdot \mathbf{y} - \mathbf{x} \cdot \mathbf{y} = 0.\]</div>
<p>Next, <span class="arithmatex">\(\mathbf{y}\)</span> is not zero (we have assumed this). Also, <span class="arithmatex">\(\mathbf{x}'\)</span> is not zero: if it were zero then <span class="arithmatex">\(\mathbf{x} = \text{Proj}_{\mathbf{y}}(\mathbf{x})\)</span>, yet such a projection is always a scalar multiple of <span class="arithmatex">\(\mathbf{y}\)</span> and we have assumed <span class="arithmatex">\(\mathbf{x}\)</span> is not a scalar multiple of <span class="arithmatex">\(\mathbf{y}\)</span>. Therefore <span class="arithmatex">\(\{\mathbf{x}', \mathbf{y}\}\)</span> is a pair of nonzero orthogonal vectors belonging to <span class="arithmatex">\(\text{span}(\mathbf{x}, \mathbf{y})\)</span> by design (note that <span class="arithmatex">\(\mathbf{y} = 0 \cdot \mathbf{x} + 1 \cdot \mathbf{y}\)</span>), and they exhaust that span since we can also write each of <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span> as linear combinations of <span class="arithmatex">\(\mathbf{x}'\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span>: <span class="arithmatex">\(\mathbf{x} = \mathbf{x}' + \text{Proj}_{\mathbf{y}}(\mathbf{x}) = \mathbf{x}' + ((\mathbf{x} \cdot \mathbf{y})/(\mathbf{y} \cdot \mathbf{y}))\mathbf{y}\)</span> and <span class="arithmatex">\(\mathbf{y} = 0 \cdot \mathbf{x}' + 1 \cdot \mathbf{y}\)</span>. Since any collection of pairwise orthogonal nonzero vectors is a basis for its span, we conclude that <span class="arithmatex">\(\{\mathbf{x}', \mathbf{y}\}\)</span> is an orthogonal basis of <span class="arithmatex">\(\text{span}(\mathbf{x}', \mathbf{y}) = \text{span}(\mathbf{x}, \mathbf{y})\)</span>.</p>
<p>Now suppose we are given <span class="arithmatex">\(n\)</span> data points <span class="arithmatex">\((x_i, y_i)\)</span>, assembled into <span class="arithmatex">\(n\)</span>-vectors</p>
<div class="arithmatex">\[X = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \quad \text{and} \quad Y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}\]</div>
<p>Earlier, we described the relationship between the correlation coefficient <span class="arithmatex">\(r\)</span> for the recentered data (corresponding to the <span class="arithmatex">\(n\)</span>-vectors <span class="arithmatex">\(\hat{X}\)</span> and <span class="arithmatex">\(\hat{Y}\)</span>) and the line of best fit. Let's restate that in terms of <span class="arithmatex">\(r^2\)</span>, which we expressed as the formula</p>
<div class="arithmatex">\[r^2 = \frac{(\hat{X} \cdot \hat{Y})^2}{\|\hat{X}\|^2\|\hat{Y}\|^2} = \frac{(\hat{X} \cdot \hat{Y})^2}{(\hat{X} \cdot \hat{X})(\hat{Y} \cdot \hat{Y})}\]</div>
<p>We stated that <span class="arithmatex">\(r^2\)</span> is near 0 when the line of best fit is a bad fit, and near 1 when it is a good fit (note that this could happen either when <span class="arithmatex">\(r\)</span> is near 1, or when <span class="arithmatex">\(r\)</span> is near −1). We made the role of <span class="arithmatex">\(r^2\)</span> as a measure of quality of fit precise. Here is the derivation of <span class="arithmatex">\(\|Y - (mX + b\mathbf{1})\|^2 = \|\hat{Y}\|^2 (1 - r^2)\)</span>.</p>
<p><strong>Proof</strong>: We know that the closest vector to <span class="arithmatex">\(Y\)</span> in <span class="arithmatex">\(\text{span}(X, \mathbf{1})\)</span> is</p>
<div class="arithmatex">\[\frac{Y \cdot \hat{X}}{\hat{X} \cdot \hat{X}} \hat{X} + \frac{Y \cdot \mathbf{1}}{\mathbf{1} \cdot \mathbf{1}} \mathbf{1} = \frac{Y \cdot \hat{X}}{\hat{X} \cdot \hat{X}} \hat{X} + \bar{y} \mathbf{1}\]</div>
<p>where <span class="arithmatex">\(\bar{y} = (1/n) \sum_{i=1}^n y_i\)</span> is the average of the <span class="arithmatex">\(y_i\)</span>'s.</p>
<div class="arithmatex">\[Y - (mX + b\mathbf{1}) = Y - \left(\frac{Y \cdot \hat{X}}{\hat{X} \cdot \hat{X}}\hat{X} + \frac{Y \cdot \mathbf{1}}{\mathbf{1} \cdot \mathbf{1}}\mathbf{1}\right) = \left(Y - \frac{Y \cdot \mathbf{1}}{\mathbf{1} \cdot \mathbf{1}}\mathbf{1}\right) - \frac{Y \cdot \hat{X}}{\hat{X} \cdot \hat{X}}\hat{X}\]</div>
<p>where <span class="arithmatex">\(Y - \frac{Y \cdot \mathbf{1}}{\mathbf{1} \cdot \mathbf{1}}\mathbf{1} = Y - \bar{y}\mathbf{1}\)</span> is indeed equal to <span class="arithmatex">\(\hat{Y}\)</span>.</p>
<p>Note that <span class="arithmatex">\(\hat{Y} \cdot \hat{X} = Y \cdot \hat{X}\)</span> because the difference <span class="arithmatex">\(\hat{Y} - Y = -\bar{y}\mathbf{1}\)</span> is orthogonal to <span class="arithmatex">\(\hat{X}\)</span>.</p>
<p>To understand why <span class="arithmatex">\(\hat{Y} \cdot \hat{X} = Y \cdot \hat{X}\)</span>, let's examine the orthogonality of <span class="arithmatex">\(\hat{Y} - Y = -\bar{y}\mathbf{1}\)</span> to <span class="arithmatex">\(\hat{X}\)</span>. Recall that <span class="arithmatex">\(\hat{X} = X - \bar{x}\mathbf{1}\)</span>, which means <span class="arithmatex">\(\hat{X}\)</span> is the vector <span class="arithmatex">\(X\)</span> with the mean <span class="arithmatex">\(\bar{x}\)</span> subtracted from each component. We need to show that <span class="arithmatex">\((-\bar{y}\mathbf{1}) \cdot \hat{X} = 0\)</span>. This is:</p>
<div class="arithmatex">\[(-\bar{y}\mathbf{1}) \cdot \hat{X} = -\bar{y}\mathbf{1} \cdot (X - \bar{x}\mathbf{1}) = -\bar{y}(\mathbf{1} \cdot X) + \bar{y}\bar{x}(\mathbf{1} \cdot \mathbf{1})\]</div>
<p>But <span class="arithmatex">\(\mathbf{1} \cdot X = \sum_{i=1}^n x_i = n\bar{x}\)</span> and <span class="arithmatex">\(\mathbf{1} \cdot \mathbf{1} = n\)</span>.</p>
<p>So: <span class="arithmatex">\(-\bar{y}(\mathbf{1} \cdot X) + \bar{y}\bar{x}(\mathbf{1} \cdot \mathbf{1}) = -\bar{y}(n\bar{x}) + \bar{y}\bar{x}(n) = -n\bar{x}\bar{y} + n\bar{x}\bar{y} = 0\)</span></p>
<div class="arithmatex">\[\hat{Y} \cdot \hat{X} = (Y + (\hat{Y} - Y)) \cdot \hat{X} = Y \cdot \hat{X} + (\hat{Y} - Y) \cdot \hat{X} = Y \cdot \hat{X} + 0 = Y \cdot \hat{X}\]</div>
<p>Putting this into the numerator of the final coefficient on the right side yields</p>
<div class="arithmatex">\[Y - (mX + b\mathbf{1}) = \hat{Y} - \frac{\hat{Y} \cdot \hat{X}}{\hat{X} \cdot \hat{X}}\hat{X} = \hat{Y} - \text{Proj}_{\hat{X}}\hat{Y}\]</div>
<p>The vectors <span class="arithmatex">\(\hat{Y} - \text{Proj}_{\hat{X}}\hat{Y}\)</span> and <span class="arithmatex">\(\text{Proj}_{\hat{X}}\hat{Y}\)</span> are perpendicular to each other. Therefore, by the Pythagorean Theorem in <span class="arithmatex">\(\mathbb{R}^n\)</span>, we have</p>
<div class="arithmatex">\[\|\hat{Y}\|^2 = \|(\hat{Y} - \text{Proj}_{\hat{X}}\hat{Y}) + \text{Proj}_{\hat{X}}\hat{Y}\|^2 = \|\hat{Y} - \text{Proj}_{\hat{X}}\hat{Y}\|^2 + \|\text{Proj}_{\hat{X}}\hat{Y}\|^2\]</div>
<p>so <span class="arithmatex">\(\|\hat{Y} - \text{Proj}_{\hat{X}}\hat{Y}\|^2 = \|\hat{Y}\|^2 - \|\text{Proj}_{\hat{X}}\hat{Y}\|^2\)</span>. But the vector difference on the left side is exactly <span class="arithmatex">\(Y - (mX + b\mathbf{1})\)</span>, so</p>
<div class="arithmatex">\[\|Y - (mX + b\mathbf{1})\|^2 = \|\hat{Y}\|^2 - \|\text{Proj}_{\hat{X}}\hat{Y}\|^2\]</div>
<p>Finally, using the definition of <span class="arithmatex">\(\text{Proj}_{\hat{X}}\hat{Y}\)</span>, we have</p>
<div class="arithmatex">\[\|\text{Proj}_{\hat{X}}\hat{Y}\|^2 = \left(\frac{\hat{Y} \cdot \hat{X}}{\hat{X} \cdot \hat{X}}\hat{X}\right) \cdot \left(\frac{\hat{Y} \cdot \hat{X}}{\hat{X} \cdot \hat{X}}\hat{X}\right) = \left(\frac{\hat{Y} \cdot \hat{X}}{\hat{X} \cdot \hat{X}}\right)^2\hat{X} \cdot \hat{X} = \frac{(\hat{Y} \cdot \hat{X})^2}{\hat{X} \cdot \hat{X}} = r^2(\hat{Y} \cdot \hat{Y})\]</div>
<p>so plugging into <span class="arithmatex">\(\|Y - (mX + b\mathbf{1})\|^2 = \|\hat{Y}\|^2 - \|\text{Proj}_{\hat{X}}\hat{Y}\|^2\)</span> yields <span class="arithmatex">\(\|Y - (mX + b\mathbf{1})\|^2 = \|\hat{Y}\|^2(1 - r^2)\)</span>, which is exactly the desired identity.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2025 <a href="https://github.com/adi14041999"  target="_blank" rel="noopener">Aditya Prabhu</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>