{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my wiki!","text":""},{"location":"AI/deep_generative_models/autoregressive_models/","title":"Autoregressive models","text":"<p>We assume we are given access to a dataset: $$ \\mathcal{D} = { \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_m } $$ where each datapoint is n-dimensional. For simplicity, we assume the datapoints are binary. $$ x_i \\in {0,1}^n $$</p>"},{"location":"AI/deep_generative_models/autoregressive_models/#representation","title":"Representation","text":"<p>If you have n random variables: $$ X_1, X_2, \\dots, X_n $$ then their joint probability can be written as a product of conditional probabilities: $$ P(X_1, X_2, \\dots, X_n) = P(X_1) \\cdot P(X_2 \\mid X_1) \\cdot P(X_3 \\mid X_1, X_2) \\cdot \\dots \\cdot P(X_n \\mid X_1, X_2, \\dots, X_{n-1}) $$ In words:</p> <p>The probability of all n variables taking particular values equals: \u2192 the probability of the first variable, \u2192 times the probability of the second variable given the first, \u2192 times the probability of the third variable given the first two, \u2192 and so on, until the n-th variable.</p> <p>By this chain rule of probability, we can factorize the joint distribution over the n-dimensions as:</p> \\[ p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i \\mid x_1, x_2, \\dots, x_{i-1}) = \\prod_{i=1}^n p(x_i \\mid x_{&lt;i}) \\] <p>where</p> \\[ x_{&lt;i} = [x_1, x_2, \\dots, x_{i-1}] \\] <p>denotes the vector of random variables with index less than i.</p> <p>The chain rule factorization can be expressed graphically as a Bayesian network.</p> <p></p> <p>Such a Bayesian network that makes no conditional independence assumptions is said to obey the autoregressive property. The term autoregressive originates from the literature on time-series models where observations from the previous time-steps are used to predict the value at the current time step. Here, we fix an ordering of the variables x1, x2, \u2026, xn and the distribution for the i-th random variable depends on the values of all the preceding random variables in the chosen ordering x1, x2, \u2026, xi\u22121.</p> <p>If we allow for every conditional p(xi|x&lt;i) to be specified in a tabular form, then such a representation is fully general and can represent any possible distribution over n random variables. However, the space complexity for such a representation grows exponentially with n.</p> <p>To see why, let us consider the conditional for the last dimension, given by p(xn|x&lt;n). In order to fully specify this conditional, we need to specify a probability for 2^(n\u22121) configurations of the variables x1, x2, \u2026, xn\u22121. Since the probabilities should sum to 1, the total number of parameters for specifying this conditional is given by 2^(n\u22121)\u22121. Hence, a tabular representation for the conditionals is impractical for learning the joint distribution factorized via chain rule.</p> <p>In an autoregressive generative model, the conditionals are specified as parameterized functions with a fixed number of parameters. Specifically, we assume that each conditional distribution corresponds to a Bernoulli random variable. We then learn a function that maps the preceding random variables to the parameter (mean) of this Bernoulli distribution. Hence, we have:</p> \\[ p_{\\theta_i}(x_i \\mid x_{&lt;i}) = \\text{Bern} \\left( f_i(x_1, x_2, \\dots, x_{i-1}) \\right) \\] <p>where the function is defined as:</p> \\[ f_i : \\{0,1\\}^{i-1} \\to [0,1] \\] <p>and theta_i denotes the set of parameters used to specify this function. This function takes in a vector of size (i-1) where each element is a 0 or a 1, and outputs a scalar bit.</p> <p>The total number of parameters in an autoregressive generative model is given by:</p> \\[ \\sum_{i=1}^n \\left| \\theta_i \\right| \\]"},{"location":"AI/deep_generative_models/introduction/","title":"Introduction","text":"<p>Natural agents excel at discovering patterns, extracting knowledge, and performing complex reasoning based on the data they observe. How can we build artificial learning systems to do the same?</p> <p>Generative models view the world under the lens of probability. In such a worldview, we can think of any kind of observed data, say , as a finite set of samples from an underlying distribution, say  pdata. At its very core, the goal of any generative model is then to approximate this data distribution given access to the dataset . The hope is that if we are able to  learn  a good generative model, we can use the learned model for downstream  inference.</p>"},{"location":"AI/deep_generative_models/introduction/#learning","title":"Learning","text":"<p>We will be primarily interested in parametric approximations (parametric models assume a specific data distribution (like a normal distribution) and estimate parameters (like the mean and standard deviation) of that distribution, while non-parametric models make no assumptions about the underlying distribution) to the data distribution, which summarize all the information about the dataset  in a finite set of parameters. In contrast with non-parametric models, parametric models scale more efficiently with large datasets but are limited in the family of distributions they can represent.</p> <p>In the parametric setting, we can think of the task of learning a generative model as picking the parameters within a family of model distributions that minimizes some notion of distance between the model distribution and the data distribution.</p> <p> </p> <p>For instance, we might be given access to a dataset of dog images  and our goal is to learn the parameters of a generative model \u03b8 within a model family M such that the model distribution p\u03b8 is close to the data distribution over dogs  pdata. Mathematically, we can specify our goal as the following optimization problem:</p> <p>min d(pdata,p\u03b8) where \u03b8\u2208M</p> <p>where pdata is accessed via the dataset  and  d(\u22c5) is a notion of distance between probability distributions.</p> <p>It is interesting to take note of the difficulty of the problem at hand. A typical image from a modern phone camera has a resolution of approximately  700\u00d71400700\u00d71400 pixels. Each pixel has three channels: R(ed), G(reen) and B(lue) and each channel can take a value between 0 to 255. Hence, the number of possible images is given by 256700\u00d71400\u00d73\u224810800000256700\u00d71400\u00d73\u224810800000. In contrast, Imagenet, one of the largest publicly available datasets, consists of only about 15 million images. Hence, learning a generative model with such a limited dataset is a highly underdetermined problem.</p> <p>Fortunately, the real world is highly structured and automatically discovering the underlying structure is key to learning generative models. For example, we can hope to learn some basic artifacts about dogs even with just a few images: two eyes, two ears, fur etc. Instead of incorporating this prior knowledge explicitly, we will hope the model learns the underlying structure directly from data.  We will be primarily interested in the following questions:</p> <ul> <li>What is the representation for the model family M?</li> <li>What is the objective function  d(\u22c5)?</li> <li>What is the optimization procedure for minimizing  d(\u22c5)?</li> </ul>"},{"location":"AI/deep_generative_models/introduction/#inference","title":"Inference","text":"<p>Discriminative models, also referred to as  conditional models, are a class of models frequently used for  classification. They are typically used to solve  binary classification  problems, i.e. assign labels, such as pass/fail, win/lose, alive/dead or healthy/sick, to existing datapoints.</p> <p>Types of discriminative models include  logistic regression  (LR),  conditional random fields  (CRFs),  decision trees  among many others.  Generative model  approaches which uses a joint probability distribution instead, include  naive Bayes classifiers,  Gaussian mixture models,  variational autoencoders,  generative adversarial networks  and others.</p> <p>Unlike generative modelling, which studies the joint probability  P(x,y), discriminative modeling studies the P(y|x) or maps the given unobserved variable (target) x to a class label y dependent on the observed variables (training samples). For example, in object recognition, x is likely to be a vector of raw pixels (or features extracted from the raw pixels of the image). Within a probabilistic framework, this is done by modeling the conditional probability distribution  P(y|x), which can be used for predicting y from x.</p> <p>For a discriminative model such as logistic regression, the fundamental inference task is to predict a label for any given datapoint. Generative models, on the other hand, learn a joint distribution over the entire data. While the range of applications to which generative models have been used continue to grow, we can identify three fundamental inference queries for evaluating a generative model.:</p> <ol> <li> <p>Density estimation:  Given a datapoint  x, what is the probability assigned by the model, i.e.,  p\u03b8(x)?</p> </li> <li> <p>Sampling:  How can we  generate  novel data from the model distribution, i.e.,  xnew\u223cp\u03b8(x)?</p> </li> <li> <p>Unsupervised representation learning:  How can we learn meaningful feature representations for a datapoint  x?</p> </li> </ol> <p>Going back to our example of learning a generative model over dog images, we can intuitively expect a good generative model to work as follows. For density estimation, we expect  p\u03b8(x) to be high for dog images and low otherwise. Alluding to the name  generative model, sampling involves generating novel images of dogs beyond the ones we observe in our dataset. Finally, representation learning can help discover high-level structure in the data such as the breed of dogs.</p>"},{"location":"math/probability/probability_and_counting/","title":"Probability and counting","text":"<p>Mathematics is the logic of certainty; probability is the logic of uncertainty.</p>"},{"location":"math/probability/probability_and_counting/#sample-spaces","title":"Sample spaces","text":"<p>The mathematical framework for probability is built around sets. Imagine that an experiment is performed, resulting in one out of a set of possible outcomes. Before the experiment is performed, it is unknown which outcome will be the result; after, the result \"crystallizes\" into the actual outcome. The sample space S of an experiment is the set of all possible outcomes of the experiment. An event A is a subset of the sample space S, and we say that A occurred if the actual outcome is in A.  When the sample space is finite, we can visualize it as Pebble World (figure above). Each pebble represents an outcome, and an event is a set of pebbles. Performing the experiment amounts to randomly selecting one pebble. If all the pebbles are of the same mass, all the pebbles are equally likely to be chosen.</p> <p>Set theory is very useful in probability, since it provides a rich language for express_ing and working with events. Set operations, especially unions, intersections, and complements, make it easy to build new events in terms of already defined events.</p> <p>Example:  A coin is flipped 10 times. Writing Heads as H and Tails as T, a possible outcome is: HHHTHHTTHT. The sample space is the set of all possible strings of length 10 consisting of H's and T's. We can (and will) encode H as <code>1</code> and T as <code>0</code>, so that an outcome is a sequence: $$ (s_1, s_2, \\dots, s_{10}) \\quad \\text{with} \\quad s_j \\in {0, 1} $$ The sample space is the set of all such sequences.</p> <p>Some Events:</p> <ol> <li>Event A_1: the first flip is Heads. As a set: $$ A_1 =  (1, s_2, \\dots, s_{10}) \\; \\mid \\; s_j \\in {0,1}, \\; 2 \\leq j \\leq 10  $$ This is a subset of the sample space, so it is indeed an event. Saying that A_1 occurs is equivalent to saying that the first flip is Heads. Similarly, let A_j be the event that the j-th flip is Heads, for: $$ j = 2, 3, \\dots, 10 $$</li> <li>Event B: at least one flip was Heads. As a set: $$ B = \\bigcup_{j=1}^{10} A_j $$</li> <li>Event C: all the flips were Heads. As a set: $$ C = \\bigcap_{j=1}^{10} A_j $$</li> <li>Event D: there were at least two consecutive Heads. As a set: $$ D = \\bigcup_{j=1}^{9} \\left( A_j \\cap A_{j+1} \\right) $$ </li> </ol>"}]}