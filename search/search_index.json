{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my wiki!","text":""},{"location":"AI/deep_generative_models/autoregressive_models/","title":"Autoregressive models","text":"<p>We assume we are given access to a dataset: $$ \\mathcal{D} = { \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_m } $$ where each datapoint is n-dimensional. For simplicity, we assume the datapoints are binary. $$ x_i \\in {0,1}^n $$</p>"},{"location":"AI/deep_generative_models/autoregressive_models/#representation","title":"Representation","text":"<p>If you have n random variables: $$ X_1, X_2, \\dots, X_n $$ then their joint probability can be written as a product of conditional probabilities: $$ P(X_1, X_2, \\dots, X_n) = P(X_1) \\cdot P(X_2 \\mid X_1) \\cdot P(X_3 \\mid X_1, X_2) \\cdot \\dots \\cdot P(X_n \\mid X_1, X_2, \\dots, X_{n-1}) $$ In words:</p> <p>The probability of all n variables taking particular values equals: \u2192 the probability of the first variable, \u2192 times the probability of the second variable given the first, \u2192 times the probability of the third variable given the first two, \u2192 and so on, until the n-th variable.</p> <p>By this chain rule of probability, we can factorize the joint distribution over the n-dimensions as:</p> \\[ p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i \\mid x_1, x_2, \\dots, x_{i-1}) = \\prod_{i=1}^n p(x_i \\mid x_{&lt;i}) \\] <p>where</p> \\[ x_{&lt;i} = [x_1, x_2, \\dots, x_{i-1}] \\] <p>denotes the vector of random variables with index less than i.</p> <p>The chain rule factorization can be expressed graphically as a Bayesian network.</p> <p></p> <p>Such a Bayesian network that makes no conditional independence assumptions is said to obey the autoregressive property. The term autoregressive originates from the literature on time-series models where observations from the previous time-steps are used to predict the value at the current time step. Here, we fix an ordering of the variables x1, x2, \u2026, xn and the distribution for the i-th random variable depends on the values of all the preceding random variables in the chosen ordering x1, x2, \u2026, xi\u22121.</p> <p>If we allow for every conditional p(xi|x&lt;i) to be specified in a tabular form, then such a representation is fully general and can represent any possible distribution over n random variables. However, the space complexity for such a representation grows exponentially with n.</p> <p>To see why, let us consider the conditional for the last dimension, given by p(xn|x&lt;n). In order to fully specify this conditional, we need to specify a probability for 2^(n\u22121) configurations of the variables x1, x2, \u2026, xn\u22121. Since the probabilities should sum to 1, the total number of parameters for specifying this conditional is given by 2^(n\u22121)\u22121. Hence, a tabular representation for the conditionals is impractical for learning the joint distribution factorized via chain rule.</p> <p>In an autoregressive generative model, the conditionals are specified as parameterized functions with a fixed number of parameters. Specifically, we assume that each conditional distribution corresponds to a Bernoulli random variable. We then learn a function that maps the preceding random variables to the parameter (mean) of this Bernoulli distribution. Hence, we have:</p> \\[ p_{\\theta_i}(x_i \\mid x_{&lt;i}) = \\text{Bern} \\left( f_i(x_1, x_2, \\dots, x_{i-1}) \\right) \\] <p>where the function is defined as:</p> \\[ f_i : \\{0,1\\}^{i-1} \\to [0,1] \\] <p>and theta_i denotes the set of parameters used to specify this function. This function takes in a vector of size (i-1) where each element is a 0 or a 1, and outputs a scalar bit.</p> <p>The total number of parameters in an autoregressive generative model is given by:</p> \\[ \\sum_{i=1}^n \\left| \\theta_i \\right| \\] <p>In the simplest case, we can specify the function as a linear combination of the input elements followed by a sigmoid non-linearity (to restrict the output to lie between 0 and 1). This gives us the formulation of a fully-visible sigmoid belief network (FVSBN).</p> \\[ f_i(x_1, x_2, \\dots, x_{i-1}) = \\sigma(\\alpha^{(i)}_0 + \\alpha^{(i)}_1 x_1 + \\dots + \\alpha^{(i)}_{i-1} x_{i-1}) \\] <p>where \\(\\sigma\\) denotes the sigmoid function and \\(\\theta_i = \\{\\alpha^{(i)}_0, \\alpha^{(i)}_1, \\dots, \\alpha^{(i)}_{i-1}\\}\\) denote the parameters of the mean function. The conditional for variable \\(i\\) requires \\(i\\) parameters, and hence the total number of parameters in the model is given by \\(\\sum_{i=1}^n i = O(n^2)\\). Note that the number of parameters are much fewer than the exponential complexity of the tabular case.</p> <p>A natural way to increase the expressiveness of an autoregressive generative model is to use more flexible parameterizations for the mean function e.g., multi-layer perceptrons (MLP). For example, consider the case of a neural network with 1 hidden layer. The mean function for variable \\(i\\) can be expressed as</p> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{A}_i \\mathbf{x}_{&lt;i} + \\mathbf{c}_i) \\\\ f_i(x_1, x_2, \\dots, x_{i-1}) &amp;= \\sigma(\\boldsymbol{\\alpha}^{(i)} \\mathbf{h}_i + b_i) \\end{align} \\] <p>where \\(\\mathbf{h}_i \\in \\mathbb{R}^d\\) denotes the hidden layer activations for the MLP and \\(\\theta_i = \\{\\mathbf{A}_i \\in \\mathbb{R}^{d \\times (i-1)}, \\mathbf{c}_i \\in \\mathbb{R}^d, \\boldsymbol{\\alpha}^{(i)} \\in \\mathbb{R}^d, b_i \\in \\mathbb{R}\\}\\) are the set of parameters for the mean function \\(\\mu_i(\\cdot)\\). The total number of parameters in this model is dominated by the matrices \\(\\mathbf{A}_i\\) and given by \\(O(n^2d)\\).</p> <p>Note: The term \"mean function\" here refers to the function that determines the mean (expected value) of the Bernoulli distribution for each variable. Since we're modeling binary variables, the mean of the Bernoulli distribution is the probability of the variable being 1. The sigmoid function \\(\\sigma\\) ensures that this probability lies between 0 and 1.</p> <p>For a Bernoulli random variable \\(X\\) with parameter \\(p\\), the expectation (mean) is given by:</p> \\[ \\mathbb{E}[X] = 1 \\cdot p + 0 \\cdot (1-p) = p \\] <p>This is because: - \\(X\\) takes value 1 with probability \\(p\\) - \\(X\\) takes value 0 with probability \\((1-p)\\) - The expectation is the weighted sum of all possible values, where the weights are their respective probabilities</p> <p>Therefore, when we say the mean function determines the mean of the Bernoulli distribution, we're saying it determines the probability \\(p\\) of the variable being 1.</p> <p>The Neural Autoregressive Density Estimator (NADE) provides an alternate MLP-based parameterization that is more statistically and computationally efficient than the vanilla approach. In NADE, parameters are shared across the functions used for evaluating the conditionals. In particular, the hidden layer activations are specified as</p> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{W}_{.,&lt;i} \\mathbf{x}_{&lt;i} + \\mathbf{c}) \\\\ f_i(x_1, x_2, \\dots, x_{i-1}) &amp;= \\sigma(\\boldsymbol{\\alpha}^{(i)} \\mathbf{h}_i + b_i) \\end{align} \\] <p>where \\(\\theta = \\{\\mathbf{W} \\in \\mathbb{R}^{d \\times n}, \\mathbf{c} \\in \\mathbb{R}^d, \\{\\boldsymbol{\\alpha}^{(i)} \\in \\mathbb{R}^d\\}_{i=1}^n, \\{b_i \\in \\mathbb{R}\\}_{i=1}^n\\}\\) is the full set of parameters for the mean functions \\(f_1(\\cdot), f_2(\\cdot), \\dots, f_n(\\cdot)\\). The weight matrix \\(\\mathbf{W}\\) and the bias vector \\(\\mathbf{c}\\) are shared across the conditionals. Sharing parameters offers two benefits:</p> <ol> <li> <p>The total number of parameters gets reduced from \\(O(n^2d)\\) to \\(O(nd)\\).</p> </li> <li> <p>The hidden unit activations can be evaluated in \\(O(nd)\\) time via the following recursive strategy:</p> </li> </ol> \\[ \\begin{align} \\mathbf{h}_i &amp;= \\sigma(\\mathbf{a}_i) \\\\ \\mathbf{a}_{i+1} &amp;= \\mathbf{a}_i + \\mathbf{W}_{[.,i]} x_i \\end{align} \\] <p>with the base case given by \\(\\mathbf{a}_1 = \\mathbf{c}\\).</p> <p>The RNADE algorithm extends NADE to learn generative models over real-valued data. Here, the conditionals are modeled via a continuous distribution such as a equi-weighted mixture of \\(K\\) Gaussians. Instead of learning a mean function, we now learn the means \\(\\mu_{i,1}, \\mu_{i,2}, \\dots, \\mu_{i,K}\\) and variances \\(\\Sigma_{i,1}, \\Sigma_{i,2}, \\dots, \\Sigma_{i,K}\\) of the \\(K\\) Gaussians for every conditional. For statistical and computational efficiency, a single function \\(g_i: \\mathbb{R}^{i-1} \\to \\mathbb{R}^{2K}\\) outputs all the means and variances of the \\(K\\) Gaussians for the \\(i\\)-th conditional distribution.</p> <p>The conditional distribution \\(p_{\\theta_i}(x_i \\mid \\mathbf{x}_{&lt;i})\\) in RNADE is given by:</p> \\[ p_{\\theta_i}(x_i \\mid \\mathbf{x}_{&lt;i}) = \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(x_i; \\mu_{i,k}, \\Sigma_{i,k}) \\] <p>where \\(\\mathcal{N}(x; \\mu, \\Sigma)\\) denotes the probability density of a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\Sigma\\) evaluated at \\(x\\). The parameters \\(\\{\\mu_{i,k}, \\Sigma_{i,k}\\}_{k=1}^K\\) are the outputs of the function \\(g_i(\\mathbf{x}_{&lt;i})\\).</p> <p>This is how RNADE is autoregressive. Example sequence showing autoregressive dependencies:</p> <p>\\(x_1\\):    - Input to \\(g_1\\): \\(\\mathbf{x}_{&lt;1} = []\\) (empty)   - Output: \\(\\{\\mu_{1,k}, \\Sigma_{1,k}\\}_{k=1}^K\\) for \\(p(x_1)\\)</p> <p>\\(x_2\\):    - Input to \\(g_2\\): \\(\\mathbf{x}_{&lt;2} = [x_1]\\)   - Output: \\(\\{\\mu_{2,k}, \\Sigma_{2,k}\\}_{k=1}^K\\) for \\(p(x_2 \\mid x_1)\\)</p> <p>\\(x_3\\):    - Input to \\(g_3\\): \\(\\mathbf{x}_{&lt;3} = [x_1, x_2]\\)   - Output: \\(\\{\\mu_{3,k}, \\Sigma_{3,k}\\}_{k=1}^K\\) for \\(p(x_3 \\mid x_1, x_2)\\)</p> <p>\\(x_4\\):    - Input to \\(g_4\\): \\(\\mathbf{x}_{&lt;4} = [x_1, x_2, x_3]\\)   - Output: \\(\\{\\mu_{4,k}, \\Sigma_{4,k}\\}_{k=1}^K\\) for \\(p(x_4 \\mid x_1, x_2, x_3)\\)</p> <p>This sequential, conditional generation process is what makes RNADE an autoregressive model. The mixture of Gaussians is just the form of the conditional distribution, but the autoregressive property comes from how these distributions are parameterized based on previous variables.</p>"},{"location":"AI/deep_generative_models/autoregressive_models/#learning-and-inference","title":"Learning and inference","text":"<p>Recall that learning a generative model involves optimizing the closeness between the data and model distributions. One commonly used notion of closeness is the KL divergence between the data and the model distributions:</p> \\[ \\min_{\\theta \\in \\Theta} d_{KL}(p_{data}, p_{\\theta}) = \\min_{\\theta \\in \\Theta} \\mathbb{E}_{x \\sim p_{data}}[\\log p_{data}(x) - \\log p_{\\theta}(x)] \\] <p>where: - \\(p_{data}\\) is the true data distribution - \\(p_{\\theta}\\) is our model distribution parameterized by \\(\\theta\\) - \\(\\Theta\\) is the set of all possible parameter values - \\(d_{KL}\\) is the Kullback-Leibler divergence</p> <p>Let's break down how this minimization works:</p> <ol> <li>For a fixed value of \\(\\theta\\), we compute:</li> <li>The expectation over all possible data points \\(x\\) from \\(p_{data}\\)</li> <li>For each \\(x\\), we compute \\(\\log p_{data}(x) - \\log p_{\\theta}(x)\\)</li> <li> <p>This gives us a single scalar value for this particular \\(\\theta\\)</p> </li> <li> <p>The minimization operator \\(\\min_{\\theta \\in \\Theta}\\) then:</p> </li> <li>Tries different values of \\(\\theta\\) in the parameter space \\(\\Theta\\)</li> <li> <p>Finds the \\(\\theta\\) that gives the smallest expected value</p> </li> <li> <p>Since \\(p_{data}\\) is constant with respect to \\(\\theta\\), minimizing the KL divergence is equivalent to maximizing the expected log-likelihood of the data under our model:</p> </li> </ol> \\[ \\max_{\\theta \\in \\Theta} \\mathbb{E}_{x \\sim p_{data}}[\\log p_{\\theta}(x)] \\] <p>This is because \\(\\log p_{data}(x)\\) doesn't depend on \\(\\theta\\), so it can be treated as a constant. Minimizing \\(-\\log p_{\\theta}(x)\\) is the same as maximizing \\(\\log p_{\\theta}(x)\\)</p> <p>To approximate the expectation over the unknown \\(p_{data}\\), we make an assumption: points in the dataset \\(\\mathcal{D}\\) are sampled i.i.d. from \\(p_{data}\\). This allows us to obtain an unbiased Monte Carlo estimate of the objective as:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\log p_{\\theta}(x) = \\mathcal{L}(\\theta | \\mathcal{D}) \\] <p>The maximum likelihood estimation (MLE) objective has an intuitive interpretation: pick the model parameters \\(\\theta \\in \\Theta\\) that maximize the log-probability of the observed datapoints in \\(\\mathcal{D}\\).</p> <p>In practice, we optimize the MLE objective using mini-batch gradient ascent. The algorithm operates in iterations. At every iteration \\(t\\), we sample a mini-batch \\(\\mathcal{B}_t\\) of datapoints sampled randomly from the dataset (\\(|\\mathcal{B}_t| &lt; |\\mathcal{D}|\\)) and compute gradients of the objective evaluated for the mini-batch. These parameters at iteration \\(t+1\\) are then given via the following update rule:</p> \\[ \\theta^{(t+1)} = \\theta^{(t)} + r_t \\nabla_{\\theta} \\mathcal{L}(\\theta^{(t)} | \\mathcal{B}_t) \\] <p>where \\(\\theta^{(t+1)}\\) and \\(\\theta^{(t)}\\) are the parameters at iterations \\(t+1\\) and \\(t\\) respectively, and \\(r_t\\) is the learning rate at iteration \\(t\\). Typically, we only specify the initial learning rate \\(r_1\\) and update the rate based on a schedule.</p> <p>Now that we have a well-defined objective and optimization procedure, the only remaining task is to evaluate the objective in the context of an autoregressive generative model. To this end, we first write the MLE objective in terms of the joint probability:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\log p_{\\theta}(x) \\] <p>Then, we substitute the factorized joint distribution of an autoregressive model. Since \\(p_{\\theta}(x) = \\prod_{i=1}^n p_{\\theta_i}(x_i | x_{&lt;i})\\), we have:</p> \\[ \\log p_{\\theta}(x) = \\log \\prod_{i=1}^n p_{\\theta_i}(x_i | x_{&lt;i}) = \\sum_{i=1}^n \\log p_{\\theta_i}(x_i | x_{&lt;i}) \\] <p>Substituting this into the MLE objective, we get:</p> \\[ \\max_{\\theta \\in \\Theta} \\frac{1}{|\\mathcal{D}|} \\sum_{x \\in \\mathcal{D}} \\sum_{i=1}^n \\log p_{\\theta_i}(x_i | x_{&lt;i}) \\] <p>where \\(\\theta = \\{\\theta_1, \\theta_2, \\dots, \\theta_n\\}\\) now denotes the collective set of parameters for the conditionals.</p> <p>Inference in an autoregressive model is straightforward. For density estimation of an arbitrary point \\(x\\), we simply evaluate the log-conditionals \\(\\log p_{\\theta_i}(x_i | x_{&lt;i})\\) for each \\(i\\) and add these up to obtain the log-likelihood assigned by the model to \\(x\\). Since we have the complete vector \\(x = [x_1, x_2, \\dots, x_n]\\), we know all the values needed for each conditional \\(x_{&lt;i}\\), so each of the conditionals can be evaluated in parallel. Hence, density estimation is efficient on modern hardware.</p> <p>For example, given a 4-dimensional vector \\(x = [x_1, x_2, x_3, x_4]\\), we can compute all conditionals in parallel:</p> <ul> <li>\\(\\log p_{\\theta_1}(x_1)\\) (no conditioning needed)</li> <li>\\(\\log p_{\\theta_2}(x_2 | x_1)\\) (using known \\(x_1\\))</li> <li>\\(\\log p_{\\theta_3}(x_3 | x_1, x_2)\\) (using known \\(x_1, x_2\\))</li> <li>\\(\\log p_{\\theta_4}(x_4 | x_1, x_2, x_3)\\) (using known \\(x_1, x_2, x_3\\))</li> </ul> <p>Then sum them to get the total log-likelihood: \\(\\log p_{\\theta}(x) = \\sum_{i=1}^4 \\log p_{\\theta_i}(x_i | x_{&lt;i})\\)</p> <p>Sampling from an autoregressive model is a sequential procedure. Here, we first sample \\(x_1\\), then we sample \\(x_2\\) conditioned on the sampled \\(x_1\\), followed by \\(x_3\\) conditioned on both \\(x_1\\) and \\(x_2\\) and so on until we sample \\(x_n\\) conditioned on the previously sampled \\(x_{&lt;n}\\). For applications requiring real-time generation of high-dimensional data such as audio synthesis, the sequential sampling can be an expensive process.</p> <p>Finally, an autoregressive model does not directly learn unsupervised representations of the data. This is because:</p> <ol> <li>The model directly models the data distribution \\(p(x)\\) through a sequence of conditional distributions \\(p(x_i | x_{&lt;i})\\)</li> <li>There is no explicit latent space or bottleneck that forces the model to learn a compressed representation</li> <li>Each variable is modeled based on previous variables, but there's no mechanism to learn a global, compressed representation of the entire data point</li> <li>The model's parameters \\(\\theta_i\\) are specific to each conditional distribution and don't encode a meaningful representation of the data</li> </ol> <p>In contrast, latent variable models like variational autoencoders explicitly learn a compressed representation by: 1. Introducing a latent space \\(z\\) that captures the essential features of the data 2. Learning an encoder that maps data to this latent space 3. Learning a decoder that reconstructs data from the latent space 4. Using a bottleneck that forces the model to learn meaningful representations</p>"},{"location":"AI/deep_generative_models/energy_based_models/","title":"Energy-Based Models","text":""},{"location":"AI/deep_generative_models/energy_based_models/#parameterizing-probability-distributions","title":"Parameterizing Probability Distributions","text":"<p>Probability distributions \\(p(x)\\) are a key building block in generative modeling. Building a neural network that ensures \\(p(x) \\geq 0\\) is not hard. However, the real challenge lies in ensuring that the distribution satisfies the normalization constraint: for discrete variables, the sum over all possible values of \\(x\\) must equal 1, while for continuous variables, the integral over the entire domain must equal 1.</p> <p>Problem: \\(g_\\theta(x) \\geq 0\\) is easy. But \\(\\sum_x g_\\theta(x) = Z(\\theta) \\neq 1\\) in general, so \\(g_\\theta(x)\\) is not a valid probability mass function. For continuous variables, \\(\\int g_\\theta(x) dx = Z(\\theta) \\neq 1\\) in general, so \\(g_\\theta(x)\\) is not a valid probability density function.</p> <p>Solution:</p> \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} g_\\theta(x) = \\frac{1}{\\int g_\\theta(x) dx} g_\\theta(x) = \\frac{1}{\\text{Volume}(g_\\theta)} g_\\theta(x)\\] <p>Then by definition,</p> \\[\\int p_\\theta(x) dx = \\int \\frac{1}{Z(\\theta)} g_\\theta(x) dx = \\frac{Z(\\theta)}{Z(\\theta)} = 1\\] <p>Here, \\(g_\\theta(x)\\) is the output of the neural network with parameters \\(\\theta\\) at input \\(x\\). The volume of \\(g_\\theta\\), denoted as \\(\\text{Volume}(g_\\theta)\\), is defined as the integral of \\(g_\\theta(x)\\) over the entire domain: \\(\\text{Volume}(g_\\theta) = \\int g_\\theta(x) dx = Z(\\theta)\\). It is a normalizing constant (w.r.t. \\(x\\)) but changes for different \\(\\theta\\). For example, we choose \\(g_\\theta(x)\\) so that we know the volume analytically as a function of \\(\\theta\\).</p> <p>The partition function \\(Z(\\theta)\\) is the normalization constant that ensures a probability distribution integrates (or sums) to 1. It's called a \"partition function\" because it partitions the unnormalized function \\(g_\\theta(x)\\) into a proper probability distribution.</p> <p>Example: \\(g_{(\\mu, \\sigma)}(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)</p> \\[\\text{Volume}(g_{(\\mu, \\sigma)}) = \\int_{-\\infty}^{\\infty} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx = \\sqrt{2\\pi\\sigma^2}\\] <p>Therefore, the normalized probability density function is:</p> \\[p_{(\\mu, \\sigma)}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] <p>This is the standard normal (Gaussian) distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Functional forms \\(g_\\theta(x)\\) need to allow analytical integration. Despite being restrictive, they are very useful as building blocks for more complex distributions.</p> <p>Note: What we've been doing with autoregressive models, flow models, and VAEs are essentially tricks for composing simple functions that are normalized to build more complex probabilistic models that are by construction guaranteed to be normalized. These approaches avoid the intractability of computing the partition function for complex distributions by designing architectures where normalization is preserved through the composition of simple, analytically tractable components.</p>"},{"location":"AI/deep_generative_models/energy_based_models/#energy-based-models_1","title":"Energy Based Models","text":"<p>We are going to formalize EBMs the following way:</p> \\[p_\\theta(x) = \\frac{1}{\\int e^{f_\\theta(x)} dx} \\cdot e^{f_\\theta(x)}\\] \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} \\cdot e^{f_\\theta(x)}\\] <p>Why \\(e^{f_\\theta(x)}\\) and not \\(f_\\theta(x)^2\\)?</p> <p>Both \\(e^{f_\\theta(x)}\\) and \\(f_\\theta(x)^2\\) produce non-negative outputs, but we choose the exponential form for several important reasons:</p> <ol> <li> <p>Additive Energy: The exponential form allows us to work with additive energy functions. If we have \\(f_\\theta(x) = f_1(x) + f_2(x)\\), then \\(e^{f_\\theta(x)} = e^{f_1(x)} \\cdot e^{f_2(x)}\\), which is a natural way to combine energy terms.</p> </li> <li> <p>Log-Probability Interpretation: Taking the logarithm gives us \\(\\log p_\\theta(x) = f_\\theta(x) - \\log Z(\\theta)\\). This means \\(f_\\theta(x)\\) directly represents the unnormalized log-probability, making it easier to work with in practice.</p> </li> <li> <p>Gradient Properties: The exponential function has the property that \\(\\frac{d}{dx}e^{f(x)} = e^{f(x)} \\cdot f'(x)\\). This makes gradient-based learning more stable and interpretable.</p> </li> <li> <p>Numerical Stability: The exponential function grows more smoothly than quadratic functions, which can lead to better numerical stability during training.</p> </li> <li> <p>Dynamic Range: The exponential function can capture much larger variations in probability compared to quadratic functions. While \\(f_\\theta(x)^2\\) is bounded by the square of the function's range, \\(e^{f_\\theta(x)}\\) can represent probabilities that vary by many orders of magnitude.</p> </li> <li> <p>Statistical Mechanics Connection: The exponential form follows from the Boltzmann distribution in statistical mechanics, where \\(p(x) \\propto e^{-E(x)/kT}\\), where \\(-E(x)\\) is the energy of state \\(x\\). Hence the name.</p> </li> </ol> <p>Pros: Very flexible, can use any \\(f_\\theta(x)\\)</p> <p>Cons: \\(Z(\\theta)\\) is intractable, so no access to likelihood. Thus, evaluating and optimizing likelihood \\(p_\\theta(x)\\) is hard (learning is hard). Also, sampling from \\(p_\\theta(x)\\) is hard. Another con is there is no feature learning (but can add latent variables). EBMs also suffer from the curse of dimensionality - as the dimension of \\(x\\) increases, the volume of the space grows exponentially, making it increasingly difficult to learn meaningful energy functions and sample efficiently.</p> <p>Given two points \\(x_1\\) and \\(x_2\\), evaluating \\(p_\\theta(x_1)\\) or \\(p_\\theta(x_2)\\) requires calculating \\(Z(\\theta)\\). However, their ratio does not involve calculating \\(Z(\\theta)\\).</p> \\[\\frac{p_\\theta(x_1)}{p_\\theta(x_2)} = \\frac{\\frac{1}{Z(\\theta)} \\cdot e^{f_\\theta(x_1)}}{\\frac{1}{Z(\\theta)} \\cdot e^{f_\\theta(x_2)}} = \\frac{e^{f_\\theta(x_1)}}{e^{f_\\theta(x_2)}} = e^{f_\\theta(x_1) - f_\\theta(x_2)}\\] <p>The partition function \\(Z(\\theta)\\) cancels out in the ratio, so we only need to evaluate the energy function at the two points and take their difference. This means we can determine which of \\(x_1\\) or \\(x_2\\) is more likely under our model without needing to compute the intractable partition function.</p>"},{"location":"AI/deep_generative_models/energy_based_models/#training-ebms-with-contrastive-divergence","title":"Training EBMs with Contrastive Divergence","text":"<p>Let's assume we want to maximize \\(\\frac{\\exp(f_\\theta(x_{train}))}{Z(\\theta)}\\). \\(x_{train}\\) is the 'correct answer'- we want to increase the probability of this under the model. Let's also assume we have a 'wrong answer'. The objective is to not just maximize \\(\\exp(f_\\theta(x_{train}))\\) but also minimize \\(Z(\\theta)\\) because that's going to result in the 'wrong' answer being pushed down.</p> <p>Instead of evaluating \\(Z(\\theta)\\) exactly, we use a Monte Carlo estimate.</p>"},{"location":"AI/deep_generative_models/energy_based_models/#contrastive-divergence-algorithm","title":"Contrastive Divergence Algorithm","text":""},{"location":"AI/deep_generative_models/energy_based_models/#high-level-idea","title":"High-Level Idea","text":"<p>The contrastive divergence algorithm works as follows:</p> <p>Algorithm:</p> <ol> <li> <p>Assuming we can sample from the model, sample \\(x_{sample} \\sim p_\\theta\\)</p> </li> <li> <p>Take a step on the gradient: \\(\\nabla_\\theta(f_\\theta(x_{train}) - f_\\theta(x_{sample}))\\)</p> </li> <li> <p>Keep repeating this to make the training data more likely than typical samples from the model</p> </li> </ol>"},{"location":"AI/deep_generative_models/energy_based_models/#why-does-this-work","title":"Why Does This Work?","text":"<p>We want to maximize the log-likelihood: \\(\\max_\\theta(f_\\theta(x_{train}) - \\log Z(\\theta))\\)</p> <p>Mathematical Derivation:</p> <p>The gradient of the log-likelihood is:</p> \\[\\nabla_\\theta \\log p_\\theta(x_{train}) = \\nabla_\\theta(f_\\theta(x_{train}) - \\log Z(\\theta))\\] <p>Let's split the terms and take the derivative:</p> \\[\\nabla_\\theta \\log p_\\theta(x_{train}) = \\nabla_\\theta f_\\theta(x_{train}) - \\nabla_\\theta \\log Z(\\theta)\\] <p>Now we need to compute \\(\\nabla_\\theta \\log Z(\\theta)\\). Let's expand this:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\nabla_\\theta \\log \\int e^{f_\\theta(x)} dx\\] <p>Using the chain rule and the fact that \\(\\nabla \\log f(x) = \\frac{\\nabla f(x)}{f(x)}\\):</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\nabla_\\theta \\int e^{f_\\theta(x)} dx\\] <p>Since the integral and derivative can be exchanged:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\int \\nabla_\\theta e^{f_\\theta(x)} dx\\] <p>Using the chain rule again:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\int e^{f_\\theta(x)} \\nabla_\\theta f_\\theta(x) dx\\] <p>Notice that \\(\\frac{e^{f_\\theta(x)}}{Z(\\theta)} = p_\\theta(x)\\), so:</p> \\[\\nabla_\\theta \\log Z(\\theta) = \\int p_\\theta(x) \\nabla_\\theta f_\\theta(x) dx = \\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)]\\] <p>Final Result:</p> <p>Putting it all together:</p> \\[\\nabla_\\theta \\log p_\\theta(x_{train}) = \\nabla_\\theta f_\\theta(x_{train}) - \\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)]\\] <p>The Key Insight:</p> <p>The second term \\(\\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)]\\) is an expectation over the model distribution. We approximate (Monte Carlo approximation) this expectation using samples from the model:</p> \\[\\mathbb{E}_{x \\sim p_\\theta}[\\nabla_\\theta f_\\theta(x)] \\approx \\nabla_\\theta f_\\theta(x_{sample})\\] <p>where \\(x_{sample} \\sim p_\\theta\\) is a sample from our model.</p> <p>Important note on sampling:</p> <p>Unlike autoregressive models or normalizing flow models, Energy-Based Models do not provide a direct way to sample from \\(p_\\theta(x)\\). In autoregressive models, we can sample sequentially by conditioning on previous values. In flow models, we can sample from a simple base distribution and transform it through invertible functions. However, in EBMs, we need to use approximate sampling methods like:</p> <ul> <li>Langevin Dynamics: Gradient-based sampling with noise</li> <li>Gibbs Sampling: For discrete variables, updating one variable at a time</li> <li>Metropolis-Hastings: Markov chain Monte Carlo methods</li> <li>Hamiltonian Monte Carlo: More sophisticated MCMC methods</li> </ul> <p>This sampling challenge is one of the main difficulties in training EBMs, as we need to run these sampling procedures every time we want to estimate the gradient.</p>"},{"location":"AI/deep_generative_models/energy_based_models/#sampling-from-ebms-with-markov-monte-carlo-methods","title":"Sampling from EBMs with Markov Monte Carlo Methods","text":""},{"location":"AI/deep_generative_models/energy_based_models/#metropolis-hastings-algorithm","title":"Metropolis-Hastings Algorithm","text":"<p>Metropolis-Hastings (MH) is a general-purpose Markov Chain Monte Carlo (MCMC) method for sampling from complex probability distributions. It's particularly useful for Energy-Based Models where direct sampling is not possible.</p> <p>The Algorithm</p> <p>Step 1: Initialize Start with an initial sample \\(x^{(0)}\\) (could be random or from training data)</p> <p>Step 2: Propose a New Sample For each iteration \\(t\\):</p> <ul> <li> <p>Generate a proposal \\(x^*\\) from a proposal distribution \\(q(x^* | x^{(t)})\\)</p> </li> <li> <p>The proposal distribution should be easy to sample from (e.g., Gaussian centered at current point)</p> </li> </ul> <p>Step 3: Accept or Reject</p> <p>Compute the acceptance probability:</p> \\[\\alpha = \\min\\left(1, \\frac{e^{f_\\theta(x^*)} \\cdot q(x^{(t)} | x^*)}{e^{f_\\theta(x^{(t)})} \\cdot q(x^* | x^{(t)})}\\right)\\] <p>The <code>min(1, ...)</code> ensures the acceptance probability is between 0 and 1. When the ratio is &gt; 1, we always accept (probability = 1). When the ratio is \u2264 1, we accept with probability equal to the ratio.</p> <p>Step 4: Update With probability \\(\\alpha\\), accept the proposal: \\(x^{(t+1)} = x^*\\). With probability \\(1-\\alpha\\), reject and keep current: \\(x^{(t+1)} = x^{(t)}\\)</p> <p>Step 5: Repeat Continue for many iterations until convergence</p> <p>This algorithm provides a robust foundation for sampling from Energy-Based Models, though it may require careful tuning and monitoring for optimal performance.</p>"},{"location":"AI/deep_generative_models/energy_based_models/#unadjusted-langevin-mcmc","title":"Unadjusted Langevin MCMC","text":"<p>Unadjusted Langevin MCMC (ULMCMC) is another popular method for sampling from Energy-Based Models. Unlike Metropolis-Hastings, it doesn't use an accept/reject step, making it computationally more efficient.</p> <p>The Algorithm</p> <p>Step 1: Initialize Start with an initial sample \\(x^{(0)}\\) (could be random or from training data)</p> <p>Step 2: Langevin Dynamics Update For each iteration \\(t\\):</p> \\[x^{(t+1)} = x^{(t)} + \\epsilon \\nabla_x f_\\theta(x^{(t)}) + \\sqrt{2\\epsilon} \\eta_t\\] <p>where:</p> <ul> <li> <p>\\(\\epsilon\\) is the step size (learning rate)</p> </li> <li> <p>\\(\\nabla_x f_\\theta(x^{(t)})\\) is the gradient of the energy function</p> </li> <li> <p>\\(\\eta_t \\sim \\mathcal{N}(0, I)\\) is Gaussian noise</p> </li> </ul> <p>Step 3: Repeat Continue for many iterations until convergence</p> <p>Intuition</p> <p>The update rule can be understood as:</p> <ol> <li> <p>Gradient Ascent: \\(\\epsilon \\nabla_x f_\\theta(x^{(t)})\\) moves the sample toward higher energy regions</p> </li> <li> <p>Noise Injection: \\(\\sqrt{2\\epsilon} \\eta_t\\) adds randomness to prevent getting stuck in local optima</p> </li> <li> <p>Balance: The step size \\(\\epsilon\\) controls the trade-off between exploration and exploitation</p> </li> </ol> <p>High-Dimensional Expense</p> <p>In high dimensions, gradient computation becomes expensive, and the noise term \\(\\sqrt{2\\epsilon} \\eta_t\\) scales with dimension, making each step computationally costly. This computational burden is particularly problematic when training Energy-Based Models using Contrastive Divergence.</p> <p>The Training Bottleneck:</p> <p>Each training step in Contrastive Divergence requires sampling from the model distribution \\(p_\\theta(x)\\). This sampling process itself is computationally expensive:</p> <ol> <li>Single Sampling Step: Each Langevin step requires computing gradients and adding noise, both of which scale with dimension</li> <li>Multiple Sampling Steps: To get a good sample, we typically need hundreds or thousands of Langevin steps</li> <li>Per Training Step: Each gradient update of the model parameters requires multiple samples</li> </ol> <p>Computational Complexity:</p> <ul> <li>Gradient Computation: \\(O(d)\\) where \\(d\\) is the dimension</li> <li>Noise Generation: \\(O(d)\\) for generating \\(\\eta_t \\sim \\mathcal{N}(0, I)\\)</li> <li>Per Langevin Step: \\(O(d)\\) total cost</li> <li>Sampling Process: \\(O(k \\cdot d)\\) where \\(k\\) is the number of Langevin steps (typically 100-1000)</li> <li>Per Training Step: \\(O(n \\cdot k \\cdot d)\\) where \\(n\\) is the number of samples needed</li> </ul> <p>Practical Impact:</p> <p>This means that training an EBM using Contrastive Divergence with Langevin sampling can be extremely slow, especially for high-dimensional data like images. The sampling process becomes the computational bottleneck, making it difficult to scale EBMs to large datasets or high-dimensional problems.</p>"},{"location":"AI/deep_generative_models/evaluating_generative_models/","title":"Evaluating Generative Models","text":"<p>In any research field, evaluation drives progress. How do we evaluate generative models? The evaluation of discriminative models (classification, regression, etc.) is well understood because:</p> <p>Clear ground truth: For discriminative tasks, we have access to labeled data that serves as ground truth. We can directly compare the model's predictions with the true labels.</p> <p>Simple Metrics: Evaluation metrics are straightforward and interpretable:</p> <ul> <li> <p>Classification: Accuracy, precision, recall, F1-score, ROC-AUC</p> </li> <li> <p>Regression: Mean squared error (MSE), mean absolute error (MAE), R\u00b2</p> </li> <li> <p>Ranking: NDCG, MAP, MRR</p> </li> </ul> <p>Domain-Agnostic: These metrics work across different domains (computer vision, NLP, etc.) with minimal adaptation.</p> <p>Example: For a binary classifier, we can compute accuracy as \\(\\frac{\\text{correct predictions}}{\\text{total predictions}}\\) and immediately understand how well the model performs.</p> <p>Evaluating generative models is highly non-trivial.  Key question: What is the task you care about? Density estimation- do you care about evaluating probabilities of images? Compression? Pure sampling/generation? Representation learning from unlabelled data? More than one task?</p>"},{"location":"AI/deep_generative_models/evaluating_generative_models/#density-estimation-or-compression","title":"Density Estimation or Compression","text":"<p>Likelihood as a metric is pretty good for Density Estimation.</p> <ul> <li> <p>Split data into train, validation and test sets.</p> </li> <li> <p>Learn model \\(p_{\\theta}\\) using the train set.</p> </li> <li> <p>Tune hyperparameters on the validation set.</p> </li> <li> <p>Evaluate generalization with likelihood on test set: \\(\\mathbb{E}_{p_{data}}[\\log p_\\theta]\\)</p> </li> </ul> <p>Note: This is the same as compression because, by Shannon's source coding theorem, the optimal code length for encoding data from distribution \\(p_{data}\\) using model \\(p_\\theta\\) is \\(-\\log p_\\theta(x)\\). The average number of bits needed to encode data from \\(p_{data}\\) using model \\(p_\\theta\\) is:</p> \\[\\text{Average Code Length} = \\mathbb{E}_{p_{data}}[-\\log p_\\theta(x)] = -\\mathbb{E}_{p_{data}}[\\log p_\\theta(x)]\\] <p>Therefore, maximizing \\(\\mathbb{E}_{p_{data}}[\\log p_\\theta]\\) is equivalent to minimizing the expected code length, which is the goal of compression. The intuition is that we assign short codes to frequent data points.</p> <p>Perplexity: Another common metric for evaluating generative models is perplexity, defined as:</p> \\[\\text{Perplexity} = 2^{-\\frac{1}{D}\\mathbb{E}_{p_{data}}[\\log p_\\theta(x)]}\\] <p>where \\(D\\) is the dimension of the data. This normalizes the log-likelihood by the data dimension, making perplexity comparable across different dimensionalities.</p> <p>Perplexity measures how \"surprised\" the model is by the data. Lower perplexity indicates better performance. For language models, perplexity represents the average number of choices the model has at each step when predicting the next token.</p> <p>Not all generative models have tractable likelihoods. For models where exact likelihood computation is intractable, we need alternative evaluation approaches:</p> <p>VAEs: We can compare models using the Evidence Lower BOund (ELBO):</p> \\[\\text{ELBO} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p(z))\\] <p>While ELBO is a lower bound on the true likelihood, it provides a reasonable proxy for model comparison within the VAE framework.</p> <p>GANs: GANs pose a unique challenge because they don't provide explicit likelihood estimates.</p> <p>In general, unbiased estimation of probability density functions from samples is impossible.</p>"},{"location":"AI/deep_generative_models/evaluating_generative_models/#sample-quality","title":"Sample Quality","text":"<p>Human evaluations are the gold standard.</p> <p>HYPE_time: A metric that measures the minimum time it takes for a human to distinguish between real and generated samples. Higher HYPE_time indicates better sample quality, as it takes humans longer to detect that samples are fake.</p> <p>HYPE_infinity: The percentage of samples that deceive people under unlimited time. The larger the better.</p> <p>Key Insight: HYPE metrics provide a human-centric evaluation of generative models, measuring how convincingly the model can fool human evaluators. This is particularly relevant for applications where human perception is the ultimate judge of quality.</p> <p>Human evaluations are expensive, biased and hard to reproduce.</p>"},{"location":"AI/deep_generative_models/evaluating_generative_models/#inception-score","title":"Inception Score","text":"<p>The Inception Score measures the quality and diversity of generated samples using a pre-trained classifier (typically Inception-v3 for images). It is based on two key principles:</p> <ol> <li>Sharpness: Generated samples should be easily classifiable (high confidence predictions)</li> <li>Diversity: The model should generate samples from different classes</li> </ol>"},{"location":"AI/deep_generative_models/evaluating_generative_models/#frechet-inception-distance-fid","title":"Fr\u00e9chet Inception Distance (FID)","text":"<p>The Fr\u00e9chet Inception Distance (FID) measures similarities in the feature representations for datapoints sampled from \\(p_{\\theta}\\) and the test dataset.</p> <p>How FID is Computed:</p> <p>Feature Extraction: Use a pre-trained Inception network (typically Inception-v3) to extract features from both real and generated samples. Let \\(f_r(x)\\) and \\(f_g(x)\\) be the feature extractors for real and generated samples respectively.</p> <p>Distribution Modeling: Model the feature distributions as multivariate Gaussians.</p> <ul> <li> <p>For real data: \\(\\mathcal{N}(\\mu_r, \\Sigma_r)\\) where:</p> <ul> <li> <p>\\(\\mu_r = \\mathbb{E}_{x \\sim p_{data}}[f_r(x)]\\) (mean of real features)</p> </li> <li> <p>\\(\\Sigma_r = \\mathbb{E}_{x \\sim p_{data}}[(f_r(x) - \\mu_r)(f_r(x) - \\mu_r)^T]\\) (covariance of real features)</p> </li> </ul> </li> <li> <p>For generated data: \\(\\mathcal{N}(\\mu_g, \\Sigma_g)\\) where:</p> <ul> <li> <p>\\(\\mu_g = \\mathbb{E}_{x \\sim p_\\theta}[f_g(x)]\\) (mean of generated features)</p> </li> <li> <p>\\(\\Sigma_g = \\mathbb{E}_{x \\sim p_\\theta}[(f_g(x) - \\mu_g)(f_g(x) - \\mu_g)^T]\\) (covariance of generated features)</p> </li> </ul> </li> </ul> <p>Fr\u00e9chet Distance Calculation: Compute the Fr\u00e9chet distance between the two Gaussian distributions:</p> \\[\\text{FID} = \\|\\mu_r - \\mu_g\\|^2 + \\text{tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})\\] <p>where:</p> <ul> <li> <p>\\(\\|\\mu_r - \\mu_g\\|^2\\) is the squared Euclidean distance between means</p> </li> <li> <p>\\(\\text{tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})\\) is the trace of the covariance difference term</p> </li> <li> <p>The matrix square root \\((\\Sigma_r \\Sigma_g)^{1/2}\\) is computed using eigendecomposition</p> </li> </ul> <p>Note: Check this resource on evaluating Text-To-Image Models: HEIM</p>"},{"location":"AI/deep_generative_models/evaluating_generative_models/#evaluating-latent-representations-and-prompting","title":"Evaluating Latent Representations and Prompting","text":""},{"location":"AI/deep_generative_models/evaluating_generative_models/#clustering","title":"Clustering","text":"<p>Clustering is a powerful method for evaluating the quality and structure of latent representations learned by generative models. It provides insights into how well the model organizes and separates different concepts in its latent space. Clusters can be obtained by applying k-means or any other algorithm in the latent space of the generative model.</p>"},{"location":"AI/deep_generative_models/evaluating_generative_models/#lossy-compression-or-reconstruction","title":"Lossy Compression or Reconstruction","text":"<p>Latent representations can be evaluated based on the maximum compression they can achieve without significant loss in reconstruction accuracy. This involves measuring the trade-off between the dimensionality of the latent space and the quality of reconstructed samples. A good latent representation should maintain high reconstruction fidelity while using a compact, low-dimensional encoding that captures the essential features of the data. There are many quantitative evaluation metrics for this.</p>"},{"location":"AI/deep_generative_models/evaluating_generative_models/#disentanglement","title":"Disentanglement","text":"<p>Intuitively, we want representations that disentangle independent and interpretable attributes of the observed data. Disentanglement means that different dimensions of the latent space should correspond to distinct, meaningful factors of variation in the data. For example, in face generation, one latent dimension might control facial expression while another controls hair color, allowing for independent manipulation of these attributes. There are many quantitative evaluation metrics for this.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/","title":"Generative Adversarial Networks","text":""},{"location":"AI/deep_generative_models/generative_adversarial_networks/#introduction-gans-as-a-paradigm-shift","title":"Introduction: GANs as a Paradigm Shift","text":"<p>GANs are unique from all the other model families that we have seen so far, such as autoregressive models, VAEs, and normalizing flow models, because we do not train them using maximum likelihood.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#the-traditional-likelihood-based-paradigm","title":"The Traditional Likelihood-Based Paradigm","text":"<p>All the generative models we've explored so far follow a similar training paradigm:</p> <ol> <li>Autoregressive Models: Maximize \\(\\log p_\\theta(x) = \\sum_{i=1}^N \\log p_\\theta(x_i|x_{&lt;i})\\)</li> <li>Variational Autoencoders: Maximize the ELBO \\(\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z))\\)</li> <li>Normalizing Flow Models: Maximize \\(\\log p_\\theta(x) = \\log p_z(f^{-1}_\\theta(x)) + \\log |\\det(\\frac{\\partial f^{-1}_\\theta(x)}{\\partial x})|\\)</li> </ol> <p>Common Theme: All these models are trained by maximizing some form of likelihood or likelihood approximation.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#gans-a-different-approach","title":"GANs: A Different Approach","text":"<p>GANs break away from this paradigm entirely. Instead of maximizing likelihood, GANs use adversarial training - a fundamentally different approach to generative modeling. We'll get to what a Generator and a Discriminator are in a bit but here is a quick table showing how GAN is different.</p> <p>Key Differences:</p> Aspect Likelihood-Based Models GANs Training Objective Maximize likelihood/ELBO Minimax game between generator and discriminator Loss Function \\(\\mathcal{L} = -\\log p_\\theta(x)\\) \\(\\mathcal{L}_G = -\\log D(G(z))\\), \\(\\mathcal{L}_D = -\\log D(x) - \\log(1-D(G(z)))\\) Model Evaluation Direct likelihood computation No explicit likelihood computation Training Stability Generally stable Can be unstable, requires careful tuning Sample Quality May produce blurry samples Often produces sharp, realistic samples"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#likelihood-free-learning","title":"Likelihood-Free Learning","text":"<p>Why not use maximum likelihood? In fact, it is not so clear that better likelihood numbers necessarily correspond to higher sample quality. We know that the optimal generative model will give us the best sample quality and highest test log-likelihood. However, models with high test log-likelihoods can still yield poor samples, and vice versa.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#the-likelihood-vs-sample-quality-disconnect","title":"The Likelihood vs. Sample Quality Disconnect","text":"<p>To see why, consider pathological cases in which our model is comprised almost entirely of noise, or our model simply memorizes the training set:</p> <ol> <li> <p>Noise Model: A model that outputs pure noise might assign some probability to real data points, leading to a non-zero (though poor) likelihood, but produces completely useless samples.</p> </li> <li> <p>Memorization Model: A model that perfectly memorizes the training set will have very high likelihood on training data but will only reproduce exact training examples, lacking generalization and diversity.</p> </li> </ol> <p>Therefore, we turn to likelihood-free training with the hope that optimizing a different objective will allow us to disentangle our desiderata of obtaining high likelihoods as well as high-quality samples.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#the-two-sample-test-framework","title":"The Two-Sample Test Framework","text":"<p>Recall that maximum likelihood required us to evaluate the likelihood of the data under our model \\(p_\\theta\\). A natural way to set up a likelihood-free objective is to consider the two-sample test, a statistical test that determines whether or not a finite set of samples from two distributions are from the same distribution using only samples from \\(P\\) and \\(Q\\).</p> <p>Concretely, given \\(S_1 = \\{x \\sim P\\}\\) and \\(S_2 = \\{x \\sim Q\\}\\), we compute a test statistic \\(T\\) according to the difference in \\(S_1\\) and \\(S_2\\) that, when less than a threshold \\(\\alpha\\), accepts the null hypothesis that \\(P = Q\\).</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#application-to-generative-modeling","title":"Application to Generative Modeling","text":"<p>Analogously, we have in our generative modeling setup access to our training set \\(S_1 = \\{x \\sim p_{data}\\}\\) and \\(S_2 = \\{x \\sim p_\\theta\\}\\). The key idea is to train the model to minimize a two-sample test objective between \\(S_1\\) and \\(S_2\\).</p> <p>However, this objective becomes extremely difficult to work with in high dimensions, so we choose to optimize a surrogate objective that instead maximizes some distance between \\(S_1\\) and \\(S_2\\).</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#why-this-approach-makes-sense","title":"Why this approach makes sense","text":"<p>1. Avoiding pathological cases: - The two-sample test framework naturally avoids the noise and memorization problems - It forces the model to learn the true underlying distribution structure</p> <p>3. Flexibility: - We can choose different distance metrics or test statistics - This allows us to focus on different aspects of sample quality</p> <p>Key Insight: GANs implement likelihood-free learning by using a neural network (the discriminator) to learn an optimal test statistic for distinguishing between real and generated data, and then training the generator to minimize this learned distance.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#gan-objective","title":"GAN Objective","text":"<p>We thus arrive at the generative adversarial network formulation. There are two components in a GAN: (1) a generator and (2) a discriminator. The generator \\(G_\\theta\\) is a directed latent variable model that deterministically generates samples \\(x\\) from \\(z\\), and the discriminator \\(D_\\phi\\) is a function whose job is to distinguish samples from the real dataset and the generator.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#components","title":"Components","text":"<ul> <li>Generator \\(G_\\theta\\): A neural network that transforms noise \\(z \\sim p(z)\\) to samples \\(G_\\theta(z)\\)</li> <li>Discriminator \\(D_\\phi\\): A neural network that outputs a probability \\(D_\\phi(x) \\in [0,1]\\) indicating whether \\(x\\) is real or generated</li> </ul>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#the-minimax-game","title":"The Minimax Game","text":"<p>The generator and discriminator both play a two player minimax game, where: - Generator: Minimizes a two-sample test objective (\\(p_{data} = p_\\theta\\)) - Discriminator: Maximizes the objective (\\(p_{data} \\neq p_\\theta\\))</p> <p>Intuitively, the generator tries to fool the discriminator to the best of its ability by generating samples that look indistinguishable from \\(p_{data}\\).</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#formal-objective","title":"Formal Objective","text":"<p>The GAN objective can be written as:</p> \\[\\min_\\theta \\max_\\phi V(G_\\theta, D_\\phi) = \\mathbb{E}_{x \\sim p_{data}}[\\log D_\\phi(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1-D_\\phi(G_\\theta(z)))]\\]"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#understanding-the-objective","title":"Understanding the Objective","text":"<p>Let's unpack this expression:</p> <p>For the Discriminator (maximizing with respect to \\(\\phi\\)): - Given a fixed generator \\(G_\\theta\\), the discriminator performs binary classification - It tries to assign probability 1 to data points from the training set \\(x \\sim p_{data}\\) - It tries to assign probability 0 to generated samples \\(x \\sim p_G\\)</p> <p>For the Generator (minimizing with respect to \\(\\theta\\)): - Given a fixed discriminator \\(D_\\phi\\), the generator tries to maximize \\(D_\\phi(G_\\theta(z))\\) - This is equivalent to minimizing \\(\\log(1-D_\\phi(G_\\theta(z)))\\)</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#optimal-discriminator","title":"Optimal Discriminator","text":"<p>In this setup, the optimal discriminator is:</p> \\[D^*_G(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_G(x)}\\] <p>Derivation: The discriminator's objective is to maximize:</p> \\[\\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{x \\sim p_G}[\\log(1-D(x))]\\] <p>This is maximized when:</p> \\[D(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_G(x)}\\] <p>On the other hand, the generator minimizes this objective for a fixed discriminator \\(D_\\phi\\).</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#connection-to-jensen-shannon-divergence","title":"Connection to Jensen-Shannon Divergence","text":"<p>After performing some algebra, plugging in the optimal discriminator \\(D^*_G(\\cdot)\\) into the overall objective \\(V(G_\\theta, D^*_G(x))\\) gives us:</p> \\[2D_{JSD}[p_{data}, p_G] - \\log 4\\] <p>The \\(D_{JSD}\\) term is the Jensen-Shannon Divergence, which is also known as the symmetric form of the KL divergence:</p> \\[D_{JSD}[p,q] = \\frac{1}{2}\\left(D_{KL}\\left[p, \\frac{p+q}{2}\\right] + D_{KL}\\left[q, \\frac{p+q}{2}\\right]\\right)\\]"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#properties-of-jsd","title":"Properties of JSD","text":"<p>The JSD satisfies all properties of the KL divergence, and has the additional perk that \\(D_{JSD}[p,q] = D_{JSD}[q,p]\\) (symmetry).</p> <p>Key Properties:</p> <ol> <li> <p>Non-negative: \\(D_{JSD}[p,q] \\geq 0\\)</p> </li> <li> <p>Symmetric: \\(D_{JSD}[p,q] = D_{JSD}[q,p]\\)</p> </li> <li> <p>Zero iff equal: \\(D_{JSD}[p,q] = 0\\) if and only if \\(p = q\\)</p> </li> <li> <p>Bounded: \\(D_{JSD}[p,q] \\leq \\log 2\\)</p> </li> </ol>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#optimal-solution","title":"Optimal Solution","text":"<p>With this distance metric, the optimal generator for the GAN objective becomes \\(p_G = p_{data}\\), and the optimal objective value that we can achieve with optimal generators and discriminators \\(G^*(\\cdot)\\) and \\(D^*_{G^*}(x)\\) is \\(-\\log 4\\).</p> <p>Why \\(-\\log 4\\)? - When \\(p_G = p_{data}\\), we have \\(D_{JSD}[p_{data}, p_G] = 0\\) - Therefore, \\(V(G^*, D^*) = 2 \\cdot 0 - \\log 4 = -\\log 4\\)</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#gan-training-algorithm","title":"GAN Training Algorithm","text":"<p>Thus, the way in which we train a GAN is as follows:</p> <p>For epochs \\(1, \\ldots, N\\) do:</p> <ol> <li>Sample minibatch of size \\(m\\) from data: \\(x^{(1)}, \\ldots, x^{(m)} \\sim p_{data}\\)</li> <li>Sample minibatch of size \\(m\\) of noise: \\(z^{(1)}, \\ldots, z^{(m)} \\sim p_z\\)</li> <li>Take a gradient descent step on the generator parameters \\(\\theta\\):</li> </ol> \\[\\nabla_\\theta V(G_\\theta, D_\\phi) = \\frac{1}{m}\\nabla_\\theta \\sum_{i=1}^m \\log(1-D_\\phi(G_\\theta(z^{(i)})))\\] <ol> <li>Take a gradient ascent step on the discriminator parameters \\(\\phi\\):</li> </ol> \\[\\nabla_\\phi V(G_\\theta, D_\\phi) = \\frac{1}{m}\\nabla_\\phi \\sum_{i=1}^m [\\log D_\\phi(x^{(i)}) + \\log(1-D_\\phi(G_\\theta(z^{(i)})))]\\] <p>Key Points:</p> <ol> <li>Alternating Updates: We update the generator and discriminator in alternating steps</li> <li>Minibatch Training: We use minibatches of both real data and noise samples</li> <li>Generator Update: Minimizes the probability that the discriminator correctly identifies generated samples</li> <li>Discriminator Update: Maximizes the probability of correctly classifying real vs. generated samples</li> </ol> <p>Practical Considerations:</p> <ul> <li>Learning Rate Balance: The learning rates for generator and discriminator must be carefully balanced</li> <li>Update Frequency: Often the discriminator is updated multiple times per generator update</li> <li>Convergence Monitoring: Training progress is monitored through discriminator accuracy and sample quality</li> <li>Early Stopping: Training may be stopped when the discriminator can no longer distinguish real from fake</li> </ul> <p>This formulation shows that GANs are essentially implementing an adaptive two-sample test, where the discriminator learns the optimal way to distinguish between real and generated data, and the generator learns to minimize this learned distance.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#challenges","title":"Challenges","text":"<p>Although GANs have been successfully applied to several domains and tasks, working with them in practice is challenging because of their: (1) unstable optimization procedure, (2) potential for mode collapse, (3) difficulty in evaluation.</p> <p>1. Unstable Optimization Procedure</p> <p>During optimization, the generator and discriminator loss often continue to oscillate without converging to a clear stopping point. Due to the lack of a robust stopping criteria, it is difficult to know when exactly the GAN has finished training.</p> <p>Causes of Instability: - Minimax Nature: The adversarial game creates competing objectives - Gradient Issues: Vanishing/exploding gradients can occur - Learning Rate Sensitivity: Small changes in learning rates can cause divergence - Network Capacity Imbalance: If one network becomes too powerful, training collapses</p> <p>Symptoms: - Oscillating loss curves - No clear convergence pattern - Sudden collapse of training - Generator or discriminator loss going to zero/infinity</p> <p>2. Mode Collapse</p> <p>The generator of a GAN can often get stuck producing one of a few types of samples over and over again (mode collapse). This occurs when the generator finds a few \"safe\" modes that consistently fool the discriminator and stops exploring the full data distribution.</p> <p>What is Mode Collapse: - Definition: Generator only produces samples from a subset of the true distribution modes - Example: In image generation, only producing images of one type (e.g., only front-facing faces) - Problem: Lack of diversity in generated samples</p> <p>Causes: - Discriminator Overfitting: Discriminator becomes too good at detecting certain types of fake samples - Generator Optimization: Generator finds local optima that work well against current discriminator - Training Imbalance: One network becomes too powerful relative to the other</p> <p>3. Difficulty in Evaluation</p> <p>Unlike likelihood-based models, GANs don't provide explicit likelihood values, making evaluation challenging.</p> <p>Evaluation Challenges: - No Likelihood: Can't use traditional metrics like log-likelihood - Subjective Quality: Sample quality is often subjective and domain-specific - Diversity vs. Quality Trade-off: Hard to balance sample quality with diversity - Mode Coverage: Difficult to measure if all modes of the data distribution are captured</p> <p>Addressing the Challenges:</p> <p>Most fixes to these challenges are empirically driven, and there has been a significant amount of work put into developing new architectures, regularization schemes, and noise perturbations in an attempt to circumvent these issues.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#selected-gans","title":"Selected GANs","text":"<p>Next, we focus our attention to a few select types of GAN architectures and explore them in more detail.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#f-gan","title":"f-GAN","text":"<p>The f-GAN optimizes the variant of the two-sample test objective that we have discussed so far, but using a very general notion of distance: the f-divergence. Given two densities \\(p\\) and \\(q\\), the f-divergence can be written as:</p> \\[D_f(p,q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>where \\(f\\) is any convex, lower-semicontinuous function with \\(f(1) = 0\\). Several of the distance \"metrics\" that we have seen so far fall under the class of f-divergences, such as KL and Jensen-Shannon.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#understanding-the-requirements","title":"Understanding the Requirements","text":"<p>What is a convex function? A function \\(f\\) is convex if for any two points \\(x, y\\) and any \\(\\lambda \\in [0,1]\\), we have:</p> \\[f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\\] <p>This means that the line segment between any two points on the function lies above or on the function itself. </p> <p>Understanding the Line Segment Property:</p> <p>Let's break down what this means geometrically:</p> <p>Two Points: Consider any two points \\((x, f(x))\\) and \\((y, f(y))\\) on the graph of the function \\(f\\)</p> <p>Line Segment: The line segment connecting these points consists of all points of the form:</p> \\[(\\lambda x + (1-\\lambda)y, \\lambda f(x) + (1-\\lambda)f(y))\\] <p>where \\(\\lambda \\in [0,1]\\)</p> <p>Function Value: At the same \\(x\\)-coordinate \\(\\lambda x + (1-\\lambda)y\\), the function value is:</p> \\[f(\\lambda x + (1-\\lambda)y)\\] <p>Convexity Condition: The inequality \\(f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\\) means that the function value at any point on the line segment is less than or equal to the corresponding point on the straight line connecting \\((x, f(x))\\) and \\((y, f(y))\\)</p> <p>Visual Interpretation: If you draw a straight line between any two points on a convex function's graph. The entire function between those points must lie on or below that straight line.</p> <p>Convex functions have important properties:</p> <ul> <li> <p>Single minimum: If a minimum exists, it's global</p> </li> <li> <p>Well-behaved gradients: Useful for optimization</p> </li> <li> <p>Jensen's inequality: \\(\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X])\\) for convex \\(f\\)</p> </li> </ul> <p>What is a lower-semicontinuous function? A function \\(f\\) is lower-semicontinuous at a point \\(x_0\\) if:</p> \\[\\liminf_{x \\to x_0} f(x) \\geq f(x_0)\\] <p>Understanding \\(\\liminf\\) and the Infimum:</p> <p>The notation \\(\\liminf_{x \\to x_0} f(x)\\) involves two concepts:</p> <ol> <li> <p>Infimum (inf): The infimum of a set is the greatest lower bound. For a set \\(S\\), \\(\\inf S\\) is the largest number that is less than or equal to all elements in \\(S\\).</p> </li> <li> <p>Limit Inferior: \\(\\liminf_{x \\to x_0} f(x)\\) is the infimum of all limit points of \\(f(x)\\) as \\(x\\) approaches \\(x_0\\).</p> </li> </ol> <p>How \\(\\liminf\\) works:</p> <p>Consider all sequences \\(\\{x_n\\}\\) that converge to \\(x_0\\). For each sequence, we look at the limit of \\(f(x_n)\\) (if it exists). The \\(\\liminf\\) is the infimum of all these possible limit values.</p> <p>Mathematical Definition:</p> \\[\\liminf_{x \\to x_0} f(x) = \\inf \\left\\{ \\lim_{n \\to \\infty} f(x_n) : x_n \\to x_0 \\text{ and } \\lim_{n \\to \\infty} f(x_n) \\text{ exists} \\right\\}\\] <p>Why consider Sequences even for Continuous Functions?</p> <p>You might wonder: \"If \\(f\\) is continuous, why do we need sequences? Can't we just use the regular limit?\"</p> <p>Key Insight: Lower-semicontinuity is a weaker condition than continuity. A function can be lower-semicontinuous without being continuous.</p> <p>The Relationship:</p> <ol> <li>Continuous functions are always lower-semicontinuous</li> <li>Lower-semicontinuous functions may have discontinuities (but only \"jumps up\")</li> </ol> <p>Example of Lower-Semicontinuous but NOT Continuous: Consider \\(f(x) = \\begin{cases} 0 &amp; \\text{if } x &lt; 0 \\\\ 1 &amp; \\text{if } x \\geq 0 \\end{cases}\\). This function is lower-semicontinuous at \\(x = 0\\) (no \"jump down\"). But it's NOT continuous at \\(x = 0\\) (there's a \"jump up\")</p> <p>Example of NOT Lower-Semicontinuous: Consider \\(f(x) = \\begin{cases} 0 &amp; \\text{if } x &lt; 0 \\\\ 1 &amp; \\text{if } x = 0 \\\\ 0 &amp; \\text{if } x &gt; 0 \\end{cases}\\) at \\(x_0 = 0\\):</p> <p>\\(\\liminf_{x \\to 0^-} f(x) = 0\\) (approaching from left)</p> <p>\\(\\liminf_{x \\to 0^+} f(x) = 0\\) (approaching from right)</p> <p>\\(\\liminf_{x \\to 0} f(x) = 0\\) (overall limit inferior)</p> <p>Since \\(f(0) = 1\\) and \\(0 \\not\\geq 1\\), this function is NOT lower-semicontinuous at \\(x = 0\\)</p> <p>Why the Sequence definition is uiversal:</p> <p>The sequence-based definition works for ALL functions, whether they're: - Continuous everywhere - Lower-semicontinuous but not continuous - Neither continuous nor lower-semicontinuous</p> <p>For Continuous Functions: If \\(f\\) is continuous at \\(x_0\\), then:</p> \\[\\liminf_{x \\to x_0} f(x) = \\lim_{x \\to x_0} f(x) = f(x_0)\\] <p>So the sequence definition \"reduces\" to the regular limit, but it's still the same mathematical concept.</p> <p>Why this matters for f-Divergences: The f-divergence framework needs to work with functions that might not be continuous everywhere, so we need the more general sequence-based definition.</p> <p>Why do we need \\(f(1) = 0\\)? This requirement ensures that the f-divergence has the correct properties for a distance measure:</p> <p>Zero when distributions are equal: When \\(p = q\\), we have \\(\\frac{p(x)}{q(x)} = 1\\) everywhere, so:</p> \\[D_f(p,p) = \\mathbb{E}_{x \\sim p}[f(1)] = \\mathbb{E}_{x \\sim p}[0] = 0\\] <p>Distance-like behavior: This property ensures that the f-divergence behaves like a proper distance measure, being zero only when the distributions are identical</p> <p>Example: For KL divergence, \\(f(u) = u \\log u\\) satisfies \\(f(1) = 1 \\cdot \\log 1 = 0\\), making it a valid choice for an f-divergence.</p> <p>Important Clarification: KL Divergence and Convexity</p> <p>You might be wondering: \"But \\(\\log u\\) is concave, so how can KL divergence be an f-divergence?\" This is a great observation! The key is that the \\(f\\) function for KL divergence is \\(f(u) = u \\log u\\), not just \\(\\log u\\).</p> <p>The KL Divergence Formula: The KL divergence between distributions \\(p\\) and \\(q\\) is:</p> \\[D_{KL}(p||q) = \\mathbb{E}_{x \\sim p}\\left[\\log\\frac{p(x)}{q(x)}\\right] = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)}\\log\\frac{p(x)}{q(x)}\\right]\\] <p>Notice that the second form matches the f-divergence formula with \\(f(u) = u \\log u\\).</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#f-divergence-examples","title":"f-Divergence Examples","text":"<p>Common f-divergences:</p> <ol> <li>KL Divergence: \\(f(u) = u \\log u\\)</li> <li>Reverse KL: \\(f(u) = -\\log u\\)</li> <li>Jensen-Shannon: \\(f(u) = u \\log u - (u+1) \\log \\frac{u+1}{2}\\)</li> <li>Total Variation: \\(f(u) = \\frac{1}{2}|u-1|\\)</li> <li>Pearson \u03c7\u00b2: \\(f(u) = (u-1)^2\\)</li> </ol>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#setting-up-the-f-gan-objective","title":"Setting up the f-GAN Objective","text":"<p>To set up the f-GAN objective, we borrow two commonly used tools from convex optimization: the Fenchel conjugate and duality. Specifically, we obtain a lower bound to any f-divergence via its Fenchel conjugate:</p> \\[D_f(p,q) \\geq \\sup_{T \\in \\mathcal{T}} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>Where \\(f^*\\) is the Fenchel conjugate of \\(f\\):</p> \\[f^*(t) = \\sup_{u \\in \\text{dom}(f)} (tu - f(u))\\] <p>What is the Fenchel Conjugate?</p> <p>The Fenchel conjugate \\(f^*\\) of a function \\(f\\) is defined as:</p> \\[f^*(t) = \\sup_{u \\in \\text{dom}(f)} (tu - f(u))\\] <p>Economic Intuition for the Fenchel Conjugate:</p> <p>One of the most intuitive ways to understand the convex conjugate function \\(f^*(t)\\) is through an economic lens. Imagine \\(f(u)\\) as the cost function representing the total expense incurred to produce a quantity \\(u\\) of a certain product. The variable \\(y\\) corresponds to the market price per unit of that product.</p> <p>In this context, the product \\(xy\\) represents the revenue generated by selling \\(x\\) units at price \\(t\\). The term \\(f(u)\\), as mentioned, is the cost of producing those units. Therefore, the expression \\(tu - f(u)\\) represents the profit earned by producing and selling \\(u\\) units at price \\(t\\).</p> <p>The convex conjugate \\(f^*(t)\\) is defined as the supremum (or maximum) of this profit over all possible production quantities \\(u\\):</p> \\[f^*(t) = \\sup_u (tu - f(u))\\] <p>Thus, \\(f^*(t)\\) gives the optimal profit achievable at the market price \\(t\\), assuming the producer chooses the best production quantity \\(u\\) to maximize profit.</p> <p>Geometric Interpretation in Economics:</p> <p>Now, consider the graph of the cost function \\(f(u)\\). Assume \\(f\\) is convex, continuous, and differentiable, which is a reasonable assumption for many cost functions in economics.</p> <p>The slope of the cost curve at any point \\(u\\) is given by the derivative \\(f'(u)\\). This derivative represents the marginal cost \u2014 the additional cost to produce one more unit at quantity \\(u\\).</p> <p>The condition for optimal production quantity \\(u\\) at price \\(t\\) arises from maximizing profit:</p> \\[\\max_u \\{tu - f(u)\\}\\] <p>Taking the derivative with respect to \\(u\\) and setting it to zero for an optimum:</p> \\[t - f'(u) = 0 \\implies t = f'(u)\\] <p>This means the optimal production quantity \\(u\\) is found where the price \\(t\\) equals the marginal cost \\(f'(u)\\).</p> <p>Geometrically, this corresponds to finding a tangent line to the graph of \\(f(u)\\) that has slope \\(t\\). Using a ruler, you can \"slide\" the line around until it just touches the cost curve without crossing it. The point of tangency corresponds to the optimal \\(u\\).</p> <p>Importantly, the vertical intercept of this tangent line relates directly to the optimal profit. The tangent line can be expressed as:</p> \\[\\ell(u) = f(u_0) + f'(u_0)(u - u_0)\\] <p>At \\(u = 0\\), the intercept is:</p> \\[\\ell(0) = f(u_0) - u_0 f'(u_0)\\] <p>Notice that:</p> \\[-(u_0 t - f(u_0)) = f(u_0) - u_0 t\\] <p>Since \\(t = f'(u_0)\\), the intercept equals the negative of the optimal profit. Therefore, the intercept of the tangent line with slope \\(t\\) gives \\(-f^*(t)\\).</p> <p></p> <p>In the above diagram, for a given \\(y\\), we are trying to maximize the difference between the line \\(xy\\) and \\(f(x)\\). For that given \\(y\\), it turns out that the maximum value that \\(xy - f(x)\\) occurs when we draw a tangent to \\(f(x)\\) with slope \\(y\\). The point at which the tangent occurs is the optimum \\(x\\). It also turns out that the vertical intercept is \\(-f^*(y)\\) = maximum value that \\(xy - f(x)\\) occurs for the given \\(y\\). For a different \\(y\\), there will be a different \\(x\\) where a line parallel to the line \\(xy\\) becomes tangent. The graph of \\(f^*(y)\\) is essentially how the negative of the vertical intercept varies over the domain \\(y\\). The graphs of \\(f(x)\\) and \\(f^*(y)\\) live in different dual spaces. \\(f(x)\\) is a function of \"quantities\", while \\(f^*(y)\\) is a function of \"prices\" or \"slopes\". </p> <p>Key Properties: 1. Convexity: If \\(f\\) is convex, then \\(f^*\\) is also convex 2. Duality: \\((f^*)^* = f\\) (the conjugate of the conjugate is the original function) 3. Domain: The domain of \\(f^*\\) depends on the behavior of \\(f\\)</p> <p>Examples of Fenchel Conjugates:</p> <p>Example 1: KL Divergence</p> <p>For \\(f(u) = u \\log u\\):</p> \\[f^*(t) = \\sup_{u &gt; 0} (tu - u \\log u)\\] <p>To find this, we set the derivative to zero:</p> \\[\\frac{d}{du}(tu - u \\log u) = t - \\log u - 1 = 0\\] \\[\\log u = t - 1\\] \\[u = e^{t-1}\\] <p>Substituting back:</p> \\[f^*(t) = te^{t-1} - e^{t-1}(t-1) = e^{t-1}\\] <p>Example 2: Reverse KL</p> <p>For \\(f(u) = -\\log u\\):</p> \\[f^*(t) = \\sup_{u &gt; 0} (tu + \\log u)\\] <p>Setting derivative to zero:</p> \\[\\frac{d}{du}(tu + \\log u) = t + \\frac{1}{u} = 0\\] \\[u = -\\frac{1}{t}\\] <p>Substituting back:</p> \\[f^*(t) = t(-\\frac{1}{t}) + \\log(-\\frac{1}{t}) = -1 + \\log(-\\frac{1}{t}) = -1 - \\log(-t)\\] <p>Example 3: Total Variation</p> <p>For \\(f(u) = \\frac{1}{2}|u-1|\\):</p> \\[f^*(t) = \\sup_{u} (tu - \\frac{1}{2}|u-1|)\\] <p>This gives:</p> \\[f^*(t) = \\begin{cases} t &amp; \\text{if } |t| \\leq \\frac{1}{2} \\\\ +\\infty &amp; \\text{otherwise} \\end{cases}\\] <p>The Duality Principle:</p> <p>The Fenchel conjugate provides a way to transform optimization problems. The key insight is that:</p> <p>Primal Problem: \\(\\inf_{u} f(u)\\)</p> <p>Dual Problem: \\(\\sup_{t} -f^*(t)\\)</p> <p>The Primal-Dual Relationship:</p> <p>The f-divergence can be expressed in both forms:</p> \\[D_f(p,q) = \\mathbb{E}_{x \\sim q}\\left[f\\left(\\frac{p(x)}{q(x)}\\right)\\right] = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)]- \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>Where:</p> <ul> <li> <p>Primal form: \\(\\mathbb{E}_{x \\sim q}\\left[f\\left(\\frac{p(x)}{q(x)}\\right)\\right]\\) (direct computation)</p> </li> <li> <p>Dual form: \\(\\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\) (optimization problem)</p> </li> </ul> <p>Derivation: From Primal to Dual Form</p> <p>Let's walk through the step-by-step derivation of how we transform the primal form into the dual form:</p> <p>Step 1: Start with the Primal Form</p> <p>The f-divergence is defined as:</p> \\[D_f(p,q) = \\mathbb{E}_{x \\sim q}\\left[f\\left(\\frac{p(x)}{q(x)}\\right)\\right]\\] <p>This is the \"primal form\" - it directly computes the divergence by evaluating the function \\(f\\) at the ratio \\(\\frac{p(x)}{q(x)}\\).</p> <p>Step 2: Apply the Fenchel Conjugate Identity</p> <p>The key insight comes from the Fenchel conjugate identity. For any convex function \\(f\\) and any point \\(u\\), we have:</p> \\[f(u) = \\sup_{t} (tu - f^*(t))\\] <p>This is a fundamental result in convex analysis known as the Fenchel-Moreau theorem. It states that a convex function can be recovered from its conjugate.</p> <p>Step 3: Substitute the Identity</p> <p>We substitute this identity into our primal form:</p> \\[D_f(p,q) = \\mathbb{E}_{x \\sim q}\\left[\\sup_{t} \\left(t \\cdot \\frac{p(x)}{q(x)} - f^*(t)\\right)\\right]\\] <p>Step 4: Exchange Supremum and Expectation</p> <p>This is the crucial step. We can exchange the supremum and expectation under certain conditions (satisfied for convex \\(f\\)):</p> \\[D_f(p,q) = \\sup_{T} \\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)} - f^*(T(x))\\right]\\] <p>Here, we've replaced the variable \\(t\\) with a function \\(T(x)\\) that can depend on \\(x\\).</p> <p>Step 5: Simplify the Expression</p> <p>We can rewrite the expectation:</p> \\[\\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)} - f^*(T(x))\\right] = \\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)}\\right] - \\mathbb{E}_{x \\sim q}[f^*(T(x))]\\] <p>The first term can be simplified using the definition of expectation:</p> \\[\\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)}\\right] = \\int T(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x) dx = \\int T(x) \\cdot p(x) dx = \\mathbb{E}_{x \\sim p}[T(x)]\\] <p>Step 6: Arrive at the Dual Form</p> <p>Putting it all together:</p> \\[D_f(p,q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x \\sim q}[f^*(T(x))]\\right)\\] <p>In other words (to distinguish the two different inputs to the Discriminator):</p> \\[D_f(p,q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x_{fake} \\sim q}[f^*(T(x_{fake}))]\\right)\\] <p>This is the dual form of the f-divergence.</p> <p>Why This Derivation Works:</p> <ol> <li>Convexity: The convexity of \\(f\\) ensures that the Fenchel conjugate identity holds</li> <li>Exchange of Supremum and Expectation: This is valid because we're optimizing over a convex set of functions</li> <li>Duality Gap: Under certain conditions, there is no duality gap, meaning the primal and dual forms give the same value</li> </ol> <p>Key Insights from the Derivation:</p> <ol> <li> <p>From Direct Computation to Optimization: The primal form requires direct computation of \\(f(\\frac{p(x)}{q(x)})\\), while the dual form transforms this into an optimization problem over functions \\(T\\).</p> </li> <li> <p>Role of the Fenchel Conjugate: The conjugate \\(f^*\\) appears naturally in the dual form.</p> </li> <li> <p>Connection to GANs: The dual form is perfect for GANs because we can parameterize \\(T\\) as a neural network (the discriminator). The optimization becomes a minimax game. We can use gradient-based optimization.</p> </li> </ol> <p>Example: KL Divergence Derivation</p> <p>Let's see this in action for KL divergence where \\(f(u) = u \\log u\\):</p> <p>Primal Form:</p> \\[D_{KL}(p||q) = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)} \\log \\frac{p(x)}{q(x)}\\right]\\] <p>Step 1: Use the Fenchel conjugate identity for \\(f(u) = u \\log u\\)</p> <p>We know that \\(f^*(t) = e^{t-1}\\) (from our earlier examples)</p> <p>Step 2: Substitute:</p> \\[D_{KL}(p||q) = \\mathbb{E}_{x \\sim q}\\left[\\sup_{t} \\left(t \\cdot \\frac{p(x)}{q(x)} - e^{t-1}\\right)\\right]\\] <p>Step 3: Exchange supremum and expectation:</p> \\[D_{KL}(p||q) = \\sup_{T} \\mathbb{E}_{x \\sim q}\\left[T(x) \\cdot \\frac{p(x)}{q(x)} - e^{T(x)-1}\\right]\\] <p>Step 4: Simplify:</p> \\[D_{KL}(p||q) = \\sup_{T} \\left(\\mathbb{E}_{x \\sim p}[T(x)] - \\mathbb{E}_{x \\sim q}[e^{T(x)-1}]\\right)\\] <p>This gives us the dual form for KL divergence, which can be used in f-GAN training.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#steps-in-f-gan","title":"Steps in f-GAN","text":"<p>Step 1: Choose an f-divergence Select a convex function \\(f\\) with \\(f(1) = 0\\) (e.g., KL divergence, Jensen-Shannon, etc.)</p> <p>Step 2: Compute the Fenchel conjugate Find \\(f^*\\) analytically or numerically</p> <p>Step 3: Parameterize the dual variable Replace \\(T\\) with a neural network \\(T_\\phi\\) parameterized by \\(\\phi\\)</p> <p>Step 4: Set up the minimax game</p> \\[\\min_\\theta \\max_\\phi F(\\theta,\\phi) = \\mathbb{E}_{x \\sim p_{data}}[T_\\phi(x)] - \\mathbb{E}_{x_{fake} \\sim p_{G_\\theta}}[f^*(T_\\phi(x_{fake}))]\\] <p>Understanding the Roles:</p> <ul> <li>Generator (\\(G_\\theta\\)): Tries to minimize the divergence estimate</li> <li>Discriminator (\\(T_\\phi\\)): Tries to tighten the lower bound by maximizing the dual objective</li> </ul> <p>Key Insight: The discriminator \\(T_\\phi\\) is not a binary classifier like in standard GANs, but rather a function.</p> <p>Important Distinction: Vanilla GAN vs f-GAN Generator</p> <p>There is a fundamental difference between how generators work in vanilla GANs versus f-GANs:</p> <p>Vanilla GAN Generator:</p> <ul> <li> <p>Explicit generator network: \\(G_\\theta(z)\\) where \\(z \\sim p(z)\\) (noise)</p> </li> <li> <p>Direct transformation: Noise \\(z\\) \u2192 Generated sample \\(G_\\theta(z)\\)</p> </li> <li> <p>Objective: \\(\\min_\\theta \\max_\\phi V(G_\\theta, D_\\phi) = \\mathbb{E}_{x \\sim p_{data}}[\\log D_\\phi(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1-D_\\phi(G_\\theta(z)))]\\)</p> </li> </ul> <p>f-GAN Generator:</p> <ul> <li> <p>No explicit generator network: We work with \\(p_{G_\\theta}\\) as a distribution directly</p> </li> <li> <p>Implicit generator: The \"generator\" is whatever mechanism produces samples from \\(p_{G_\\theta}\\)</p> </li> <li> <p>Objective: \\(\\min_\\theta \\max_\\phi F(\\theta,\\phi) = \\mathbb{E}_{x \\sim p_{data}}[T_\\phi(x)] - \\mathbb{E}_{x_{fake} \\sim p_{G_\\theta}}[f^*(T_\\phi(x_{fake}))]\\)</p> </li> </ul> <p>The Key Insight:</p> <p>In f-GAN, the \"generator\" is implicit - it's whatever mechanism produces samples from the distribution \\(p_{G_\\theta}\\). This could be:</p> <ol> <li>A neural network \\(G_\\theta\\) that transforms noise (like in vanilla GANs)</li> <li>A flow-based model that transforms a base distribution</li> <li>A VAE decoder that generates from a latent space</li> <li>Any other generative model that produces samples from \\(p_{G_\\theta}\\)</li> </ol> <p>Why This Matters:</p> <p>The f-GAN framework is more general than vanilla GANs because:</p> <ul> <li>Vanilla GANs: Require a specific generator architecture \\(G_\\theta(z)\\)</li> <li>f-GANs: Can work with any generative model that produces samples from \\(p_{G_\\theta}\\)</li> </ul> <p>Practical Implementation:</p> <p>In practice, when implementing f-GAN, you would typically:</p> <ol> <li>Choose a generative model (e.g., a neural network \\(G_\\theta\\))</li> <li>Use it to generate samples from \\(p_{G_\\theta}\\)</li> <li>Apply the f-GAN objective to train both the generator and discriminator</li> </ol> <p>So in the f-GAN formulation, there's no explicit \\(G_\\theta\\) network like in vanilla GANs. The \"generator\" is the abstract distribution \\(p_{G_\\theta}\\), and the actual implementation depends on what generative model you choose to use.</p>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#advantages-of-f-gan","title":"Advantages of f-GAN","text":"<ol> <li>Unified Framework: One formulation covers many different GAN variants</li> <li>Theoretical Rigor: Based on well-established convex optimization theory</li> <li>Flexibility: Can adapt the divergence measure to the specific problem</li> <li>Stability: Some f-divergences may lead to more stable training</li> </ol>"},{"location":"AI/deep_generative_models/generative_adversarial_networks/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>Choice of f: Different f-divergences have different properties</li> <li>Fenchel Conjugate: Must be computable for the chosen f-divergence</li> <li>Training: Similar alternating optimization as standard GANs</li> <li>Evaluation: Same challenges as other GAN variants</li> </ul>"},{"location":"AI/deep_generative_models/introduction/","title":"Introduction","text":"<p>Natural agents excel at discovering patterns, extracting knowledge, and performing complex reasoning based on the data they observe. How can we build artificial learning systems to do the same?</p> <p>Generative models view the world under the lens of probability. In such a worldview, we can think of any kind of observed data, say , as a finite set of samples from an underlying distribution, say  pdata. At its very core, the goal of any generative model is then to approximate this data distribution given access to the dataset . The hope is that if we are able to  learn  a good generative model, we can use the learned model for downstream  inference.</p>"},{"location":"AI/deep_generative_models/introduction/#learning","title":"Learning","text":"<p>We will be primarily interested in parametric approximations (parametric models assume a specific data distribution (like a normal distribution) and estimate parameters (like the mean and standard deviation) of that distribution, while non-parametric models make no assumptions about the underlying distribution) to the data distribution, which summarize all the information about the dataset  in a finite set of parameters. In contrast with non-parametric models, parametric models scale more efficiently with large datasets but are limited in the family of distributions they can represent.</p> <p>In the parametric setting, we can think of the task of learning a generative model as picking the parameters within a family of model distributions that minimizes some notion of distance between the model distribution and the data distribution.</p> <p> </p> <p>For instance, we might be given access to a dataset of dog images  and our goal is to learn the parameters of a generative model \u03b8 within a model family M such that the model distribution p\u03b8 is close to the data distribution over dogs  pdata. Mathematically, we can specify our goal as the following optimization problem:</p> <p>min d(pdata,p\u03b8) where \u03b8\u2208M</p> <p>where pdata is accessed via the dataset  and  d(\u22c5) is a notion of distance between probability distributions.</p> <p>It is interesting to take note of the difficulty of the problem at hand. A typical image from a modern phone camera has a resolution of approximately  700\u00d71400700\u00d71400 pixels. Each pixel has three channels: R(ed), G(reen) and B(lue) and each channel can take a value between 0 to 255. Hence, the number of possible images is given by 256700\u00d71400\u00d73\u224810800000256700\u00d71400\u00d73\u224810800000. In contrast, Imagenet, one of the largest publicly available datasets, consists of only about 15 million images. Hence, learning a generative model with such a limited dataset is a highly underdetermined problem.</p> <p>Fortunately, the real world is highly structured and automatically discovering the underlying structure is key to learning generative models. For example, we can hope to learn some basic artifacts about dogs even with just a few images: two eyes, two ears, fur etc. Instead of incorporating this prior knowledge explicitly, we will hope the model learns the underlying structure directly from data.  We will be primarily interested in the following questions:</p> <ul> <li>What is the representation for the model family M?</li> <li>What is the objective function  d(\u22c5)?</li> <li>What is the optimization procedure for minimizing  d(\u22c5)?</li> </ul>"},{"location":"AI/deep_generative_models/introduction/#inference","title":"Inference","text":"<p>Discriminative models, also referred to as  conditional models, are a class of models frequently used for  classification. They are typically used to solve  binary classification  problems, i.e. assign labels, such as pass/fail, win/lose, alive/dead or healthy/sick, to existing datapoints.</p> <p>Types of discriminative models include  logistic regression  (LR),  conditional random fields  (CRFs),  decision trees  among many others.  Generative model  approaches which uses a joint probability distribution instead, include  naive Bayes classifiers,  Gaussian mixture models,  variational autoencoders,  generative adversarial networks  and others.</p> <p>Unlike generative modelling, which studies the joint probability  P(x,y), discriminative modeling studies the P(y|x) or maps the given unobserved variable (target) x to a class label y dependent on the observed variables (training samples). For example, in object recognition, x is likely to be a vector of raw pixels (or features extracted from the raw pixels of the image). Within a probabilistic framework, this is done by modeling the conditional probability distribution  P(y|x), which can be used for predicting y from x.</p> <p>For a discriminative model such as logistic regression, the fundamental inference task is to predict a label for any given datapoint. Generative models, on the other hand, learn a joint distribution over the entire data. While the range of applications to which generative models have been used continue to grow, we can identify three fundamental inference queries for evaluating a generative model.:</p> <ol> <li> <p>Density estimation:  Given a datapoint  x, what is the probability assigned by the model, i.e.,  p\u03b8(x)?</p> </li> <li> <p>Sampling:  How can we  generate  novel data from the model distribution, i.e.,  xnew\u223cp\u03b8(x)?</p> </li> <li> <p>Unsupervised representation learning:  How can we learn meaningful feature representations for a datapoint  x?</p> </li> </ol> <p>Going back to our example of learning a generative model over dog images, we can intuitively expect a good generative model to work as follows. For density estimation, we expect  p\u03b8(x) to be high for dog images and low otherwise. Alluding to the name  generative model, sampling involves generating novel images of dogs beyond the ones we observe in our dataset. Finally, representation learning can help discover high-level structure in the data such as the breed of dogs.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/","title":"Normalizing Flow Models","text":"<p>So far we have learned two types of likelihood based generative models:</p> <p>Autoregressive Models: \\(p_\\theta(x) = \\prod_{i=1}^N p_\\theta(x_i|x_{&lt;i})\\)</p> <p>Variational autoencoders: \\(p_\\theta(x) = \\int p_\\theta(x,z)dz\\)</p> <p>The two methods have relative strengths and weaknesses. Autoregressive models provide tractable likelihoods but no direct mechanism for learning features, whereas variational autoencoders can learn feature representations but have intractable marginal likelihoods.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#change-of-variables-formula","title":"Change of Variables Formula","text":"<p>In normalizing flows, we wish to map simple distributions (easy to sample and evaluate densities) to complex ones (learned via data). The change of variables formula describes how to evaluate densities of a random variable that is a deterministic transformation from another variable.</p> <p>Let's start with the univariate case and then generalize to multivariate random variables.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#univariate-case","title":"Univariate Case","text":"<p>Consider two random variables \\(Z\\) and \\(X\\) related by a strictly monotonic function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) such that \\(X = f(Z)\\). We want to find the probability density function of \\(X\\) in terms of the density of \\(Z\\).</p> <p>The key insight comes from the fact that probabilities must be preserved under the transformation. For any interval \\([a, b]\\) in the \\(X\\) space:</p> \\[P(a \\leq X \\leq b) = P(f^{-1}(a) \\leq Z \\leq f^{-1}(b))\\] <p>This can be written as:</p> \\[\\int_a^b p_X(x) dx = \\int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz\\] <p>To perform the substitution \\(z = f^{-1}(x)\\), we need to express \\(dz\\) in terms of \\(dx\\). Since \\(z = f^{-1}(x)\\), we can use the chain rule to find:</p> \\[\\frac{dz}{dx} = \\frac{d}{dx}f^{-1}(x) = \\frac{1}{f'(f^{-1}(x))}\\] <p>This follows from the inverse function theorem: if \\(y = f(x)\\), then \\(\\frac{dx}{dy} = \\frac{1}{f'(x)}\\).</p> <p>Therefore, \\(dz = \\frac{1}{f'(f^{-1}(x))} dx\\). However, we need to take the absolute value because probability densities must be non-negative. If \\(f'(f^{-1}(x)) &lt; 0\\) (meaning \\(f\\) is decreasing), then \\(\\frac{1}{f'(f^{-1}(x))} &lt; 0\\), which would make the density negative. Therefore, we use:</p> \\[dz = \\frac{1}{|f'(f^{-1}(x))|} dx\\] <p>Substituting this into our integral:</p> \\[\\int_a^b p_X(x) dx = \\int_{f^{-1}(a)}^{f^{-1}(b)} p_Z(z) dz = \\int_a^b p_Z(f^{-1}(x)) \\cdot \\frac{1}{|f'(f^{-1}(x))|} dx\\] <p>Since this equality must hold for all intervals \\([a, b]\\), the integrands must be equal:</p> \\[p_X(x) = p_Z(f^{-1}(x)) \\cdot \\frac{1}{|f'(f^{-1}(x))|}\\] <p>This is the univariate change of variables formula. The factor \\(\\frac{1}{|f'(f^{-1}(x))|}\\) accounts for how the transformation stretches or compresses the probability mass.</p> <p>Why should \\(f\\) be monotonic? The monotonicity requirement ensures that \\(f\\) is invertible (one-to-one), which is crucial for the change of variables formula to work correctly. If \\(f\\) were not monotonic, there could be multiple values of \\(z\\) that map to the same value of \\(x\\), making the inverse function \\(f^{-1}\\) ill-defined. This would violate the fundamental assumption that we can uniquely determine the original variable \\(z\\) from the transformed variable \\(x\\).</p> <p>For example, if \\(f(z) = z^2\\) (which is not monotonic on \\(\\mathbb{R}\\)), then both \\(z = 2\\) and \\(z = -2\\) map to \\(x = 4\\). This creates ambiguity in the inverse mapping and would require special handling to account for multiple pre-images.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#multivariate-case","title":"Multivariate Case","text":"<p>For the multivariate case, we have random variables \\(\\mathbf{Z}\\) and \\(\\mathbf{X}\\) related by a bijective function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(\\mathbf{X} = f(\\mathbf{Z})\\).</p> <p>The key insight is that the probability mass in any region must be preserved under the transformation. For any region \\(A\\) in the \\(\\mathbf{X}\\) space:</p> \\[P(\\mathbf{X} \\in A) = P(\\mathbf{Z} \\in f^{-1}(A))\\] <p>This can be written as:</p> \\[\\int_A p_X(\\mathbf{x}) d\\mathbf{x} = \\int_{f^{-1}(A)} p_Z(\\mathbf{z}) d\\mathbf{z}\\] <p>To perform the multivariate substitution \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\), we need to understand how the volume element \\(d\\mathbf{z}\\) transforms. The Jacobian matrix \\(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\) is an \\(n \\times n\\) matrix where:</p> \\[\\left[\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right]_{ij} = \\frac{\\partial f^{-1}_i(\\mathbf{x})}{\\partial x_j}\\] <p>This matrix describes how small changes in \\(\\mathbf{x}\\) correspond to changes in \\(\\mathbf{z}\\). In multivariate calculus, when we perform a change of variables \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\), the volume element transforms as:</p> \\[d\\mathbf{z} = \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right| d\\mathbf{x}\\] <p>This is the multivariate generalization of the univariate substitution \\(dz = \\frac{1}{|f'(f^{-1}(x))|} dx\\). The determinant of the Jacobian matrix measures how the transformation affects the volume of a small region: - If \\(|\\det(J)| &gt; 1\\), the transformation expands volume - If \\(|\\det(J)| &lt; 1\\), the transformation contracts volume - If \\(|\\det(J)| = 1\\), the transformation preserves volume</p> <p>Substituting this into our integral:</p> \\[\\int_A p_X(\\mathbf{x}) d\\mathbf{x} = \\int_{f^{-1}(A)} p_Z(\\mathbf{z}) d\\mathbf{z} = \\int_A p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right| d\\mathbf{x}\\] <p>Since this equality must hold for all regions \\(A\\), the integrands must be equal:</p> \\[p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>This is the multivariate change of variables formula. The determinant of the Jacobian matrix accounts for how the transformation affects the volume of probability mass.</p> <p>Alternative Form Using Forward Mapping: Using the property that \\(\\det(A^{-1}) = \\det(A)^{-1}\\) for any invertible matrix \\(A\\), we can rewrite this as:</p> \\[p_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right|^{-1}\\] <p>This form is often more convenient in practice because it uses the forward mapping \\(f\\) rather than the inverse mapping \\(f^{-1}\\).</p> <p>Final result: Let \\(Z\\) and \\(X\\) be random variables which are related by a mapping \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(X = f(Z)\\) and \\(Z = f^{-1}(X)\\). Then</p> \\[p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>There are several things to note here:</p> <ul> <li>\\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) need to be continuous and have the same dimension.</li> <li>\\(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\) is a matrix of dimension \\(n \\times n\\), where each entry at location \\((i,j)\\) is defined as \\(\\frac{\\partial f^{-1}(\\mathbf{x})_i}{\\partial x_j}\\). This matrix is also known as the Jacobian matrix.</li> <li>\\(\\det(A)\\) denotes the determinant of a square matrix \\(A\\).</li> </ul> <p>For any invertible matrix \\(A\\), \\(\\det(A^{-1}) = \\det(A)^{-1}\\), so for \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\) we have</p> \\[p_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right|^{-1}\\] <p>If \\(\\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right| = 1\\), then the mapping is volume preserving, which means that the transformed distribution \\(p_X\\) will have the same \"volume\" compared to the original one \\(p_Z\\).</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#normalizing-flow-models-deep-dive","title":"Normalizing Flow Models deep dive","text":"<p>Let us consider a directed, latent-variable model over observed variables \\(X\\) and latent variables \\(Z\\). In a normalizing flow model, the mapping between \\(Z\\) and \\(X\\), given by \\(f_\\theta: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\), is deterministic and invertible such that \\(X = f_\\theta(Z)\\) and \\(Z = f^{-1}_\\theta(X)\\).</p> <p>Using change of variables, the marginal likelihood \\(p(x)\\) is given by</p> \\[p_X(\\mathbf{x}; \\theta) = p_Z(f^{-1}_\\theta(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f^{-1}_\\theta(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>The name \"normalizing flow\" can be interpreted as the following:</p> <ul> <li> <p>\"Normalizing\" means that the change of variables gives a normalized density after applying an invertible transformation. When we transform a random variable through an invertible function, the resulting density automatically integrates to 1 (is normalized) because the change of variables formula preserves the total probability mass. This is different from other methods where we might need to explicitly normalize or approximate the density.</p> </li> <li> <p>\"Flow\" means that the invertible transformations can be composed with each other to create more complex invertible transformations. If we have two invertible functions \\(f_1\\) and \\(f_2\\), then their composition \\(f_2 \\circ f_1\\) is also invertible. This allows us to build complex transformations by chaining simpler ones, creating a \"flow\" of transformations.</p> </li> </ul> <p>Different from autoregressive models and variational autoencoders, deep normalizing flow models require specific architectural structures:</p> <ol> <li> <p>The input and output dimensions must be the same - This is necessary for the transformation to be invertible. If the dimensions don't match, we can't uniquely map back and forth between the spaces.</p> </li> <li> <p>The transformation must be invertible - This is fundamental to the change of variables formula and allows us to compute both the forward transformation (for sampling) and the inverse transformation (for density evaluation).</p> </li> <li> <p>Computing the determinant of the Jacobian needs to be efficient (and differentiable) - The change of variables formula requires computing the determinant of the Jacobian matrix. For high-dimensional spaces, this can be computationally expensive, so we need architectures that make this computation tractable.</p> </li> </ol> <p>Next, we introduce several popular forms of flow models that satisfy these properties.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#planar-flow","title":"Planar Flow","text":"<p>The Planar Flow introduces the following invertible transformation:</p> \\[\\mathbf{x} = f_\\theta(\\mathbf{z}) = \\mathbf{z} + \\mathbf{u}h(\\mathbf{w}^\\top\\mathbf{z} + b)\\] <p>where \\(\\mathbf{u}, \\mathbf{w}, b\\) are parameters.</p> <p>The absolute value of the determinant of the Jacobian is given by:</p> \\[\\left|\\det\\left(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right| = |1 + h'(\\mathbf{w}^\\top\\mathbf{z} + b)\\mathbf{u}^\\top\\mathbf{w}|\\] <p>However, \\(\\mathbf{u}, \\mathbf{w}, b, h(\\cdot)\\) need to be restricted in order to be invertible. For example, \\(h = \\tanh\\) and \\(h'(\\mathbf{w}^\\top\\mathbf{z} + b)\\mathbf{u}^\\top\\mathbf{w} \\geq -1\\). Note that while \\(f_\\theta(\\mathbf{z})\\) is invertible, computing \\(f^{-1}_\\theta(\\mathbf{z})\\) could be difficult analytically. The following models address this problem, where both \\(f_\\theta\\) and \\(f^{-1}_\\theta\\) have simple analytical forms.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#nice-and-realnvp","title":"NICE and RealNVP","text":"<p>The Nonlinear Independent Components Estimation (NICE) model and Real Non-Volume Preserving (RealNVP) model compose two kinds of invertible transformations: additive coupling layers and rescaling layers. The coupling layer in NICE partitions a variable \\(\\mathbf{z}\\) into two disjoint subsets, say \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\). Then it applies the following transformation:</p> <p>Forward mapping \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\):</p> <ul> <li>\\(\\mathbf{x}_1 = \\mathbf{z}_1\\), which is an identity mapping.</li> <li>\\(\\mathbf{x}_2 = \\mathbf{z}_2 + m_\\theta(\\mathbf{z}_1)\\), where \\(m_\\theta\\) is a neural network.</li> </ul> <p>Inverse mapping \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\):</p> <ul> <li>\\(\\mathbf{z}_1 = \\mathbf{x}_1\\), which is an identity mapping.</li> <li>\\(\\mathbf{z}_2 = \\mathbf{x}_2 - m_\\theta(\\mathbf{x}_1)\\), which is the inverse of the forward transformation.</li> </ul> <p>Therefore, the Jacobian of the forward mapping is lower triangular, whose determinant is simply the product of the elements on the diagonal, which is 1. Therefore, this defines a volume preserving transformation. RealNVP adds scaling factors to the transformation:</p> \\[\\mathbf{x}_2 = \\exp(s_\\theta(\\mathbf{z}_1)) \\odot \\mathbf{z}_2 + m_\\theta(\\mathbf{z}_1)\\] <p>where \\(\\odot\\) denotes elementwise product. This results in a non-volume preserving transformation.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#autoregressive-flow-models","title":"Autoregressive Flow Models","text":"<p>Some autoregressive models can also be interpreted as flow models. For a Gaussian autoregressive model, one receives some Gaussian noise for each dimension of \\(\\mathbf{x}\\), which can be treated as the latent variables \\(\\mathbf{z}\\). Such transformations are also invertible, meaning that given \\(\\mathbf{x}\\) and the model parameters, we can obtain \\(\\mathbf{z}\\) exactly.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#masked-autoregressive-flow-maf","title":"Masked Autoregressive Flow (MAF)","text":"<p>Masked Autoregressive Flow (MAF) uses this interpretation, where the forward mapping is an autoregressive model. However, sampling is sequential and slow, in \\(O(n)\\) time where \\(n\\) is the dimension of the samples.</p> <p>MAF Architecture and Mathematical Formulation:</p> <p>The MAF is comprised of Masked Autoencoder for Distribution Estimation (MADE) blocks, which has a special masking scheme at each layer such that the autoregressive property is preserved. In particular, we consider a Gaussian autoregressive model:</p> \\[p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i | \\mathbf{x}_{&lt;i})\\] <p>such that the conditional Gaussians \\(p(x_i | \\mathbf{x}_{&lt;i}) = \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\) are parameterized by neural networks \\(\\mu_i = f_{\\mu_i}(\\mathbf{x}_{&lt;i})\\) and \\(\\alpha_i = f_{\\alpha_i}(\\mathbf{x}_{&lt;i})\\). Note that \\(\\alpha_i\\) denotes the log standard deviation of the Gaussian \\(p(x_i | \\mathbf{x}_{&lt;i})\\).</p> <p>As seen in the change of variables formula, a normalizing flow uses a series of deterministic and invertible mappings \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) such that \\(\\mathbf{x} = f(\\mathbf{z})\\) and \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\) to transform a simple prior distribution \\(p_z\\) (e.g. isotropic Gaussian) into a more expressive one. In particular, a normalizing flow which composes \\(k\\) invertible transformations \\(\\{f_j\\}_{j=1}^k\\) such that \\(\\mathbf{x} = f_k \\circ f_{k-1} \\circ \\cdots \\circ f_1(\\mathbf{z}_0)\\) takes advantage of the change-of-variables property:</p> \\[\\log p(\\mathbf{x}) = \\log p_z(f^{-1}(\\mathbf{x})) + \\sum_{j=1}^k \\log \\left|\\det\\left(\\frac{\\partial f_j^{-1}(\\mathbf{x}_j)}{\\partial \\mathbf{x}_j}\\right)\\right|\\] <p>In MAF, the forward mapping is: \\(x_i = \\mu_i + z_i \\cdot \\exp(\\alpha_i)\\), and the inverse mapping is: \\(z_i = (x_i - \\mu_i)/\\exp(\\alpha_i)\\). The log of the absolute value of the determinant of the Jacobian is:</p> \\[\\log \\left|\\det\\left(\\frac{\\partial f^{-1}}{\\partial \\mathbf{x}}\\right)\\right| = -\\sum_{i=1}^n \\alpha_i\\] <p>where \\(\\mu_i\\) and \\(\\alpha_i\\) are as defined above.</p> <p>Connection between \\(p(\\mathbf{x})\\) and \\(\\log p(\\mathbf{x})\\) formulations:</p> <p>The two formulations are equivalent but serve different purposes:</p> <ol> <li>\\(p(\\mathbf{x})\\) formulation (autoregressive view):</li> </ol> \\[p(\\mathbf{x}) = \\prod_{i=1}^n p(x_i | \\mathbf{x}_{&lt;i}) = \\prod_{i=1}^n \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\] <ol> <li>\\(\\log p(\\mathbf{x})\\) formulation (flow view):</li> </ol> \\[\\log p(\\mathbf{x}) = \\log p_z(f^{-1}(\\mathbf{x})) + \\sum_{j=1}^k \\log \\left|\\det\\left(\\frac{\\partial f_j^{-1}(\\mathbf{x}_j)}{\\partial \\mathbf{x}_j}\\right)\\right|\\] <p>How they relate:</p> <p>Taking the logarithm of the autoregressive formulation:</p> \\[\\log p(\\mathbf{x}) = \\sum_{i=1}^n \\log p(x_i | \\mathbf{x}_{&lt;i}) = \\sum_{i=1}^n \\log \\mathcal{N}(x_i | \\mu_i, (\\exp(\\alpha_i))^2)\\] <p>For a Gaussian distribution \\(\\mathcal{N}(x | \\mu, \\sigma^2)\\), we have:</p> \\[\\log \\mathcal{N}(x | \\mu, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) - \\frac{(x-\\mu)^2}{2\\sigma^2}\\] <p>Substituting \\(\\sigma = \\exp(\\alpha_i)\\) and using the inverse mapping \\(z_i = (x_i - \\mu_i)/\\exp(\\alpha_i)\\):</p> \\[\\log p(\\mathbf{x}) = \\sum_{i=1}^n \\left[-\\frac{1}{2}\\log(2\\pi) - \\alpha_i - \\frac{z_i^2}{2}\\right] = \\sum_{i=1}^n \\log \\mathcal{N}(z_i | 0, 1) - \\sum_{i=1}^n \\alpha_i\\] <p>This shows that the autoregressive formulation (using conditional Gaussians) is equivalent to the flow formulation (using change of variables with a standard normal prior and the Jacobian determinant term \\(-\\sum_{i=1}^n \\alpha_i\\)).</p> <p>Key insight: The \\(\\alpha_i\\) terms serve dual purposes - they parameterize the conditional standard deviations in the autoregressive view, and they contribute to the Jacobian determinant in the flow view.</p> <p>What are \\(\\mu_1\\) and \\(\\alpha_1\\) in MAF?</p> <p>In MAF, for the first dimension (\\(i=1\\)):</p> <ul> <li> <p>\\(\\mu_1\\): This is the mean of the first conditional distribution \\(p(x_1)\\). Since \\(x_1\\) has no previous dimensions to condition on (\\(\\mathbf{x}_{&lt;1}\\) is empty), \\(\\mu_1\\) is typically a learned constant parameter or computed from a bias term in the neural network.</p> </li> <li> <p>\\(\\alpha_1\\): This is the log standard deviation of the first conditional distribution \\(p(x_1)\\). The actual standard deviation is \\(\\exp(\\alpha_1)\\), and \\(\\alpha_1\\) is also typically a learned constant parameter.</p> </li> </ul> <p>This makes sense because the first dimension has no autoregressive dependencies - it's the starting point of the autoregressive chain.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#made-blocks","title":"MADE Blocks","text":"<p>MADE (Masked Autoencoder for Distribution Estimation) is a key architectural component that enables efficient autoregressive modeling. MADE uses a special masking scheme to ensure that the autoregressive property is preserved while allowing for efficient parallel computation of all conditional parameters.</p> <p>How MADE Works:</p> <ol> <li> <p>Masking Scheme: Each layer in the neural network has a mask that ensures each output unit only depends on a subset of input units, maintaining the autoregressive ordering.</p> </li> <li> <p>Autoregressive Property: For dimension \\(i\\), the network can only access inputs \\(x_j\\) where \\(j &lt; i\\), ensuring that \\(p(x_i | \\mathbf{x}_{&lt;i})\\) only depends on previous dimensions.</p> </li> <li> <p>Parallel Parameter Computation: Despite the autoregressive constraints, MADE can compute all \\(\\mu_i\\) and \\(\\alpha_i\\) parameters in parallel during training, making it much more efficient than sequential autoregressive models.</p> </li> </ol> <p>Mathematical Implementation:</p> <p>The masking is implemented by multiplying the weight matrices with binary masks:</p> \\[W_{masked} = W \\odot M\\] <p>where \\(M\\) is a binary mask matrix that enforces the autoregressive dependencies. The mask ensures that: - Output \\(i\\) can only depend on inputs \\(j &lt; i\\) - This creates a lower triangular dependency structure</p> <p>Connection to MAF: MAF uses MADE blocks as its core building blocks, allowing it to efficiently compute all the conditional parameters \\(\\mu_i\\) and \\(\\alpha_i\\) while maintaining the autoregressive structure required for the flow transformation.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#detailed-maf-implementation-analysis","title":"Detailed MAF Implementation Analysis","text":"<p>Let's analyze a complete MAF implementation that demonstrates the concepts discussed above:</p> <p>Core Components:</p> <ol> <li>MaskedLinear: Implements the masking mechanism for autoregressive dependencies</li> <li>PermuteLayer: Reorders dimensions between flow layers</li> <li>MADE: Single MADE block with forward and inverse transformations</li> <li>MAF: Complete model with multiple MADE blocks</li> </ol> <p>1. MaskedLinear Layer:</p> <pre><code>class MaskedLinear(nn.Linear):\n    def __init__(self, input_size, output_size, mask):\n        super().__init__(input_size, output_size)\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, x):\n        return F.linear(x, self.mask * self.weight, self.bias)\n</code></pre> <p>Key Features: - Masking: The mask is a binary matrix that enforces autoregressive dependencies - Element-wise Multiplication: <code>self.mask * self.weight</code> zeros out forbidden connections - Autoregressive Property: Ensures output \\(i\\) only depends on inputs \\(j &lt; i\\)</p> <p>2. PermuteLayer:</p> <pre><code>class PermuteLayer(nn.Module):\n    def __init__(self, num_inputs):\n        super().__init__()\n        self.perm = np.array(np.arange(0, num_inputs)[::-1])\n\n    def forward(self, inputs):\n        return inputs[:, self.perm], torch.zeros(inputs.size(0), 1, device=inputs.device)\n\n    def inverse(self, inputs):\n        return inputs[:, self.perm], torch.zeros(inputs.size(0), 1, device=inputs.device)\n</code></pre> <p>Purpose: - Dimension Reordering: Reverses the order of dimensions between flow layers - Expressiveness: Allows different autoregressive orderings across layers - Jacobian: Since it's just a permutation, the Jacobian determinant is 1 (log_det = 0)</p> <p>3. MADE Block Implementation:</p> <p>Forward Method (z \u2192 x): <pre><code>def forward(self, z):\n    x = torch.zeros_like(z)\n    log_det = None\n    for i in range(self.input_size):\n        out = self.net(x)  # MADE network with masking\n        mean, alpha = out.chunk(2, dim=1)  # Split into mean and log_std\n        x[:, i] = mean[:, i] + z[:, i] * torch.exp(alpha[:, i])  # Transform\n        if log_det is None:\n            log_det = alpha[:, i].unsqueeze(1)\n        else:\n            log_det = torch.cat((log_det, alpha[:, i].unsqueeze(1)), dim=1)\n    log_det = -torch.sum(log_det, dim=1)  # Negative sum for change of variables\n    return x, log_det\n</code></pre></p> <p>Key Implementation Details: - Sequential Processing: Each dimension is processed one by one - Autoregressive Access: The MADE network can only access previously computed \\(x\\) values - Transformation: \\(x_i = \\mu_i + z_i \\cdot \\exp(\\alpha_i)\\) - Log Determinant: Accumulates \\(\\alpha_i\\) values and takes negative sum</p> <p>Inverse Method (x \u2192 z): <pre><code>def inverse(self, x):\n    out = self.net(x)  # MADE network with masking\n    mean, alpha = out.chunk(2, dim=1)  # Split into mean and log_std\n    z = (x - mean) * torch.exp(-alpha)  # Inverse transform\n    log_det = -torch.sum(alpha, dim=1)  # Negative sum for change of variables\n    return z, log_det\n</code></pre></p> <p>Key Implementation Details: - Parallel Processing: All dimensions can be processed simultaneously - Autoregressive Masking: The masking ensures proper dependencies - Inverse Transformation: \\(z_i = (x_i - \\mu_i) / \\exp(\\alpha_i)\\) - Log Determinant: Same formula as forward, but computed in parallel</p> <p>4. Complete MAF Model:</p> <p>Architecture: <pre><code>def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n    nf_blocks = []\n    for i in range(self.n_flows):\n        nf_blocks.append(MADE(self.input_size, hidden_size, n_hidden))\n        nf_blocks.append(PermuteLayer(self.input_size))\n    self.nf = nn.Sequential(*nf_blocks)\n</code></pre></p> <p>Structure: <pre><code>Input \u2192 MADE\u2081 \u2192 Permute\u2081 \u2192 MADE\u2082 \u2192 Permute\u2082 \u2192 ... \u2192 MADE\u2096 \u2192 Permute\u2096 \u2192 Output\n</code></pre></p> <p>Log Probability Computation: <pre><code>def log_probs(self, x):\n    log_det_list = []\n    for flow in self.nf:\n        x, log_det = flow.inverse(x)  # Transform x \u2192 z\n        log_det_list.append(log_det)\n\n    sum_log_det = torch.stack(log_det_list, dim=1).sum(dim=1)\n    z = x  # Final z after all transformations\n    p_z = self.base_dist.log_prob(z).sum(-1)  # Prior log probability\n    log_prob = (p_z + sum_log_det).mean()  # Change of variables formula\n    return log_prob\n</code></pre></p> <p>Sampling Process: <pre><code>def sample(self, device, n):\n    x_sample = torch.randn(n, self.input_size).to(device)  # Sample from prior\n    for flow in self.nf[::-1]:  # Reverse order for sampling\n        x_sample, log_det = flow.forward(x_sample)  # Transform z \u2192 x\n    return x_sample.cpu().data.numpy()\n</code></pre></p> <p>Understanding the Flow Methods:</p> <ol> <li> <p>During Training (likelihood computation): <pre><code># We have x, want to compute log p(x)\nfor flow in self.nf:  # Forward order\n    x, log_det = flow.inverse(x)  # x \u2192 z (inverse of this flow)\n</code></pre></p> </li> <li> <p>During Sampling: <pre><code># We have z, want to get x\nfor flow in self.nf[::-1]:  # Reverse order\n    x_sample, log_det = flow.forward(x_sample)  # z \u2192 x (forward of this flow)\n</code></pre></p> </li> </ol> <p>In other words:</p> <ul> <li>Each flow's <code>forward()</code> method: Transforms \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\) for that specific flow</li> <li>Each flow's <code>inverse()</code> method: Transforms \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\) for that specific flow</li> <li>During training: We use <code>inverse()</code> to go from data space to latent space</li> <li>During sampling: We use <code>forward()</code> to go from latent space to data space</li> </ul> <p>Mathematical Perspective: Let \\(f_i\\) denote the forward transformation of the \\(i\\)-th flow (from \\(\\mathbf{z}\\) to \\(\\mathbf{x}\\)), and \\(f_i^{-1}\\) denote its inverse transformation (from \\(\\mathbf{x}\\) to \\(\\mathbf{z}\\)).</p> <ul> <li>Training: \\(f_k^{-1} \\circ f_{k-1}^{-1} \\circ \\cdots \\circ f_1^{-1}(\\mathbf{x}) = \\mathbf{z}\\) (using <code>inverse()</code> methods)</li> <li>Sampling: \\(f_1 \\circ f_2 \\circ \\cdots \\circ f_k(\\mathbf{z}) = \\mathbf{x}\\) (using <code>forward()</code> methods)</li> </ul> <p>What is \\(k\\)?</p> <p>The parameter \\(k\\) represents the total number of flow layers in the MAF model. In the implementation, this corresponds to <code>n_flows</code> in the MAF constructor.</p> <p>In the MAF Architecture: <pre><code>def __init__(self, input_size, hidden_size, n_hidden, n_flows):\n    # n_flows = k (total number of flow layers)\n    for i in range(self.n_flows):  # i goes from 0 to k-1\n        nf_blocks.append(MADE(self.input_size, hidden_size, n_hidden))\n        nf_blocks.append(PermuteLayer(self.input_size))\n</code></pre></p> <p>Flow Composition Structure: <pre><code>Input \u2192 MADE\u2081 \u2192 Permute\u2081 \u2192 MADE\u2082 \u2192 Permute\u2082 \u2192 ... \u2192 MADE\u2096 \u2192 Permute\u2096 \u2192 Output\n</code></pre></p> <p>Where: - \\(f_1\\): First MADE block (MADE\u2081) - \\(f_2\\): Second MADE block (MADE\u2082) - ... - \\(f_k\\): Last MADE block (MADE\u2096)</p> <p>Example with \\(k = 3\\): - Training: \\(f_3^{-1} \\circ f_2^{-1} \\circ f_1^{-1}(\\mathbf{x}) = \\mathbf{z}\\) - Sampling: \\(f_1 \\circ f_2 \\circ f_3(\\mathbf{z}) = \\mathbf{x}\\)</p> <p>Key Insight: The <code>forward()</code> method of each flow is designed to be the inverse transformation for the overall model's training direction. This is why we use <code>forward()</code> during sampling in reverse order.</p> <p>This implementation demonstrates how the theoretical concepts of MAF translate into practical code, showing the interplay between autoregressive structure, masking, and flow transformations.</p>"},{"location":"AI/deep_generative_models/normalizing_flow_models/#inverse-autoregressive-flow-iaf","title":"Inverse Autoregressive Flow (IAF)","text":"<p>To address the sampling problem (sequential) in MAF, the Inverse Autoregressive Flow (IAF) simply inverts the generating process. In this case, the sampling (generation), is still parallelized. However, computing the likelihood of new data points is slow.</p> <p>Forward mapping from \\(\\mathbf{z} \\rightarrow \\mathbf{x}\\) (parallel):</p> <ol> <li> <p>Sample \\(z_i \\sim \\mathcal{N}(0,1)\\) for \\(i = 1, \\ldots, n\\)</p> </li> <li> <p>Compute all \\(\\mu_i, \\alpha_i\\) (can be done in parallel)</p> </li> <li> <p>Let \\(x_1 = \\exp(\\alpha_1)z_1 + \\mu_1\\)</p> </li> <li> <p>Let \\(x_2 = \\exp(\\alpha_2)z_2 + \\mu_2\\)</p> </li> <li> <p>\\(\\ldots\\)</p> </li> </ol> <p>Inverse mapping from \\(\\mathbf{x} \\rightarrow \\mathbf{z}\\) (sequential):</p> <ol> <li> <p>Let \\(z_1 = (x_1 - \\mu_1)/\\exp(\\alpha_1)\\)</p> </li> <li> <p>Compute \\(\\mu_2(z_1), \\alpha_2(z_1)\\)</p> </li> <li> <p>Let \\(z_2 = (x_2 - \\mu_2)/\\exp(\\alpha_2)\\)</p> </li> <li> <p>Compute \\(\\mu_3(z_1,z_2), \\alpha_3(z_1,z_2)\\)</p> </li> <li> <p>\\(\\ldots\\)</p> </li> </ol> <p>Key insight: Fast to sample from, slow to evaluate likelihoods of data points (train).</p> <p>Efficient Likelihood for Generated Points: However, for generated points the likelihood can be computed efficiently (since the noise are already obtained). When we generate samples using IAF, we start with known noise values \\(\\mathbf{z}\\) and transform them to get \\(\\mathbf{x}\\). Since we already have the noise values, we don't need to perform the expensive sequential inverse mapping to recover them. We can directly compute the likelihood using the change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) - \\sum_{i=1}^n \\alpha_i\\] <p>where we already know all the \\(\\alpha_i\\) values from the forward pass. This is much faster than the \\(O(n)\\) sequential computation required for arbitrary data points.</p> <p>Derivation of the Change of Variables Formula for IAF:</p> <p>Let's derive how we get this formula. Starting with the general change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) + \\log \\left|\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)\\right|\\] <p>For IAF, the forward transformation is:</p> \\[x_i = \\exp(\\alpha_i)z_i + \\mu_i\\] <p>The inverse transformation is:</p> \\[z_i = \\frac{x_i - \\mu_i}{\\exp(\\alpha_i)}\\] <p>The Jacobian matrix \\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\) is diagonal because each \\(z_i\\) only depends on \\(x_i\\):</p> \\[\\frac{\\partial z_i}{\\partial x_j} = \\begin{cases}  \\frac{1}{\\exp(\\alpha_i)} &amp; \\text{if } i = j \\\\ 0 &amp; \\text{if } i \\neq j \\end{cases}\\] <p>Therefore, the determinant is the product of the diagonal elements:</p> \\[\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right) = \\prod_{i=1}^n \\frac{1}{\\exp(\\alpha_i)} = \\exp\\left(-\\sum_{i=1}^n \\alpha_i\\right)\\] <p>Taking the absolute value and logarithm:</p> \\[\\log \\left|\\det\\left(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right)\\right| = \\log \\exp\\left(-\\sum_{i=1}^n \\alpha_i\\right) = -\\sum_{i=1}^n \\alpha_i\\] <p>Substituting back into the change of variables formula:</p> \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}) - \\sum_{i=1}^n \\alpha_i\\] <p>This derivation shows why the likelihood computation is efficient for generated samples - we already have all the \\(\\alpha_i\\) values from the forward pass, so we just need to sum them up.</p>"},{"location":"AI/deep_generative_models/recap_at_this_point/","title":"Recap at this point","text":""},{"location":"AI/deep_generative_models/recap_at_this_point/#the-generative-modeling-framework","title":"The Generative Modeling Framework","text":"<p>We have some data from an unknown probability distribution, that we denote \\(p_{data}\\). We have a model family \\(\\mathcal{M}\\) which is a set of probability distributions. We denote some kind of notion of similarity between \\(p_{data}\\) and the distributions in \\(\\mathcal{M}\\). We try to find the probability distribution \\(p_\\theta\\) that is the closest to \\(p_{data}\\) in this notion of similarity.</p>"},{"location":"AI/deep_generative_models/recap_at_this_point/#model-families-weve-explored","title":"Model Families We've Explored","text":"<p>We have seen different ways of constructing probability distributions in \\(\\mathcal{M}\\):</p> <ol> <li>Autoregressive Models: \\(p_\\theta(x) = \\prod_{i=1}^N p_\\theta(x_i|x_{&lt;i})\\)</li> <li>Variational Autoencoders (VAEs): \\(p_\\theta(x) = \\int p_\\theta(x,z)dz\\)</li> <li>Normalizing Flow Models: \\(p_\\theta(x) = p_z(f^{-1}_\\theta(x)) \\left|\\det\\left(\\frac{\\partial f^{-1}_\\theta(x)}{\\partial x}\\right)\\right|\\)</li> </ol>"},{"location":"AI/deep_generative_models/recap_at_this_point/#the-likelihood-based-training-paradigm","title":"The Likelihood-Based Training Paradigm","text":"<p>The key thing is we always try to assign some probability assigned by the model to a data point. All the above families are trained by minimizing KL divergence \\(D_{KL}(p_{data} || p_\\theta)\\), or equivalently maximizing likelihoods (or approximations). In these techniques, the machinery involves setting up models such that we can evaluate likelihoods (or approximations) pretty efficiently.</p> <p>Mathematical Foundation:</p> \\[\\arg\\min_\\theta D_{KL}(p_{data} || p_\\theta) = \\arg\\max_\\theta \\mathbb{E}_{x \\sim p_{data}}[\\log p_\\theta(x)]\\] <p>This equivalence shows that minimizing KL divergence is equivalent to maximizing the expected log-likelihood of the data.</p>"},{"location":"AI/deep_generative_models/recap_at_this_point/#alternative-similarity-measures","title":"Alternative Similarity Measures","text":"<p>However, the training objective of maximizing likelihoods is not the only way to measure similarity between \\(p_{data}\\) and \\(p_\\theta\\). There are other ways to measure the notion of similarity:</p> <ol> <li>Wasserstein Distance: Measures the minimum \"cost\" of transporting mass from one distribution to another</li> <li>Maximum Mean Discrepancy (MMD): Compares distributions using kernel methods</li> <li>Adversarial Training: Uses a discriminator to distinguish between real and generated samples</li> <li>Energy-Based Models: Learn an energy function that assigns low energy to real data and high energy to fake data</li> </ol>"},{"location":"AI/deep_generative_models/recap_at_this_point/#the-likelihood-vs-sample-quality-dilemma","title":"The Likelihood vs. Sample Quality Dilemma","text":"<p>The Problem: It is possible that models with high likelihood could be bad at sample generation and vice versa. This creates a fundamental tension in generative modeling.</p>"},{"location":"AI/deep_generative_models/recap_at_this_point/#why-this-disconnect-occurs","title":"Why This Disconnect Occurs","text":"<p>1. Likelihood Measures Average Performance:</p> \\[\\mathbb{E}_{x \\sim p_{data}}[\\log p_\\theta(x)] = \\int p_{data}(x) \\log p_\\theta(x) dx\\] <p>This measures how well the model assigns probability to the average data point, not necessarily how well it captures the fine-grained structure needed for high-quality generation.</p> <p>2. Sample Quality Requires Fine-Grained Structure: High-quality generation requires the model to capture: - Sharp boundaries between different modes - Fine details and textures - Proper spatial relationships - Realistic variations within modes</p> <p>3. Different Optimization Objectives: - Likelihood: Optimizes for probability assignment over the entire distribution - Sample Quality: Requires optimization of perceptual and structural properties</p>"},{"location":"AI/deep_generative_models/recap_at_this_point/#implications-for-model-design","title":"Implications for Model Design","text":"<p>The Training Objective Trade-off: Although training objective of maximizing likelihoods could be a good one, it might not be the best one if the objective is to generate the best samples.</p> <p>This suggests that it might be useful to disentangle likelihoods and sample quality.</p>"},{"location":"AI/deep_generative_models/recap_at_this_point/#alternative-training-paradigms","title":"Alternative Training Paradigms","text":"<p>1. Adversarial Training (GANs): - Objective: Direct optimization of sample quality through adversarial training - Advantage: Can generate very high-quality samples - Disadvantage: No explicit likelihood computation, training instability</p> <p>2. Hybrid Approaches: - VAE-GAN: Combines likelihood-based training with adversarial training - Flow-GAN: Combines normalizing flows with adversarial training - Objective: Balance between likelihood and sample quality</p> <p>3. Perceptual Losses: - Objective: Use pre-trained networks to measure perceptual similarity - Advantage: Better alignment with human perception - Example: LPIPS (Learned Perceptual Image Patch Similarity)</p> <p>4. Multi-Objective Training: - Objective: Combine multiple loss functions - Example: \\(\\mathcal{L} = \\mathcal{L}_{likelihood} + \\lambda \\mathcal{L}_{perceptual} + \\mu \\mathcal{L}_{adversarial}\\)</p>"},{"location":"AI/deep_generative_models/recap_at_this_point/#conclusion","title":"Conclusion","text":"<p>The tension between likelihood and sample quality is a fundamental challenge in generative modeling. While likelihood-based training provides a principled framework, it may not always lead to the best sample quality. This motivates the exploration of alternative training paradigms and evaluation metrics that better align with the ultimate goal of generating high-quality, diverse samples.</p> <p>The key insight is that generative modeling is not just about fitting a distribution to data, but about creating models that can generate samples that are both high-quality and diverse. This requires careful consideration of both the training objective and the evaluation metrics used to assess model performance.</p>"},{"location":"AI/deep_generative_models/score_based_diffusion_models/","title":"Score Based Diffusion Models","text":""},{"location":"AI/deep_generative_models/score_based_diffusion_models/#quick-recap-score-based-models","title":"Quick Recap: Score Based Models","text":"<p>From our exploration of score-based generative modeling, we learned several key concepts:</p> <p>Score Function: The gradient of the log probability density, \\(\\nabla_x \\log p(x)\\), which points \"uphill\" in the probability landscape toward high-density regions.</p> <p>Score Matching: A training objective that learns the score function by minimizing the Fisher divergence between the learned and true score functions.</p> <p>Score Matching Objective: The original score matching objective is:</p> \\[\\mathcal{L}(\\theta) = \\mathbb{E}_{x \\sim p_{data}(x)} \\left[ \\frac{1}{2} \\| s_\\theta(x) \\|_2^2 + \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] <p>where \\(\\text{tr}(\\nabla_x s_\\theta(x))\\) is the trace of the Jacobian of the score function, which is computationally expensive to evaluate.</p> <p>Denoising Score Matching (DSM): A practical variant that trains the score function to predict the direction from noisy to clean data, avoiding the need to compute the true score function.</p> <p>DSM Objective: The denoising score matching objective is:</p> \\[\\mathcal{L}(\\theta) = \\mathbb{E}_{y \\sim p_{data}(y)} \\mathbb{E}_{x \\sim \\mathcal{N}(x; y, \\sigma^2 I)} \\left[ \\frac{1}{2} \\left\\| s_\\theta(x) - \\frac{y - x}{\\sigma^2} \\right\\|_2^2 \\right]\\] <p>where \\(s_\\theta(x)\\) learns to predict the score function of the noise-perturbed distribution, and \\(\\frac{y - x}{\\sigma^2}\\) is the target score function that points from noisy sample \\(x\\) toward clean data \\(y\\).</p> <p>Langevin Dynamics: A continuous-time stochastic process that uses the score function to guide sampling:</p> \\[dx_t = \\nabla_x \\log p(x_t) dt + \\sqrt{2} dW_t\\] <p>Discretized Form: For practical implementation:</p> \\[x_{t+1} = x_t + \\frac{\\epsilon}{2} \\cdot s_\\theta(x_t) + \\sqrt{2\\epsilon} \\cdot \\eta_t\\] <p>Mode Collapse: Standard Langevin dynamics struggles with multi-modal distributions and low-density regions.</p> <p>Annealed Langevin Dynamics: Addresses this by using multiple noise scales \\(\\sigma_1 &lt; \\sigma_2 &lt; \\ldots &lt; \\sigma_L\\), creating a sequence of increasingly noisy distributions that are easier to sample from.</p> <p>Stochastic Differential Equations (SDEs): General framework for continuous-time stochastic processes:</p> \\[dx = f(x, t)dt + g(t)dw\\] <p>Reverse SDE: Any SDE has a corresponding reverse process for sampling:</p> \\[dx = [f(x, t) - g^2(t)\\nabla_x \\log p_t(x)]dt + g(t)d\\bar{w}\\] <p>Time-Dependent Score Models: Neural networks that learn \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\) for continuous-time processes.</p> <p>Key insights:</p> <ol> <li>Score functions act as denoisers: They point from noisy to clean data</li> <li>Multiple noise scales help: Annealing from high to low noise improves sampling</li> <li>Continuous-time generalizes discrete: SDEs provide a unified framework</li> <li>Reverse processes enable generation: The reverse SDE naturally incorporates the score function for sampling</li> </ol>"},{"location":"AI/deep_generative_models/score_based_diffusion_models/#diffusion-models-as-score-based-models-hierarchical-vaes","title":"Diffusion Models as Score Based Models &amp; Hierarchical VAEs","text":"<p>Iterative Denoising perspective: In annealed Langevin dynamics with multiple noise scales, the sampling process can be viewed as iterative denoising. Starting from high noise levels and gradually reducing noise, each step uses the score function to denoise the sample, progressively refining it from a noisy state toward the clean data distribution.</p> <p>Training perspective: The inverse process involves iteratively adding Gaussian noise to clean data during training. By corrupting data with increasing levels of noise, the model learns to predict the score function at each noise level, enabling it to reverse the corruption process during sampling.</p> <p></p> <p>VAE Perspective: This entire framework can be viewed as a VAE where:</p> <ul> <li> <p>Encoder process: The forward process that converts clean data to noise through iterative corruption</p> </li> <li> <p>Decoder process: The reverse process that generates samples by iteratively denoising from noise</p> </li> </ul> <p>Noise Perturbation process: Each \\(x_t\\) represents a noise-perturbed density that is obtained by adding Gaussian noise to \\(x_{t-1}\\). This creates a Markov chain where each step adds a small amount of noise to the previous state.</p> <p>We can write the forward process as a conditional distribution:</p> \\[q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)\\] <p>where \\(\\beta_t\\) is the noise schedule that determines how much noise is added at each step.</p> <p>The joint distribution of the entire forward process is:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\] <p>This factorization follows from the chain rule of probability and the Markov property of the forward process:</p> <p>Chain Rule: For any joint distribution, we can write:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = q(x_1 | x_0) \\cdot q(x_2 | x_0, x_1) \\cdot q(x_3 | x_0, x_1, x_2) \\cdots q(x_T | x_0, x_1, \\ldots, x_{T-1})\\] <p>Markov Property: In the forward process, each \\(x_t\\) depends only on \\(x_{t-1}\\), not on earlier states:</p> \\[q(x_t | x_0, x_1, \\ldots, x_{t-1}) = q(x_t | x_{t-1})\\] <p>Substituting the Markov property into the chain rule:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = q(x_1 | x_0) \\cdot q(x_2 | x_1) \\cdot q(x_3 | x_2) \\cdots q(x_T | x_{T-1})\\] <p>This can be written compactly as:</p> \\[q(x_1, x_2, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\] <p>This represents the probability of the entire noise corruption sequence, where each step depends only on the previous step (Markov property).</p> <p>Comparison with VAEs: In a typical VAE, you would take \\(x_0\\) and map it via a neural network to obtain some mean and standard deviation to parameterize the distribution of the latent variable. Here, we obtain the distribution of the latent variables through the predefined noise corruption procedure we defined above, rather than learning it with a neural network.</p> <p>Multistep transitions: A key advantage of this process is that we can compute transitions between any two time steps efficiently. For example, we can directly compute \\(q(x_t | x_0)\\) without going through all intermediate steps.</p> <p>Starting from \\(x_0\\), we can write:</p> \\[x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}\\] <p>where \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\epsilon_{t-1} \\sim \\mathcal{N}(0, I)\\).</p> <p>Recursively substituting:</p> \\[x_t = \\sqrt{\\alpha_t} (\\sqrt{\\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_{t-1}} \\epsilon_{t-2}) + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1}\\] <p>Continuing this recursion, we get:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>where \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\) and \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>Result: The multistep transition is:</p> \\[q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\] <p>This allows us to sample \\(x_t\\) directly from \\(x_0\\) in a single step, making training much more efficient.</p> <p>Diffusion analogy: We can think of this as a diffusion process. This is like a diffuser where given an initial state, we keep adding noise at every step. This is analogous to heat diffusion in a space- just as heat spreads out and becomes more uniform over time, our data distribution becomes increasingly noisy and uniform Gaussian as we add more noise at each step.</p> <p>The process gradually \"diffuses\" the structured information in the data into random noise, creating a smooth transition from the complex data distribution to a simple Gaussian noise distribution.</p> <p></p> <p>The ideal sampling process would be:</p> <ol> <li>Sample \\(x_T\\) from \\(\\pi(x_T)\\). Start with pure noise from the prior distribution</li> <li>Iteratively sample from the true denoising distribution \\(q(x_{t-1} | x_t)\\).</li> </ol> <p>This would generate samples by following the exact reverse of the forward diffusion process, gradually denoising from pure noise back to clean data.</p> <p>The challenge however, is that we don't know the true denoising distributions \\(q(x_{t-1} | x_t)\\). While the forward process \\(q(x_t | x_{t-1})\\) is predefined and tractable, the reverse process is not.</p> <p>However, we can learn an approximation \\(p_\\theta(x_{t-1} | x_t)\\) which is a Gaussian distribution with learned parameters:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>where \\(\\mu_\\theta(x_t, t)\\) is a neural network that learns the mean of the denoising distribution, and \\(\\sigma_t^2 I\\) is the fixed variance schedule.</p> <p>This is similar to a VAE decoder:</p> <p>VAE Decoder:</p> \\[p_\\theta(x | z) = \\mathcal{N}(x; \\mu_\\theta(z), \\sigma_\\theta^2(z) I)\\] <p>Diffusion reverse process:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>The diffusion decoder \\(p_\\theta(x_{t-1} | x_t)\\) is trying to learn to approximate the true denoising distributions \\(q(x_{t-1} | x_t)\\).</p> <p>The joint distribution of the learned reverse process is:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>Let's derive the joint distribution of the learned reverse process step by step.</p> <p>In the general case of \\(n\\) random variables \\(X_1, X_2, \\ldots, X_n\\), the values of an arbitrary subset of variables can be known and one can ask for the joint probability of all other variables. For example, if the values of \\(X_{k+1}, X_{k+2}, \\ldots, X_n\\) are known, the probability for \\(X_1, X_2, \\ldots, X_k\\) given these known values is:</p> \\[P(X_1, X_2, \\ldots, X_k|X_{k+1}, X_{k+2}, \\ldots, X_n) = \\frac{P(X_1, X_2, \\ldots, X_n)}{P(X_{k+1}, X_{k+2}, \\ldots, X_n)}\\] <p>This is the fundamental definition of conditional probability for multiple random variables.</p> <p>For any three events \\(A\\), \\(B\\), and \\(C\\), the joint conditional probability is defined as:</p> \\[P(A, B|C) = \\frac{P(A, B, C)}{P(C)}\\] <p>We can write the joint probability \\(P(A, B, C)\\) using the chain rule:</p> \\[P(A, B, C) = P(A|B, C) \\cdot P(B, C)\\] <p>Substituting this into our definition:</p> \\[P(A, B|C) = \\frac{P(A|B, C) \\cdot P(B, C)}{P(C)}\\] <p>We can write \\(P(B, C)\\) as:</p> \\[P(B, C) = P(B|C) \\cdot P(C)\\] \\[P(A, B|C) = \\frac{P(A|B, C) \\cdot P(B|C) \\cdot P(C)}{P(C)}\\] <p>The \\(P(C)\\) terms cancel out:</p> \\[P(A, B|C) = P(A|B, C) \\cdot P(B|C)\\] <p>The learned reverse process consists of a sequence of conditional distributions:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>where \\(\\mu_\\theta(x_t, t)\\) is a neural network that learns the mean of the denoising distribution.</p> <p>For the joint distribution \\(p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T)\\), we can apply the chain rule:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = p_\\theta(x_0 | x_1, \\ldots, x_T) \\cdot p_\\theta(x_1 | x_2, \\ldots, x_T) \\cdots p_\\theta(x_{T-1} | x_T)\\] <p>In the reverse process, we assume that each \\(x_{t-1}\\) depends only on \\(x_t\\), not on future states. This is the reverse Markov property:</p> \\[p_\\theta(x_{t-1} | x_t, x_{t+1}, \\ldots, x_T) = p_\\theta(x_{t-1} | x_t)\\] <p>Substituting the reverse Markov property into the chain rule:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = p_\\theta(x_0 | x_1) \\cdot p_\\theta(x_1 | x_2) \\cdots p_\\theta(x_{T-1} | x_T)\\] <p>This can be written compactly as:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>To get the complete joint distribution, we need to include the prior distribution over \\(x_T\\):</p> \\[p_\\theta(x_0, x_1, \\ldots, x_T) = p(x_T) \\cdot p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T)\\] \\[p_\\theta(x_0, x_1, \\ldots, x_T) = p(x_T) \\cdot \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>A crucial aspect of the diffusion process is choosing the values of \\(\\bar{\\alpha}_t\\) such that after many steps, we are left with pure noise. This ensures that the forward process converges to a simple, known distribution.</p> <p>Common choices for the noise schedule include:</p> <ol> <li>Linear Schedule: \\(\\beta_t = \\frac{t}{T} \\cdot \\beta_{\\text{max}}\\)</li> <li>Cosine Schedule: \\(\\beta_t = \\cos\\left(\\frac{t}{T} \\cdot \\frac{\\pi}{2}\\right)\\)</li> <li>Quadratic Schedule: \\(\\beta_t = \\left(\\frac{t}{T}\\right)^2 \\cdot \\beta_{\\text{max}}\\)</li> </ol> <p>Example: For a linear schedule with \\(\\beta_{\\text{max}} = 0.02\\) and \\(T = 1000\\), we get \\(\\beta_1 = 0.00002\\), \\(\\beta_{500} = 0.01\\) and \\(\\beta_{1000} = 0.02\\).</p> <p>Once we have trained the diffusion model and learned the reverse process \\(p_\\theta(x_{t-1} | x_t)\\), we can generate new samples by running the reverse process. Here's how sampling works. </p> <p>Sample \\(x_T\\) from the prior distribution \\(x_T \\sim \\mathcal{N}(x_T; 0, I)\\).</p> <p>For \\(t = T, T-1, \\ldots, 1\\), sample from the learned reverse process \\(x_{t-1} \\sim p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\).</p> <p>After \\(T\\) steps, we obtain \\(x_0\\), which is our generated sample.</p> <p>This entire diffusion framework can be viewed as a Variational Autoencoder (VAE) with a crucial difference: the encoder is fixed and predefined, while only the decoder is learned.</p> <p>Standard VAE Structure:</p> <ul> <li> <p>Encoder: \\(q_\\phi(z | x) = \\mathcal{N}(z; \\mu_\\phi(x), \\sigma_\\phi^2(x) I)\\)</p> </li> <li> <p>Decoder: \\(p_\\theta(x | z) = \\mathcal{N}(x; \\mu_\\theta(z), \\sigma_\\theta^2(z) I)\\)</p> </li> <li> <p>Prior: \\(p(z) = \\mathcal{N}(z; 0, I)\\)</p> </li> </ul> <p>Vanilla VAE ELBO (Non-KL form):</p> \\[ELBO_{\\text{VAE}} = \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right]\\] <p>Hierarchical VAE Structure (z\u2082 \u2192 z\u2081 \u2192 x):</p> <ul> <li> <p>Encoder: \\(q_\\phi(z_1, z_2 | x) = q_\\phi(z_1 | x) \\cdot q_\\phi(z_2 | z_1)\\)</p> </li> <li> <p>\\(q_\\phi(z_1 | x) = \\mathcal{N}(z_1; \\mu_\\phi(x), \\sigma_\\phi^2(x) I)\\)</p> </li> <li> <p>\\(q_\\phi(z_2 | z_1) = \\mathcal{N}(z_2; \\mu_\\phi(z_1), \\sigma_\\phi^2(z_1) I)\\)</p> </li> <li> <p>Decoder: \\(p_\\theta(x, z_1 | z_2) = p_\\theta(x | z_1) \\cdot p_\\theta(z_1 | z_2)\\)</p> </li> <li> <p>\\(p_\\theta(x | z_1) = \\mathcal{N}(x; \\mu_\\theta(z_1), \\sigma_\\theta^2(z_1) I)\\)</p> </li> <li> <p>\\(p_\\theta(z_1 | z_2) = \\mathcal{N}(z_1; \\mu_\\theta(z_2), \\sigma_\\theta^2(z_2) I)\\)</p> </li> <li> <p>Prior: \\(p(z_2) = \\mathcal{N}(z_2; 0, I)\\)</p> </li> </ul> <p>Hierarchical VAE ELBO (Non-KL form):</p> \\[ELBO_{\\text{HVAE}} = \\mathbb{E}_{q_\\phi(z_1,z_2|x)} \\left[ \\log \\frac{p_\\theta(x, z_1, z_2)}{q_\\phi(z_1, z_2|x)} \\right]\\] <p>Following the hierarchical VAE formulation, we can write the ELBO for diffusion models. In diffusion models, we have a sequence of latent variables \\(x_1, x_2, \\ldots, x_T\\) where \\(x_T\\) is the most abstract (pure noise) and \\(x_0\\) is the data.</p> <p>Diffusion Model Structure (x_T \u2192 x_{T-1} \u2192 ... \u2192 x_1 \u2192 x_0):</p> <ul> <li> <p>Encoder: \\(q(x_1, x_2, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\) - Fixed noise corruption process</p> </li> <li> <p>Decoder: \\(p_\\theta(x_0, x_1, \\ldots, x_{T-1} | x_T) = \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\) - Learned denoising process</p> </li> <li> <p>Prior: \\(p(x_T) = \\mathcal{N}(x_T; 0, I)\\)</p> </li> </ul> <p>Diffusion Model ELBO (Non-KL form):</p> \\[ELBO_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ \\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>The Negative Evidence Lower BOund (NELBO) is the negative of the ELBO, which is what we actually minimize during training:</p> \\[\\mathcal{L}_{\\text{Diff}} = -\\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ \\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>This can be rewritten as:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>The decoder learns to predict the mean function \\(\\mu_\\theta(x_t, t)\\) for the reverse process. Let's derive how this function is parameterized.</p> <p>The true reverse process \\(q(x_{t-1} | x_t, x_0)\\) can be derived using Bayes' theorem. For Gaussian distributions, this gives us:</p> \\[q(x_{t-1} | x_t, x_0) = \\mathcal{N}(x_{t-1}; \\mu_t(x_t, x_0), \\sigma_t^2 I)\\] <p>where it can be shown that:</p> \\[\\mu_t(x_t, x_0) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\right)\\] <p>and:</p> \\[\\sigma_t^2 = \\frac{\\beta_t(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\\] <p>The learned reverse process is:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)\\] <p>Since we want the learned process to approximate the true reverse process, we parameterize \\(\\mu_\\theta(x_t, t)\\) to match the form of \\(\\mu_t(x_t, x_0)\\):</p> \\[\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\] <p>where \\(\\epsilon_\\theta(x_t, t)\\) is a neural network that predicts the noise \\(\\epsilon\\) given \\(x_t\\) and \\(t\\).</p>"},{"location":"AI/deep_generative_models/score_based_diffusion_models/#rewriting-the-elbo-for-diffusion-models","title":"Rewriting the ELBO for Diffusion Models","text":"<p>Let's rewrite the diffusion model ELBO and transform it to resemble denoising score matching.</p> <p>Starting with the diffusion model ELBO:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log \\frac{p_\\theta(x_0, x_1, \\ldots, x_T)}{q(x_1, \\ldots, x_T|x_0)} \\right]\\] <p>We can expand this as:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log p_\\theta(x_0, x_1, \\ldots, x_T) + \\log q(x_1, \\ldots, x_T|x_0) \\right]\\] <p>The learned joint distribution is:</p> \\[p_\\theta(x_0, x_1, \\ldots, x_T) = p(x_T) \\cdot \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\\] <p>The true joint distribution is:</p> \\[q(x_1, \\ldots, x_T | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\\] <p>Substituting these:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log p(x_T) - \\sum_{t=1}^T \\log p_\\theta(x_{t-1} | x_t) + \\sum_{t=1}^T \\log q(x_t | x_{t-1}) \\right]\\] <p>For Gaussian distributions, the log-likelihood is:</p> \\[\\log \\mathcal{N}(x; \\mu, \\sigma^2 I) = -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 + C\\] <p>where \\(C\\) is a constant that doesn't depend on the parameters.</p> <p>For the learned reverse process:</p> \\[\\log p_\\theta(x_{t-1} | x_t) = -\\frac{1}{2\\sigma_t^2} \\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 + C\\] <p>For the true forward process:</p> \\[\\log q(x_t | x_{t-1}) = -\\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2 + C\\] <p>Using the definition of \\(\\mu_\\theta(x_t, t)\\):</p> \\[\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\] <p>The squared error term becomes:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\left\\|x_{t-1} - \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\right\\|^2\\] <p>From the forward process, we know:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>And from the multistep transition:</p> \\[x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\epsilon_{t-1}\\] <p>Substituting these into the squared error:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\left\\|\\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\epsilon_{t-1} - \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\\right\\|^2\\] <p>Using the relationship \\(\\bar{\\alpha}_t = \\bar{\\alpha}_{t-1} \\cdot \\alpha_t\\), we can simplify:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\left\\|\\frac{\\beta_t}{\\sqrt{\\alpha_t(1 - \\bar{\\alpha}_t)}} (\\epsilon - \\epsilon_\\theta(x_t, t))\\right\\|^2\\] <p>This simplifies to:</p> \\[\\|x_{t-1} - \\mu_\\theta(x_t, t)\\|^2 = \\frac{\\beta_t^2}{\\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\] <p>Substituting back into the ELBO:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)} \\left[ -\\log p(x_T) + \\sum_{t=1}^T \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2 + \\sum_{t=1}^T \\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2 \\right]\\] <p>The key term in the ELBO is:</p> \\[\\sum_{t=1}^T \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\] <p>Let's define:</p> \\[\\lambda_t = \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)}\\] <p>From the forward process, we know:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>The expectation \\(\\mathbb{E}_{q(x_1,\\ldots,x_T|x_0)}\\) can be rewritten as:</p> \\[\\mathbb{E}_{x_0 \\sim p_{data}(x_0)} \\mathbb{E}_{\\epsilon_1, \\ldots, \\epsilon_T \\sim \\mathcal{N}(0, I)}\\] <p>Since each \\(x_t\\) is generated as:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t\\] <p>The ELBO can be simplified to:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\lambda_t \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|^2 \\right] + \\text{constant terms}\\] <p>where:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t\\] <p>This can be written more compactly as:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\lambda_t \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|^2 \\right] + \\text{constant terms}\\] <p>where \\(x_0 \\sim p_{data}(x_0)\\) (clean data), \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) (noise), \\(t \\sim \\text{Uniform}(1, T)\\) (timestep)</p> <p>The diffusion model ELBO is equivalent to:</p> \\[\\mathcal{L}_{\\text{Diff}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\lambda_t \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|^2 \\right]\\] <p>where \\(\\lambda_t = \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)}\\) is the weighting factor for each timestep.</p> <p>Note In the original ELBO, we had two summation-terms:</p> <ol> <li> <p>\\(\\sum_{t=1}^T \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1 - \\bar{\\alpha}_t)} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\) (noise prediction term)</p> </li> <li> <p>\\(\\sum_{t=1}^T \\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2\\) (forward process term)</p> </li> </ol> <p>The second summation-term \\(\\sum_{t=1}^T \\frac{1}{2\\beta_t} \\|x_t - \\sqrt{1 - \\beta_t} x_{t-1}\\|^2\\) represents the log-likelihood of the forward process \\(q(x_t | x_{t-1})\\). This summation-term does not depend on the model parameters \\(\\theta\\) because the forward process is fixed and predefined. It only depends on the noise schedule \\(\\beta_t\\) and the data. When we take the gradient with respect to \\(\\theta\\) to optimize the model, this summation-term vanishes.</p>"},{"location":"AI/deep_generative_models/score_based_diffusion_models/#sampling","title":"Sampling","text":"<p>While the ELBO loss \\(\\mathcal{L}_{\\text{Diff}}\\) and the score-based objective are roughly equivalent in terms of what they learn, the sampling procedures differ between these two approaches.</p> <p>In a Score-Based Model (SBM), sampling is performed using Langevin dynamics. In a Diffusion Model (VAE form), sampling follows the learned reverse process.</p> <p>The connection between the two approaches comes from the relationship between the score function and the noise predictor:</p> <p>Score function: \\(s_\\theta(x_t, t) = \\nabla_x \\log p_t(x_t)\\)</p> <p>Noise predictor: \\(\\epsilon_\\theta(x_t, t)\\) predicts the noise added during the forward process</p> <p>Relationship: For Gaussian noise, the score function is proportional to the negative noise.</p> <p>From the forward process, we have:</p> \\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] <p>where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>The distribution of \\(x_t\\) given \\(x_0\\) is:</p> \\[q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\] <p>The score function is the gradient of the log probability density:</p> \\[s(x_t, t) = \\nabla_{x_t} \\log q(x_t | x_0)\\] <p>For the Gaussian distribution \\(q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\):</p> \\[\\log q(x_t | x_0) = -\\frac{1}{2(1 - \\bar{\\alpha}_t)} \\|x_t - \\sqrt{\\bar{\\alpha}_t} x_0\\|^2 + C\\] <p>where \\(C\\) is a constant that doesn't depend on \\(x_t\\).</p> <p>Taking the gradient with respect to \\(x_t\\):</p> \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} (x_t - \\sqrt{\\bar{\\alpha}_t} x_0)\\] <p>From the forward process, we can express \\(x_0\\) in terms of \\(x_t\\) and \\(\\epsilon\\):</p> \\[x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon}{\\sqrt{\\bar{\\alpha}_t}}\\] \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} \\left(x_t - \\sqrt{\\bar{\\alpha}_t} \\cdot \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon}{\\sqrt{\\bar{\\alpha}_t}}\\right)\\] <p>Simplifying the expression:</p> \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} \\left(x_t - (x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon)\\right)\\] \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{1}{1 - \\bar{\\alpha}_t} \\cdot \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\] \\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{\\epsilon}{\\sqrt{1 - \\bar{\\alpha}_t}}\\] <p>Therefore, the score function is:</p> \\[s(x_t, t) = \\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{\\epsilon}{\\sqrt{1 - \\bar{\\alpha}_t}}\\] <p>In practice, we learn:</p> <ul> <li> <p>Score function: \\(s_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log q(x_t | x_0)\\)</p> </li> <li> <p>Noise predictor: \\(\\epsilon_\\theta(x_t, t) \\approx \\epsilon\\)</p> </li> </ul> <p>Therefore, \\(s_\\theta(x_t, t) \\approx -\\frac{\\epsilon_\\theta(x_t, t)}{\\sqrt{1 - \\bar{\\alpha}_t}}\\).</p> <p>Both sampling methods work because they're learning the same underlying structure:</p> <ol> <li>Score-based: Learns the gradient of the log-density at each noise level</li> <li>Diffusion: Learns the noise that was added during the forward process</li> </ol> <p>Since the score function and noise predictor are mathematically related, both approaches can generate high-quality samples, but they use different sampling algorithms.</p>"},{"location":"AI/deep_generative_models/score_based_generative_modeling/","title":"Score Based Generative Modeling","text":""},{"location":"AI/deep_generative_models/score_based_generative_modeling/#langevin-dynamics-sampling","title":"Langevin Dynamics Sampling","text":"<p>Langevin dynamics is a powerful MCMC method that uses gradient information to efficiently sample from complex probability distributions. For score-based models, it provides a natural way to generate samples by following the learned score field.</p> <p>Mathematical Foundation</p> <p>Langevin dynamics is based on the Langevin equation, a stochastic differential equation that describes the motion of particles in a potential field:</p> \\[dx_t = \\nabla_x \\log p(x_t) dt + \\sqrt{2} dW_t\\] <p>where:</p> <ul> <li> <p>\\(x_t\\) is the particle position at time \\(t\\)</p> </li> <li> <p>\\(\\nabla_x \\log p(x_t)\\) is the score function (gradient of log probability)</p> </li> <li> <p>\\(W_t\\) is a Wiener process (Brownian motion)</p> </li> <li> <p>The first term is the drift term (gradient guidance)</p> </li> <li> <p>The second term is the diffusion term (random exploration)</p> </li> </ul> <p>Discretized Langevin Dynamics</p> <p>For practical implementation, we discretize the continuous-time equation:</p> \\[x_{t+1} = x_t + \\epsilon \\cdot \\nabla_x \\log p(x_t) + \\sqrt{2\\epsilon} \\cdot \\eta_t\\] <p>where:</p> <ul> <li> <p>\\(\\epsilon\\) is the step size (time discretization)</p> </li> <li> <p>\\(\\eta_t \\sim \\mathcal{N}(0, I)\\) is Gaussian noise</p> </li> <li> <p>\\(t\\) indexes the discrete time steps</p> </li> </ul> <p>Score-Based Langevin Sampling</p> <p>For our trained score function \\(s_\\theta(x) \\approx \\nabla_x \\log p_{data}(x)\\), the sampling algorithm becomes:</p> <p>Algorithm: Score-Based Langevin Sampling</p> <ol> <li> <p>Initialize: \\(x_0 \\sim \\mathcal{N}(0, I)\\) (random noise)</p> </li> <li> <p>Iterate: For \\(t = 0, 1, 2, \\ldots, T-1\\):</p> </li> <li> <p>Compute score: \\(s_t = s_\\theta(x_t)\\)</p> </li> <li> <p>Add gradient step: \\(x_{t+1} = x_t + \\frac{\\epsilon}{2} \\cdot s_t + \\sqrt{2\\epsilon} \\cdot \\eta_t\\)</p> </li> <li> <p>Where \\(\\eta_t \\sim \\mathcal{N}(0, I)\\)</p> </li> <li> <p>Return: \\(x_T\\) as the generated sample</p> </li> </ol> <p>Intuition Behind Langevin Dynamics</p> <p>The Drift Term (\\(\\frac{\\epsilon}{2} \\cdot s_\\theta(x_t)\\)):</p> <ul> <li> <p>Pushes the sample toward high-probability regions</p> </li> <li> <p>The score function points \"uphill\" in the probability landscape</p> </li> <li> <p>Larger step sizes \\(\\epsilon\\) lead to more aggressive movement</p> </li> <li> <p>The factor of \\(\\frac{1}{2}\\) comes from proper discretization of the continuous Langevin equation</p> </li> </ul> <p>The Diffusion Term (\\(\\sqrt{2\\epsilon} \\cdot \\eta_t\\)):</p> <ul> <li> <p>Adds random exploration to avoid getting stuck in local modes</p> </li> <li> <p>Balances the deterministic gradient guidance</p> </li> <li> <p>Ensures the chain can escape local optima and explore the full distribution</p> </li> </ul> <p>Balance Between Drift and Diffusion:</p> <ul> <li> <p>Small \\(\\epsilon\\): More exploration, slower convergence, better mixing</p> </li> <li> <p>Large \\(\\epsilon\\): Faster convergence, but may miss modes or become unstable</p> </li> <li> <p>Optimal \\(\\epsilon\\): Depends on the data distribution and model architecture</p> </li> </ul> <p>Convergence Guarantees</p> <p>Under mild conditions on the target distribution and score function, Langevin dynamics provides strong theoretical guarantees:</p> <p>Asymptotic Convergence:</p> <p>If \\(\\epsilon \\to 0\\) and \\(T \\to \\infty\\), we are guaranteed that \\(x_T \\sim p_{data}(x)\\).</p> <p>Mathematical Interpretation:</p> <ul> <li>\\(\\epsilon \\to 0\\): The discretization becomes arbitrarily fine, approaching the continuous Langevin equation</li> <li>\\(T \\to \\infty\\): The Markov chain runs for an infinite number of steps, allowing it to reach the stationary distribution</li> <li>\\(x_T \\sim p_{data}(x)\\): The final sample is distributed according to the target data distribution</li> </ul> <p>Challenge in Low Density Regions:</p> <p>One significant limitation of Langevin dynamics is its poor performance in low density regions of the target distribution:</p> <ul> <li>Weak Score Signals: In regions where \\(p_{data}(x) \\approx 0\\), the score function \\(\\nabla_x \\log p_{data}(x)\\) becomes very small or noisy</li> <li>Mode Collapse Risk: The algorithm may fail to explore all modes (mode is a region where the probability density is high, i.e., data points are concentrated) of a multi-modal distribution</li> <li>Slow convergence: Langevin Dynamics converges very slowly. Might not even converge if we have zero probability somewhere.</li> </ul> <p>This challenge motivates the development of annealed Langevin dynamics and other advanced sampling techniques that can better handle complex, multi-modal distributions.</p>"},{"location":"AI/deep_generative_models/score_based_generative_modeling/#annealed-langevin-dynamics","title":"Annealed Langevin Dynamics","text":"<p>Mathematical Formulation</p> <p>We define a sequence of annealed distributions indexed by noise level \\(\\sigma_t\\):</p> \\[p_t(x) = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_t^2 I) dy\\] <p>where each \\(p_t(x)\\) is a smoothed version of the original data distribution.</p> <p>This equation is derived from the convolution of the data distribution with the noise distribution. Here's the step-by-step reasoning:</p> <p>If \\(Y \\sim p_{data}(y)\\) and \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_i^2 I)\\), then the noisy sample is \\(X = Y + \\epsilon\\).</p> <p>The joint distribution of \\((Y, X)\\) is:</p> \\[p(y, x) = p_{data}(y) \\cdot \\mathcal{N}(x; y, \\sigma_i^2 I)\\] <p>The joint distribution is derived using the chain rule of probability:</p> \\[p(y, x) = p(y) \\cdot p(x | y)\\] <p>where:</p> <ul> <li> <p>\\(p(y) = p_{data}(y)\\) is the marginal distribution of the clean data</p> </li> <li> <p>\\(p(x | y)\\) is the conditional distribution of the noisy sample given the clean data</p> </li> </ul> <p>Since \\(X = Y + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_i^2 I)\\), the conditional distribution is:</p> \\[p(x | y) = \\mathcal{N}(x; y, \\sigma_i^2 I)\\] <p>This is because adding a constant (\\(y\\)) to a Gaussian random variable shifts the mean but preserves the variance. Therefore:</p> \\[p(y, x) = p_{data}(y) \\cdot \\mathcal{N}(x; y, \\sigma_i^2 I)\\] <p>To get the distribution of \\(X\\) alone, we marginalize over \\(Y\\):</p> \\[p_{\\sigma_i}(x) = \\int p(y, x) dy = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_i^2 I) dy\\] <p>We're using the law of total probability (also called marginalization). When we have a joint distribution \\(p(y, x)\\), to find the marginal distribution of \\(x\\) alone, we integrate out the other variable:</p> \\[p_{\\sigma_i}(x) = \\int p(y, x) dy\\] <p>This is because:</p> <ul> <li> <p>The joint distribution \\(p(y, x)\\) gives us the probability of both \\(y\\) AND \\(x\\) occurring together</p> </li> <li> <p>To find the probability of just \\(x\\) (regardless of what \\(y\\) is), we sum over all possible values of \\(y\\)</p> </li> <li> <p>In continuous probability, \"summing\" becomes integration</p> </li> </ul> <p>Intuition: We're asking \"What's the probability of observing a noisy sample \\(x\\)?\" The answer is the sum of probabilities over all possible clean samples \\(y\\) that could have generated this noisy sample.</p> <p>Final Form: The noise-perturbed distribution is:</p> \\[p_{\\sigma_i}(x) = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_i^2 I) dy\\] <p>We use multiple scales of noise perturbations simultaneously. Suppose we always perturb the data with isotropic Gaussian noise, and let there be a total of \\(L\\) increasing standard deviations \\(\\sigma_1 &lt; \\sigma_2 &lt; \\ldots &lt; \\sigma_L\\). We first perturb the data distribution \\(p_{data}(y)\\) with each of the Gaussian noise \\(\\mathcal{N}(0, \\sigma_i^2 I)\\) to obtain a noise-perturbed distribution (the final form we derived above):</p> \\[p_{\\sigma_i}(x) = \\int p_{data}(y) \\mathcal{N}(x; y, \\sigma_i^2 I) dy\\] <p>Note that we can easily draw samples from \\(p_{\\sigma_i}(x)\\) by sampling \\(y \\sim p_{data}(y)\\) and computing \\(x = y + \\sigma_i \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>We estimate the score function of each noise-perturbed distribution, \\(\\nabla_x \\log p_{\\sigma_i}(x)\\), by training a Denoising Score Matching Model (when parameterized with a neural network) with score matching, such that \\(s_\\theta(x, \\sigma_i) \\approx \\nabla_x \\log p_{\\sigma_i}(x)\\) for all \\(i\\). The training objective for \\(s_\\theta\\) is a weighted sum of Fisher divergences for all noise scales. In particular, we use the objective below:</p> \\[\\mathcal{L}(\\theta) = \\frac{1}{L} \\sum_{i=1}^L \\lambda(\\sigma_i) \\mathbb{E}_{p_{\\sigma_i}(x)} \\left[ \\| s_\\theta(x, \\sigma_i) - \\nabla_x \\log p_{\\sigma_i}(x) \\|_2^2 \\right]\\] <p>where \\(\\lambda(\\sigma_i)\\) is a positive weighting function, often chosen to be \\(\\lambda(\\sigma_i) = \\sigma_i^2\\). The objective \\(\\mathcal{L}(\\theta)\\) can be optimized with score matching, exactly as in optimizing the naive score-based model.</p> <p>Denoising Score Matching Format:</p> <p>We can rewrite the objective in Denoising Score Matching model format:</p> \\[\\mathcal{L}(\\theta) = \\frac{1}{L} \\sum_{i=1}^L \\lambda(\\sigma_i) \\mathbb{E}_{y \\sim p_{data}(y), x \\sim \\mathcal{N}(x; y, \\sigma_i^2 I)} \\left[ \\left\\| s_\\theta(x, \\sigma_i) - \\frac{y - x}{\\sigma_i^2} \\right\\|_2^2 \\right]\\] <p>Note: The noise scales \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_L\\) are typically chosen to be in a geometric progression, meaning \\(\\sigma_{i+1} = \\alpha \\cdot \\sigma_i\\) for some constant \\(\\alpha &lt; 1\\). This ensures that the noise levels decrease exponentially, providing a smooth annealing schedule from high noise to low noise.</p> <p>Perturbing an image with multiple scales of Gaussian noise: </p> <p>After training our score-based model \\(s_\\theta(x, \\sigma_i)\\), we can produce samples from it by running Langevin dynamics for \\(\\sigma_L, \\sigma_{L-1}, \\ldots, \\sigma_1\\) in sequence. This method is called Annealed Langevin dynamics, since the noise scale decreases (anneals) gradually over time.</p> <p>We can start from unstructured noise, modify images according to the scores, and generate nice samples: </p>"},{"location":"AI/deep_generative_models/score_based_generative_modeling/#generative-modeling-via-stochastic-differential-equations-sdes","title":"Generative Modeling via Stochastic Differential Equations (SDEs)","text":"<p>When the number of noise scales approaches infinity, we essentially perturb the data distribution with continuously growing levels of noise. In this case, the noise perturbation procedure is a continuous-time stochastic process, as demonstrated below.</p> <p></p> <p>How can we represent a stochastic process in a concise way? Many stochastic processes are solutions of stochastic differential equations (SDEs). In general, an SDE possesses the following form:</p> \\[dx = f(x, t)dt + g(t)dw\\] <p>where \\(f(x, t)\\) is a vector-valued function called the drift coefficient, \\(g(t)\\) is a real-valued function called the diffusion coefficient, \\(w\\) denotes a standard Brownian motion, and \\(dw\\) can be viewed as infinitesimal white noise. The solution of a stochastic differential equation is a continuous collection of random variables \\(\\{x(t)\\}_{t \\in [0, T]}\\).</p> <p>These random variables trace stochastic trajectories as the time index \\(t\\) grows from the start time \\(0\\) to the end time \\(T\\). Let \\(p_t(x)\\) denote the (marginal) probability density function of \\(x(t)\\). Here \\(p_t(x)\\) is analogous to \\(p_{\\sigma_i}(x)\\) when we had a finite number of noise scales, and \\(t\\) is analogous to \\(\\sigma_i\\). Clearly, \\(p_0(x)\\) is the data distribution since no perturbation is applied to data at \\(t = 0\\). After perturbing \\(p_0(x)\\) with the stochastic process for a sufficiently long time \\(T\\), \\(p_T(x)\\) becomes close to a tractable noise distribution \\(p_T(x) \\approx \\pi(x)\\), called a prior distribution. We note that \\(\\pi(x)\\) is analogous to \\(p_{\\sigma_L}(x)\\) in the case of finite noise scales, which corresponds to applying the largest noise perturbation \\(\\sigma_L\\) to the data.</p> <p>There are numerous ways to add noise perturbations, and the choice of SDEs is not unique. For example, the following SDE</p> \\[dx = e^t dw\\] <p>perturbs data with a Gaussian noise of mean zero and exponentially growing variance. Therefore, the SDE should be viewed as part of the model, much like \\(\\sigma_i\\).</p> <p>Recall that with a finite number of noise scales, we can generate samples by reversing the perturbation process with annealed Langevin dynamics, i.e., sequentially sampling from each noise-perturbed distribution using Langevin dynamics. For infinite noise scales, we can analogously reverse the perturbation process for sample generation by using the reverse SDE.</p> <p>Importantly, any SDE has a corresponding reverse SDE, whose closed form is given by</p> \\[dx = [f(x, t) - g^2(t)\\nabla_x \\log p_t(x)]dt + g(t)d\\bar{w}\\] <p>Here \\(dt\\) represents a negative infinitesimal time step, since the SDE needs to be solved backwards in time (from \\(T\\) to \\(0\\)). In order to compute the reverse SDE, we need to estimate \\(\\nabla_x \\log p_t(x)\\), which is exactly the score function of \\(p_t(x)\\).</p> <p></p> <p>Note: Langevin dynamics is a specific instance of the reverse SDE where \\(f(x, t) = 0\\) (no forward drift) and \\(g(t) = \\sqrt{2}\\) (constant diffusion). This shows how Langevin dynamics naturally emerges as a special case of the reverse SDE when we want to sample from a target distribution.</p>"},{"location":"AI/deep_generative_models/score_based_generative_modeling/#estimating-the-reverse-sde-with-score-based-models-and-score-matching","title":"Estimating the reverse SDE with score-based models and score matching","text":"<p>Solving the reverse SDE requires us to know the terminal distribution \\(p_T(x)\\), and the score function \\(\\nabla_x \\log p_t(x)\\). By design, the former is close to the prior distribution \\(\\pi(x)\\) which is fully tractable. In order to estimate \\(\\nabla_x \\log p_t(x)\\), we train a Time-Dependent Score-Based Model \\(s_\\theta(x, t)\\), such that \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\). This is analogous to the denoising score matching model \\(s_\\theta(x, \\sigma_i)\\) used for finite noise scales, trained such that \\(s_\\theta(x, \\sigma_i) \\approx \\nabla_x \\log p_{\\sigma_i}(x)\\).</p> <p>Our training objective for \\(s_\\theta(x, t)\\) is a continuous weighted combination of Fisher divergences, given by</p> \\[\\mathcal{L}(\\theta) = \\mathbb{E}_{t \\sim \\mathcal{U}[0, T]} \\mathbb{E}_{x \\sim p_t(x)} \\left[ \\lambda(t) \\| s_\\theta(x, t) - \\nabla_x \\log p_t(x) \\|_2^2 \\right]\\] <p>where \\(\\mathcal{U}[0, T]\\) denotes a uniform distribution over the time interval \\([0, T]\\), and \\(\\lambda(t)\\) is a positive weighting function.</p> <p>As before, our weighted combination of Fisher divergences can be efficiently optimized with score matching methods, such as denoising score matching and sliced score matching. Once our score-based model \\(s_\\theta(x, t)\\) is trained to optimality, we can plug it into the expression of the reverse SDE to obtain an estimated reverse SDE.</p> <p>We can start with \\(x(T) \\sim p_T(x)\\), and solve the above reverse SDE to obtain a sample \\(x(0)\\). Let us denote the distribution of \\(x(0)\\) obtained in such way as \\(p_\\theta(x)\\). When the score-based model \\(s_\\theta(x, t)\\) is well-trained, we have \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\), in which case \\(x(0)\\) is an approximate sample from the data distribution \\(p_0(x)\\).</p> <p>By solving the estimated reverse SDE with numerical SDE solvers, we can simulate the reverse stochastic process for sample generation. Perhaps the simplest numerical SDE solver is the Euler-Maruyama method. When applied to our estimated reverse SDE, it discretizes the SDE using finite time steps and small Gaussian noise. Specifically, it chooses a small negative time step \\(\\Delta t\\), initializes \\(x(T) \\sim p_T(x)\\), and iterates the following procedure until \\(t = 0\\):</p> \\[\\Delta x = [f(x, t) - g^2(t)s_\\theta(x, t)]\\Delta t + g(t)\\sqrt{|\\Delta t|}\\eta_t\\] <p>where \\(\\eta_t \\sim \\mathcal{N}(0, I)\\).</p> <p>Then update: \\(x \\leftarrow x + \\Delta x\\) and \\(t \\leftarrow t + \\Delta t\\).</p> <p>Note: The function \\(f(x, t)\\) in the Euler-Maruyama equation is the drift coefficient from the original forward SDE. Common examples include:</p> <ul> <li> <p>\\(f(x, t) = 0\\) (pure diffusion): Used in simple noise perturbation</p> </li> <li> <p>\\(f(x, t) = -\\frac{1}{2}\\beta(t)x\\) (linear drift): Used in variance-preserving diffusion</p> </li> <li> <p>\\(f(x, t) = -x^2\\) (quadratic drift): Creates potential wells</p> </li> <li> <p>\\(f(x, t) = x - x^3\\) (polynomial drift): Creates multiple stable equilibria</p> </li> </ul> <p>Most Common in Practice: For score-based generative modeling, the most commonly used forms are \\(f(x, t) = 0\\) (pure diffusion) and \\(f(x, t) = -\\frac{1}{2}\\beta(t)x\\) (VP diffusion). The choice of \\(f(x, t)\\) determines how the data is perturbed during the forward process.</p> <p>The Euler-Maruyama method is qualitatively similar to Langevin dynamics\u2014 both update \\(x\\) by following score functions perturbed with Gaussian noise.</p>"},{"location":"AI/deep_generative_models/score_based_models/","title":"Score Based Models","text":""},{"location":"AI/deep_generative_models/score_based_models/#score-matching","title":"Score Matching","text":"<p>Energy-Based Model Probability Distribution</p> <p>In Energy-Based Models, the probability distribution is defined as:</p> \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} e^{f_\\theta(x)}\\] <p>where:</p> <ul> <li> <p>\\(f_\\theta(x)\\) is the energy function (neural network)</p> </li> <li> <p>\\(Z(\\theta) = \\int e^{f_\\theta(x)} dx\\) is the partition function (intractable)</p> </li> </ul> <p>Taking the logarithm of the probability distribution:</p> \\[\\log p_\\theta(x) = f_\\theta(x) - \\log Z(\\theta)\\] <p>Notice that the partition function \\(Z(\\theta)\\) appears as a constant term that doesn't depend on \\(x\\).</p> <p>Stein Score Function</p> <p>The Stein score function \\(s_\\theta(x)\\) is defined as the gradient of the log probability with respect to \\(x\\):</p> \\[s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\] <p>For Energy-Based Models, the score function equals the gradient of the energy function:</p> \\[s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = \\nabla_x (f_\\theta(x) - \\log Z(\\theta)) = \\nabla_x f_\\theta(x)\\] <p>The partition function term \\(\\log Z(\\theta)\\) disappears because it doesn't depend on \\(x\\).</p> <p>Score as a Vector Field</p> <p>The score function \\(s_\\theta(x)\\) is a vector field that assigns a vector to each point \\(x\\) in the data space. This vector has both:</p> <ol> <li> <p>Magnitude: How quickly the log probability changes</p> </li> <li> <p>Direction: The direction of steepest increase in log probability</p> </li> </ol> <p>Intuition: The score vector points \"uphill\" in the log probability landscape, indicating the direction where the model assigns higher probability.</p> <p>Example: Gaussian Distribution</p> <p>Consider a Gaussian distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\):</p> \\[p(x) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\right)\\] <p>Log Probability:</p> \\[\\log p(x) = -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu) - \\frac{1}{2}\\log((2\\pi)^d |\\Sigma|)\\] <p>Score Function:</p> \\[s(x) = \\nabla_x \\log p(x) = -\\Sigma^{-1}(x - \\mu)\\] <p>Interpretation:</p> <ul> <li> <p>The score points toward the mean \\(\\mu\\) (direction of higher probability)</p> </li> <li> <p>The magnitude is proportional to the distance from the mean</p> </li> <li> <p>For isotropic Gaussian (\\(\\Sigma = \\sigma^2 I\\)): \\(s(x) = -\\frac{1}{\\sigma^2}(x - \\mu)\\)</p> </li> </ul> <p>This example shows how the score function naturally guides samples toward high-probability regions of the distribution.</p>"},{"location":"AI/deep_generative_models/score_based_models/#score-matching-comparing-distributions-via-vector-fields","title":"Score Matching: Comparing Distributions via Vector Fields","text":"<p>The core idea of score matching is that we want to compare two probability distributions by comparing their respective vector fields of gradients (score functions).</p> <p>The Key Insight:</p> <p>Instead of directly comparing probability densities \\(p_{data}(x)\\) and \\(p_\\theta(x)\\) (which requires computing the intractable partition function), we compare their score functions:</p> <ul> <li>Data Score: \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\)</li> <li>Model Score: \\(s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = \\nabla_x f_\\theta(x)\\)</li> </ul> <p>This measures how different the \"pointing directions\" are at each location \\(x\\).</p> <p>L2 Distance Between Score Functions</p> <p>One way to compare the score functions is to calculate the average L2 distance between the score of \\(p_{data}\\) and \\(p_\\theta\\):</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x) - s_{data}(x)\\|^2 \\right]\\] <p>Note: This loss function is also called the Fisher divergence between \\(p_{data}(x)\\) and \\(p_\\theta(x)\\). The Fisher divergence measures the difference between two probability distributions by comparing their score functions (gradients of log densities) rather than the densities themselves.</p> <p>Understanding the L2 Distance:</p> <p>The L2 norm \\(\\|s_\\theta(x) - s_{data}(x)\\|^2\\) measures the squared Euclidean distance between two vectors:</p> \\[\\|s_\\theta(x) - s_{data}(x)\\|^2 = \\sum_{i=1}^d (s_\\theta(x)_i - s_{data}(x)_i)^2\\] <p>where \\(d\\) is the dimension of the data space.</p> <p>Score matching is a method for training Energy-Based Models by minimizing the Fisher divergence between the data distribution \\(p_{data}(x)\\) and the model distribution \\(p_\\theta(x)\\):</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x) - s_{data}(x)\\|^2 \\right]\\] <p>where \\(s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\) and \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\) are the score functions of the model and data distributions respectively.</p> <p>But how do we figure out \\(\\nabla_x \\log p_{data}(x)\\) given only samples?</p> <p>Score Matching Reformulation (Univariate Case)</p> <p>For the univariate case where \\(x \\in \\mathbb{R}\\), we can rewrite the score matching objective to avoid needing the data score. Let's expand the squared difference:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_\\theta(x) - \\frac{d}{dx} \\log p_{data}(x)\\right)^2 \\right]\\] <p>Expanding the square:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_\\theta(x)\\right)^2 - \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) + \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_{data}(x)\\right)^2 \\right]\\] <p>The key insight is to use integration by parts on the cross term. For any function \\(f(x)\\) and \\(g(x)\\):</p> \\[\\int f(x) \\frac{d}{dx} g(x) dx = f(x)g(x) - \\int \\frac{d}{dx} f(x) \\cdot g(x) dx\\] <p>Setting \\(f(x) = \\frac{d}{dx} \\log p_\\theta(x)\\) and \\(g(x) = p_{data}(x)\\), we get:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\right] = \\int \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\cdot p_{data}(x) dx\\] <p>Using the chain rule: \\(\\frac{d}{dx} \\log p_{data}(x) \\cdot p_{data}(x) = \\frac{d}{dx} p_{data}(x)\\), we get:</p> \\[= \\int \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} p_{data}(x) dx\\] <p>Using integration by parts:</p> \\[= \\left. \\frac{d}{dx} \\log p_\\theta(x) \\cdot p_{data}(x) \\right|_{-\\infty}^{\\infty} - \\int \\frac{d^2}{dx^2} \\log p_\\theta(x) \\cdot p_{data}(x) dx\\] <p>Why does the boundary term vanish?</p> <p>The boundary term \\(\\left. \\frac{d}{dx} \\log p_\\theta(x) \\cdot p_{data}(x) \\right|_{-\\infty}^{\\infty}\\) vanishes under reasonable assumptions:</p> <ol> <li>Data distribution decay: \\(p_{data}(x) \\to 0\\) as \\(|x| \\to \\infty\\) (most real-world distributions have finite support or decay to zero)</li> <li>Model score boundedness: \\(\\frac{d}{dx} \\log p_\\theta(x)\\) grows at most polynomially as \\(|x| \\to \\infty\\)</li> <li>Product decay: The product \\(\\frac{d}{dx} \\log p_\\theta(x) \\cdot p_{data}(x) \\to 0\\) as \\(|x| \\to \\infty\\)</li> </ol> <p>This is a standard assumption in score matching literature and holds for most practical distributions.</p> <p>Assuming the boundary term vanishes (which is reasonable for well-behaved distributions), we get:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d}{dx} \\log p_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\right] = -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d^2}{dx^2} \\log p_\\theta(x) \\right]\\] <p>Substituting back into the original objective:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\left(\\frac{d}{dx} \\log p_\\theta(x)\\right)^2 + \\frac{d^2}{dx^2} \\log p_\\theta(x) \\right] + \\text{constant}\\] <p>where the constant term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}} \\left[ \\left(\\frac{d}{dx} \\log p_{data}(x)\\right)^2 \\right]\\) doesn't depend on \\(\\theta\\) and can be ignored during optimization.</p> <p>Key Insight: This reformulation allows us to train the model using only samples from \\(p_{data}(x)\\) and the derivatives of our model's log probability, without needing access to the data score function.</p> <p>Score Matching Reformulation (Multivariate Case)</p> <p>For the multivariate case where \\(x \\in \\mathbb{R}^d\\), we can extend the univariate derivation. The score matching objective becomes:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x) - \\nabla_x \\log p_{data}(x)\\|^2 \\right]\\] <p>Expanding the squared norm:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x)\\|^2 - \\nabla_x \\log p_\\theta(x)^T \\nabla_x \\log p_{data}(x) + \\frac{1}{2} \\|\\nabla_x \\log p_{data}(x)\\|^2 \\right]\\] <p>The key insight is to use integration by parts on the cross term. For the multivariate case, we need to handle each component separately. Let \\(s_\\theta(x)_i\\) and \\(s_{data}(x)_i\\) denote the \\(i\\)-th component of the respective score functions.</p> <p>For each component \\(i\\), we have:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)_i \\cdot s_{data}(x)_i \\right] = \\int s_\\theta(x)_i \\cdot s_{data}(x)_i \\cdot p_{data}(x) dx\\] <p>Using the chain rule: \\(s_{data}(x)_i \\cdot p_{data}(x) = \\frac{\\partial}{\\partial x_i} p_{data}(x)\\), we get:</p> \\[= \\int s_\\theta(x)_i \\cdot \\frac{\\partial}{\\partial x_i} p_{data}(x) dx\\] <p>Using integration by parts (assuming boundary terms vanish):</p> \\[= -\\int \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\cdot p_{data}(x) dx = -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\right]\\] <p>Why do the boundary terms vanish in the multivariate case?</p> <p>For each component \\(i\\), the boundary term is:</p> \\[\\left. s_\\theta(x)_i \\cdot p_{data}(x) \\right|_{x_i = -\\infty}^{x_i = \\infty}\\] <p>This vanishes under similar assumptions as the univariate case:</p> <ol> <li>Data distribution decay: \\(p_{data}(x) \\to 0\\) as \\(\\|x\\| \\to \\infty\\) in any direction</li> <li>Model score boundedness: Each component \\(s_\\theta(x)_i\\) grows at most polynomially as \\(\\|x\\| \\to \\infty\\)</li> <li>Product decay: The product \\(s_\\theta(x)_i \\cdot p_{data}(x) \\to 0\\) as \\(\\|x\\| \\to \\infty\\) for each component</li> </ol> <p>These assumptions ensure that the boundary terms vanish for all components, allowing us to apply integration by parts component-wise.</p> <p>Summing over all components:</p> \\[\\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)_i \\cdot s_{data}(x)_i \\right] = -\\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\right] = -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] <p>where \\(\\text{tr}(\\nabla_x s_\\theta(x)) = \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i\\) is the trace of the Jacobian matrix of the score function.</p> <p>Substituting back into the original objective:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x)\\|^2 + \\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x)) \\right] + \\text{constant}\\] <p>where the constant term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}} \\left[ \\|\\nabla_x \\log p_{data}(x)\\|^2 \\right]\\) doesn't depend on \\(\\theta\\) and can be ignored during optimization.</p> <p>Key Insight: The multivariate case introduces the trace of the Hessian matrix \\(\\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x))\\).</p>"},{"location":"AI/deep_generative_models/score_based_models/#score-matching-algorithm","title":"Score Matching Algorithm","text":"<p>The score matching algorithm follows these steps:</p> <p>Sample a mini-batch of datapoints: \\(\\{x_1, x_2, \\ldots, x_n\\} \\sim p_{data}(x)\\)</p> <p>Estimate the score matching loss with the empirical mean: </p> \\[\\mathcal{L}_{SM}(\\theta) \\approx \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x_i)\\|^2 + \\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x_i)) \\right]\\] <p>Stochastic gradient descent: Update parameters using gradients of the estimated loss</p> <p>Advantages: * No need to sample from EBM: Unlike other training methods for energy-based models, score matching doesn't require generating samples from the model during training. This avoids the computational expense and potential instability of MCMC sampling. * Direct optimization: The objective directly measures how well the model's score function matches the data distribution's score function. * Theoretically sound: Score matching provides a consistent estimator under mild conditions.</p> <p>Disadvantages: * Computing the Hessian is expensive: The term \\(\\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x))\\) requires computing second derivatives, which scales quadratically with the input dimension and can be computationally prohibitive for large models. * Memory requirements: Storing and computing Hessians for large neural networks requires significant memory. * Numerical instability: Second derivatives can be numerically unstable, especially for deep networks.</p> <p>Computational Complexity: For a model with \\(d\\) input dimensions and \\(m\\) parameters, computing the Hessian trace requires \\(O(d^2 \\cdot m)\\) operations, making it impractical for high-dimensional data like images.</p>"},{"location":"AI/deep_generative_models/score_based_models/#recap-distances-for-training-ebms","title":"Recap: Distances for Training EBMs","text":"<p>When training Energy-Based Models, we need to measure how close our model distribution \\(p_\\theta(x)\\) is to the data distribution \\(p_{data}(x)\\). Here are the main approaches:</p>"},{"location":"AI/deep_generative_models/score_based_models/#contrastive-divergence","title":"Contrastive Divergence","text":"<p>Contrastive divergence measures the difference between the data distribution and the model distribution using KL divergence:</p> \\[\\mathcal{L}_{CD}(\\theta) = D_{KL}(p_{data}(x) \\| p_\\theta(x)) - D_{KL}(p_\\theta(x) \\| p_{data}(x))\\] <p>Key insight: This objective encourages the model to match the data distribution while preventing mode collapse.</p> <p>Challenge: Computing the KL divergence requires sampling from the model distribution \\(p_\\theta(x)\\), which is typically done using MCMC methods like Langevin dynamics or Hamiltonian Monte Carlo.</p>"},{"location":"AI/deep_generative_models/score_based_models/#fisher-divergence-score-matching","title":"Fisher Divergence (Score Matching)","text":"<p>Fisher divergence measures the difference between the score functions (gradients of log densities) of the two distributions:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|\\nabla_x \\log p_\\theta(x) - \\nabla_x \\log p_{data}(x)\\|^2 \\right]\\] <p>Key insight: Instead of comparing probability densities directly, we compare their gradients, which avoids the need to compute the intractable partition function.</p> <p>Advantage: No need to sample from the model during training, making it computationally more efficient than contrastive divergence.</p> <p>Challenge: Requires computing second derivatives (Hessian) of the log probability, which can be expensive for high-dimensional data.</p>"},{"location":"AI/deep_generative_models/score_based_models/#noise-contrastive-estimation","title":"Noise Contrastive Estimation","text":"<p>Learning an EBM by contrasting it with a noise distribution.</p> <p>We have the data distribution \\(p_{data}(x)\\). We have the noise distribution \\(p_n(x)\\) which should be analytically tractable and easy to sample from. We can train a discriminator \\(D(x) \\in [0, 1]\\) to distinguish between data samples and noise samples.</p>"},{"location":"AI/deep_generative_models/score_based_models/#optimal-discriminator","title":"Optimal Discriminator","text":"<p>The optimal discriminator \\(D^*(x)\\) that maximizes this objective is given by:</p> \\[D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\]"},{"location":"AI/deep_generative_models/score_based_models/#parameterizing-the-discriminator-as-an-ebm","title":"Parameterizing the Discriminator as an EBM","text":"<p>Key Insight: Instead of training a separate discriminator, we can parameterize it directly in terms of an Energy-Based Model.</p> <p>Let's define a parameterized version of the discriminator as:</p> \\[D_\\theta(x) = \\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)}\\] <p>where \\(p_\\theta(x) = \\frac{1}{Z(\\theta)} e^{f_\\theta(x)}\\) is our Energy-Based Model.</p> <p>Implicit Learning of the Data Distribution</p> <p>By training the discriminator \\(D_\\theta(x)\\) to distinguish between data samples and noise samples, we are implicitly learning the Energy-Based Model \\(p_\\theta(x)\\) to approximate the true data distribution \\(p_{data}(x)\\).</p> <p>Why This Works:</p> <p>Recall that the optimal discriminator (when trained to perfection) satisfies:</p> \\[D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\] <p>But we've parameterized our discriminator as:</p> \\[D_\\theta(x) = \\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)}\\] <p>The Key Insight: When we train \\(D_\\theta(x)\\) to match the optimal discriminator \\(D^*(x)\\), we're essentially forcing:</p> \\[\\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)} \\approx \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\] <p>This equality holds if and only if \\(p_\\theta(x) \\approx p_{data}(x)\\) (assuming \\(p_n(x) &gt; 0\\) everywhere).</p> <p>Mathematical Justification:</p> <p>If \\(\\frac{p_\\theta(x)}{p_\\theta(x) + p_n(x)} = \\frac{p_{data}(x)}{p_{data}(x) + p_n(x)}\\), then:</p> \\[p_\\theta(x) \\cdot (p_{data}(x) + p_n(x)) = p_{data}(x) \\cdot (p_\\theta(x) + p_n(x))\\] \\[p_\\theta(x) \\cdot p_{data}(x) + p_\\theta(x) \\cdot p_n(x) = p_{data}(x) \\cdot p_\\theta(x) + p_{data}(x) \\cdot p_n(x)\\] \\[p_\\theta(x) \\cdot p_n(x) = p_{data}(x) \\cdot p_n(x)\\] <p>Since \\(p_n(x) &gt; 0\\), we can divide both sides to get:</p> \\[p_\\theta(x) = p_{data}(x)\\] <p>Modeling the Partition Function as a Trainable Parameter</p> <p>The EBM Equation:</p> <p>Our Energy-Based Model is defined as:</p> \\[p_\\theta(x) = \\frac{1}{Z(\\theta)} e^{f_\\theta(x)}\\] <p>where \\(f_\\theta(x)\\) is the energy function (neural network) and \\(Z(\\theta) = \\int e^{f_\\theta(x)} dx\\) is the partition function.</p> <p>The Partition Function Constraint Problem:</p> <p>The constraint \\(Z(\\theta) = \\int e^{f_\\theta(x)} dx\\) is computationally intractable to satisfy exactly because:</p> <ol> <li>High-dimensional integration: Computing \\(\\int e^{f_\\theta(x)} dx\\) over high-dimensional spaces is extremely expensive</li> <li>No closed form: For complex energy functions, there's no analytical solution</li> <li>Dynamic updates: The integral changes every time we update the energy function parameters</li> </ol> <p>Solution: Treat Z as a Trainable Parameter</p> <p>Instead of enforcing the constraint, we model \\(Z(\\theta)\\) as an additional trainable parameter \\(Z\\) that is not explicitly constrained to satisfy \\(Z = \\int e^{f_\\theta(x)} dx\\).</p> <p>This gives us the modified EBM:</p> \\[p_{\\theta, Z}(x) = \\frac{e^{f_\\theta(x)}}{Z}\\] <p>Why Z Converges to the Correct Partition Function:</p> <p>As we train \\(p_{\\theta, Z}(x)\\) to approximate \\(p_{data}(x)\\), the parameter \\(Z\\) automatically converges to the correct partition function value.</p> <p>Mathematical Justification:</p> <p>When training converges, we have \\(p_{\\theta, Z}(x) \\approx p_{data}(x)\\). This means:</p> \\[\\frac{e^{f_\\theta(x)}}{Z} \\approx p_{data}(x)\\] <p>A direct argument comes from the fact that \\(p_{\\theta, Z}(x)\\) must approximate \\(p_{data}(x)\\), which must integrate to 1:</p> \\[\\int p_{\\theta, Z}(x) dx = \\int \\frac{e^{f_\\theta(x)}}{Z} dx \\approx 1\\] <p>This immediately gives us:</p> \\[Z \\approx \\int e^{f_\\theta(x)} dx\\] <p>Deriving the Discriminator for the Modified EBM</p> <p>Now let's derive the discriminator \\(D_{\\theta, Z}(x)\\) for our modified EBM \\(p_{\\theta, Z}(x) = \\frac{e^{f_\\theta(x)}}{Z}\\).</p> <p>Starting with the discriminator definition:</p> \\[D_{\\theta, Z}(x) = \\frac{p_{\\theta, Z}(x)}{p_{\\theta, Z}(x) + p_n(x)}\\] <p>Substituting our modified EBM:</p> \\[D_{\\theta, Z}(x) = \\frac{\\frac{e^{f_\\theta(x)}}{Z}}{\\frac{e^{f_\\theta(x)}}{Z} + p_n(x)}\\] \\[D_{\\theta, Z}(x) = \\frac{e^{f_\\theta(x)}}{e^{f_\\theta(x)} + Z \\cdot p_n(x)}\\] <p>Noise Contrastive Estimation Training Objective</p> <p>The NCE objective maximizes the log-likelihood of correctly classifying data vs noise samples:</p> \\[\\mathcal{L}_{NCE}(\\theta, Z) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log D_{\\theta, Z}(x) \\right] + \\mathbb{E}_{x \\sim p_n} \\left[ \\log(1 - D_{\\theta, Z}(x)) \\right]\\] <p>In theory, we could have any noise distribution to make this work. But in pratice, a noise distribution that similar (if we can manage) to the data distribution works very well. At the end of the day you learn an EBM and you learn a partition function. In the limit of infinite data and perfect optimization, the EBM matches the data distribution and Z matches the true partition function of the EBM.</p> <p>There is no evolving Generator like we had in GAN. The generator here is fixed, which is the noise distribution. We are training a special Discriminator.</p> <p>Note: The NCE objective function does not guide us to sample any data. We still need to use something like MCMC (e.g., Langevin dynamics, Hamiltonian Monte Carlo) to generate samples from the trained EBM. NCE only provides a way to train the energy function and partition function without computing the intractable partition function integral.</p> <p>Substituting our discriminator:</p> \\[\\mathcal{L}_{NCE}(\\theta, Z) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log \\frac{e^{f_\\theta(x)}}{e^{f_\\theta(x)} + Z \\cdot p_n(x)} \\right] + \\mathbb{E}_{x \\sim p_n} \\left[ \\log \\frac{Z \\cdot p_n(x)}{e^{f_\\theta(x)} + Z \\cdot p_n(x)} \\right]\\] <p>Using the sigmoid formulation with \\(h_{\\theta, Z}(x) = f_\\theta(x) - \\log p_n(x) - \\log Z\\):</p> \\[\\mathcal{L}_{NCE}(\\theta, Z) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log \\sigma(h_{\\theta, Z}(x)) \\right] + \\mathbb{E}_{x \\sim p_n} \\left[ \\log(1 - \\sigma(h_{\\theta, Z}(x))) \\right]\\] <p>This is exactly the binary cross-entropy loss for a binary classifier that distinguishes between data and noise samples.</p> <p>Training Algorithm:</p> <ol> <li> <p>Sample data batch: \\(\\{x_1, x_2, \\ldots, x_n\\} \\sim p_{data}(x)\\)</p> </li> <li> <p>Sample noise batch: \\(\\{\\tilde{x}_1, \\tilde{x}_2, \\ldots, \\tilde{x}_n\\} \\sim p_n(x)\\)</p> </li> <li> <p>Compute logits: \\(h_{\\theta, Z}(x_i)\\) and \\(h_{\\theta, Z}(\\tilde{x}_i)\\)</p> </li> <li> <p>Compute binary cross-entropy loss</p> </li> <li> <p>Update both \\(\\theta\\) (energy function parameters) and \\(Z\\) (partition function parameter) via gradient descent</p> </li> </ol> <p>Key Advantage: Unlike other EBM training methods (like contrastive divergence), NCE does not require sampling from the EBM during the training process. This eliminates the computational expense and potential instability of MCMC sampling during training, making NCE much more efficient and stable.</p>"},{"location":"AI/deep_generative_models/score_based_models/#comparing-nce-and-gan","title":"Comparing NCE and GAN","text":"<p>Both NCE and GANs use binary classification objectives to train generative models, but they differ significantly in their approach and properties.</p> <p>Similarities</p> <ol> <li>Binary Classification Objective: Both use binary cross-entropy loss to distinguish between real and fake samples</li> <li>No Likelihood Computation: Neither requires computing or maximizing explicit likelihood</li> <li>Stable Training: Both avoid the computational challenges of direct likelihood-based training</li> </ol> <p>Key Differences</p> Aspect NCE GAN Generator Fixed noise distribution \\(p_n(x)\\) Learnable generator network \\(G_\\phi(z)\\) Discriminator Parameterized as EBM: \\(D_{\\theta,Z}(x) = \\frac{e^{f_\\theta(x)}}{e^{f_\\theta(x)} + Z \\cdot p_n(x)}\\) Separate neural network \\(D_\\theta(x)\\) Training Single objective: \\(\\mathcal{L}_{NCE}(\\theta, Z)\\) Min-max game: \\(\\min_G \\max_D \\mathcal{L}_{GAN}(G, D)\\) Sampling Requires MCMC after training Direct sampling via generator Mode Coverage Depends on noise distribution choice Can adapt to cover all data modes Convergence Single optimization problem Requires careful balance between generator and discriminator <p>When to Use Each</p> <p>Use NCE when: - You need interpretable energy functions - Training stability is crucial - You want theoretical guarantees - You can afford MCMC sampling at inference time</p> <p>Use GAN when: - Fast sampling is required - You need high-quality, diverse samples - You have computational resources for adversarial training - You want to avoid MCMC entirely</p>"},{"location":"AI/deep_generative_models/score_based_models/#training-score-based-models","title":"Training Score Based Models","text":"<p>Is Score Matching Limited to EBMs?</p> <p>No, score matching is not limited to Energy-Based Models. We can use score matching for other generative model types as well:</p> <ul> <li>Autoregressive Models: Can be trained using score matching</li> <li>Normalizing Flow Models: Can also be trained using score matching</li> <li>Variational Autoencoders: Score matching can be applied to VAEs</li> </ul> <p>But what's the point since likelihoods are tractable?</p> <p>For models like autoregressive models and normalizing flows, the likelihood is indeed tractable, so we could use maximum likelihood estimation (MLE) instead of score matching. However, in principle, we could still train these models using score matching.</p> <p>Practical Considerations: MLE is often preferred when likelihood is tractable because it's more direct and efficient. Score matching might be useful when the likelihood computation is numerically unstable</p> <p>The core ddea of Score-Based Models</p> <p>The fundamental insight behind score-based models is that instead of modeling the energy function or probability density directly, we model the score function \\(s_\\theta(x)\\).</p> <p>What is the Score Function?</p> <p>The score function is the gradient of the log probability density:</p> \\[s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\] <p>Direct Modeling Approach:</p> <p>Instead of learning an energy function \\(f_\\theta(x)\\) and computing \\(s_\\theta(x) = \\nabla_x f_\\theta(x)\\), we directly model:</p> \\[s_\\theta(x): \\mathbb{R}^d \\rightarrow \\mathbb{R}^d\\] <p>This is a vector-valued function that maps from the data space to the same space, representing the gradient field.</p> <p>Key Properties:</p> <ol> <li>Vector Field: \\(s_\\theta(x)\\) is a vector field that assigns a gradient vector to each point \\(x\\) in the data space</li> <li>No Partition Function: We don't need to compute or approximate the partition function \\(Z(\\theta)\\)</li> <li>Direct Approximation: \\(s_\\theta(x) \\approx \\nabla_x \\log p_{data}(x)\\)</li> </ol> <p></p> <p>Deriving the Score Matching Objective</p> <p>Fisher Divergence objective:</p> <p>We want to minimize the Fisher divergence between the data distribution and the distribution induced by our score function:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x) - s_{data}(x)\\|^2 \\right]\\] <p>This measures how well our learned score function \\(s_\\theta(x)\\) approximates the true score function \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\).</p> <p>The Challenge:</p> <p>We don't have access to \\(s_{data}(x) = \\nabla_x \\log p_{data}(x)\\) since we only have samples from \\(p_{data}(x)\\), not its analytical form.</p> <p>We can rewrite the Fisher divergence to avoid needing the true score function. Let's expand the squared norm:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x)\\|^2 - s_\\theta(x)^T s_{data}(x) + \\frac{1}{2} \\|s_{data}(x)\\|^2 \\right]\\] <p>The key insight is to handle the cross term \\(s_\\theta(x)^T s_{data}(x)\\) using integration by parts.</p> <p>For the univariate case (\\(x \\in \\mathbb{R}\\)), we have:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x) \\cdot s_{data}(x) \\right] = \\int s_\\theta(x) \\cdot \\frac{d}{dx} \\log p_{data}(x) \\cdot p_{data}(x) dx\\] \\[= \\int s_\\theta(x) \\cdot \\frac{d}{dx} p_{data}(x) dx\\] <p>Using integration by parts: \\(\\int u \\cdot \\frac{d}{dx} v \\, dx = u \\cdot v - \\int \\frac{d}{dx} u \\cdot v \\, dx\\)</p> <p>Setting \\(u = s_\\theta(x)\\) and \\(v = p_{data}(x)\\):</p> \\[= \\left. s_\\theta(x) \\cdot p_{data}(x) \\right|_{-\\infty}^{\\infty} - \\int \\frac{d}{dx} s_\\theta(x) \\cdot p_{data}(x) dx\\] <p>Assuming the boundary term vanishes (reasonable for well-behaved distributions):</p> \\[= -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{d}{dx} s_\\theta(x) \\right]\\] <p>Multivariate Case:</p> <p>For \\(x \\in \\mathbb{R}^d\\), we apply integration by parts component-wise:</p> \\[\\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)^T s_{data}(x) \\right] = \\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ s_\\theta(x)_i \\cdot s_{data}(x)_i \\right]\\] \\[= -\\sum_{i=1}^d \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i \\right]\\] \\[= -\\mathbb{E}_{x \\sim p_{data}} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] <p>where \\(\\text{tr}(\\nabla_x s_\\theta(x)) = \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} s_\\theta(x)_i\\) is the trace of the Jacobian matrix.</p> <p>Final Score Matching Objective:</p> <p>Substituting back into the Fisher divergence:</p> \\[\\mathcal{L}_{SM}(\\theta) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\frac{1}{2} \\|s_\\theta(x)\\|^2 + \\text{tr}(\\nabla_x s_\\theta(x)) \\right] + \\text{constant}\\] <p>where the constant term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}} \\left[ \\|s_{data}(x)\\|^2 \\right]\\) doesn't depend on \\(\\theta\\) and can be ignored during optimization.</p> <p>Key Insight:</p> <p>This reformulation allows us to train the score function using only samples from \\(p_{data}(x)\\) and the derivatives of our score model, without needing access to the true score function \\(s_{data}(x)\\).</p> <p>The computational cost of the second term makes score matching challenging for high-dimensional data, which motivates alternative approaches like denoising score matching and sliced score matching.</p>"},{"location":"AI/deep_generative_models/score_based_models/#denoising-score-matching","title":"Denoising Score Matching","text":"<p>Denoising score matching addresses the computational challenges of standard score matching by adding noise to the data.</p> <p>The Key Idea:</p> <p>Instead of trying to learn the score function of the original data distribution \\(p_{data}(x)\\), we learn the score function of a noisy version of the data.</p> <p>Noise Distribution:</p> <p>We define a noise distribution \\(q_\\sigma(\\tilde{x} | x)\\) that adds noise to clean data points. A common choice is Gaussian noise:</p> \\[q_\\sigma(\\tilde{x} | x) = \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)\\] <p>This means: \\(\\tilde{x} = x + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\)</p> <p>Noisy Data Distribution:</p> <p>The noisy data distribution is the convolution of the original data distribution with the noise:</p> \\[q_\\sigma(\\tilde{x}) = \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx\\] <p>The denoising score matching objective minimizes the Fisher divergence between the noise perturbed data distribution \\(q_\\sigma(\\tilde{x})\\) and our score model \\(s_\\theta(\\tilde{x})\\):</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\frac{1}{2} \\|s_\\theta(\\tilde{x}) - \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right]\\] \\[\\mathcal{L}_{DSM}(\\theta) = \\int q_\\sigma(\\tilde{x}) \\left[ \\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2 - s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) + \\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right] d\\tilde{x}\\] <p>Focusing on the Cross Term:</p> <p>The cross term \\(-s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\) is the challenging part. First, let's write the cross term as an integral:</p> \\[\\int q_\\sigma(\\tilde{x}) \\left[ -s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) \\right] d\\tilde{x}\\] <p>Using the chain rule: \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) \\cdot q_\\sigma(\\tilde{x}) = \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x})\\), we get:</p> \\[= -\\int s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x}) d\\tilde{x}\\] <p>Substituting the noisy data distribution:</p> <p>Recall that \\(q_\\sigma(\\tilde{x}) = \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx\\). Substituting this into the integral:</p> \\[= -\\int s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\left[ \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx \\right] d\\tilde{x}\\] <p>Since the gradient operator \\(\\nabla_{\\tilde{x}}\\) acts only on \\(\\tilde{x}\\) and not on \\(x\\), we can interchange the gradient and the integral over \\(x\\):</p> \\[= -\\int s_\\theta(\\tilde{x})^T \\left[ \\int \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x} | x) p_{data}(x) dx \\right] d\\tilde{x}\\] <p>We can rearrange this as a double integral:</p> \\[= -\\iint s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x} | x) \\cdot p_{data}(x) \\, dx \\, d\\tilde{x}\\] <p>Now we can use the chain rule in reverse: \\(\\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x} | x) = \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) \\cdot q_\\sigma(\\tilde{x} | x)\\)</p> \\[= -\\iint s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) \\cdot q_\\sigma(\\tilde{x} | x) \\cdot p_{data}(x) \\, dx \\, d\\tilde{x}\\] <p>Final Expression:</p> <p>This can be written as an expectation:</p> \\[= -\\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) \\right]\\] <p>Or equivalently:</p> \\[= -\\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right]\\] <p>Completing the Denoising Score Matching Objective:</p> <p>Now let's bring this back to the complete objective function. Recall that we started with:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\int q_\\sigma(\\tilde{x}) \\left[ \\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2 - s_\\theta(\\tilde{x})^T \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) + \\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right] d\\tilde{x}\\] <p>We've derived the cross term. Now let's handle all three terms:</p> <p>Term 1: \\(\\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2\\)</p> \\[\\int q_\\sigma(\\tilde{x}) \\cdot \\frac{1}{2} \\|s_\\theta(\\tilde{x})\\|^2 d\\tilde{x} = \\frac{1}{2} \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right]\\] <p>Substituting \\(q_\\sigma(\\tilde{x}) = \\int q_\\sigma(\\tilde{x} | x) p_{data}(x) dx\\):</p> \\[= \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right]\\] <p>Term 2: Cross term (already derived)</p> \\[- \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right]\\] <p>Term 3: \\(\\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2\\)</p> \\[\\int q_\\sigma(\\tilde{x}) \\cdot \\frac{1}{2} \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 d\\tilde{x} = \\frac{1}{2} \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\|^2 \\right]\\] <p>This term is a constant with respect to \\(\\theta\\) and can be ignored during optimization.</p> <p>Combining all terms:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right] - \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right] + \\text{const}\\] <p>Simplifying to the final form:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right] - \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right] + \\text{const}\\] <p>To see how this becomes the final form, let's expand the squared difference:</p> \\[\\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2 = \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\|^2 - 2 \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) + \\|s_\\theta(\\tilde{x})\\|^2\\] <p>Taking the expectation and multiplying by \\(\\frac{1}{2}\\):</p> \\[\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2 \\right] = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|s_\\theta(\\tilde{x})\\|^2 \\right] - \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)^T s_\\theta(\\tilde{x}) \\right] + \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\|^2 \\right]\\] <p>The last term \\(\\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\|^2 \\right]\\) is a constant with respect to \\(\\theta\\) and can be absorbed into the constant term.</p> <p>This can be written as:</p> \\[\\mathcal{L}_{DSM}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x \\sim p_{data}, \\tilde{x} \\sim q_\\sigma(\\tilde{x} | x)} \\left[ \\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2 \\right] + \\text{const}\\] <p>Key Advantage: easy computation of the Target Score Function</p> <p>The major advantage of denoising score matching is that \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\) is easy to compute analytically, unlike the true data score function \\(\\nabla_x \\log p_{data}(x)\\).</p> <p>For Gaussian Noise:</p> <p>The most common choice is Gaussian noise: \\(q_\\sigma(\\tilde{x} | x) = \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)\\)</p> <p>The log probability is:</p> \\[\\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{2\\sigma^2} \\|\\tilde{x} - x\\|^2 - \\frac{d}{2} \\log(2\\pi\\sigma^2)\\] <p>Taking the gradient with respect to \\(\\tilde{x}\\):</p> \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\] <p>This gradient is analytically tractable and computationally cheap.</p> <p>Comparison with Standard Score Matching:</p> <p>In standard score matching, we need to compute:</p> <ul> <li> <p>\\(\\nabla_x \\log p_\\theta(x)\\) (our model's score function)</p> </li> <li> <p>\\(\\text{tr}(\\nabla_x \\nabla_x \\log p_\\theta(x))\\) (Hessian trace - expensive!)</p> </li> </ul> <p>In denoising score matching, we only need:</p> <ul> <li> <p>\\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\) (known analytically)</p> </li> <li> <p>\\(s_\\theta(\\tilde{x})\\) (our model's score function)</p> </li> </ul> <p>Training Algorithm:</p> <ol> <li>Sample clean data: \\(x \\sim p_{data}(x)\\)</li> <li>Add noise: \\(\\tilde{x} = x + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\)</li> <li>Compute target: \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x) = -\\frac{1}{\\sigma^2} \\epsilon\\)</li> <li>Compute prediction: \\(s_\\theta(\\tilde{x})\\)</li> <li>Minimize: \\(\\|\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) - s_\\theta(\\tilde{x})\\|^2\\)</li> </ol> <p>Monte Carlo Estimation:</p> <p>For a batch of \\(N\\) samples \\(\\{x_1, x_2, \\ldots, x_N\\}\\), the Monte Carlo estimate of the loss is:</p> \\[\\mathcal{L}_{DSM}(\\theta) \\approx \\frac{1}{2N} \\sum_{i=1}^N \\|\\nabla_{\\tilde{x}_i} \\log q_\\sigma(\\tilde{x}_i | x_i) - s_\\theta(\\tilde{x}_i)\\|^2\\] <p>where \\(\\tilde{x}_i = x_i + \\epsilon_i\\) with \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I)\\).</p> <p>Practical Implementation:</p> <pre><code># For a batch of data\ndef denoising_score_matching_loss(model, data_batch, sigma):\n    # Add noise to data\n    noise = torch.randn_like(data_batch) * sigma\n    noisy_data = data_batch + noise\n\n    # Compute target score (gradient of log noise distribution)\n    target_score = -noise / (sigma ** 2)\n\n    # Compute model prediction\n    predicted_score = model(noisy_data)\n\n    # Compute loss\n    loss = 0.5 * torch.mean((target_score - predicted_score) ** 2)\n\n    return loss\n</code></pre> <p>Intuition behind the loss function</p> <p>The denoising score matching objective has a beautiful intuition: we're teaching our model to estimate the score function of noisy data, and when the noise is very small, this approximates the score function of the clean data distribution.</p> <p>The Core Idea:</p> <ol> <li> <p>Learning Noisy Data Structure: Instead of trying to learn the score function of the complex, unknown data distribution \\(p_{data}(x)\\), we learn the score function of a simpler, known noisy distribution \\(q_\\sigma(\\tilde{x})\\).</p> </li> <li> <p>Denoising as Learning: By learning to predict \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\), our model learns to \"point\" from noisy points \\(\\tilde{x}\\) back toward their clean counterparts \\(x\\).</p> </li> </ol> <p>What does \"point\" mean?</p> <p>The score function \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x)\\) is a vector field that assigns a direction vector to each point \\(\\tilde{x}\\) in the data space. This vector \"points\" in the direction of steepest increase in the log probability.</p> <p>For Gaussian noise, this vector is:</p> \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\] <p>Interpretation:</p> <ul> <li> <p>Direction: The vector points from the noisy point \\(\\tilde{x}\\) toward the clean point \\(x\\)</p> </li> <li> <p>Magnitude: The length of the vector is proportional to the distance between \\(\\tilde{x}\\) and \\(x\\), scaled by \\(\\frac{1}{\\sigma^2}\\)</p> </li> <li> <p>Purpose: This vector tells us \"if you want to increase the probability of this noisy point, move in this direction\"</p> </li> </ul> <p>Visual Example: Imagine a 2D space where:</p> <ul> <li> <p>Clean point \\(x = (0, 0)\\)</p> </li> <li> <p>Noisy point \\(\\tilde{x} = (1, 1)\\) </p> </li> <li> <p>Noise level \\(\\sigma = 1\\)</p> </li> </ul> <p>The score vector at \\(\\tilde{x}\\) is:</p> \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -(1, 1)\\] <p>This vector points from \\((1, 1)\\) toward \\((0, 0)\\), indicating the direction to move to increase the probability of the noisy point under the noise distribution.</p> <p>Why this matters: When our model learns to predict this vector, it's learning to identify the direction that leads back to the clean data. This implicitly teaches it about the local structure of the data manifold - where the \"good\" data points are located relative to any given noisy point.</p> <p>Why this is a denoiser:</p> <p>The score function \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{1}{\\sigma^2} (\\tilde{x} - x)\\) acts as a denoiser because it provides the exact direction and magnitude needed to remove the noise from a noisy data point.</p> <p>When our model learns to predict this score function, it's learning to:</p> <ul> <li> <p>Identify noise: Recognize what part of the data is noise</p> </li> <li> <p>Compute denoising direction: Determine which direction to move to remove the noise</p> </li> <li> <p>Estimate noise magnitude: Understand how much to move in that direction</p> </li> </ul> <p>This is why denoising score matching is so powerful - by learning to denoise, the model implicitly learns the structure of the clean data distribution, even though it never directly sees the clean data score function.</p> <p>Con: Estimates the score of the noise-perturbed data, not the score of the actual data. We have shifted the goal post basically.</p> <p>Generating Samples with MCMC:</p> <p>Once we've trained our score function \\(s_\\theta(x)\\), we can generate samples using Markov Chain Monte Carlo (MCMC) methods, typically Langevin dynamics.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/","title":"Variational autoencoders","text":""},{"location":"AI/deep_generative_models/variational_autoencoders/#representation","title":"Representation","text":"<p>Consider a directed, latent variable model as shown below.</p> <p></p> <p>In the model above, \\(z\\) and \\(x\\) denote the latent and observed variables respectively. The joint distribution expressed by this model is given as</p> \\[p(x,z) = p(x|z)p(z).\\] <p>From a generative modeling perspective, this model describes a generative process for the observed data \\(x\\) using the following procedure:</p> \\[z \\sim p(z)\\] \\[x \\sim p(x|z)\\] <p>If one adopts the belief that the latent variables \\(z\\) somehow encode semantically meaningful information about \\(x\\), it is natural to view this generative process as first generating the \"high-level\" semantic information about \\(x\\) first before fully generating \\(x\\).</p> <p>We now consider a family of distributions \\(\\mathcal{Z}\\) where \\(p(z) \\in \\mathcal{Z}\\) describes a probability distribution over \\(z\\). Next, consider a family of conditional distributions \\(\\mathcal{X|Z}\\) where \\(p(x|z) \\in \\mathcal{X|Z}\\) describes a conditional probability distribution over \\(x\\) given \\(z\\). Then our hypothesis class of generative models is the set of all possible combinations</p> \\[\\mathcal{X,Z} = \\{p(x,z) \\mid p(z) \\in \\mathcal{Z}, p(x|z) \\in \\mathcal{X|Z}\\}.\\] <p>Given a dataset \\(\\mathcal{D} = \\{x^{(1)}, \\ldots, x^{(n)}\\}\\), we are interested in the following learning and inference tasks:</p> <ol> <li>Selecting \\(p \\in \\mathcal{X,Z}\\) that \"best\" fits \\(\\mathcal{D}\\).</li> <li>Given a sample \\(x\\) and a model \\(p \\in \\mathcal{X,Z}\\), what is the posterior distribution over the latent variables \\(z\\)?</li> </ol> <p>The posterior distribution \\(p(z|x)\\) represents our updated beliefs about the latent variables \\(z\\) after observing the data \\(x\\). In other words, it tells us what values of \\(z\\) are most likely to have generated the observed \\(x\\). This is particularly useful for tasks like feature extraction, where we want to understand what latent factors might have generated our observed data.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#learning-directed-latent-variable-models","title":"Learning Directed Latent Variable Models","text":"<p>One way to measure how closely \\(p(x,z)\\) fits the observed dataset \\(\\mathcal{D}\\) is to measure the Kullback-Leibler (KL) divergence between the data distribution (which we denote as \\(p_{data}(x)\\)) and the model's marginal distribution \\(p(x) = \\int p(x,z)dz\\). The distribution that \"best\" fits the data is thus obtained by minimizing the KL divergence.</p> \\[\\min_{p \\in \\mathcal{X,Z}} D_{KL}(p_{data}(x) \\| p(x)).\\] <p>As we have seen previously, optimizing an empirical estimate of the KL divergence is equivalent to maximizing the marginal log-likelihood \\(\\log p(x)\\) over \\(\\mathcal{D}\\):</p> \\[\\max_{p \\in \\mathcal{X,Z}} \\sum_{x \\in \\mathcal{D}} \\log p(x) = \\max_{p \\in \\mathcal{X,Z}} \\sum_{x \\in \\mathcal{D}} \\log \\int p(x,z)dz.\\] <p>However, it turns out this problem is generally intractable for high-dimensional \\(z\\) as it involves an integration (or sums in the case \\(z\\) is discrete) over all the possible latent sources of variation \\(z\\). This intractability arises from several challenges:</p> <ol> <li> <p>Computational Complexity: The integral \\(\\int p(x,z)dz\\) requires evaluating the joint distribution \\(p(x,z)\\) for all possible values of \\(z\\). In high-dimensional spaces, this becomes computationally prohibitive as the number of points to evaluate grows exponentially with the dimension of \\(z\\).</p> </li> <li> <p>Numerical Integration: Even if we could evaluate the integrand at all points, computing the integral numerically becomes increasingly difficult as the dimension of \\(z\\) grows. Traditional numerical integration methods like quadrature become impractical in high dimensions.</p> </li> <li> <p>Posterior Inference: The intractability of the marginal likelihood also makes it difficult to compute the posterior distribution \\(p(z|x)\\), which is crucial for tasks like feature extraction and data generation.</p> </li> </ol> <p>This intractability motivates the need for approximate inference methods, such as variational inference. One option is to estimate the objective via Monte Carlo. For any given datapoint \\(x\\), we can obtain the following estimate for its marginal log-likelihood:</p> \\[\\log p(x) \\approx \\log \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>This Monte Carlo estimate is derived as follows:</p> <p>First, recall that the marginal likelihood \\(p(x)\\) can be written as an expectation:</p> \\[p(x) = \\int p(x|z)p(z)dz = \\mathbb{E}_{z \\sim p(z)}[p(x|z)]\\] <p>The Monte Carlo method approximates this expectation by drawing \\(k\\) samples from \\(p(z)\\) and computing their average:</p> \\[\\mathbb{E}_{z \\sim p(z)}[p(x|z)] \\approx \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>Taking the logarithm of both sides gives us our final estimate:</p> \\[\\log p(x) \\approx \\log \\frac{1}{k} \\sum_{i=1}^k p(x|z^{(i)}), \\text{ where } z^{(i)} \\sim p(z)\\] <p>This approximation becomes more accurate as \\(k\\) increases, but at the cost of more computational resources. The key insight is that we're using random sampling to approximate the intractable integral, trading exact computation for statistical estimation.</p> <p>Rather than maximizing the log-likelihood directly, an alternate is to instead construct a lower bound that is more amenable to optimization. To do so, we note that evaluating the marginal likelihood \\(p(x)\\) is at least as difficult as as evaluating the posterior \\(p(z|x)\\) for any latent vector \\(z\\) since by definition \\(p(z|x) = p(x,z)/p(x)\\).</p> <p>Next, we introduce a variational family \\(\\mathcal{Q}\\) of distributions that approximate the true, but intractable posterior \\(p(z|x)\\). Further henceforth, we will assume a parameteric setting where any distribution in the model family \\(\\mathcal{X,Z}\\) is specified via a set of parameters \\(\\theta \\in \\Theta\\) and distributions in the variational family \\(\\mathcal{Q}\\) are specified via a set of parameters \\(\\lambda \\in \\Lambda\\).</p> <p>Given \\(\\mathcal{X,Z}\\) and \\(\\mathcal{Q}\\), we note that the following relationships hold true for any \\(x\\) and all variational distributions \\(q_\\lambda(z) \\in \\mathcal{Q}\\):</p> \\[\\log p_\\theta(x) = \\log \\int p_\\theta(x,z)dz = \\log \\int \\frac{q_\\lambda(z)}{q_\\lambda(z)}p_\\theta(x,z)dz \\geq \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] := \\text{ELBO}(x;\\theta,\\lambda)\\] <p>where we have used Jensen's inequality in the final step. The key insight here is that since the logarithm function is concave, Jensen's inequality tells us that for any random variable \\(X\\) and concave function \\(f\\), we have \\(\\mathbb{E}[f(X)] \\leq f(\\mathbb{E}[X])\\). In our case:</p> <p>We first multiply and divide by \\(q_\\lambda(z)\\) inside the integral to get:</p> \\[\\log \\int \\frac{q_\\lambda(z)}{q_\\lambda(z)}p_\\theta(x,z)dz = \\log \\int q_\\lambda(z)\\frac{p_\\theta(x,z)}{q_\\lambda(z)}dz\\] <p>The integral \\(\\int q_\\lambda(z)\\frac{p_\\theta(x,z)}{q_\\lambda(z)}dz\\) can be seen as an expectation \\(\\mathbb{E}_{q_\\lambda(z)}\\left[\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\)</p> <p>Since \\(\\log\\) is a concave function, Jensen's inequality gives us:</p> \\[\\log \\mathbb{E}_{q_\\lambda(z)}\\left[\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] \\geq \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\] <p>This inequality is what allows us to obtain a lower bound on the log-likelihood, which we call the Evidence Lower BOund (ELBO). The ELBO admits a tractable unbiased Monte Carlo estimator</p> \\[\\frac{1}{k}\\sum_{i=1}^k \\log\\frac{p_\\theta(x,z^{(i)})}{q_\\lambda(z^{(i)})}, \\text{ where } z^{(i)} \\sim q_\\lambda(z),\\] <p>so long as it is easy to sample from and evaluate densities for \\(q_\\lambda(z)\\).</p> <p>In summary, we can learn a latent variable model by maximizing the ELBO with respect to both the model parameters \\(\\theta\\) and the variational parameters \\(\\lambda\\) for any given datapoint \\(x\\):</p> \\[\\max_\\theta \\sum_{x \\in \\mathcal{D}} \\max_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right].\\] <p>This optimization objective can be broken down into two parts:</p> <ol> <li>Inner Optimization: For each datapoint \\(x\\), we find the best variational parameters \\(\\lambda\\) that make \\(q_\\lambda(z)\\) as close as possible to the true posterior \\(p(z|x)\\). This is done by maximizing the ELBO with respect to \\(\\lambda\\). </li> </ol> <p>Why do we need \\(q_\\lambda(z)\\) to approximate \\(p(z|x)\\)? Since \\(p(x) = p(x,z)/p(z|x)\\), as \\(q_\\lambda(z)\\) tends to \\(p(z|x)\\), the ratio \\(p(x,z)/q_\\lambda(z)\\) tends to \\(p(x)\\). This means that by making our variational approximation closer to the true posterior, we get a better estimate of the marginal likelihood \\(p(x)\\).</p> <ol> <li>Outer Optimization: Across all datapoints in the dataset \\(\\mathcal{D}\\), we find the best model parameters \\(\\theta\\) that maximize the average ELBO. This improves the generative model's ability to explain the data.</li> </ol> <p>The outer sum \\(\\sum_{x \\in \\mathcal{D}}\\) is necessary because we want to learn a model that works well for all datapoints in our dataset, not just a single example. This is equivalent to maximizing the average ELBO across all datapoints.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#black-box-variational-inference","title":"Black-Box Variational Inference","text":"<p>We shall focus on first-order stochastic gradient methods for optimizing the ELBO. This inspires Black-Box Variational Inference (BBVI), a general-purpose Expectation-Maximization-like algorithm for variational learning of latent variable models, where, for each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), the following two steps are performed.</p> <p>Step 1</p> <p>We first do per-sample optimization of \\(q\\) by iteratively applying the update</p> \\[\\lambda^{(i)} \\leftarrow \\lambda^{(i)} + \\tilde{\\nabla}_\\lambda \\text{ELBO}(x^{(i)}; \\theta, \\lambda^{(i)}),\\] <p>where \\(\\text{ELBO}(x; \\theta, \\lambda) = \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right]\\), and \\(\\tilde{\\nabla}_\\lambda\\) denotes an unbiased estimate of the ELBO gradient. This step seeks to approximate the log-likelihood \\(\\log p_\\theta(x^{(i)})\\).</p> <p>Step 2</p> <p>We then perform a single update step based on the mini-batch</p> \\[\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_i \\text{ELBO}(x^{(i)}; \\theta, \\lambda^{(i)}),\\] <p>which corresponds to the step that hopefully moves \\(p_\\theta\\) closer to \\(p_{data}\\).</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#gradient-estimation","title":"Gradient Estimation","text":"<p>The gradients \\(\\nabla_\\lambda \\text{ELBO}\\) and \\(\\nabla_\\theta \\text{ELBO}\\) can be estimated via Monte Carlo sampling. While it is straightforward to construct an unbiased estimate of \\(\\nabla_\\theta \\text{ELBO}\\) by simply pushing \\(\\nabla_\\theta\\) through the expectation operator, the same cannot be said for \\(\\nabla_\\lambda\\). Instead, we see that</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\mathbb{E}_{q_\\lambda(z)}\\left[\\left(\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z)\\right].\\] <p>This equality follows from the log-derivative trick (also commonly referred to as the REINFORCE trick). To derive this, we start with the gradient of the expectation:</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\nabla_\\lambda \\int q_\\lambda(z) \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>Using the product rule and chain rule:</p> \\[= \\int \\nabla_\\lambda q_\\lambda(z) \\cdot \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} + q_\\lambda(z) \\cdot \\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>The second term vanishes because: \\(\\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} = \\nabla_\\lambda [\\log p_\\theta(x,z) - \\log q_\\lambda(z)]\\). Since \\(p_\\theta(x,z)\\) doesn't depend on \\(\\lambda\\), \\(\\nabla_\\lambda \\log p_\\theta(x,z) = 0\\). Therefore, \\(\\nabla_\\lambda \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} = -\\nabla_\\lambda \\log q_\\lambda(z)\\).  When we multiply by \\(q_\\lambda(z)\\) and integrate, we get:</p> \\[\\int q_\\lambda(z) \\cdot (-\\nabla_\\lambda \\log q_\\lambda(z)) dz = -\\int \\nabla_\\lambda q_\\lambda(z) dz = -\\nabla_\\lambda \\int q_\\lambda(z) dz = -\\nabla_\\lambda 1 = 0\\] <p>where we used the fact that \\(\\int q_\\lambda(z) dz = 1\\) for any valid probability distribution.</p> <p>For the first term, we use the identity \\(\\nabla_\\lambda q_\\lambda(z) = q_\\lambda(z) \\nabla_\\lambda \\log q_\\lambda(z)\\):</p> \\[= \\int q_\\lambda(z) \\nabla_\\lambda \\log q_\\lambda(z) \\cdot \\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)} dz\\] <p>This can be rewritten as an expectation:</p> \\[= \\mathbb{E}_{q_\\lambda(z)}\\left[\\left(\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z)\\right]\\] <p>The gradient estimator \\(\\tilde{\\nabla}_\\lambda \\text{ELBO}\\) is thus</p> \\[\\frac{1}{k}\\sum_{i=1}^k \\left[\\left(\\log\\frac{p_\\theta(x,z^{(i)})}{q_\\lambda(z^{(i)})}\\right) \\cdot \\nabla_\\lambda \\log q_\\lambda(z^{(i)})\\right], \\text{ where } z^{(i)} \\sim q_\\lambda(z).\\] <p>However, it is often noted that this estimator suffers from high variance. One of the key contributions of the variational autoencoder paper is the reparameterization trick, which introduces a fixed, auxiliary distribution \\(p(\\epsilon)\\) and a differentiable function \\(T(\\epsilon; \\lambda)\\) such that the procedure</p> \\[\\epsilon \\sim p(\\epsilon)\\] \\[z \\leftarrow T(\\epsilon; \\lambda),\\] <p>is equivalent to sampling from \\(q_\\lambda(z)\\). This two-step procedure works as follows:</p> <ol> <li>First, we sample \\(\\epsilon\\) from a fixed distribution \\(p(\\epsilon)\\) that doesn't depend on \\(\\lambda\\) (e.g., standard normal)</li> <li>Then, we transform this sample using a deterministic function \\(T(\\epsilon; \\lambda)\\) that depends on \\(\\lambda\\)</li> </ol> <p>The key insight is that if we choose \\(T\\) appropriately, the distribution of \\(z = T(\\epsilon; \\lambda)\\) will be exactly \\(q_\\lambda(z)\\). For example, if \\(q_\\lambda(z)\\) is a normal distribution with mean \\(\\mu_\\lambda\\) and standard deviation \\(\\sigma_\\lambda\\), we can use:</p> <p>\\(p(\\epsilon) = \\mathcal{N}(0, 1)\\)</p> <p>\\(T(\\epsilon; \\lambda) = \\mu_\\lambda + \\sigma_\\lambda \\cdot \\epsilon\\)</p> <p>By the Law of the Unconscious Statistician, we can see that</p> \\[\\nabla_\\lambda \\mathbb{E}_{q_\\lambda(z)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\lambda(z)}\\right] = \\mathbb{E}_{p(\\epsilon)}\\left[\\nabla_\\lambda \\log\\frac{p_\\theta(x,T(\\epsilon; \\lambda))}{q_\\lambda(T(\\epsilon; \\lambda))}\\right].\\] <p>In contrast to the REINFORCE trick, the reparameterization trick is often noted empirically to have lower variance and thus results in more stable training.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#parameterizing-distributions-via-deep-neural-networks","title":"Parameterizing Distributions via Deep Neural Networks","text":"<p>So far, we have described \\(p_\\theta(x,z)\\) and \\(q_\\lambda(z)\\) in the abstract. To instantiate these objects, we consider choices of parametric distributions for \\(p_\\theta(z)\\), \\(p_\\theta(x|z)\\), and \\(q_\\lambda(z)\\). A popular choice for \\(p_\\theta(z)\\) is the unit Gaussian</p> \\[p_\\theta(z) = \\mathcal{N}(z|0,I),\\] <p>in which case \\(\\theta\\) is simply the empty set since the prior is a fixed distribution.</p> <p>In the case where \\(p_\\theta(x|z)\\) is a Gaussian distribution, we can thus represent it as</p> \\[p_\\theta(x|z) = \\mathcal{N}(x|\\mu_\\theta(z), \\Sigma_\\theta(z)),\\] <p>where \\(\\mu_\\theta(z)\\) and \\(\\Sigma_\\theta(z)\\) are neural networks that specify the mean and covariance matrix for the Gaussian distribution over \\(x\\) when conditioned on \\(z\\).</p> <p>Finally, the variational family for the proposal distribution \\(q_\\lambda(z)\\) needs to be chosen judiciously so that the reparameterization trick is possible. Many continuous distributions in the location-scale family can be reparameterized. In practice, a popular choice is again the Gaussian distribution, where</p> \\[\\begin{align*} \\lambda &amp;= (\\mu, \\Sigma) \\\\ q_\\lambda(z) &amp;= \\mathcal{N}(z|\\mu, \\Sigma) \\\\ p(\\varepsilon) &amp;= \\mathcal{N}(z|0,I) \\\\ T(\\varepsilon; \\lambda) &amp;= \\mu + \\Sigma^{1/2}\\varepsilon, \\end{align*}\\] <p>where \\(\\Sigma^{1/2}\\) is the Cholesky decomposition of \\(\\Sigma\\). For simplicity, practitioners often restrict \\(\\Sigma\\) to be a diagonal matrix (which restricts the distribution family to that of factorized Gaussians).</p> <p>The reparameterization trick consists of four key steps:</p> <ol> <li> <p>Parameter Definition: We define the variational parameters \\(\\lambda\\) as a tuple containing the mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\) of our Gaussian distribution. These parameters will be learned during training.</p> </li> <li> <p>Variational Distribution: We specify that our variational distribution \\(q_\\lambda(z)\\) is a Gaussian distribution parameterized by \\(\\mu\\) and \\(\\Sigma\\). This is the distribution we ideally want to sample from.</p> </li> <li> <p>Auxiliary Distribution: Instead of sampling directly from \\(q_\\lambda(z)\\), we introduce a fixed auxiliary distribution \\(p(\\varepsilon)\\) which is a standard normal distribution (mean 0, identity covariance). This distribution doesn't depend on our parameters \\(\\lambda\\).</p> </li> <li> <p>Transformation Function: We define a deterministic function \\(T(\\varepsilon; \\lambda)\\) that transforms samples from the auxiliary distribution into samples from our variational distribution. The transformation is given by \\(\\mu + \\Sigma^{1/2}\\varepsilon\\), where \\(\\Sigma^{1/2}\\) is the Cholesky decomposition of \\(\\Sigma\\).</p> </li> </ol> <p>The key insight is that instead of sampling directly from \\(q_\\lambda(z)\\), we can: 1. Sample \\(\\varepsilon\\) from the standard normal distribution \\(p(\\varepsilon)\\) 2. Transform it using \\(T(\\varepsilon; \\lambda)\\) to make it seem like we're getting a sample from \\(q_\\lambda(z)\\)</p> <p>This trick is crucial because it allows us to compute gradients with respect to \\(\\lambda\\) through the sampling process. Since the transformation \\(T\\) is differentiable, we can backpropagate through it to update the parameters \\(\\lambda\\) during training. This is why the reparameterization trick often leads to lower variance in gradient estimates compared to the REINFORCE trick.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#amortized-variational-inference","title":"Amortized Variational Inference","text":"<p>A noticeable limitation of black-box variational inference is that Step 1 executes an optimization subroutine that is computationally expensive. Recall that the goal of Step 1 is to find</p> \\[\\lambda^* = \\arg\\max_{\\lambda \\in \\Lambda} \\text{ELBO}(x; \\theta, \\lambda).\\] <p>For a given choice of \\(\\theta\\), there is a well-defined mapping from \\(x \\mapsto \\lambda^*\\). A key realization is that this mapping can be learned. In particular, one can train an encoding function (parameterized by \\(\\phi\\)) \\(f_\\phi: \\mathcal{X} \\to \\Lambda\\) (where \\(\\Lambda\\) is the space of \\(\\lambda\\) parameters) on the following objective</p> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, f_\\phi(x)).\\] <p>It is worth noting at this point that \\(f_\\phi(x)\\) can be interpreted as defining the conditional distribution \\(q_\\phi(z|x)\\). With a slight abuse of notation, we define</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right],\\] <p>and rewrite the optimization problem as</p> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, \\phi).\\] <p>It is also worth noting that optimizing \\(\\phi\\) over the entire dataset as a subroutine every time we sample a new mini-batch is clearly not reasonable. However, if we believe that \\(f_\\phi\\) is capable of quickly adapting to a close-enough approximation of \\(\\lambda^*\\) given the current choice of \\(\\theta\\), then we can interleave the optimization of \\(\\phi\\) and \\(\\theta\\). This yields the following procedure, where for each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), we perform the following two updates jointly:</p> \\[\\begin{align*} \\phi &amp;\\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\\\ \\theta &amp;\\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi), \\end{align*}\\] <p>rather than running BBVI's Step 1 as a subroutine. By leveraging the learnability of \\(x \\mapsto \\lambda^*\\), this optimization procedure amortizes the cost of variational inference. If one further chooses to define \\(f_\\phi\\) as a neural network, the result is the variational autoencoder.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#steps-of-amortized-variational-inference","title":"Steps of Amortized Variational Inference","text":"<p>Let's break down the amortized variational inference procedure in detail:</p> <ol> <li>Initial Setup:</li> <li>We have a dataset \\(\\mathcal{D} = \\{x^{(1)}, \\ldots, x^{(n)}\\}\\)</li> <li>We have a generative model \\(p_\\theta(x,z)\\) with parameters \\(\\theta\\)</li> <li> <p>We want to learn both the model parameters \\(\\theta\\) and the variational parameters \\(\\lambda\\) for each datapoint</p> </li> <li> <p>Traditional BBVI Approach:</p> </li> <li>For each datapoint \\(x\\), we would need to run an optimization to find:</li> </ol> \\[\\lambda^* = \\arg\\max_{\\lambda \\in \\Lambda} \\text{ELBO}(x; \\theta, \\lambda)\\] <ul> <li> <p>This is computationally expensive as it requires running an optimization subroutine for each datapoint</p> </li> <li> <p>Key Insight - Learnable Mapping:</p> </li> <li>Instead of optimizing \\(\\lambda\\) separately for each \\(x\\), we realize that there's a mapping from \\(x\\) to \\(\\lambda^*\\)</li> <li>This mapping can be learned using a function \\(f_\\phi: \\mathcal{X} \\to \\Lambda\\) parameterized by \\(\\phi\\)</li> <li> <p>The function \\(f_\\phi\\) takes a datapoint \\(x\\) and outputs the variational parameters \\(\\lambda\\)</p> </li> <li> <p>Training the Encoder:</p> </li> <li>We train \\(f_\\phi\\) to maximize the ELBO across all datapoints:</li> </ul> \\[\\max_\\phi \\sum_{x \\in \\mathcal{D}} \\text{ELBO}(x; \\theta, f_\\phi(x))\\] <ul> <li> <p>This is equivalent to learning a conditional distribution \\(q_\\phi(z|x)\\)</p> </li> <li> <p>Joint Optimization:</p> </li> <li>Instead of running BBVI's Step 1 as a subroutine, we interleave the optimization of \\(\\phi\\) and \\(\\theta\\)</li> <li>For each mini-batch \\(\\mathcal{B} = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\), we perform two updates:</li> </ul> \\[\\begin{align*} \\phi &amp;\\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\\\ \\theta &amp;\\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi) \\end{align*}\\] <ol> <li>Practical Implementation:</li> <li>When \\(f_\\phi\\) is implemented as a neural network, we get a variational autoencoder</li> <li>The encoder network \\(f_\\phi\\) maps inputs \\(x\\) to variational parameters</li> <li>The decoder network maps latent variables \\(z\\) to reconstructed inputs</li> <li>Both networks are trained end-to-end using the ELBO objective</li> </ol> <p>In practice, the encoder neural network \\(f_\\phi\\) outputs the parameters of a diagonal Gaussian distribution:</p> \\[q_\\phi(z|x) = \\mathcal{N}(z|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))\\] <p>where \\(\\mu_\\phi(x)\\) and \\(\\sigma^2_\\phi(x)\\) are the mean and variance vectors output by the encoder network. To sample from this distribution during training, we use the reparameterization trick:</p> \\[z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,I)\\] <p>where \\(\\odot\\) denotes element-wise multiplication. This allows us to backpropagate through the sampling process and train the encoder network end-to-end.</p> <p>The key advantage of this approach is that it amortizes the cost of variational inference by learning a single function \\(f_\\phi\\) that can quickly approximate the optimal variational parameters for any input \\(x\\), rather than running a separate optimization for each datapoint.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#decomposition-of-the-negative-elbo","title":"Decomposition of the Negative ELBO","text":"<p>Starting with the definition of the ELBO:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right]\\] <p>We can expand the joint distribution \\(p_\\theta(x,z)\\) using the chain rule of probability:</p> \\[p_\\theta(x,z) = p_\\theta(x|z)p_\\theta(z)\\] <p>Substituting this into the ELBO:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{p_\\theta(x|z)p_\\theta(z)}{q_\\phi(z|x)}\\right]\\] <p>Using the properties of logarithms, we can split this into three terms:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)]\\] <p>The second and third terms can be combined to form the KL divergence between \\(q_\\phi(z|x)\\) and \\(p_\\theta(z)\\):</p> \\[\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)] = -\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log\\frac{q_\\phi(z|x)}{p_\\theta(z)}\\right] = -D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\] <p>Therefore, the ELBO can be written as:</p> \\[\\text{ELBO}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\] <p>It is insightful to note that the negative ELBO can be decomposed into two terms:</p> \\[-\\text{ELBO}(x; \\theta, \\phi) = \\underbrace{-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{Reconstruction Loss}} + \\underbrace{D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))}_{\\text{KL Divergence}}\\] <p>This decomposition reveals two key components of the training objective:</p> <ol> <li>Reconstruction Loss: \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\)</li> <li>This term measures how well the model can reconstruct the input \\(x\\) from its latent representation \\(z\\)</li> <li>It encourages the encoder to produce latent codes that preserve the essential information about the input</li> <li> <p>In practice, this is often implemented as the mean squared error or binary cross-entropy between the input and its reconstruction</p> </li> <li> <p>KL Divergence: \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\)</p> </li> <li>This term measures how far the approximate posterior \\(q_\\phi(z|x)\\) is from the prior \\(p_\\theta(z)\\)</li> <li>It encourages the latent space to follow the prior distribution (typically a standard normal distribution)</li> </ol>"},{"location":"AI/deep_generative_models/variational_autoencoders/#practical-implementation-of-elbo-computation","title":"Practical Implementation of ELBO Computation","text":"<p>Let's look at how the ELBO is actually computed in practice. Here's a detailed implementation with explanations:</p> <p>We implement the (rec+kl) decomposed form for practicality and clarity because:</p> <ul> <li>KL has a closed form (for two Gaussians \\(q_\\phi(z|x) \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and \\(p(z) \\sim \\mathcal{N}(0,I)\\), the KL term can be computed analytically). A closed form means we can compute the exact value using a finite number of standard operations (addition, multiplication, logarithms, etc.) without needing numerical integration or approximation. This closed form is derived as follows:</li> </ul> <p>For two multivariate Gaussians \\(q_\\phi(z|x) = \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(z) = \\mathcal{N}(0,I)\\), the KL divergence is:</p> \\[D_{KL}(q_\\phi(z|x) \\| p(z)) = \\frac{1}{2}\\left[\\text{tr}(\\Sigma) + \\mu^T\\mu - d - \\log|\\Sigma|\\right]\\] <p>where \\(\\text{tr}(\\Sigma)\\) is the trace of the covariance matrix \\(\\Sigma\\) (the sum of its diagonal elements), \\(\\mu^T\\mu\\) is the squared L2 norm of the mean vector, \\(d\\) is the dimension of the latent space, and \\(|\\Sigma|\\) is the determinant of \\(\\Sigma\\). For diagonal covariance matrices \\(\\Sigma = \\text{diag}(\\sigma^2)\\), this simplifies to:</p> \\[D_{KL}(q_\\phi(z|x) \\| p(z)) = \\frac{1}{2}\\sum_{i=1}^d (\\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1)\\] <p>This analytical solution is not only computationally efficient but also provides exact gradients, unlike Monte Carlo estimates which would require sampling.</p> <ul> <li> <p>The analytical KL avoids noisy gradients that arise from computing KL via sampling so the decomposition makes training more stable. When using Monte Carlo estimation, the gradients can have high variance due to the randomness in sampling. The analytical form provides deterministic gradients, which leads to more stable optimization. This is particularly important because the KL term acts as a regularizer, and having stable gradients for this term helps prevent the model from either collapsing to a degenerate solution (where the KL term becomes too small) or failing to learn meaningful representations (where the KL term dominates).</p> </li> <li> <p>The decomposed form allows you to monitor reconstruction loss and KL separately which is very helpful in debugging and understanding model behavior</p> </li> </ul> <pre><code>def negative_elbo_bound(self, x):\n    \"\"\"\n    Computes the Evidence Lower Bound, KL and, Reconstruction costs\n\n    Args:\n        x: tensor: (batch, dim): Observations\n\n    Returns:\n        nelbo: tensor: (): Negative evidence lower bound\n        kl: tensor: (): ELBO KL divergence to prior\n        rec: tensor: (): ELBO Reconstruction term\n    \"\"\"\n    # Step 1: Get the parameters of the approximate posterior q_phi(z|x)\n    q_phi_z_given_x_m, q_phi_z_given_x_v = self.enc(x)\n\n    # Step 2: Compute the KL divergence term\n    # This computes D_KL(q_phi(z|x) || p_theta(z))\n    kl = ut.kl_normal(q_phi_z_given_x_m, q_phi_z_given_x_v,\n                      self.z_prior_m, self.z_prior_v)\n\n    # Step 3: Take m samples from the approximate posterior using reparameterization\n    # This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)\n    z_samples = ut.sample_gaussian(\n        q_phi_z_given_x_m.expand(x.shape[0], self.z_dim),\n        q_phi_z_given_x_v.expand(x.shape[0], self.z_dim))\n\n    # Step 4: Get the decoder outputs (logits)\n    # These parameterize the Bernoulli distributions for reconstruction\n    f_theta_of_z = self.dec(z_samples)\n\n    # Step 5: Compute the reconstruction term\n    # This computes -E_q[log p_theta(x|z)] using binary cross-entropy\n    rec = -ut.log_bernoulli_with_logits(x, f_theta_of_z)\n\n    # Step 6: Combine terms to get the negative ELBO\n    nelbo = kl + rec\n\n    # Step 7: Average over the batch\n    nelbo_avg = torch.mean(nelbo)\n    kl_avg = torch.mean(kl)\n    rec_avg = torch.mean(rec)\n\n    return nelbo_avg, kl_avg, rec_avg\n</code></pre> <p>Let's break down each step:</p> <ol> <li>Encoder Output: </li> <li>The encoder network takes input \\(x\\) and outputs the parameters of the approximate posterior \\(q_\\phi(z|x)\\)</li> <li> <p>These parameters are the mean (\\(\\mu_\\phi(x)\\)) and variance (\\(\\sigma^2_\\phi(x)\\)) of a diagonal Gaussian</p> </li> <li> <p>KL Divergence:</p> </li> <li>Computes \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\)</li> <li>For diagonal Gaussians, this has a closed-form solution</li> <li> <p>The prior \\(p_\\theta(z)\\) is typically a standard normal distribution</p> </li> <li> <p>Sampling:</p> </li> <li>Uses the reparameterization trick to sample from \\(q_\\phi(z|x)\\)</li> <li>Implements \\(z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon\\) where \\(\\varepsilon \\sim \\mathcal{N}(0,I)\\)</li> <li> <p>The samples are used to estimate the reconstruction term</p> </li> <li> <p>Decoder Output:</p> </li> <li>The decoder network takes the sampled \\(z\\) and outputs logits</li> <li> <p>These logits parameterize Bernoulli distributions for each element of \\(x\\)</p> </li> <li> <p>Reconstruction Term:</p> </li> <li>Computes \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\)</li> <li>Uses binary cross-entropy loss which takes logits directly</li> <li> <p>The sigmoid function is incorporated into the loss computation</p> </li> <li> <p>Final ELBO:</p> </li> <li>Combines the KL divergence and reconstruction terms</li> <li> <p>The negative ELBO is what we minimize during training</p> </li> <li> <p>Batch Averaging:</p> </li> <li>Averages the losses over the batch</li> <li>This gives us the final training objective</li> </ol> <p>This implementation shows how the theoretical ELBO decomposition we discussed earlier is actually computed in practice, with all the necessary components for training a VAE on binary data.</p> <p>Note on Sampling from \\(q_\\phi(z|x)\\): The sampling step in the implementation is crucial for two reasons:</p> <p>Monte Carlo Estimation: The reconstruction term \\(-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\) involves an expectation over \\(q_\\phi(z|x)\\). We estimate this expectation using Monte Carlo sampling:</p> \\[-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] \\approx -\\frac{1}{K}\\sum_{k=1}^K \\log p_\\theta(x|z^{(k)})\\] <p>where \\(z^{(k)} \\sim q_\\phi(z|x)\\). In practice, we often use \\(K=1\\) (a single sample) as it works well and is computationally efficient.</p> <p>Gradient Estimation: We need to compute gradients of this expectation with respect to both \\(\\phi\\) (encoder parameters) and \\(\\theta\\) (decoder parameters). The reparameterization trick allows us to: - Sample from a fixed distribution \\(p(\\varepsilon)\\) that doesn't depend on \\(\\phi\\) - Transform these samples using a deterministic function that depends on \\(\\phi\\) - Backpropagate through this transformation to compute gradients - This results in lower variance gradient estimates compared to the REINFORCE trick</p> <p>The sampling step is therefore essential for both estimating the ELBO and computing its gradients during training.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#-vae","title":"\u03b2-VAE","text":"<p>A popular variation of the normal VAE is called the \u03b2-VAE. The \u03b2-VAE optimizes the following objective:</p> \\[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\beta D_{KL}(q_\\phi(z|x) || p(z)) \\] <p>Here, \u03b2 is a positive real number. From a training objective, we want to decrease the negative of ELBO, also called NELBO:</p> \\[ \\text{NELBO} = \\text{Reconstruction Loss} + \\beta D_{KL}(q_\\phi(z|x) \\| p(z)) \\] <p>We see that the second term acts as a regularization term. \u03b2 can be thought of as a hyperparameter that adjusts how much we want to regularize. Greater the \u03b2, more is the training optimized to reduce KL divergence, and a higher possibility of overfitting (and also more the \\(q_\\phi(z|x)\\) closely approximates \\(p(z)\\)). Lesser the \u03b2, optimization is geared towards increasing the KL divergence, leading to a more general model. When \u03b2 is set to 1 however, we get the standard VAE.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#importance-weighted-autoencoder-iwae","title":"Importance Weighted Autoencoder (IWAE)","text":"<p>While the ELBO serves as a lower bound to the true marginal log-likelihood, it may be loose if the variational posterior \\(q_\\phi(z|x)\\) is a poor approximation to the true posterior \\(p_\\theta(z|x)\\). The key idea behind IWAE is to use \\(m &gt; 1\\) samples from the approximate posterior \\(q_\\phi(z|x)\\) to obtain the following IWAE bound:</p> \\[ \\mathcal{L}_m(x; \\theta,\\phi) = \\mathbb{E}_{z^{(1)},...,z^{(m)} \\text{ i.i.d.} \\sim q_\\phi(z|x)} \\log \\frac{1}{m}\\sum_{i=1}^m \\frac{p_\\theta(x,z^{(i)})}{q_\\phi(z^{(i)}|x)} \\] <p>Notice that for the special case of \\(m=1\\), the IWAE objective \\(\\mathcal{L}_m\\) reduces to the standard ELBO \\(\\mathcal{L}_1 = \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\).</p> <p>As a pseudocode, the main modification to the standard VAE would be:</p> <pre><code># Step 3: Take m samples from the approximate posterior using reparameterization\n# This implements z = mu + sigma * epsilon, where epsilon ~ N(0,I)\nz_samples = ut.sample_gaussian(\n    q_phi_z_given_x_m.expand(x.shape[0], self.z_dim),\n    q_phi_z_given_x_v.expand(x.shape[0], self.z_dim))\n</code></pre>"},{"location":"AI/deep_generative_models/variational_autoencoders/#gaussian-mixture-vae-gmvae","title":"Gaussian Mixture VAE (GMVAE)","text":"<p>The VAE's prior distribution was a parameter-free isotropic Gaussian \\(p_\\theta(z) = \\mathcal{N}(z|0,I)\\). While this original setup works well, there are settings in which we desire more expressivity to better model our data. Let's look at GMVAE, which has a mixture of Gaussians as the prior distribution.</p> \\[p_\\theta(z) = \\sum_{i=1}^k \\frac{1}{k}\\mathcal{N}(z|\\mu_i, \\text{diag}(\\sigma^2_i))\\] <p>where \\(i \\in \\{1, ..., k\\}\\) denotes the \\(i\\)th cluster index. For notational simplicity, we shall subsume our mixture of Gaussian parameters \\(\\{\\mu_i, \\sigma_i\\}_{i=1}^k\\) into our generative model parameters \\(\\theta\\). For simplicity, we have also assumed fixed uniform weights \\(1/k\\) over the possible different clusters.</p> <p>Apart from the prior, the GMVAE shares an identical setup as the VAE:</p> \\[q_\\phi(z|x) = \\mathcal{N}(z|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))\\] \\[p_\\theta(x|z) = \\text{Bern}(x|f_\\theta(z))\\] <p>Although the ELBO for the GMVAE: \\(\\mathbb{E}_{q_\\phi(z)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\) is identical to that of the VAE, we note that the KL term \\(D_{KL}(q_\\phi(z|x) \\| p_\\theta(z))\\) cannot be computed analytically between a Gaussian distribution \\(q_\\phi(z|x)\\) and a mixture of Gaussians \\(p_\\theta(z)\\). However, we can obtain its unbiased estimator via Monte Carlo sampling:</p> \\[D_{KL}(q_\\phi(z|x) \\| p_\\theta(z)) \\approx \\log q_\\phi(z^{(1)}|x) - \\log p_\\theta(z^{(1)})\\] \\[= \\underbrace{\\log\\mathcal{N}(z^{(1)}|\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))}_{\\text{log normal}} - \\underbrace{\\log\\sum_{i=1}^k \\frac{1}{k}\\mathcal{N}(z^{(1)}|\\mu_i, \\text{diag}(\\sigma^2_i))}_{\\text{log normal mixture}}\\] <p>where \\(z^{(1)} \\sim q_\\phi(z|x)\\) denotes a single sample.</p>"},{"location":"AI/deep_generative_models/variational_autoencoders/#the-semi-supervised-vae-ssvae","title":"The Semi-Supervised VAE (SSVAE)","text":"<p>The Semi-Supervised VAE (SSVAE) extends the standard VAE to handle both labeled and unlabeled data. In a semi-supervised setting, we have a dataset \\(\\mathcal{D}\\) that consists of: - Labeled data: \\(\\mathcal{D}_l = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N_l}\\) - Unlabeled data: \\(\\mathcal{D}_u = \\{x^{(i)}\\}_{i=1}^{N_u}\\)</p> <p>where \\(y^{(i)}\\) represents the class label for the \\(i\\)-th labeled example. The SSVAE introduces an additional latent variable \\(y\\) to model the class labels, and the joint distribution is factorized as:</p> \\[ p_\\theta(x, y, z) = p_\\theta(x|y,z)p_\\theta(y|z)p_\\theta(z) \\] <p>This factorization is derived from the chain rule of probability. We first factorize \\(p_\\theta(x, y, z)\\) as \\(p_\\theta(x|y,z)p_\\theta(y,z)\\), and then further factorize \\(p_\\theta(y,z)\\) as \\(p_\\theta(y|z)p_\\theta(z)\\). This reflects the generative process where: 1. First, we sample \\(z\\) from the prior \\(p_\\theta(z)\\) 2. Then, we sample \\(y\\) conditioned on \\(z\\) from \\(p_\\theta(y|z)\\) 3. Finally, we generate \\(x\\) conditioned on both \\(y\\) and \\(z\\) from \\(p_\\theta(x|y,z)\\)</p> <p>The approximate posterior for labeled data is:</p> \\[ q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y|x) \\] <p>This factorization is derived from the chain rule of probability for the approximate posterior. The chain rule states that for any random variables \\(A\\), \\(B\\), and \\(C\\), we can write:</p> \\[p(A,B|C) = p(A|B,C)p(B|C)\\] <p>This equation is derived from the definition of conditional probability. Let's break it down step by step:</p> <ol> <li>First, recall that conditional probability is defined as:</li> </ol> \\[p(A|B) = \\frac{p(A,B)}{p(B)}\\] <ol> <li>For our case with three variables, we can write:</li> </ol> \\[p(A,B|C) = \\frac{p(A,B,C)}{p(C)}\\] <ol> <li>We can also write:</li> </ol> \\[p(A|B,C) = \\frac{p(A,B,C)}{p(B,C)}\\] <p>and</p> \\[p(B|C) = \\frac{p(B,C)}{p(C)}\\] <ol> <li>Multiplying these last two equations:</li> </ol> \\[p(A|B,C)p(B|C) = \\frac{p(A,B,C)}{p(B,C)} \\cdot \\frac{p(B,C)}{p(C)} = \\frac{p(A,B,C)}{p(C)} = p(A,B|C)\\] <p>Therefore, we have proven that:</p> \\[p(A,B|C) = p(A|B,C)p(B|C)\\] <p>In our case, we can identify: - \\(A\\) as \\(z\\) (the latent code) - \\(B\\) as \\(y\\) (the label) - \\(C\\) as \\(x\\) (the observed data)</p> <p>Therefore, applying the chain rule:</p> \\[q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y|x)\\] <p>This means: 1. First, we predict the label \\(y\\) from \\(x\\) using \\(q_\\phi(y|x)\\) 2. Then, we infer the latent code \\(z\\) using both \\(x\\) and the predicted \\(y\\) through \\(q_\\phi(z|x,y)\\)</p> <p>and for unlabeled data:</p> \\[ q_\\phi(y,z|x) = q_\\phi(z|x,y)q_\\phi(y) \\] <p>For unlabeled data, since we don't know the true label \\(y\\), we use a prior distribution \\(q_\\phi(y)\\) (typically a uniform distribution over classes) instead of \\(q_\\phi(y|x)\\). The factorization reflects that: 1. We sample a label \\(y\\) from the prior \\(q_\\phi(y)\\) 2. Then, we infer the latent code \\(z\\) using both \\(x\\) and the sampled \\(y\\) through \\(q_\\phi(z|x,y)\\)</p> <p>The training objective for SSVAE combines: 1. The ELBO for labeled data 2. The ELBO for unlabeled data 3. A classification loss for labeled data</p> <p>This allows the model to learn both the data distribution and the class labels in a semi-supervised manner.</p>"},{"location":"math/linear_algebra/basis_and_orthogonality/","title":"Basis and orthogonality","text":"<p>A basis for a nonzero linear subspace \\(V\\) in \\(\\mathbb{R}^n\\) is a spanning set for \\(V\\) consisting of exactly \\(\\dim(V)\\) vectors.</p> <p>If \\(\\dim(V) = 2\\) then a basis for \\(V\\) consists of any v, w for which \\(\\text{span}(v,w) = V\\).</p> <p>One basis of \\(\\mathbb{R}^3\\) is given by \\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\), \\(\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\\), \\(\\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\); this is called the standard basis of \\(\\mathbb{R}^3\\). But many other triples of vectors are also a basis of \\(\\mathbb{R}^3\\).</p> <p>Although we have a way to figure out the dimension of the span of 2 or 3 nonzero vectors, we have to confront the reality that for the span of 4 or more nonzero vectors in \\(\\mathbb{R}^n\\) it becomes rather cumbersome to figure out the dimension via algebra alone; we need another way.</p> <p>A collection of vectors v\\(_1\\), . . . , v\\(_k\\) in \\(\\mathbb{R}^n\\) is called orthogonal if \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) whenever \\(i \\neq j\\). In words, the vectors are all perpendicular to one another.</p> <p>If v\\(_1\\), . . . , v\\(_k\\) is an orthogonal collection of nonzero vectors in \\(\\mathbb{R}^n\\) then it is a basis for \\(\\text{span}(v_1, \\ldots, v_k)\\). In particular, \\(\\text{span}(v_1, \\ldots, v_k)\\) then has dimension \\(k\\) and we call v\\(_1\\), . . . , v\\(_k\\) an orthogonal basis for its span (a single nonzero vector is always an orthogonal basis for its span!).</p> <p>The span of a collection of \\(k\\) vectors in \\(\\mathbb{R}^n\\) has dimension at most \\(k\\) (e.g., three vectors in \\(\\mathbb{R}^3\\) lying in a common plane through 0 have span with dimension less than 3). Orthogonality is a useful way to guarantee that \\(k\\) given nonzero \\(n\\)-vectors have a \\(k\\)-dimensional span.</p> <p>Example: Consider the span \\(V\\) of the following three vectors in \\(\\mathbb{R}^5\\):</p> \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\end{bmatrix}\\] <p>This collection of three vectors is not orthogonal, since, for example, \\(\\mathbf{v}_1 \\cdot \\mathbf{v}_2 = 1 + 0 + 6 + 0 + 3 = 10\\). We can show that \\(\\dim(V) = 3\\), so the triple \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) is a basis of \\(V\\) (if \\(\\dim(V) = 2\\), then the triple would not be a basis of V, just a regular spanning set), but not an orthogonal basis of \\(V\\).</p> <p>Note: There is a systematic process for finding an orthogonal basis for the span of \\(k\\) vectors in \\(\\mathbb{R}^n\\) called the \"Gram\u2013Schmidt process\".</p> <p>Every nonzero linear subspace of \\(\\mathbb{R}^n\\) has an orthogonal basis.</p> <p>There is an especially convenient type of orthogonal basis for a nonzero linear subspace of \\(\\mathbb{R}^n\\). A collection of vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\) is called orthonormal if they are orthogonal to each other and in addition they are all unit vectors; that is, \\(\\mathbf{v}_i \\cdot \\mathbf{v}_i = 1\\) for all \\(i\\) (ensuring \\(\\|\\mathbf{v}_i\\| = \\sqrt{\\mathbf{v}_i \\cdot \\mathbf{v}_i} = \\sqrt{1} = 1\\) for all \\(i\\)).</p> <p>Any orthonormal collection of vectors is a basis of its span.</p> <p>For any \\(n\\) the analogous orthonormal collection of \\(n\\) vectors \\(\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\) in \\(\\mathbb{R}^n\\) can be written down (i.e., \\(\\mathbf{e}_i\\) has its \\(i\\)th entry equal to \\(1\\) and all other entries are \\(0\\)), and this spans \\(\\mathbb{R}^n\\); it is called the standard basis of \\(\\mathbb{R}^n\\), and shows \\(\\dim(\\mathbb{R}^n) = n\\) (as we expect). In particular, (with \\(V = \\mathbb{R}^n\\)), every linear subspace of \\(\\mathbb{R}^n\\) has dimension at most \\(n\\) and the only \\(n\\)-dimensional one is \\(\\mathbb{R}^n\\) itself (as geometric intuition may suggest).</p> <p>In the special case \\(n = 3\\), the vectors \\(\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3 \\in \\mathbb{R}^3\\) are often respectively denoted as \\(\\mathbf{i}, \\mathbf{j}, \\mathbf{k}\\) in physics and engineering contexts.</p> <p>Example: The triple</p> \\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\end{bmatrix}, \\quad \\begin{bmatrix} -6 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} -2 \\\\ -25 \\\\ 13 \\end{bmatrix}\\] <p>is an orthogonal basis for \\(\\mathbb{R}^3\\). How can one \"see\" this? One can check by hand that it is an orthogonal collection of vectors, so this collection of \\(3\\) nonzero vectors must be a basis of its span by, and hence its span has dimension \\(3\\).</p>"},{"location":"math/linear_algebra/basis_and_orthogonality/#fourier-formula","title":"Fourier formula","text":"<p>If \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) are nonzero vectors in \\(\\mathbb{R}^n\\), by definition any vector \\(\\mathbf{v} \\in \\text{span}(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k)\\) can be written as a linear combination</p> \\[\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i\\] <p>for some scalars \\(c_1, \\ldots, c_k\\). If the collection of \\(\\mathbf{v}_i\\)'s is orthogonal, we can actually solve for the \\(c_i\\)'s in terms of \\(\\mathbf{v}\\) by the following slick technique that has useful generalizations throughout mathematics (with Fourier series, special function theory, and so on).</p> <p>For instance, if we form the dot product against \\(\\mathbf{v}_1\\) then we obtain</p> \\[\\mathbf{v} \\cdot \\mathbf{v}_1 = c_1(\\mathbf{v}_1 \\cdot \\mathbf{v}_1) + c_2(\\mathbf{v}_2 \\cdot \\mathbf{v}_1) + c_3(\\mathbf{v}_3 \\cdot \\mathbf{v}_1) + \\cdots = c_1(\\mathbf{v}_1 \\cdot \\mathbf{v}_1),\\] <p>where the tremendous cancellation at the final equality is precisely due to the orthogonality of the collection of \\(\\mathbf{v}_i\\)'s. Since \\(\\mathbf{v}_1\\) is nonzero, so \\(\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = \\|\\mathbf{v}_1\\|^2\\) is nonzero, we can now divide by it at both ends of our string of equalities above to obtain</p> \\[\\frac{\\mathbf{v} \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1} = c_1.\\] <p>In this way we have solved for \\(c_1\\)!</p> <p>The same procedure works likewise to solve for each \\(c_i\\) via forming dot products against \\(\\mathbf{v}_i\\), yielding the general formula</p> \\[c_i = \\frac{\\mathbf{v} \\cdot \\mathbf{v}_i}{\\mathbf{v}_i \\cdot \\mathbf{v}_i}\\] <p>for each \\(i\\). Substituting back into the right side of the equation for \\(\\mathbf{v}\\), we obtain the following result.</p> <p>Theorem (Fourier formula). For any orthogonal collection of nonzero vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\) and vector \\(\\mathbf{v}\\) in their span,</p> \\[\\mathbf{v} = \\sum_{i=1}^k \\frac{\\mathbf{v} \\cdot \\mathbf{v}_i}{\\mathbf{v}_i \\cdot \\mathbf{v}_i} \\mathbf{v}_i.\\] <p>In particular, if the \\(\\mathbf{v}_i\\)'s are all unit vectors (so \\(\\mathbf{v}_i \\cdot \\mathbf{v}_i = 1\\) for all \\(i\\)) then \\(\\mathbf{v} = \\sum_{i=1}^k (\\mathbf{v} \\cdot \\mathbf{v}_i)\\mathbf{v}_i\\).</p> <p>Example: For the orthonormal basis \\(\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3, \\mathbf{e}_4\\) of \\(\\mathbb{R}^4\\) and any \\(\\mathbf{v} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} \\in \\mathbb{R}^4\\), the coefficients \\(\\mathbf{v} \\cdot \\mathbf{e}_i\\) work out as follows:</p> \\[\\mathbf{v} \\cdot \\mathbf{e}_1 = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = a_1\\] <p>and similarly \\(\\mathbf{v} \\cdot \\mathbf{e}_i = a_i\\) for each \\(i = 1, 2, 3, 4\\). Thus (since \\(\\mathbf{e}_i \\cdot \\mathbf{e}_i = 1\\)), \\(\\mathbf{v} = \\sum_{i=1}^4 (\\mathbf{v} \\cdot \\mathbf{e}_i)\\mathbf{e}_i = \\sum_{i=1}^4 a_i\\mathbf{e}_i\\). Unpacking the summation notation, this is just asserting</p> \\[\\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} = a_1\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + a_2\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + a_3\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + a_4\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\\] <p>which can be directly verified by hand since the right side is exactly</p> \\[\\begin{bmatrix} a_1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ a_2 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ a_3 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ a_4 \\end{bmatrix}.\\] <p>In other words, the Fourier formula in the special case that \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) is the orthonormal basis \\(\\{\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\}\\) of \\(\\mathbb{R}^n\\) is precisely the familiar fact that any vector in \\(\\mathbb{R}^n\\) can be decomposed as the sum of its \"components\" along the various coordinate directions. This is neither surprising nor perhaps particularly interesting, so we next give a more \"typical\" example.</p> <p>Example: Consider the span \\(V\\) of the following three vectors in \\(\\mathbb{R}^5\\):</p> \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\end{bmatrix}\\] <p>Consider the following three nonzero vectors in \\(V\\), which form an orthogonal basis for their span:</p> \\[\\mathbf{w}_1 = \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{w}_2 = -2\\mathbf{v}_1 + 3\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 3 \\\\ 0 \\\\ -4 \\\\ 7 \\end{bmatrix}, \\quad \\mathbf{w}_3 = -9\\mathbf{v}_1 - 24\\mathbf{v}_2 + 75\\mathbf{v}_3 = \\begin{bmatrix} -33 \\\\ 201 \\\\ -75 \\\\ 132 \\\\ -6 \\end{bmatrix}\\] <p>Consider the vector</p> \\[\\mathbf{v} = 2\\mathbf{v}_1 - \\mathbf{v}_2 + \\mathbf{v}_3 = \\begin{bmatrix} 2 \\\\ 0 \\\\ 6 \\\\ 4 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\\\ 6 \\\\ 0 \\end{bmatrix}\\] <p>in \\(V\\). Since \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}\\) is a basis of \\(V\\), we know that there is some expression of the form</p> \\[\\mathbf{v} = c_1\\mathbf{w}_1 + c_2\\mathbf{w}_2 + c_3\\mathbf{w}_3\\] <p>for unknown scalars \\(c_1, c_2, c_3\\). What are these scalars? A brute-force approach would be to write everything out as explicit vectors to obtain</p> \\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\\\ 6 \\\\ 0 \\end{bmatrix} = \\mathbf{v} = c_1\\mathbf{w}_1 + c_2\\mathbf{w}_2 + c_3\\mathbf{w}_3 = c_1\\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix} + c_2\\begin{bmatrix} 1 \\\\ 3 \\\\ 0 \\\\ -4 \\\\ 7 \\end{bmatrix} + c_3\\begin{bmatrix} -33 \\\\ 201 \\\\ -75 \\\\ 132 \\\\ -6 \\end{bmatrix} = \\begin{bmatrix} c_1 + c_2 - 33c_3 \\\\ 3c_2 + 201c_3 \\\\ 3c_1 - 75c_3 \\\\ 2c_1 - 4c_2 + 132c_3 \\\\ c_1 + 7c_2 - 6c_3 \\end{bmatrix},\\] <p>and then equate corresponding vector entries on the left and right sides to get a huge system of \\(5\\) equations in \\(3\\) unknowns. We can entirely bypass that by computing dot products for our specific \\(\\mathbf{v}\\)!</p> <p>To carry this out, we use the explicit descriptions of \\(\\mathbf{v}\\) and the \\(\\mathbf{w}_i\\)'s to compute</p> \\[\\mathbf{v} \\cdot \\mathbf{w}_1 = 25, \\quad \\mathbf{v} \\cdot \\mathbf{w}_2 = -17, \\quad \\mathbf{v} \\cdot \\mathbf{w}_3 = 861,\\] <p>so the Fourier formula says for this particular \\(\\mathbf{v}\\) that</p> \\[\\mathbf{v} = \\frac{25}{15}\\mathbf{w}_1 - \\frac{17}{75}\\mathbf{w}_2 + \\frac{861}{64575}\\mathbf{w}_3 = \\frac{5}{3}\\mathbf{w}_1 - \\frac{17}{75}\\mathbf{w}_2 + \\frac{1}{75}\\mathbf{w}_3.\\] <p>That's it! This is the expression for \\(\\mathbf{v}\\) as a linear combination of the orthogonal basis \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}\\) of \\(V\\).</p>"},{"location":"math/linear_algebra/planes_in_r3/","title":"Planes in \\(\\mathbb{R}^3\\)","text":"<p>The collection of points \\((x, y, z)\\) in \\(\\mathbb{R}^3\\) satisfying an equation of the form</p> \\[ax + by + cz = d\\] <p>with at least one of the constants \\(a\\), \\(b\\), or \\(c\\) nonzero, is a plane in \\(\\mathbb{R}^3\\). Note that although the equation \\(x = 0\\) on \\(\\mathbb{R}^2\\) defines a line (the \\(y\\)-axis, consisting of points \\((0, y)\\)), the \"same\" equation \\(x = 0\\) on \\(\\mathbb{R}^3\\) defines a plane, namely the vertical \\(yz\\)-plane consisting of points \\((0, y, z)\\).</p>"},{"location":"math/linear_algebra/planes_in_r3/#lines-in-mathbbr3","title":"Lines in \\(\\mathbb{R}^3\\)","text":"<p>In \\(\\mathbb{R}^3\\), a line can be represented in several forms:</p> <p>Parametric form (most common): \\(\\mathbf{r}(t) = \\mathbf{r}_0 + t\\mathbf{v}\\) where \\(\\mathbf{r}_0\\) is a point on the line and \\(\\mathbf{v}\\) is a direction vector.</p> <p>As an example, consider a line that passes through the point \\((1, 0, -2)\\) and has direction vector \\((3, 1, 4)\\). The parametric equation is:</p> \\[\\mathbf{r}(t) = (1, 0, -2) + t(3, 1, 4) = (1 + 3t, 0 + t, -2 + 4t)\\] <p>This means any point on the line has coordinates \\((1 + 3t, t, -2 + 4t)\\) for some real number \\(t\\). For example:</p> <ul> <li> <p>When \\(t = 0\\): \\((1, 0, -2)\\) (the base point)</p> </li> <li> <p>When \\(t = 1\\): \\((4, 1, 2)\\)</p> </li> <li> <p>When \\(t = -1\\): \\((-2, -1, -6)\\)</p> </li> </ul> <p>Parametric form through two points: Given two points \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\), the line passing through them has parametric equation:</p> \\[\\mathbf{r}(t) = \\mathbf{p} + t(\\mathbf{q} - \\mathbf{p}) = (1-t)\\mathbf{p} + t\\mathbf{q}\\] <p>This form uses the direction vector \\(\\mathbf{q} - \\mathbf{p}\\) and parameter \\(t\\) ranges from 0 to 1 to give all points between \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\).</p> <p>Example: For points \\(\\mathbf{p} = (1, 2, 3)\\) and \\(\\mathbf{q} = (4, 1, 0)\\), the line equation is:</p> \\[\\mathbf{r}(t) = (1, 2, 3) + t(3, -1, -3) = (1 + 3t, 2 - t, 3 - 3t)\\] <p>Symmetric form (when all components of \\(\\mathbf{v}\\) are nonzero): \\(\\frac{x - x_0}{v_1} = \\frac{y - y_0}{v_2} = \\frac{z - z_0}{v_3}\\)</p> <p>In the symmetric form:</p> <ul> <li> <p>\\((x_0, y_0, z_0)\\) is a point on the line (the base point)</p> </li> <li> <p>\\((v_1, v_2, v_3)\\) are the components of the direction vector \\(\\mathbf{v}\\)</p> </li> </ul> <p>So if we have a line with parametric form \\(\\mathbf{r}(t) = \\mathbf{r}_0 + t\\mathbf{v}\\), then \\((x_0, y_0, z_0) = \\mathbf{r}_0\\) and \\((v_1, v_2, v_3) = \\mathbf{v}\\).</p> <p>For example, if a line passes through the point \\((2, -1, 3)\\) and has direction vector \\((1, 2, -1)\\), then the symmetric form would be:</p> \\[\\frac{x - 2}{1} = \\frac{y - (-1)}{2} = \\frac{z - 3}{-1}\\] <p>This form eliminates the parameter \\(t\\) and gives a direct relationship between the coordinates, but it only works when all components of the direction vector are nonzero (to avoid division by zero).</p> <p>Intersection of two planes:  \\(\\begin{cases} a_1x + b_1y + c_1z = d_1 \\\\ a_2x + b_2y + c_2z = d_2 \\end{cases}\\)</p> <p>The key difference from \\(\\mathbb{R}^2\\) is that in \\(\\mathbb{R}^3\\), a single linear equation \\(ax + by + cz = d\\) defines a plane, not a line. To define a line in \\(\\mathbb{R}^3\\), you need either a parametric equation with one parameter, the intersection of two planes (two linear equations), or, a point and a direction vector.</p>"},{"location":"math/linear_algebra/planes_in_r3/#forms-of-planes","title":"Forms of Planes","text":"<p>Planes in \\(\\mathbb{R}^3\\) can be represented in several different forms, each useful for different purposes:</p> <p>1. Point-normal form: \\((x - x_0, y - y_0, z - z_0) \\cdot \\mathbf{n} = 0\\)</p> <p>This form uses a point \\((x_0, y_0, z_0)\\) on the plane and a normal vector \\(\\mathbf{n} = (a, b, c)\\) perpendicular to the plane.</p> <p></p> <p>Example: A plane (shown above) passing through the point \\((0, 1, 1)\\) with normal vector \\((3, -2, 1)\\) has equation:</p> \\[(x - 0, y - 1, z - 1) \\cdot (3, -2, 1) = 0\\] \\[3(x - 0) - 2(y - 1) + 1(z - 1) = 0\\] \\[3x - 2y + z + 1 = 0\\] <p>From the coefficients we again read off that (3, -2, 1) is a normal vector to the plane (not to the individual points in the plane, but rather to differences between such points). This is no surprise, in view of how the plane was originally defined.</p> <p>2. General form: \\(ax + by + cz + d = 0\\)</p> <p>This is the standard form where \\((a, b, c)\\) is a normal vector to the plane and \\(d\\) determines the position of the plane in space.</p> <p>Example: The plane \\(2x - 3y + 4z = 12\\) has normal vector \\((2, -3, 4)\\) and can be rewritten as \\(2x - 3y + 4z - 12 = 0\\).</p> <p>The plane \\(ax + by + cz + d = 0\\) is at a distance of \\(\\frac{|d|}{\\sqrt{a^2 + b^2 + c^2}}\\) from the origin. All planes with the same normal vector \\((a, b, c)\\) but different \\(d\\) values are parallel to each other.</p> <p>If you think of the normal vector \\((a,b,c)\\) as pointing in a fixed direction, then \\(d\\) tells you \"how far along that direction\" the plane is located. For example, the planes \\(2x - 3y + 4z = 0\\), \\(2x - 3y + 4z = 5\\), and \\(2x - 3y + 4z = -3\\) all have the same normal vector \\((2, -3, 4)\\) but are at different distances from the origin.</p> <p>3. Parametric form: \\(\\mathbf{r}(s,t) = \\mathbf{r}_0 + s\\mathbf{v}_1 + t\\mathbf{v}_2\\)</p> <p>This form uses a point \\(\\mathbf{r}_0\\) on the plane and two non-parallel direction vectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) that lie in the plane.</p> <p>Example: A plane through the point \\((1, 0, 2)\\) with direction vectors \\((1, 1, 0)\\) and \\((0, 1, 1)\\) has parametric equation: \\(\\mathbf{r}(s,t) = (1, 0, 2) + s(1, 1, 0) + t(0, 1, 1) = (1 + s, s + t, 2 + t)\\)</p> <p>An advantage of the parametric form is that as we independently vary values of the parameters \\(t\\) and \\(t\u2032\\), the vectors we get in the parametric form are guaranteed to lie exactly on the plane. For instance, if we want to trace out some path in the plane, then by varying the values of \\(t\\) and \\(t\u2032\\) continuously we trace out a continuous curve exactly in the plan.</p> <p></p> <p>The parametric description for planes and its analogues for more complicated surfaces (such as a sphere, a cylinder, etc.) is quite useful in computer graphics to generate the image of a path of motion lying exactly on a specific surface. For such applications a parametric form is far more useful than the general form; as with the parametric form we do not have to solve for anything.</p> <p>4. Three-point form: Using three non-collinear points</p> <p>Given three points \\((x_1, y_1, z_1)\\), \\((x_2, y_2, z_2)\\), and \\((x_3, y_3, z_3)\\), the plane equation is: \\(\\begin{vmatrix} x - x_1 &amp; y - y_1 &amp; z - z_1 \\\\ x_2 - x_1 &amp; y_2 - y_1 &amp; z_2 - z_1 \\\\ x_3 - x_1 &amp; y_3 - y_1 &amp; z_3 - z_1 \\end{vmatrix} = 0\\)</p> <p>Example: For points \\((1, 0, 0)\\), \\((0, 1, 0)\\), and \\((0, 0, 1)\\), the plane equation becomes:</p> \\[\\begin{vmatrix} x - 1 &amp; y &amp; z \\\\ -1 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 \\end{vmatrix} = 0\\] \\[(x - 1)(1 \\cdot 1 - 0 \\cdot 0) - y(-1 \\cdot 1 - 0 \\cdot (-1)) + z(-1 \\cdot 0 - 1 \\cdot (-1)) = 0\\] \\[(x - 1) + y + z = 0\\] \\[x + y + z = 1\\] <p>Note: We claim that the only way three different points can be on a common line in space is when the difference vectors from one of them to the other two lie along the same or opposite directions. This corresponds to those two difference vectors being scalar multiples of each other (a positive scalar when pointing in the same direction, and a negative scalar when pointing in opposite directions).</p> <p>5. Intercept form: \\(\\frac{x}{a} + \\frac{y}{b} + \\frac{z}{c} = 1\\)</p> <p>This form shows the intercepts of the plane with the coordinate axes: \\((a, 0, 0)\\), \\((0, b, 0)\\), and \\((0, 0, c)\\).</p> <p>Example: The plane \\(\\frac{x}{3} + \\frac{y}{2} + \\frac{z}{6} = 1\\) has intercepts at \\((3, 0, 0)\\), \\((0, 2, 0)\\), and \\((0, 0, 6)\\).</p> <p>Each form has its advantages. The Point-normal form is useful for finding distance from a point to a plane. The General form is standard for solving systems of equations. The Parametric form is convenient for generating points on the plane. Three-point form is natural when given three points. The Intercept form provides immediate geometric insight.</p>"},{"location":"math/linear_algebra/projections/","title":"Projections","text":"<p>For many applications in engineering, physics, and data-related problems in all scientific fields (genetics, economics, neuroscience, computer science, etc.) it is essential to go far beyond \\(\\mathbb{R}^3\\) and solve problems involving distance minimization to subspaces in \\(\\mathbb{R}^n\\) for any \\(n\\):</p> <p>If \\(V\\) is a linear subspace in \\(\\mathbb{R}^n\\) and \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is some point, then what point in \\(V\\) is closest to \\(\\mathbf{x}\\)? This closest point will be called the projection of \\(\\mathbf{x}\\) into \\(V\\).</p>"},{"location":"math/linear_algebra/projections/#the-closest-point-to-a-line","title":"The closest point to a line","text":"<p>The way we will solve the general problem of distance minimization from a point in \\(\\mathbb{R}^n\\) to a linear subspace \\(V\\) involves \"assembling\" a collection of solutions to distance minimization to certain \\(1\\)-dimensional linear subspaces of \\(V\\), or in other words, solving distance minimization problems to a collection of lines through \\(0\\) in \\(\\mathbb{R}^n\\).</p> <p>Since minimizing distance to lines will be the foundation for the general case, we begin by focusing on this special case. Consider a line \\(L\\) in \\(\\mathbb{R}^n\\) through \\(0\\), so \\(L = \\text{span}(\\mathbf{w}) = \\{c\\mathbf{w} : c \\in \\mathbb{R}\\}\\) where \\(\\mathbf{w} \\in \\mathbb{R}^n\\) is some nonzero vector. For any point \\(\\mathbf{x} \\in \\mathbb{R}^n\\), we want to show that there is a unique point in \\(L\\) closest to \\(\\mathbf{x}\\), and to actually give a formula for how to compute this nearest point to \\(\\mathbf{x}\\) in \\(L\\). </p> <p>Here is the fundamental idea: although our task (necessary for many applications!) takes place in \\(\\mathbb{R}^n\\) with completely general (and possibly huge) \\(n\\), we look at a low-dimensional instance of the problem in the hope that the low-dimensional case will suggest some feature that has a chance to adapt to the general situation. This balancing of insight from pictures in low-dimensional cases alongside algebraic work and geometric language developed for \\(\\mathbb{R}^n\\) with general \\(n\\) is an important part of linear algebra, giving visual insight into \\(\\mathbb{R}^n\\) for big \\(n\\). This doesn't justify results in \\(\\mathbb{R}^n\\) for general \\(n\\), but it inspires what we should expect and/or try to prove is true.</p> <p>Let's look at the case \\(n = 2\\) as shown in the figure below.</p> <p></p> <p>The key insight suggested by the figure above is that the point on the line \\(L\\) that is closest to \\(\\mathbf{x}\\) has another characterization (that in turn will allow us to compute it): it is the one point on \\(L\\) for which the displacement vector to \\(\\mathbf{x}\\) (the dotted line segment joining it to \\(\\mathbf{x}\\) as in the figure above) is perpendicular to \\(L\\), or equivalently is perpendicular to \\(\\mathbf{w}\\). Visually, the perpendicular direction to \\(L\\) from \\(\\mathbf{x}\\) is the \"most direct\" route. Or put another way, you may convince yourself by drawing some pictures that any deviation from perpendicularity entails a longer path from \\(\\mathbf{x}\\) to the line \\(L\\).</p> <p>To summarize, the figure above suggests a workable idea: the point \\(c\\mathbf{w} \\in L\\) for which \\(\\|\\mathbf{x} - c\\mathbf{w}\\|\\) is minimal should also have the property that \\(\\mathbf{x} - c\\mathbf{w}\\) is orthogonal to everything in \\(L\\). Although this idea is suggested by the picture in \\(\\mathbb{R}^2\\), as written it makes equally good sense in \\(\\mathbb{R}^n\\) for any \\(n\\) whatsoever. But is it true? And even once we know it is true, how can we exploit this property of the nearest point to \\(\\mathbf{x}\\) on \\(L\\) to actually compute this nearest point?</p> <p>The informal reasoning above may have already convinced you that the distance is minimized precisely when the displacement vector is orthogonal to \\(L\\).</p> <p>Method I (algebraic). In accordance with the idea inspired by the figure above in the case \\(n = 2\\), for general \\(n\\) we look for a scalar \\(c\\) for which \\(\\mathbf{x} - c\\mathbf{w}\\) is orthogonal to every vector in \\(L\\). The points of \\(L = \\text{span}(\\mathbf{w})\\) are those of the form \\(a\\mathbf{w}\\) for scalars \\(a\\), so we seek \\(c\\) making \\((\\mathbf{x} - c\\mathbf{w}) \\cdot (a\\mathbf{w}) = 0\\) for every scalar \\(a\\). The dot product has the property that \\((\\mathbf{x} - c\\mathbf{w}) \\cdot (a\\mathbf{w}) = a((\\mathbf{x} - c\\mathbf{w}) \\cdot \\mathbf{w})\\), so actually it suffices to make sure that \\((\\mathbf{x} - c\\mathbf{w}) \\cdot \\mathbf{w} = 0\\). We can use the further properties of the dot product to rewrite this as</p> \\[0 = (\\mathbf{x} - c\\mathbf{w}) \\cdot \\mathbf{w} = \\mathbf{x} \\cdot \\mathbf{w} - (c\\mathbf{w}) \\cdot \\mathbf{w}.\\] <p>We can rearrange this expression to write it as \\(c(\\mathbf{w} \\cdot \\mathbf{w}) = \\mathbf{x} \\cdot \\mathbf{w}\\). But \\(\\mathbf{w} \\cdot \\mathbf{w} = \\|\\mathbf{w}\\|^2 &gt; 0\\) (since \\(\\mathbf{w} \\neq 0\\)), so it makes sense to divide both sides by \\(\\mathbf{w} \\cdot \\mathbf{w}\\) to obtain that \\(c = \\frac{\\mathbf{x} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}}\\). This is the coefficient we had previously regarded as unknown! </p> <p>To summarize, we have shown through algebra and the properties of dot products that there is exactly one point in the line \\(L = \\text{span}(\\mathbf{w})\\) through \\(0\\) in \\(\\mathbb{R}^n\\) whose difference from \\(\\mathbf{x}\\) is orthogonal to everything in \\(L\\): it is \\(\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w}\\). We have not yet actually shown that this scalar multiple of \\(\\mathbf{w}\\) on \\(L\\) is closer to \\(\\mathbf{x}\\) than every other vector in \\(L\\), but if we believe the orthogonality insight inspired by the \\(2\\)-dimensional picture in the figure above then this must be that closest point. As a bonus, we have obtained an explicit formula for it!</p> <p>Method II (geometric). We next use some plane geometry via the figure above to obtain the same formula for the closest point. Strictly speaking, this argument only applies when \\(n = 2\\), but you might find that it gives the formula some visual meaning that is somehow lacking in the purely algebraic work in Method I.</p> <p>There is nothing to be done if \\(\\mathbf{x} \\cdot \\mathbf{w} = 0\\) (in that case \\(\\mathbf{x}\\) is perpendicular to \\(L\\) and we are thereby convinced that \\(0\\) is the closest point, as is also given by the desired formula). Hence, we can suppose \\(\\mathbf{x} \\neq 0\\) and the angle \\(\\theta\\) between \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\) satisfies either \\(0\u00b0 &lt; \\theta &lt; 90\u00b0\\) or \\(90\u00b0 &lt; \\theta &lt; 180\u00b0\\).</p> <p>The case of acute \\(\\theta\\) is shown in the figure above, whereas if \\(\\theta\\) is obtuse then the point we seek would be in the direction of \\(-\\mathbf{w}\\) (rather than in the direction of \\(\\mathbf{w}\\)). If \\(\\theta\\) is acute, as in the figure above, then by basic trigonometry, the leg along \\(L\\) for the right triangle as shown has length \\(\\|\\mathbf{x}\\| \\cos(\\theta)\\) and it points in the direction of the unit vector \\(\\mathbf{w}/\\|\\mathbf{w}\\|\\). This says that the endpoint on \\(L\\) of the dotted segment is the vector</p> \\[(\\|\\mathbf{x}\\| \\cos(\\theta)) \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}.\\] <p>But \\(\\cos(\\theta) = (\\mathbf{x} \\cdot \\mathbf{w})/(\\|\\mathbf{x}\\|\\|\\mathbf{w}\\|)\\), so plugging this into the equation above yields the desired formula since \\(\\|\\mathbf{w}\\|^2 = \\mathbf{w} \\cdot \\mathbf{w}\\). The case when \\(90\u00b0 &lt; \\theta &lt; 180\u00b0\\) goes very similarly, except now \\(\\cos(\\theta) &lt; 0\\) (so the endpoint on \\(L\\) of the dotted segment is in the direction of the opposite unit vector \\(-\\mathbf{w}/\\|\\mathbf{w}\\|\\)) and we have to work with the length \\(\\|\\mathbf{x}\\| |\\cos(\\theta)| = -\\|\\mathbf{x}\\| \\cos(\\theta)\\). Putting these together, the two signs cancel and we get the desired formula again.</p> <p>Proposition. Let \\(L = \\text{span}(\\mathbf{w}) = \\{c\\mathbf{w} : c \\in \\mathbb{R}\\}\\) be a \\(1\\)-dimensional linear subspace of \\(\\mathbb{R}^n\\) (so \\(\\mathbf{w} \\neq 0\\)), a \"line\". Choose any point \\(\\mathbf{x} \\in \\mathbb{R}^n\\). There is exactly one point in \\(L\\) closest to \\(\\mathbf{x}\\), and it is given by the scalar multiple</p> \\[\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w}\\] <p>of \\(\\mathbf{w}\\). This is called \"the projection of \\(\\mathbf{x}\\) into \\(\\text{span}(\\mathbf{w})\\)\"; we denote it by the symbol \\(\\text{Proj}_{\\mathbf{w}} \\mathbf{x}\\).</p> <p>Note: This can be seen as (dot product of \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\)), which is a scalar, times the unit vector in direction of \\(\\mathbf{w}\\).</p> <p>Example: Consider \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}\\). The numbers \\(1, 3, 4\\) represent the amount of \\(\\mathbf{v}\\) that points along the \\(x, y, z\\)-axes respectively. More precisely, \\(\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\mathbf{e}_1\\) is the component of \\(\\mathbf{v}\\) along the \\(x\\)-axis line, \\(\\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\end{bmatrix} = 3\\mathbf{e}_2\\) is the component of \\(\\mathbf{v}\\) along the \\(y\\)-axis line, and \\(\\begin{bmatrix} 0 \\\\ 0 \\\\ 4 \\end{bmatrix} = 4\\mathbf{e}_3\\) is the component of \\(\\mathbf{v}\\) along the \\(z\\)-axis line. These are the closest points to \\(\\mathbf{v}\\) on the \\(x\\)-, \\(y\\)- and \\(z\\)-axes, respectively.</p> <p>In terms of this data, we want to compute the projection of \\(\\mathbf{v}\\) on some line pointing with some other direction: if \\(\\mathbf{w}\\) is a (nonzero) vector along this new direction, we want to compute \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{v})\\). The key point is that the formula for \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{x})\\) behaves well for any linear combination of any \\(n\\)-vectors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_k\\): the projection of a linear combination of the \\(\\mathbf{x}_i\\)'s is equal to the corresponding linear combination of the projections.</p> <p>For example, with \\(k = 2\\) it says \\(\\text{Proj}_{\\mathbf{w}}(5\\mathbf{x}_1 - 7\\mathbf{x}_2) = 5 \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_1) - 7 \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_2)\\) and likewise with \\(5\\) and \\(-7\\) replaced by any two scalars. The reason this works is an algebraic calculation:</p> \\[\\text{Proj}_{\\mathbf{w}}(c_1\\mathbf{x}_1 + \\cdots + c_k\\mathbf{x}_k) = \\frac{(c_1\\mathbf{x}_1 + \\cdots + c_k\\mathbf{x}_k) \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = \\frac{c_1(\\mathbf{x}_1 \\cdot \\mathbf{w}) + \\cdots + c_k(\\mathbf{x}_k \\cdot \\mathbf{w})}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = c_1 \\frac{\\mathbf{x}_1 \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} + \\cdots + c_k \\frac{\\mathbf{x}_k \\cdot \\mathbf{w}}{\\mathbf{w} \\cdot \\mathbf{w}} \\mathbf{w} = c_1 \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_1) + \\cdots + c_k \\text{Proj}_{\\mathbf{w}}(\\mathbf{x}_k).\\] <p>Applying this to the expression \\(\\mathbf{v} = \\mathbf{e}_1 + 3\\mathbf{e}_2 + 4\\mathbf{e}_3\\) yields \\(\\text{Proj}_{\\mathbf{w}}(\\mathbf{v}) = \\text{Proj}_{\\mathbf{w}}(\\mathbf{e}_1) + 3 \\text{Proj}_{\\mathbf{w}}(\\mathbf{e}_2) + 4 \\text{Proj}_{\\mathbf{w}}(\\mathbf{e}_3)\\).</p>"},{"location":"math/linear_algebra/projections/#projection-onto-a-general-subspace","title":"Projection onto a general subspace","text":"<p>Let's look at the case of a plane \\(V\\) through the origin in \\(\\mathbb{R}^3\\) equipped with a choice of orthogonal basis \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) of this plane. In the figure below, we draw the typical situation, indicating with the notation \\(\\text{Proj}_V(\\mathbf{x})\\) the point in \\(V\\) closest to \\(\\mathbf{x}\\). The first geometric insight, similar to our experience with lines, is that since this nearest point should have displacement vector to \\(\\mathbf{x}\\) that is the \"most direct\" route to \\(V\\) from \\(\\mathbf{x}\\), the displacement should involve \"no tilting\" relative to any direction within \\(V\\).</p> <p></p> <p>If you think about it, hopefully it seems plausible that if \\(\\mathbf{v} \\in V\\) makes the displacement \\(\\mathbf{x} - \\mathbf{v}\\) perpendicular to everything in \\(V\\) then \\(\\mathbf{v}\\) should be the point in \\(V\\) for which the direction of the displacement \\(\\mathbf{x} - \\mathbf{v}\\) is the \"most direct\" route from \\(\\mathbf{x}\\) to \\(V\\), making \\(\\mathbf{v}\\) the point in \\(V\\) nearest to \\(\\mathbf{x}\\).</p> <p>Theorem (Orthogonal Projection Theorem, version I). For any \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and linear subspace \\(V\\) of \\(\\mathbb{R}^n\\), there is a unique \\(\\mathbf{v}\\) in \\(V\\) closest to \\(\\mathbf{x}\\). In symbols, \\(\\|\\mathbf{x} - \\mathbf{v}\\| &lt; \\|\\mathbf{x} - \\mathbf{v}'\\|\\) for all \\(\\mathbf{v}'\\) in \\(V\\) with \\(\\mathbf{v}' \\neq \\mathbf{v}\\). This \\(\\mathbf{v}\\) is called the projection of \\(\\mathbf{x}\\) onto \\(V\\), and is denoted \\(\\text{Proj}_V(\\mathbf{x})\\); see the figure above. The projection \\(\\text{Proj}_V(\\mathbf{x})\\) is also the only vector \\(\\mathbf{v} \\in V\\) with the property that the displacement \\(\\mathbf{x} - \\mathbf{v}\\) is perpendicular to \\(V\\) (i.e., \\(\\mathbf{x} - \\mathbf{v}\\) is perpendicular to every vector in \\(V\\)).</p> <p>If \\(V\\) is nonzero then for any orthogonal basis \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) of \\(V\\) we have</p> \\[\\text{Proj}_V(\\mathbf{x}) = \\text{Proj}_{\\mathbf{v}_1}(\\mathbf{x}) + \\text{Proj}_{\\mathbf{v}_2}(\\mathbf{x}) + \\cdots + \\text{Proj}_{\\mathbf{v}_k}(\\mathbf{x}),\\] <p>where \\(\\text{Proj}_{\\mathbf{v}_i}(\\mathbf{x}) = \\frac{\\mathbf{x} \\cdot \\mathbf{v}_i}{\\mathbf{v}_i \\cdot \\mathbf{v}_i} \\mathbf{v}_i\\). For \\(\\mathbf{x} \\in V\\) we have \\(\\text{Proj}_V(\\mathbf{x}) = \\mathbf{x}\\) \u2013 the point in \\(V\\) closest to \\(\\mathbf{x}\\) is itself! \u2013 so the equation above for \\(\\mathbf{x} \\in V\\) recovers the Fourier formula!</p> <p>Theorem (Orthogonal Projection Theorem, version II). If \\(V\\) is a linear subspace of \\(\\mathbb{R}^n\\) then every vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) can be uniquely expressed as a sum</p> \\[\\mathbf{x} = \\mathbf{v} + \\mathbf{v}'\\] <p>with \\(\\mathbf{v} \\in V\\) and \\(\\mathbf{v}'\\) orthogonal to everything in \\(V\\). Explicitly, \\(\\mathbf{v} = \\text{Proj}_V(\\mathbf{x})\\) and \\(\\mathbf{v}' = \\mathbf{x} - \\text{Proj}_V(\\mathbf{x})\\).</p> <p>Since the \\(\\mathbf{v}_i\\)'s span \\(V\\), the point \\(\\mathbf{v} \\in V\\) closest to \\(\\mathbf{x}\\) can be written in the form \\(\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i\\) for some unknown coefficients \\(c_i\\). We are going to see that the perpendicularity of \\(\\mathbf{x} - \\mathbf{v}\\) to everything in \\(V\\) forces \\(c_i = (\\mathbf{x} \\cdot \\mathbf{v}_i)/(\\mathbf{v}_i \\cdot \\mathbf{v}_i)\\) for every \\(i\\). But then \\(c_i\\mathbf{v}_i\\) is exactly the formula for \\(\\text{Proj}_{\\mathbf{v}_i}(\\mathbf{x})\\), so we would obtain \\(\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i = \\sum_{i=1}^k \\text{Proj}_{\\mathbf{v}_i}(\\mathbf{x})\\) as asserted in the equation above.</p> <p>How can we show that the coefficients \\(c_i\\) are really given by the ratios \\((\\mathbf{x} \\cdot \\mathbf{v}_i)/(\\mathbf{v}_i \\cdot \\mathbf{v}_i)\\)? For this we have to do some algebra (rather than geometry): since \\(\\mathbf{x} - \\mathbf{v}\\) is perpendicular to everything in \\(V\\), it is in particular perpendicular to every \\(\\mathbf{v}_j\\), so</p> \\[0 = (\\mathbf{x} - \\mathbf{v}) \\cdot \\mathbf{v}_j = \\mathbf{x} \\cdot \\mathbf{v}_j - \\mathbf{v} \\cdot \\mathbf{v}_j\\] <p>for every \\(j\\). This says \\(\\mathbf{x} \\cdot \\mathbf{v}_j = \\mathbf{v} \\cdot \\mathbf{v}_j\\) for every \\(j\\). But \\(\\mathbf{v} = \\sum_{i=1}^k c_i\\mathbf{v}_i\\) with some unknown \\(c_i\\)'s, so</p> \\[\\mathbf{v} \\cdot \\mathbf{v}_j = \\sum_{i=1}^k (c_i\\mathbf{v}_i) \\cdot \\mathbf{v}_j = \\sum_{i=1}^k c_i(\\mathbf{v}_i \\cdot \\mathbf{v}_j),\\] <p>and the terms for \\(i \\neq j\\) all vanish since \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) whenever \\(i \\neq j\\) (as \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) is an orthogonal basis of \\(V\\)!). In other words, \\(\\mathbf{v} \\cdot \\mathbf{v}_j = c_j(\\mathbf{v}_j \\cdot \\mathbf{v}_j)\\) for every \\(j\\). But we have seen that \\(\\mathbf{x} \\cdot \\mathbf{v}_j = \\mathbf{v} \\cdot \\mathbf{v}_j\\), so</p> \\[\\mathbf{x} \\cdot \\mathbf{v}_j = c_j(\\mathbf{v}_j \\cdot \\mathbf{v}_j)\\] <p>for every \\(j\\). We can divide by \\(\\mathbf{v}_j \\cdot \\mathbf{v}_j\\) since this is nonzero (it is equal to \\(\\|\\mathbf{v}_j\\|^2 &gt; 0\\), as \\(\\mathbf{v}_j \\neq 0\\)), so we thereby obtain the formula \\(c_j = (\\mathbf{x} \\cdot \\mathbf{v}_j)/(\\mathbf{v}_j \\cdot \\mathbf{v}_j)\\) for every \\(j\\), as desired.</p>"},{"location":"math/linear_algebra/span_subspaces_and_dimension/","title":"Span, subspaces, and dimension","text":""},{"location":"math/linear_algebra/span_subspaces_and_dimension/#span-and-linear-subspaces","title":"Span and linear subspaces","text":"<p>Consider a plane \\(P\\) in \\(\\mathbb{R}^3\\) passing through 0 = \\((0, 0, 0)\\). We want to express mathematically the idea that \\(P\\) is \"flat\" with \"two degrees of freedom\". Choose two other points in \\(P\\), denoted v and w, that do not lie on a common line through 0.</p> <p></p> <p>We can get to any point on \\(P\\) by starting at 0 and walking first some specific distance in the v-direction, then some specific distance in the w-direction. Symbolically,</p> \\[P = \\{\\text{all vectors of the form } a\\mathbf{v} + b\\mathbf{w}, \\text{ for scalars } a, b\\}\\] <p>\\(a &lt; 0\\) and \\(b &lt; 0\\) correspond to walking \"backwards\" relative to the directions of v and w respectively. The description as vectors \\(a\\mathbf{v} + b\\mathbf{w}\\) for varying scalars \\(a\\) and \\(b\\) is a way of encoding the flatness of \\(P\\) with two degrees of freedom.</p> <p>In other words, any vector that we can obtain from v and w repeatedly using the vector operations (addition and scalar multiplication) in any order is actually of the form \\(a\\mathbf{v} + b\\mathbf{w}\\) for some scalars \\(a\\), \\(b\\).</p> <p>Thus, the right side of the symbolic equation gives a parametric form of the plane through the 3 points 0, v, w and describes all vectors created from v, w using vector operations. If we instead allow the nonzero 3-vectors v, w to lie on a common line through 0, which is to say w is a scalar multiple of v, then the right side describes a line through 0 rather than a plane (as shown in the figure below).</p> <p></p> <p>The span of vectors v\\(_1\\), . . . , v\\(_k\\) in \\(\\mathbb{R}^n\\) is the collection of all vectors in \\(\\mathbb{R}^n\\) that one can obtain from v\\(_1\\), . . . , v\\(_k\\) by repeatedly using addition and scalar multiplication. In symbols,</p> \\[\\text{span}(v_1, \\ldots, v_k) = \\{\\text{all } n\\text{-vectors } \\mathbf{x} \\text{ of the form } c_1\\mathbf{v}_1 + \\cdots + c_k\\mathbf{v}_k\\}\\] <p>where \\(c_1\\), . . . , \\(c_k\\) are arbitrary scalars.</p> <p>In \\(\\mathbb{R}^3\\), for \\(k = 2\\) and nonzero v\\(_1\\), v\\(_2\\) not multiples of each other, this recovers the parametric form of a plane through \\(P = 0\\). In general, the span of a collection of finitely many \\(n\\)-vectors is the collection of all the \\(n\\)-vectors one can reach from those given \\(n\\)-vectors by forming linear combinations in every possible way.</p> <p>This is a very new kind of concept- considering such a collection of \\(n\\)-vectors all at the same time. But it is ultimately no different than how we may visualize a plane in our head yet it consists of a lot of different points. The span of two nonzero \\(n\\)-vectors that are not scalar multiples of each other should be visualized as a \"plane\" through 0 in \\(\\mathbb{R}^n\\).</p> <p>Example: Let's show that the set \\(U\\) of 4-vectors \\(\\begin{bmatrix} x \\\\ y \\\\ z \\\\ w \\end{bmatrix}\\) that are perpendicular to v = \\(\\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\\\ 7 \\end{bmatrix}\\) is a span of three 4-vectors (the same type of calculation as what we are about to do shows that the set of vectors in \\(\\mathbb{R}^n\\) perpendicular to any fixed nonzero vector in \\(\\mathbb{R}^n\\) is a span of \\(n-1\\) nonzero \\(n\\)-vectors). This is a \"higher-dimensional\" analogue of our visual experience in \\(\\mathbb{R}^3\\) that the collection of vectors in \\(\\mathbb{R}^3\\) perpendicular to a given nonzero vector is a plane through the origin (and hence is the span of two nonzero vectors in \\(\\mathbb{R}^3\\)).</p> <p>As a first step, we write out the condition of perpendicularity using the dot product: \\(2x + 3y + z + 7w = 0\\)</p> <p>Now we solve for \\(w\\) (say) to get \\(w = -(2/7)x - (3/7)y - z/7\\). Thus, points of \\(U\\) are precisely</p> \\[\\begin{bmatrix} x \\\\ y \\\\ z \\\\ w \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ z \\\\ -(2/7)x - (3/7)y - z/7 \\end{bmatrix} = x\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ -2/7 \\end{bmatrix}_{\\mathbf{a}} + y\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -3/7 \\end{bmatrix}_{\\mathbf{b}} + z\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -1/7 \\end{bmatrix}_{\\mathbf{c}}\\] <p>with arbitrary scalars \\(x\\), \\(y\\), \\(z\\). This shows that the vectors perpendicular to v are precisely those in the span of the vectors a, b, c that appear on the right side. This shows that \\(U\\) is the span of a, b, c in \\(\\mathbb{R}^4\\).</p> <p>We have noted that a line in \\(\\mathbb{R}^2\\) or \\(\\mathbb{R}^3\\) passing through 0 and a plane in \\(\\mathbb{R}^3\\) passing through 0 each arise as a span of one or two vectors. But lines and planes not passing through 0 are not a span of any collection of vectors! The reason is that the span of any collection of \\(n\\)-vectors always contains 0, by setting all coefficients \\(c_1\\), . . . , \\(c_k\\) in the span definition to be 0, since \\(0\\mathbf{v}_1 + 0\\mathbf{v}_2 + \\cdots + 0\\mathbf{v}_k = 0\\).</p> <p>If \\(V\\) is the span of some finite collection of vectors in \\(\\mathbb{R}^n\\) then there are generally many different collections of vectors v\\(_1\\), . . . , v\\(_k\\) in \\(\\mathbb{R}^n\\) with span equal to \\(V\\). For example, given one parametric form of a plane in \\(\\mathbb{R}^3\\) through 0 we get lots of others by rotating the resulting parallelogram grid around the origin in that plane by any nonzero angle we like. To refer to a span while suppressing the mention of any specific choice of vectors v\\(_1\\), . . . , v\\(_k\\) that create the span, some new terminology is convenient.</p> <p>A linear subspace of \\(\\mathbb{R}^n\\) is a subset of \\(\\mathbb{R}^n\\) that is the span of a finite collection of vectors in \\(\\mathbb{R}^n\\) (this is also referred to as a subspace, dropping the word \"linear\"). If \\(V\\) is a linear subspace of \\(\\mathbb{R}^n\\), a spanning set for \\(V\\) is a collection of \\(n\\)-vectors v\\(_1\\), . . . , v\\(_k\\) whose span equals \\(V\\) (a linear subspace has lots of spanning sets, akin to tiling a floor by parallelogram tiles in many ways).</p> <p>Planes and lines in \\(\\mathbb{R}^3\\) passing through 0 are the visual examples to keep in mind when you hear the phrase \"linear subspace\". You may wonder: what is the difference between a linear subspace and a span? There is no difference, but saying \"span\" emphasizes the input \u2013 a specific finite list v\\(_1\\), . . . , v\\(_k\\) and the dynamic process of forming their linear combinations \u2013 whereas saying \"linear subspace\" emphasizes the output collection \\(V\\) of \\(n\\)-vectors without choosing a specific v\\(_1\\), . . . , v\\(_k\\) whose span is \\(V\\). It is far more important to know that \\(V\\) can be obtained as a span of some list v\\(_1\\), . . . , v\\(_k\\) rather than to pick a specific such list.</p> <p>Proposition. If \\(V\\) is a linear subspace in \\(\\mathbb{R}^n\\) then for any vectors \\(\\mathbf{x}_1\\), . . . , \\(\\mathbf{x}_m \\in V\\) and scalars \\(a_1\\), . . . , \\(a_m\\) the linear combination \\(a_1\\mathbf{x}_1 + \\cdots + a_m\\mathbf{x}_m\\) also lies in \\(V\\). In words: all linear combinations of \\(n\\)-vectors chosen from a linear subspace of \\(\\mathbb{R}^n\\) belong to that same subspace.</p> <p>Some illustrations below of which are linear subspaces and which are not. Only the blue and green planes are linear subspaces by definition as well as the proposition above.</p> <p> </p>"},{"location":"math/linear_algebra/span_subspaces_and_dimension/#dimension","title":"Dimension","text":"<p>We would like to define the \"dimension\" of a linear subspace in a way that generalizes our familiar concept of dimension: a thread or a line is 1-dimensional, a sheet of paper is 2-dimensional, the world around us is 3-dimensional.</p> <p>In particular, we may informally say: the \"dimension\" of an object \\(X\\) tells us how many different numbers are needed to locate a point in \\(X\\).</p> <p>To turn this into something unambiguous and useful, we now focus on the case of a linear subspace \\(V\\) of \\(\\mathbb{R}^n\\), where vector algebra will provide a way to make that informal idea precise. The \"dimension\" of \\(V\\) will be, intuitively, the number of independent directions in \\(V\\). In other words, it will tell us how many numbers we need in order to specify a vector v in \\(V\\).</p> <p>More precisely, recall that by the definition of \"linear subspace\", \\(V\\) is the span of some finite collection of vectors v\\(_1\\), . . . , v\\(_k \\in \\mathbb{R}^n\\). That is, for any v \\(\\in V\\) we can write</p> \\[\\mathbf{v} = c_1\\mathbf{v}_1 + \\cdots + c_k\\mathbf{v}_k\\] <p>for some scalars \\(c_1\\), . . . , \\(c_k\\), so to determine v it is enough to tell us \\(k\\) numbers\u2013 the scalars \\(c_1\\), . . . , \\(c_k\\). But \\(V\\) can have another spanning set consisting of a different number of vectors.</p> <p>The span \\(V\\) of two nonzero vectors in \\(\\mathbb{R}^3\\) could be a line (such as if the two vectors point in the same or opposite directions), in which case \\(V\\) is also spanned by just one of those two vectors (e.g., the second vector is redundant).</p> <p>Similarly, the span of three nonzero vectors in \\(\\mathbb{R}^3\\) could be a plane in special circumstances (or even a line in especially degenerate circumstances).</p> <p>In both such cases, the initial spanning set has some redundancy. To define \"dimension\" for \\(V\\), we want to use ways of spanning \\(V\\) that (in a sense we need to make precise) don't have redundancy.</p> <p>Let \\(V\\) be a nonzero linear subspace of some \\(\\mathbb{R}^n\\). The dimension of \\(V\\), denoted as \\(\\dim(V)\\), is defined to be the smallest number of vectors needed to span \\(V\\). We define \\(\\dim(\\{0\\}) = 0\\).</p> <p>For \\(k \\geq 2\\), consider a collection v\\(_1\\), . . . , v\\(_k\\) of vectors spanning a linear subspace \\(V\\) in \\(\\mathbb{R}^n\\). We have \\(\\dim(V) = k\\) precisely when \"there is no redundancy\": each v\\(_i\\) is not a linear combination of the others, or in other words removing it from the list destroys the spanning property.</p> <p>Equivalently, \\(\\dim(V) &lt; k\\) precisely when \"there is redundancy\": some v\\(_i\\) is a linear combination of the others, or in other words removing some v\\(_i\\) from the list does not affect the span. If some v\\(_i\\) vanishes then it is a linear combination of the others and hence can be dropped from the span, so \\(\\dim(V) &lt; k\\).</p> <p>If \\(V\\) and \\(W\\) are linear subspaces of \\(\\mathbb{R}^n\\) with \\(W\\) contained in \\(V\\) (i.e., every vector in \\(W\\) also belongs to \\(V\\), much like a line inside a plane in \\(\\mathbb{R}^3\\)) then \\(\\dim(W) \\leq \\dim(V)\\), and equality holds precisely when \\(W = V\\).</p>"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/","title":"Vector geometry in \\(\\mathbb{R}^n\\) and correlation coefficients","text":""},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#angles","title":"Angles","text":"<p>The angle \\(0\u00b0 \\leq \\theta \\leq 180\u00b0\\) between nonzero 2-vectors a = \\((a_1, a_2)\\) and b = \\((b_1, b_2)\\) satisfies</p> \\[\\cos \\theta = \\frac{a_1b_1 + a_2b_2}{\\|a\\|\\|b\\|}\\] <p>The angle \\(0\u00b0 \\leq \\theta \\leq 180\u00b0\\) between two nonzero 3-vectors a = \\((a_1, a_2, a_3)\\) and b = \\((b_1, b_2, b_3)\\) satisfies</p> \\[\\cos \\theta = \\frac{a_1b_1 + a_2b_2 + a_3b_3}{\\|a\\|\\|b\\|}\\] <p>The preceding in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) motivates how to define appropriate concepts with \\(n\\)-vectors for any \\(n\\).</p> <p>Consider \\(n\\)-vectors x = \\(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\) and y = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\).</p> <p>(i) The dot product of x and y is defined to be the scalar</p> \\[x \\cdot y = x_1y_1 + x_2y_2 + \\cdots + x_ny_n = \\sum_{i=1}^n x_iy_i\\] <p>The dot product is only defined if the two vectors are \\(n\\)-vectors for the same value of \\(n\\).</p> <p>(ii) The angle \\(\\theta\\) between two nonzero \\(n\\)-vectors x, y is defined by the formula</p> \\[\\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\|\\|y\\|} \\tag{2.1.4}\\] <p>with \\(0\u00b0 \\leq \\theta \\leq 180\u00b0\\). For emphasis: x and y must be nonzero \\(n\\)-vectors for a common \\(n\\).</p> <p>(iii) When \\(x \\cdot y = 0\\) (same as \\(\\theta = 90\u00b0\\) if x, y \u2260 0), we say x and y are perpendicular; the word orthogonal is often used for this (\"orthog\u014dnios\" is ancient Greek for \"right-angled\"), though only rarely at the U.S. Supreme Court.</p> <p>Always remember that the dot product of vectors is a scalar (it is not a vector).</p> <p>The notion of angle is a definition in \\(\\mathbb{R}^n\\) for general \\(n\\): it is motivated by the case when \\(n = 3\\), but for general \\(n\\) there is nothing to \"physically justify\". The real content making this definition for general \\(n\\) is that (as you will learn with experience) this notion of angle behaves like our visual experience in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) and so provides useful visual guidance with \\(n\\)-vectors for any \\(n\\).</p> <p>Whenever we speak of an angle between two lines through the origin, there is always an ambiguity (when they're not perpendicular) of whether we want the acute angle between them or the (supplementary) obtuse angle between them. This corresponds to the fact that when we set it up as a vector problem, we have to choose a direction along each line (coming out of the intersection point). Depending on the choice, we will get the acute or the obtuse angle.</p>"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#properties-of-dot-products","title":"Properties of dot products","text":"<p>For any \\(n\\)-vectors v, w, w\\(_1\\), and w\\(_2\\), the following hold:</p> <p>(i) \\(v \\cdot w = w \\cdot v\\),</p> <p>(ii) \\(v \\cdot v = \\|v\\|^2\\),</p> <p>(iii) \\(v \\cdot (cw) = c(v \\cdot w)\\) for any scalar \\(c\\), and \\(v \\cdot (w_1 + w_2) = v \\cdot w_1 + v \\cdot w_2\\).</p> <p>(iii\u2032) Combining both rules in (iii), for any scalars \\(c_1\\), \\(c_2\\) we have</p> \\[v \\cdot (c_1w_1 + c_2w_2) = c_1(v \\cdot w_1) + c_2(v \\cdot w_2)\\]"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#pythagorean-theorem-in-mathbbrn-and-the-cauchyschwarz-inequality","title":"Pythagorean Theorem in \\(\\mathbb{R}^n\\) and the Cauchy\u2013Schwarz Inequality","text":"<p>As an application of the dot product rules, we can establish a version of the Pythagorean Theorem for \\(n\\)-vectors with any \\(n\\) (not just \\(n = 2\\)) and we can show that the subtlety lurking in the definition of \"angle\" between \\(n\\)-vectors is really not a problem at all.</p> <p>Theorem (Pythagoras). If \\(n\\)-vectors v\\(_1\\) and v\\(_2\\) are nonzero and perpendicular (i.e., at an angle of \\(90\u00b0\\)) then</p> \\[\\|v_1 + v_2\\|^2 = \\|v_1\\|^2 + \\|v_2\\|^2 \\] <p>Proof. Expand the left side as a dot product:</p> \\[(v_1 + v_2) \\cdot (v_1 + v_2) = v_1 \\cdot (v_1 + v_2) + v_2 \\cdot (v_1 + v_2) = v_1 \\cdot v_1 + v_1 \\cdot v_2 + v_2 \\cdot v_1 + v_2 \\cdot v_2\\] <p>We've used the rules for dot products at each step to expand. But the common value \\(v_1 \\cdot v_2\\) and \\(v_2 \\cdot v_1\\) is 0 because v\\(_1\\) and v\\(_2\\) are assumed to be perpendicular (which means by definition that their dot product equals 0). Thus, the right side equals \\(v_1 \\cdot v_1 + v_2 \\cdot v_2 = \\|v_1\\|^2 + \\|v_2\\|^2\\), as we wanted. \\(\\square\\)</p> <p>The motivation for our definitions of perpendicularity and more generally angle between vectors in \\(\\mathbb{R}^n\\) and length of vectors in \\(\\mathbb{R}^n\\) for general \\(n\\) (especially \\(n &gt; 3\\)) came from our knowledge of how things work in \\(\\mathbb{R}^2\\) based on knowing the Pythagorean Theorem in plane geometry. Making up definitions of words cannot ever replace the work involved in proving a real theorem.</p> <p>Since we now have several good properties of dot products in hand, we can establish a fact that is needed to confirm that our definition of \"angle\" between nonzero \\(n\\)-vectors makes sense for any \\(n\\):</p> <p>Theorem (Cauchy\u2013Schwarz Inequality). For \\(n\\)-vectors v, w, we have</p> \\[-\\|v\\| \\|w\\| \\leq v \\cdot w \\leq \\|v\\| \\|w\\|\\] <p>(or equivalently the absolute value \\(|v \\cdot w|\\) is at most \\(\\|v\\| \\|w\\|\\)). Moreover, one of the inequalities is an equality precisely when one of v or w is a scalar multiple of the other.</p> <p>Proof. If v = 0 or w = 0 then everything is clear (note that 0 is a scalar multiple of any \\(n\\)-vector: multiply it by the scalar 0), so now we assume v, w \u2260 0. The idea of the proof is to explore how the length of v + \\(x\\)w depends on \\(x\\). This is most conveniently done by analyzing the squared-length, which is a dot product:</p> \\[\\|v + xw\\|^2 = (v + xw) \\cdot (v + xw)\\] <p>Using the linearity properties of dot products, we have</p> \\[(v + xw) \\cdot (v + xw) = (v + xw) \\cdot v + x((v + xw) \\cdot w) = v \\cdot v + (xw) \\cdot v + x((v \\cdot w) + x(w \\cdot w)) = v \\cdot v + x(w \\cdot v) + x(v \\cdot w) + x^2(w \\cdot w)\\] <p>But \\(w \\cdot v = v \\cdot w\\), so combining the middle two terms yields:</p> \\[\\|v + xw\\|^2 = \\|v\\|^2 + 2(v \\cdot w)x + \\|w\\|^2x^2\\] <p>The squared length of a vector is always \u2265 0, and it equals 0 precisely when the vector equals 0. But the vector v + \\(x\\)w equals 0 for some value \\(x = c\\) precisely when v = \\(-c\\)w, which is to say v is a scalar multiple of w, and that is the same as w being a scalar multiple of v (since the scalar multiplier can be brought to the other side as its reciprocal as long as the scalar cannot be 0, and indeed such a scalar cannot be 0 since we have arranged that v, w \u2260 0). So we just need to analyze what it means that the quadratic polynomial in \\(x\\) given by</p> \\[q(x) = \\|w\\|^2x^2 + 2(v \\cdot w)x + \\|v\\|^2\\] <p>is always non-negative, and determine when this polynomial does actually attain the value 0 for some value of \\(x\\).</p> <p>Let's review when a quadratic polynomial \\(ax^2 + bx + c\\) with positive leading coefficient (such as \\(a = \\|w\\|^2\\) for \\(q(x)\\) above) is \u2265 0 everywhere. This happens precisely when its concave-up parabolic graph lies entirely on one side of the \\(x\\)-axis (possibly touching the \\(x\\)-axis at one point), which is exactly the situation that the graph does not cross the \\(x\\)-axis at two different points. This is exactly the situation when the output of the quadratic formula does not yield two different real numbers. The opposite case of having two different real roots occurs exactly when the \"\\(b^2 - 4ac\\)\" part of the quadratic formula inside the square-root is &gt; 0, so in our situation we must have the exactly opposite situation: \\(b^2 - 4ac \\leq 0\\), with equality happening precisely when there is a real root.</p> <p>Applying the preceding review with \\(a = \\|w\\|^2\\), \\(b = 2(v \\cdot w)\\), \\(c = \\|v\\|^2\\) for \\(q(x)\\), we get</p> \\[(2(v \\cdot w))^2 - 4\\|w\\|^2\\|v\\|^2 \\leq 0\\] <p>with equality happening exactly when v and w are scalar multiples of each other. Bringing the second term on the left over to the other side, we conclude that</p> \\[(2(v \\cdot w))^2 \\leq 4\\|w\\|^2\\|v\\|^2\\] <p>with equality precisely when v and w are scalar multiples of each other. Dividing each side by 4, this is the same as the inequality</p> \\[|v \\cdot w|^2 \\leq (\\|w\\| \\|v\\|)^2\\] <p>so taking square roots of both sides gives what we want. \\(\\square\\)</p>"},{"location":"math/linear_algebra/vector_geometry_in_mathbb_r_n_and_correlation_coefficients/#the-correlation-coefficient","title":"The correlation coefficient","text":"<p>Given data points \\((x_1, y_1)\\), . . . , \\((x_n, y_n)\\), it is often useful to seek a line which gives a \"best fit\" to this collection of points.</p> <p>The problem of finding a \"best fit\" line to some data is called linear regression. But at a more basic level we may seek a measure of the extent to which it is reasonable to try to find a line that could be regarded as a good fit to the data (setting aside what that specific line may be). There is a widely used measure of whether one should seek such a line: this measure is called the correlation coefficient of the data points.</p> <p>Consider \\(n\\) data points \\((x_1, y_1)\\), \\((x_2, y_2)\\), . . . , \\((x_n, y_n)\\) in \\(\\mathbb{R}^2\\). Assume they don't all lie on a common vertical line nor on a common horizontal line (i.e., the \\(x_i\\)'s are not all equal to each other, and the \\(y_i\\)'s are not all equal to each other, so in particular X, Y \u2260 0).</p> <p>In the above setup, assume furthermore that the averages \\(\\bar{x} = \\frac{1}{n}\\sum x_i\\) and \\(\\bar{y} = \\frac{1}{n}\\sum y_i\\) of the \\(x\\)-coordinates and of the \\(y\\)-coordinates both equal 0. The correlation coefficient \\(r\\) between the \\(x_i\\)'s and \\(y_i\\)'s is defined to be the cosine of the angle between X and Y, or equivalently between the unit vectors \\(\\frac{X}{\\|X\\|}\\) and \\(\\frac{Y}{\\|Y\\|}\\):</p> \\[r = \\text{cosine of the angle between X and Y} = \\frac{X \\cdot Y}{\\|X\\|\\|Y\\|} = \\frac{X}{\\|X\\|} \\cdot \\frac{Y}{\\|Y\\|}\\] <p></p> <p>Intuition: This definition makes perfect geometric sense. When the data points \\((x_i, y_i)\\) are centered (so their averages are 0), we can think of X and Y as vectors in \\(\\mathbb{R}^n\\) representing the \\(x\\)-coordinates and \\(y\\)-coordinates respectively. </p> <p>The correlation coefficient \\(r\\) measures how well the data points align along a line through the origin. When \\(r = 1\\), the vectors X and Y point in the same direction, meaning the data points lie perfectly on a line with positive slope. When \\(r = -1\\), the vectors point in opposite directions, meaning the data points lie perfectly on a line with negative slope. When \\(r = 0\\), the vectors are perpendicular, meaning there's no linear relationship between the variables.</p> <p>You may be bothered by the assumption that the averages \\(\\bar{x}\\) and \\(\\bar{y}\\) of the coordinates of the data both equal 0, since in practice it is rarely satisfied. What is done in real-world problems is that the data is recentered: we replace \\(x_i\\) with \\(\\hat{x}_i = x_i - \\bar{x}\\) and replacing \\(y_i\\) with \\(\\hat{y}_i = y_i - \\bar{y}\\). Such subtraction of the averages makes \"center of mass\" move to \\((0, 0)\\) (i.e., \\(\\hat{\\bar{x}}, \\hat{\\bar{y}} = 0\\)).</p> <p>Often people work with \\(r^2\\), which is always non-negative. This is</p> \\[r^2 = \\frac{(X \\cdot Y)^2}{\\|X\\|^2\\|Y\\|^2}\\] <p>it is near 0 when there is little correlation, and near 1 when there's a strong linear relationship (without specifying the sign of the slope: \\(r\\) may be near 1 or near \\(-1\\)).</p> <p>Don't confuse the value of \\(r\\) with the slope of a \"best-fit line\"! The nearness of \\(r^2\\) to 1 (or of \\(r\\) to \\(\\pm 1\\)) is a measure of quality of fit. The actual slope of the best-fit line (which could be any real number at all) has nothing whatsoever to do with the value of \\(r\\) (which is always between \\(-1\\) and 1).</p> <p>Correlation coefficients go hand in hand with linear regression (finding a \"best fit\" line for data) and help one to understand how meaningful the results of a linear regression are.</p> <p>Note: Let's see why the correlation coefficient equals 1 precisely when the points \\((x_i, y_i)\\) all lie exactly on a line \\(y = mx\\) whose slope \\(m\\) is positive. We assume as always that the data doesn't all lie on a common vertical line or a common horizontal line, and that the averages \\(\\bar{x}\\) and \\(\\bar{y}\\) equal 0. By then replacing \\(y_i\\) with \\(-y_i\\) everywhere, it would follow that the correlation coefficient equals \\(-1\\) precisely when the points \\((x_i, y_i)\\) all lie exactly on a line \\(y = mx\\) whose slope \\(m\\) is negative.</p> <p>Note that X, Y \u2260 0 since we assumed the data points aren't on a common horizontal line and aren't on a common vertical line. We want to show that the correlation coefficient is 1 precisely when Y = \\(m\\)X for some \\(m &gt; 0\\).</p> <p>But the correlation coefficient is the cosine of the angle between the nonzero vectors X and Y, so the correlation coefficient is equal to 1 precisely when the angle between X and Y is \\(0\u00b0\\). The angle \\(\\theta\\) between the (nonzero) vectors X and Y is \\(0\u00b0\\) precisely when Y = \\(m\\)X for some \\(m &gt; 0\\).</p>"},{"location":"math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/","title":"Vectors, vector addition, and scalar multiplication","text":"<p>Differential equations provided the mathematical framework for many of the advances of the 20th century, but linear algebra (the algebra and geometry of vectors and matrices in arbitrary dimensions) is the mathematical tool par excellence (alongside statistics) for the systematic analysis and management of the data-driven tasks of the 21st century. Even for modern applications of differential equations, linear algebra far beyond 3 dimensions is an important tool.</p> <p>Also, a good understanding of multivariable differential calculus requires first learning some ideas and computational techniques in linear algebra.</p>"},{"location":"math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/#vectors-and-their-linear-combinations","title":"Vectors and their linear combinations","text":"<p>In the physical sciences, vectors are used to represent quantities that have both a magnitude and a direction. Examples of such quantities include displacement, velocity, force, and angular momentum.</p> <p>In data science, economics, and many industrial applications of mathematics, vectors are used to keep track of collections of numerical data. This type of example is much more varied than the examples arising from natural sciences, and nearly always \\(n\\) is very large.</p> <p>The sum v + w of two vectors is defined precisely when v and w are \\(n\\)-vectors for the same \\(n\\). In that case, we define their sum by the rule</p> \\[\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} + \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} = \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\\\ \\vdots \\\\ v_n + w_n \\end{bmatrix}\\] <p>We multiply a scalar \\(c\\) against an \\(n\\)-vector v = \\(\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\) by the rule </p> \\[c\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} cv_1 \\\\ cv_2 \\\\ \\vdots \\\\ cv_n \\end{bmatrix}\\] <p></p> <p>As in the above for \\(n = 3\\), the vector u + v is represented by the diagonal arrow in the parallelogram with one vertex at the origin and two edges given by u and v. This description of vector addition for \\(n = 2\\) and \\(n = 3\\) is called the parallelogram law.</p> <p>We define subtraction as we did addition, or equivalently:</p> \\[v - w = v + (-1)w\\] <p>A linear combination of two \\(n\\)-vectors v,w is an \\(n\\)-vector \\(av + bw\\) for scalars \\(a, b\\). More generally, a linear combination of \\(k\\) such \\(n\\)-vectors v\\(_1\\), v\\(_2\\), . . . , v\\(_k\\) is \\(a_1\\)v\\(_1\\) + \\(a_2\\)v\\(_2\\) + \u00b7 \u00b7 \u00b7 + \\(a_k\\)v\\(_k\\) for scalars \\(a_1\\), \\(a_2\\), . . . , \\(a_k\\). In physical sciences, this is often called a \"superposition\" of v\\(_1\\), . . . , v\\(_k\\).</p> <p>Example: Suppose that T\\(_{2001}\\), T\\(_{2002}\\), . . . , T\\(_{2016}\\) are 365-vectors that describe the daily average temperatures in Palo Alto (say in Celsius) in years 2001, 2002, . . . , 2016 (let's ignore February 29 in leap years). Then \\(\\frac{1}{16}\\)(T\\(_{2001}\\) + \u00b7 \u00b7 \u00b7 + T\\(_{2016}\\)) is a 365-vector that tells us, for each given day, the average temperature in the years 2001\u20132016. For example, the first entry of this vector is the average January 1 temperature during this period.</p> <p>A special type of linear combination that arises in applications such as linear programming, weighted averages, and probability theory is convex combination: in the case of two \\(n\\)-vectors v and w, this means a linear combination of the form \\((1 - t)\\)v + \\(tw\\) = v + \\(t\\)(w - v) with \\(0 \\leq t \\leq 1\\). This adds to v a portion (given by \\(t\\)) of the displacement from v to w. It has the geometric interpretation (for \\(n = 2, 3\\)) of being a point on the line segment between the tips of v and w; e.g., it is v when \\(t = 0\\), it is the midpoint when \\(t = \\frac{1}{2}\\), and it is w when \\(t = 1\\).</p> <p></p> <p>For any \\(n\\)-vectors v\\(_1\\), . . . , v\\(_k\\), a convex combination of them means a linear combination \\(t_1\\)v\\(_1\\) + \u00b7 \u00b7 \u00b7 + \\(t_k\\)v\\(_k\\) for which all \\(t_j \\geq 0\\) and the sum of the coefficients is equal to 1; that is, \\(t_1\\) + \u00b7 \u00b7 \u00b7 + \\(t_k\\) = 1. When the coefficients are all equal, which is to say every \\(t_j\\) is equal to \\(\\frac{1}{k}\\), this is the average (sometimes called the centroid) of the vectors.</p> <p>In linear algebra, the phrase \"point in \\(\\mathbb{R}^n\\)\" means exactly the same thing as \"\\(n\\)-vector\" (as well as \"vector\", when we don't need to specify \\(n\\)). The mental image for a given situation may suggest a preference between the words \"point\" and \"vector\", such as \"displacement vector\" or \"closest point\", but there is absolutely no difference in the meanings of these words in linear algebra. You might imagine that a \"point\" is the tip of an arrow emanating from 0, or that a \"vector\" is a directed line segment with specified endpoints.</p> <p>In physics and engineering, a special \"cross product\" of 3-vectors (the output of which is also a 3-vector) shows up a lot. This has no analogue in \\(\\mathbb{R}^n\\) for \\(n \\neq 3\\), and it behaves very differently from products of numbers; e.g., it is neither commutative nor associative!</p>"},{"location":"math/linear_algebra/vectors_vector_addition_and_scalar_multiplication/#length-for-vectors-and-distance-in-mathbbrn","title":"Length for vectors and distance in \\(\\mathbb{R}^n\\)","text":"<p>The length or magnitude of an \\(n\\)-vector v = \\(\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\), denoted \\(\\|v\\|\\), is the number</p> \\[\\|v\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\geq 0\\] <p>Note that the length is a scalar, and \\(\\|-v\\| = \\|v\\|\\) (in accordance with the visualization of \\(-v\\) as \"a copy of v pointing in the opposite direction\") because signs disappear when squaring each \\(-v_j\\).</p> <p>If \\(c\\) is any scalar then \\(\\|cv\\| = |c|\\|v\\|\\) (i.e., if we multiply a vector by \\(c\\) then the length scales by the factor \\(|c|\\)). For example, \\((-5)\\)v has length \\(5\\|v\\|\\).</p> <p>In other references, you may see \\(\\|v\\|\\) called the \"norm\" of v.</p> <p>The distance between two \\(n\\)-vectors x, y is defined to be \\(\\|x - y\\|\\).</p> <p>In general it also equals \\(\\|y - x\\|\\) since y - x = \\(-(x - y)\\) and any vector has the same length as its negative, so the order of subtraction doesn't matter.</p> <p>There is no \"physical justification\" to be given when \\(n &gt; 3\\). What is important is that (i) for \\(n = 2, 3\\) we convince ourselves that it is the usual notion of distance, and (ii) for general \\(n\\) it satisfies reasonable properties for a notion of \"distance\" to provide helpful geometric insight.</p> <p>The zero vector in \\(\\mathbb{R}^n\\) is 0 = \\(\\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\), and a unit vector is a vector with length 1.</p> <p>Always \\(\\|v\\| \\geq 0\\), and \\(\\|v\\| = 0\\) precisely when v = 0. If v \u2260 0 then dividing v by its length (i.e., multiplying by the scalar \\(\\frac{1}{\\|v\\|} &gt; 0\\)) yields a unit vector \"pointing in the same direction\" as v.</p> <p>In general the length of \\(cv\\) for \\(c &gt; 0\\) is \\(c\\|v\\|\\), so in order that \\(cv\\) be a unit vector the condition is precisely that \\(c\\|v\\| = 1\\), which is to say \\(c = \\frac{1}{\\|v\\|}\\). In other words, the scalar multiple \\(\\frac{1}{\\|v\\|}\\)v of v is indeed the unique unit vector pointing in the same direction as v (in the opposite direction we have the unit vector \\(-\\frac{1}{\\|v\\|}\\)v).</p>"},{"location":"math/probability/probability_and_counting/","title":"Probability and counting","text":"<p>Mathematics is the logic of certainty; probability is the logic of uncertainty.</p>"},{"location":"math/probability/probability_and_counting/#sample-spaces","title":"Sample spaces","text":"<p>The mathematical framework for probability is built around sets. Imagine that an experiment is performed, resulting in one out of a set of possible outcomes. Before the experiment is performed, it is unknown which outcome will be the result; after, the result \"crystallizes\" into the actual outcome. The sample space S of an experiment is the set of all possible outcomes of the experiment. An event A is a subset of the sample space S, and we say that A occurred if the actual outcome is in A.  When the sample space is finite, we can visualize it as Pebble World (figure above). Each pebble represents an outcome, and an event is a set of pebbles. Performing the experiment amounts to randomly selecting one pebble. If all the pebbles are of the same mass, all the pebbles are equally likely to be chosen.</p> <p>Set theory is very useful in probability, since it provides a rich language for express_ing and working with events. Set operations, especially unions, intersections, and complements, make it easy to build new events in terms of already defined events.</p> <p>Example:  A coin is flipped 10 times. Writing Heads as H and Tails as T, a possible outcome is: HHHTHHTTHT. The sample space is the set of all possible strings of length 10 consisting of H's and T's. We can (and will) encode H as <code>1</code> and T as <code>0</code>, so that an outcome is a sequence: $$ (s_1, s_2, \\dots, s_{10}) \\quad \\text{with} \\quad s_j \\in {0, 1} $$ The sample space is the set of all such sequences.</p> <p>Some Events:</p> <ol> <li>Event A_1: the first flip is Heads. As a set: $$ A_1 =  (1, s_2, \\dots, s_{10}) \\; \\mid \\; s_j \\in {0,1} \\; \\text{and } 2 \\leq j \\leq 10  $$ This is a subset of the sample space, so it is indeed an event. Saying that A_1 occurs is equivalent to saying that the first flip is Heads. Similarly, let A_j be the event that the j-th flip is Heads, for: $$ j = 2, 3, \\dots, 10 $$</li> <li>Event B: at least one flip was Heads. As a set: $$ B = \\bigcup_{j=1}^{10} A_j $$</li> <li>Event C: all the flips were Heads. As a set: $$ C = \\bigcap_{j=1}^{10} A_j $$</li> <li>Event D: there were at least two consecutive Heads. As a set: $$ D = \\bigcup_{j=1}^{9} \\left( A_j \\cap A_{j+1} \\right) $$ </li> </ol>"},{"location":"math/probability/probability_and_counting/#naive-definition-of-probability","title":"Naive definition of probability","text":"<p>(Naive definition of probability). Let \\( A \\) be an event for an experiment with a finite sample space \\( S \\). The naive probability of \\( A \\) is</p> \\[ P_{\\text{naive}}(A) = \\frac{|A|}{|S|} = \\frac{\\text{number of outcomes favorable to } A}{\\text{total number of outcomes in } S} \\] <p>The naive definition is very restrictive in that it requires S to be finite, with equal mass for each pebble. It has often been misapplied by people who assume equally likely outcomes without justification and make arguments to the e\ufb00ect of \u201ceither it will happen or it won\u2019t, and we don\u2019t know which, so it\u2019s 50-50\u201d.</p>"}]}