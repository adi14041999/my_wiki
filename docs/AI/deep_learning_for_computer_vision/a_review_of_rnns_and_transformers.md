# A review of RNNs and Transformers

## Recurrent Neural Networks (RNNs)

- [Recurrent Neural Networks](../natural_language_processing/recurrent_neural_networks.md) - Introduction to RNNs and their architecture
- [Gated Recurrent Units (GRUs)](../natural_language_processing/gated_recurrent_units.md) - GRU architecture as an alternative to LSTMs
- [Long-Short Term Memory (LSTM) Networks](../natural_language_processing/long_short_term_memory_networks.md) - LSTM networks for handling long-term dependencies
- [Seq2Seq Models](../natural_language_processing/seq2seq_models.md) - Sequence-to-sequence models using RNNs

## Transformers

- [Attention Mechanism](../natural_language_processing/attention_mechanism.md) - General attention mechanism concepts
- [How large language models work, a visual intro to transformers](../natural_language_processing/how_large_language_models_work_a_visual_intro_to_transformers.md) - Visual introduction to how LLMs work
- [Attention in transformers, visually explained](../natural_language_processing/attention_in_transformers_visually_explained.md) - Visual explanation of attention mechanisms in transformers
- [The basic Transformer](../natural_language_processing/the_basic_transformer.md) - Overview of the basic transformer architecture
- [Decoder-only Transformers](../natural_language_processing/decoder_only_transformers.md) - Decoder-only transformer architecture (e.g., GPT models)
- [Differences between the basic Transformer and the Decoder-Only Transformer](../natural_language_processing/differences_between_the_basic_transformer_and_the_decoder_only_transformer.md) - Comparison of transformer architectures
- [Self-Attention and Transformers, a mathematical approach](../natural_language_processing/self_attention_and_transformers_a_mathematical_approach.md) - Comprehensive mathematical treatment of self-attention and transformers


